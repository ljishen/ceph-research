[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:42:52,667979729-07:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.1 mkdir --parents /tmp/bench-rados
[1] 14:42:52 [SUCCESS] ljishen@10.10.2.1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:42:52,855778781-07:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.5 mkdir --parents /tmp/bench-rados
[1] 14:42:53 [SUCCESS] ljishen@10.10.2.5
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:42:53,991994772-07:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.5 bash
[1] 14:42:55 [SUCCESS] ljishen@10.10.2.5


[1;7;39;49m[2021-10-28T14:42:55,144311619-07:00][RUNNING][ROUND 1/1/21] object_size=4KB[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:42:55,146656278-07:00] STAGE: Launch a Ceph cluster (CEPH_HOST: 10.10.2.1, OSD_HOST: 10.10.2.5) ...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:42:55,151126242-07:00] INFO: > Get OSD hostname[0m
## ./benchmarks/bench-rados:178 - launch_ceph_cluster() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.5 uname --nodename
## ./benchmarks/bench-rados:178 - launch_ceph_cluster() > tail -n1
# ./benchmarks/bench-rados:178 - launch_ceph_cluster() > OSD_HOSTNAME=sm1
# ./benchmarks/bench-rados:183 - launch_ceph_cluster() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.1 bash -euo pipefail
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:42:56,270708043-07:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.1 bash -euo pipefail
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T14:42:57,159380091-07:00] INFO: > Remove existing clusters\x1b[0m\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T14:42:57,164655539-07:00] INFO: > Deploy a new cluster\x1b[0m\n'
10.10.2.1: b'# bash:24 -  > /tmp/bench-rados/ceph-research/scripts/ceph-deploy -m 10.10.2.1 -o sm1/10.10.2.5:/dev/nvme0n1\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T14:42:57,176319248-07:00] STAGE: Verify passwordless SSH to hosts, passwordless sudo via ljishen, and the existence of devices on hosts...\x1b[0m\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T14:42:57,181132788-07:00] INFO: > Check the host for the monitor\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:181 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@10.10.2.1 sudo --non-interactive --validate\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T14:42:57,326191980-07:00] INFO: > Check hosts for OSDs\x1b[0m\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T14:42:57,330758145-07:00] INFO: >> Check host 'sm1'\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:200 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 sudo --non-interactive --validate\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T14:42:58,394833048-07:00] INFO: >>> Check device '/dev/nvme0n1'\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:215 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 test -b /dev/nvme0n1\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:226 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 lsblk --nodeps --noheadings --output TYPE /dev/nvme0n1\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:227 - verify_parameters() > grep --word-regexp disk\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T14:43:00,543702377-07:00] STAGE: Reset devices on hosts...\x1b[0m\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T14:43:00,548501840-07:00] INFO: > Resetting device '/dev/nvme0n1' on host 'sm1'\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:288 - reset_host_devices() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 sudo sh\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T14:43:01,732991111-07:00] STAGE: Bootstrap a new cluster...\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:317 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.1 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T14:43:01,743284613-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T14:43:01,746875495-07:00] DEBUG: CEPHADM_PARAMS: ['bootstrap', '--config', '/tmp/bench-rados/ceph-research/scripts/ceph.conf', '--mon-ip', '10.10.2.1', '--output-dir', '/tmp/bench-rados/deployment_data_root/etc/ceph', '--ssh-user', 'ljishen', '--skip-dashboard', '--no-minimize-config', '--allow-overwrite', '--allow-fqdn-hostname', '--orphan-initial-daemons', '--skip-monitoring-stack', '--single-host-defaults']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.1 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults\n'
10.10.2.1: b'Verifying podman|docker is present...\nVerifying lvm2 is present...\n'
10.10.2.1: b'Verifying time synchronization is in place...\n'
10.10.2.1: b'Unit ntp.service is enabled and running\n'
10.10.2.1: b'Repeating the final host check...\npodman|docker (/usr/bin/docker) is present\nsystemctl is present\n'
10.10.2.1: b'lvcreate is present\n'
10.10.2.1: b'Unit ntp.service is enabled and running\n'
10.10.2.1: b'Host looks OK\n'
10.10.2.1: b'Cluster fsid: 06799c3a-3838-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Verifying IP 10.10.2.1 port 3300 ...\n'
10.10.2.1: b'Verifying IP 10.10.2.1 port 6789 ...\n'
10.10.2.1: b'Mon IP `10.10.2.1` is in CIDR network `10.10.2.0/24`\n'
10.10.2.1: b'- internal network (--cluster-network) has not been provided, OSD replication will default to the public_network\n'
10.10.2.1: b'Adjusting default settings to suit single-host cluster...\n'
10.10.2.1: b'Pulling container image quay.io/ceph/ceph:v16...\n'
10.10.2.1: b'Ceph version: ceph version 16.2.6 (ee28fb57e47e9f88813e24bbf4c14496ca299d31) pacific (stable)\n'
10.10.2.1: b'Extracting ceph user uid/gid from container image...\n'
10.10.2.1: b'Creating initial keys...\n'
10.10.2.1: b'Creating initial monmap...\n'
10.10.2.1: b'Creating mon...\n'
10.10.2.1: b'Waiting for mon to start...\n'
10.10.2.1: b'Waiting for mon...\n'
10.10.2.1: b'mon is available\n'
10.10.2.1: b'Setting mon public_network to 10.10.2.0/24\n'
10.10.2.1: b'Wrote config to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf\n'
10.10.2.1: b'Wrote keyring to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring\n'
10.10.2.1: b'Creating mgr...\n'
10.10.2.1: b'Verifying port 9283 ...\n'
10.10.2.1: b'Waiting for mgr to start...\nWaiting for mgr...\n'
10.10.2.1: b'mgr not available, waiting (1/15)...\n'
10.10.2.1: b'mgr not available, waiting (2/15)...\n'
10.10.2.1: b'mgr is available\n'
10.10.2.1: b'Enabling cephadm module...\n'
10.10.2.1: b'Waiting for the mgr to restart...\n'
10.10.2.1: b'Waiting for mgr epoch 5...\n'
10.10.2.1: b'mgr epoch 5 is available\n'
10.10.2.1: b'Setting orchestrator backend to cephadm...\n'
10.10.2.1: b'Generating ssh key...\n'
10.10.2.1: b'Wrote public SSH key to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub\n'
10.10.2.1: b'Adding key to ljishen@localhost authorized_keys...\n'
10.10.2.1: b'Adding host node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us...\n'
10.10.2.1: b'Deploying unmanaged mon service...\n'
10.10.2.1: b'Deploying unmanaged mgr service...\n'
10.10.2.1: b'You can access the Ceph CLI with:\n\n\tsudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --fsid 06799c3a-3838-11ec-b51d-53e6e728d2d3 -c /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf -k /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring\n\n'
10.10.2.1: b'Please consider enabling telemetry to help improve Ceph:\n\n\tceph telemetry on\n\nFor more information see:\n\n\thttps://docs.ceph.com/docs/pacific/mgr/telemetry/\n\nBootstrap complete.\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T14:44:00,627149000-07:00] INFO: Sleep for 20 seconds\x1b[0m\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T14:44:20,634535176-07:00] INFO: > Set the default number of monitor daemons to 1\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:337 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T14:44:20,644903720-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T14:44:20,648996184-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mon', '1']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1\n'
10.10.2.1: b'Inferring fsid 06799c3a-3838-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/06799c3a-3838-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b'Scheduled mon update...\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T14:44:29,667650604-07:00] INFO: > Set the default number of manager daemons to 1\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:342 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T14:44:29,677892069-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T14:44:29,681430050-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mgr', '1']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1\n'
10.10.2.1: b'Inferring fsid 06799c3a-3838-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/06799c3a-3838-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b'Scheduled mgr update...\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T14:44:38,519135793-07:00] STAGE: Add hosts to the cluster...\x1b[0m\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T14:44:38,525243085-07:00] INFO: > Adding host 'sm1'\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:361 - add_hosts() > ssh-copy-id -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -f -i /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub ljishen@sm1\n'
10.10.2.1: b'/usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: "/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub"\n'
10.10.2.1: b'\nNumber of key(s) added: 1\n\nNow try logging into the machine, with:   "ssh -o \'GlobalKnownHostsFile=/dev/null\' -o \'LogLevel=ERROR\' -o \'PasswordAuthentication=no\' -o \'StrictHostKeyChecking=no\' -o \'UserKnownHostsFile=/dev/null\' \'ljishen@sm1\'"\nand check to make sure that only the key(s) you wanted were added.\n\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:367 - add_hosts() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add sm1 10.10.2.5\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T14:44:39,715537169-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T14:44:39,718574148-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'host', 'add', 'sm1', '10.10.2.5']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add sm1 10.10.2.5\n'
10.10.2.1: b'Inferring fsid 06799c3a-3838-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/06799c3a-3838-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b"Added host 'sm1' with addr '10.10.2.5'\n"
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T14:44:50,390369749-07:00] INFO: Sleep for 20 seconds\x1b[0m\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T14:45:10,395596141-07:00] STAGE: Add OSDs to the cluster...\x1b[0m\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T14:45:10,402368714-07:00] INFO: > Adding devices ['/dev/nvme0n1'] on host 'sm1' as OSDs\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:388 - add_host_devices_as_osds() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd sm1:/dev/nvme0n1\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T14:45:10,412978441-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T14:45:10,417074813-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'daemon', 'add', 'osd', 'sm1:/dev/nvme0n1']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd sm1:/dev/nvme0n1\n'
10.10.2.1: b'Inferring fsid 06799c3a-3838-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/06799c3a-3838-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b"Created osd(s) 0 on host 'sm1'\n"
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T14:45:34,874730715-07:00] INFO: > Sleep for 20 seconds\x1b[0m\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T14:45:54,880523005-07:00] STAGE: Show Cluster Status\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:408 - show_status() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T14:45:54,890265211-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T14:45:54,894034828-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', '--status']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status\n'
10.10.2.1: b'Inferring fsid 06799c3a-3838-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/06799c3a-3838-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b'  cluster:\n    id:     06799c3a-3838-11ec-b51d-53e6e728d2d3\n    health: HEALTH_WARN\n            1 pool(s) have no replicas configured\n \n  services:\n    mon: 1 daemons, quorum node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us (age 2m)\n    mgr: node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us.twqssw(active, since 2m)\n    osd: 1 osds: 1 up (since 20s), 1 in (since 41s)\n \n  data:\n    pools:   1 pools, 1 pgs\n    objects: 0 objects, 0 B\n    usage:   4.7 MiB used, 400 GiB / 400 GiB avail\n    pgs:     1 active+clean\n \n'
[1] 14:46:04 [SUCCESS] ljishen@10.10.2.1

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:42:57,159380091-07:00] INFO: > Remove existing clusters[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:42:57,164655539-07:00] INFO: > Deploy a new cluster[0m
# bash:24 -  > /tmp/bench-rados/ceph-research/scripts/ceph-deploy -m 10.10.2.1 -o sm1/10.10.2.5:/dev/nvme0n1


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:42:57,176319248-07:00] STAGE: Verify passwordless SSH to hosts, passwordless sudo via ljishen, and the existence of devices on hosts...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:42:57,181132788-07:00] INFO: > Check the host for the monitor[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:181 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@10.10.2.1 sudo --non-interactive --validate

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:42:57,326191980-07:00] INFO: > Check hosts for OSDs[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:42:57,330758145-07:00] INFO: >> Check host 'sm1'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:200 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 sudo --non-interactive --validate

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:42:58,394833048-07:00] INFO: >>> Check device '/dev/nvme0n1'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:215 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 test -b /dev/nvme0n1
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:226 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 lsblk --nodeps --noheadings --output TYPE /dev/nvme0n1
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:227 - verify_parameters() > grep --word-regexp disk


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:43:00,543702377-07:00] STAGE: Reset devices on hosts...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:43:00,548501840-07:00] INFO: > Resetting device '/dev/nvme0n1' on host 'sm1'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:288 - reset_host_devices() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 sudo sh


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:43:01,732991111-07:00] STAGE: Bootstrap a new cluster...[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:317 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.1 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:43:01,743284613-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:43:01,746875495-07:00] DEBUG: CEPHADM_PARAMS: ['bootstrap', '--config', '/tmp/bench-rados/ceph-research/scripts/ceph.conf', '--mon-ip', '10.10.2.1', '--output-dir', '/tmp/bench-rados/deployment_data_root/etc/ceph', '--ssh-user', 'ljishen', '--skip-dashboard', '--no-minimize-config', '--allow-overwrite', '--allow-fqdn-hostname', '--orphan-initial-daemons', '--skip-monitoring-stack', '--single-host-defaults'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.1 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults
Verifying podman|docker is present...
Verifying lvm2 is present...
Verifying time synchronization is in place...
Unit ntp.service is enabled and running
Repeating the final host check...
podman|docker (/usr/bin/docker) is present
systemctl is present
lvcreate is present
Unit ntp.service is enabled and running
Host looks OK
Cluster fsid: 06799c3a-3838-11ec-b51d-53e6e728d2d3
Verifying IP 10.10.2.1 port 3300 ...
Verifying IP 10.10.2.1 port 6789 ...
Mon IP `10.10.2.1` is in CIDR network `10.10.2.0/24`
- internal network (--cluster-network) has not been provided, OSD replication will default to the public_network
Adjusting default settings to suit single-host cluster...
Pulling container image quay.io/ceph/ceph:v16...
Ceph version: ceph version 16.2.6 (ee28fb57e47e9f88813e24bbf4c14496ca299d31) pacific (stable)
Extracting ceph user uid/gid from container image...
Creating initial keys...
Creating initial monmap...
Creating mon...
Waiting for mon to start...
Waiting for mon...
mon is available
Setting mon public_network to 10.10.2.0/24
Wrote config to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf
Wrote keyring to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring
Creating mgr...
Verifying port 9283 ...
Waiting for mgr to start...
Waiting for mgr...
mgr not available, waiting (1/15)...
mgr not available, waiting (2/15)...
mgr is available
Enabling cephadm module...
Waiting for the mgr to restart...
Waiting for mgr epoch 5...
mgr epoch 5 is available
Setting orchestrator backend to cephadm...
Generating ssh key...
Wrote public SSH key to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub
Adding key to ljishen@localhost authorized_keys...
Adding host node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us...
Deploying unmanaged mon service...
Deploying unmanaged mgr service...
You can access the Ceph CLI with:

	sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --fsid 06799c3a-3838-11ec-b51d-53e6e728d2d3 -c /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf -k /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring

Please consider enabling telemetry to help improve Ceph:

	ceph telemetry on

For more information see:

	https://docs.ceph.com/docs/pacific/mgr/telemetry/

Bootstrap complete.

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:44:00,627149000-07:00] INFO: Sleep for 20 seconds[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:44:20,634535176-07:00] INFO: > Set the default number of monitor daemons to 1[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:337 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:44:20,644903720-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:44:20,648996184-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mon', '1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1
Inferring fsid 06799c3a-3838-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/06799c3a-3838-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Scheduled mon update...

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:44:29,667650604-07:00] INFO: > Set the default number of manager daemons to 1[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:342 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:44:29,677892069-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:44:29,681430050-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mgr', '1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1
Inferring fsid 06799c3a-3838-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/06799c3a-3838-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Scheduled mgr update...


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:44:38,519135793-07:00] STAGE: Add hosts to the cluster...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:44:38,525243085-07:00] INFO: > Adding host 'sm1'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:361 - add_hosts() > ssh-copy-id -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -f -i /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub ljishen@sm1
/usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: "/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub"

Number of key(s) added: 1

Now try logging into the machine, with:   "ssh -o 'GlobalKnownHostsFile=/dev/null' -o 'LogLevel=ERROR' -o 'PasswordAuthentication=no' -o 'StrictHostKeyChecking=no' -o 'UserKnownHostsFile=/dev/null' 'ljishen@sm1'"
and check to make sure that only the key(s) you wanted were added.

# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:367 - add_hosts() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add sm1 10.10.2.5
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:44:39,715537169-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:44:39,718574148-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'host', 'add', 'sm1', '10.10.2.5'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add sm1 10.10.2.5
Inferring fsid 06799c3a-3838-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/06799c3a-3838-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Added host 'sm1' with addr '10.10.2.5'

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:44:50,390369749-07:00] INFO: Sleep for 20 seconds[0m


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:45:10,395596141-07:00] STAGE: Add OSDs to the cluster...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:45:10,402368714-07:00] INFO: > Adding devices ['/dev/nvme0n1'] on host 'sm1' as OSDs[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:388 - add_host_devices_as_osds() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd sm1:/dev/nvme0n1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:45:10,412978441-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:45:10,417074813-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'daemon', 'add', 'osd', 'sm1:/dev/nvme0n1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd sm1:/dev/nvme0n1
Inferring fsid 06799c3a-3838-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/06799c3a-3838-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Created osd(s) 0 on host 'sm1'

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:45:34,874730715-07:00] INFO: > Sleep for 20 seconds[0m


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:45:54,880523005-07:00] STAGE: Show Cluster Status[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:408 - show_status() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:45:54,890265211-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:45:54,894034828-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', '--status'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status
Inferring fsid 06799c3a-3838-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/06799c3a-3838-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
  cluster:
    id:     06799c3a-3838-11ec-b51d-53e6e728d2d3
    health: HEALTH_WARN
            1 pool(s) have no replicas configured
 
  services:
    mon: 1 daemons, quorum node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us (age 2m)
    mgr: node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us.twqssw(active, since 2m)
    osd: 1 osds: 1 up (since 20s), 1 in (since 41s)
 
  data:
    pools:   1 pools, 1 pgs
    objects: 0 objects, 0 B
    usage:   4.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     1 active+clean
 

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:46:04,063671421-07:00] INFO: > Copy configuration files to local[0m
# ./benchmarks/bench-rados:220 - launch_ceph_cluster() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.1 sudo chown --changes --recursive ljishen /tmp/bench-rados/deployment_data_root/etc/ceph
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:46:04,071182938-07:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.1 sudo chown --changes --recursive ljishen /tmp/bench-rados/deployment_data_root/etc/ceph
[1] 14:46:04 [SUCCESS] ljishen@10.10.2.1
# ./benchmarks/bench-rados:223 - launch_ceph_cluster() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.1:/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf
# ./benchmarks/bench-rados:226 - launch_ceph_cluster() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.1:/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:46:04,548874878-07:00] STAGE: Update local ceph.conf as a client...[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:46:04,551325146-07:00] STAGE: Configure Ceph pool...[0m
# ./benchmarks/bench-rados:257 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_max_backfills 32
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:46:04,574689982-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:46:04,577537569-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '06799c3a-3838-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'config', 'set', 'osd', 'osd_max_backfills', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 06799c3a-3838-11ec-b51d-53e6e728d2d3 -- ceph config set osd osd_max_backfills 32
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:258 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_active 32
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:46:08,828863576-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:46:08,831981281-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '06799c3a-3838-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_active', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 06799c3a-3838-11ec-b51d-53e6e728d2d3 -- ceph config set osd osd_recovery_max_active 32
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:259 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_single_start 8
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:46:12,904000631-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:46:12,906889746-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '06799c3a-3838-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_single_start', '8'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 06799c3a-3838-11ec-b51d-53e6e728d2d3 -- ceph config set osd osd_recovery_max_single_start 8
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:260 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_op_priority 63
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:46:16,913570303-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:46:16,916662140-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '06799c3a-3838-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_op_priority', '63'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 06799c3a-3838-11ec-b51d-53e6e728d2d3 -- ceph config set osd osd_recovery_op_priority 63
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:262 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config show-with-defaults osd.0
# ./benchmarks/bench-rados:263 - configure_ceph_pool() > grep 'osd_max_backfills\|osd_recovery'
# ./benchmarks/bench-rados:263 - configure_ceph_pool() > column -t -s ' '
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
osd_max_backfills                        32         mon
osd_recovery_cost                        20971520   default
osd_recovery_delay_start                 0.000000   default
osd_recovery_max_active                  32         mon
osd_recovery_max_active_hdd              3          default
osd_recovery_max_active_ssd              10         default
osd_recovery_max_chunk                   8388608    default
osd_recovery_max_omap_entries_per_chunk  8096       default
osd_recovery_max_single_start            8          mon
osd_recovery_op_priority                 63         mon
osd_recovery_op_warn_multiple            16         default
osd_recovery_priority                    5          default
osd_recovery_retry_interval              30.000000  default
osd_recovery_sleep                       0.000000   default
osd_recovery_sleep_hdd                   0.100000   default
osd_recovery_sleep_hybrid                0.025000   default
osd_recovery_sleep_ssd                   0.000000   default
# ./benchmarks/bench-rados:265 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool create bench_rados 128 128 replicated --size 1 --pg-num-min 128
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:46:24,921059901-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:46:24,924106643-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '06799c3a-3838-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'create', 'bench_rados', '128', '128', 'replicated', '--size', '1', '--pg-num-min', '128'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 06799c3a-3838-11ec-b51d-53e6e728d2d3 -- ceph osd pool create bench_rados 128 128 replicated --size 1 --pg-num-min 128
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
pool 'bench_rados' created
# ./benchmarks/bench-rados:267 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados min_size 1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:46:30,032842352-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:46:30,035923048-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '06799c3a-3838-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'min_size', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 06799c3a-3838-11ec-b51d-53e6e728d2d3 -- ceph osd pool set bench_rados min_size 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 min_size to 1
# ./benchmarks/bench-rados:268 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados pg_autoscale_mode off
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:46:34,352872235-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:46:34,356062208-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '06799c3a-3838-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'pg_autoscale_mode', 'off'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 06799c3a-3838-11ec-b51d-53e6e728d2d3 -- ceph osd pool set bench_rados pg_autoscale_mode off
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 pg_autoscale_mode to off
# ./benchmarks/bench-rados:269 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool application enable bench_rados benchmark
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:46:38,971362724-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:46:38,974403826-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '06799c3a-3838-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'application', 'enable', 'bench_rados', 'benchmark'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 06799c3a-3838-11ec-b51d-53e6e728d2d3 -- ceph osd pool application enable bench_rados benchmark
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
enabled application 'benchmark' on pool 'bench_rados'
# ./benchmarks/bench-rados:270 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados noscrub 1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:46:43,301402254-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:46:43,304640537-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '06799c3a-3838-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'noscrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 06799c3a-3838-11ec-b51d-53e6e728d2d3 -- ceph osd pool set bench_rados noscrub 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 noscrub to 1
# ./benchmarks/bench-rados:271 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados nodeep-scrub 1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:46:47,817419653-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:46:47,820358572-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '06799c3a-3838-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'nodeep-scrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 06799c3a-3838-11ec-b51d-53e6e728d2d3 -- ceph osd pool set bench_rados nodeep-scrub 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 nodeep-scrub to 1
# ./benchmarks/bench-rados:272 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool ls detail
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:46:52,051552629-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:46:52,054497910-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '06799c3a-3838-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'ls', 'detail'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 06799c3a-3838-11ec-b51d-53e6e728d2d3 -- ceph osd pool ls detail
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
pool 1 'device_health_metrics' replicated size 1 min_size 1 crush_rule 0 object_hash rjenkins pg_num 64 pgp_num 64 autoscale_mode on last_change 18 lfor 0/0/16 flags hashpspool stripe_width 0 pg_num_min 1 application mgr_devicehealth
pool 2 'bench_rados' replicated size 1 min_size 1 crush_rule 0 object_hash rjenkins pg_num 128 pgp_num 128 autoscale_mode off last_change 20 flags hashpspool,noscrub,nodeep-scrub stripe_width 0 pg_num_min 128 application benchmark

# ./benchmarks/bench-rados:273 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd df tree
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:46:55,978169733-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:46:55,981317896-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '06799c3a-3838-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'df', 'tree'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 06799c3a-3838-11ec-b51d-53e6e728d2d3 -- ceph osd df tree
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
ID  CLASS  WEIGHT   REWEIGHT  SIZE     RAW USE  DATA     OMAP  META     AVAIL    %USE  VAR   PGS  STATUS  TYPE NAME    
-1         0.39059         -  400 GiB  5.7 MiB  168 KiB   0 B  5.6 MiB  400 GiB  0.00  1.00    -          root default 
-3         0.39059         -  400 GiB  5.7 MiB  168 KiB   0 B  5.6 MiB  400 GiB  0.00  1.00    -              host sm1 
 0    ssd  0.39059   1.00000  400 GiB  5.7 MiB  168 KiB   0 B  5.6 MiB  400 GiB  0.00  1.00  192      up          osd.0
                       TOTAL  400 GiB  5.7 MiB  168 KiB   0 B  5.6 MiB  400 GiB  0.00                                  
MIN/MAX VAR: 1.00/1.00  STDDEV: 0


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:47:00,085637524-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:47:24,101055112-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:47:33,244592242-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:47:42,106952764-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:47:42,113450630-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:47:51,097842714-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:47:51,104325612-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:48:00,006781241-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:48:00,013135206-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:48:09,057159049-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:48:09,063247915-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:48:18,039838788-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:48:18,046287722-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:48:18,050737669-07:00] INFO: > The cluster is idle now.[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:48:18,053448468-07:00] STAGE: Start benchmarking of mode write ...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:48:18,058356548-07:00] INFO: > Start system activity collection services[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:48:18,062477735-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:341 - rados_bench() > rm --force /mnt/sda8/ceph-research/scripts/output/sys_activity_4KB_write.dat.rados_bench_host.1
# ./benchmarks/bench-rados:342 - rados_bench() > sar_pid_rados_bench_host=782007
# ./benchmarks/bench-rados:342 - rados_bench() > S_TIME_FORMAT=ISO
# ./benchmarks/bench-rados:342 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/output/sys_activity_4KB_write.dat.rados_bench_host.1 2

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:48:18,067825585-07:00] INFO: >> for OSD host[0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:48:18,076111832-07:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.5 bash
10.10.2.5: b'250725\n'
[1] 14:48:19 [SUCCESS] ljishen@10.10.2.5
250725

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:48:19,186607913-07:00] INFO: > Run rados bench[0m
# ./benchmarks/bench-rados:370 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/output/rados_bench_4KB_write.log.1
## ./benchmarks/bench-rados:361 - rados_bench() > count_cpus 32-63,96-127
# ./benchmarks/bench-rados:361 - rados_bench() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 60 write --pool bench_rados -b 4096 -O 4096 --concurrent-ios 128 --show-time --write-object --write-omap --write-xattr --no-cleanup
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:48:19,205952391-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:48:19,208961082-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '06799c3a-3838-11ec-b51d-53e6e728d2d3', '--', 'rados', 'bench', '60', 'write', '--pool', 'bench_rados', '-b', '4096', '-O', '4096', '--concurrent-ios', '128', '--show-time', '--write-object', '--write-omap', '--write-xattr', '--no-cleanup'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 06799c3a-3838-11ec-b51d-53e6e728d2d3 -- rados bench 60 write --pool bench_rados -b 4096 -O 4096 --concurrent-ios 128 --show-time --write-object --write-omap --write-xattr --no-cleanup
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
hints = 1
2021-10-28T21:48:22.180852+0000 Maintaining 128 concurrent writes of 4096 bytes to objects of size 4096 for up to 60 seconds or 0 objects
2021-10-28T21:48:22.180863+0000 Object prefix: benchmark_data_node-2.bluefield2.ucsc-cmps10_7
2021-10-28T21:48:22.181399+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-28T21:48:22.181399+0000     0       0         0         0         0         0           -           0
2021-10-28T21:48:23.181547+0000     1     128      1459      1331   5.19882   5.19922    0.102719   0.0834934
2021-10-28T21:48:24.181660+0000     2     128      3123      2995   5.84906       6.5   0.0954633   0.0831006
2021-10-28T21:48:25.181732+0000     3     128      4915      4787   6.23253         7   0.0663718   0.0795357
2021-10-28T21:48:26.181845+0000     4     128      6545      6417   6.26602   6.36719   0.0582695   0.0795538
2021-10-28T21:48:27.181965+0000     5     128      8243      8115   6.33922   6.63281   0.0731665   0.0782882
2021-10-28T21:48:28.182075+0000     6     128      9617      9489   6.17711   5.36719    0.148938   0.0804834
2021-10-28T21:48:29.182148+0000     7     128     11315     11187   6.24214   6.63281    0.060341   0.0795928
2021-10-28T21:48:30.182259+0000     8     128     13073     12945   6.32018   6.86719   0.0646902   0.0789991
2021-10-28T21:48:31.182345+0000     9     128     14643     14515    6.2993   6.13281   0.0687609   0.0789321
2021-10-28T21:48:32.182453+0000    10     128     16435     16307    6.3693         7    0.107055   0.0781446
2021-10-28T21:48:33.182524+0000    11     128     18227     18099   6.42659         7   0.0922457   0.0774788
2021-10-28T21:48:34.182634+0000    12     128     19667     19539   6.35974     5.625   0.0653782   0.0783399
2021-10-28T21:48:35.182745+0000    13     128     21427     21299   6.39931     6.875   0.0666379    0.077876
2021-10-28T21:48:36.182858+0000    14     128     23185     23057   6.43268   6.86719   0.0654757   0.0772098
2021-10-28T21:48:37.182940+0000    15     128     25030     24902   6.48426   7.20703   0.0661847   0.0768546
2021-10-28T21:48:38.183057+0000    16     128     26513     26385   6.44101   5.79297   0.0684299   0.0773802
2021-10-28T21:48:39.183172+0000    17     127     27962     27835   6.39527   5.66406   0.0586182   0.0779684
2021-10-28T21:48:40.183294+0000    18     127     29492     29365   6.37197   5.97656   0.0668351   0.0782259
2021-10-28T21:48:41.183373+0000    19     128     31155     31027   6.37827   6.49219   0.0779939   0.0781958
2021-10-28T21:48:42.183487+0000 min lat: 0.0331041 max lat: 0.275256 avg lat: 0.0779173
2021-10-28T21:48:42.183487+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-28T21:48:42.183487+0000    20     128     32913     32785   6.40267   6.86719    0.107097   0.0779173
2021-10-28T21:48:43.183613+0000    21     128     34705     34577   6.43108         7   0.0677564   0.0776278
2021-10-28T21:48:44.183726+0000    22     128     36019     35891   6.37204   5.13281   0.0701546   0.0778598
2021-10-28T21:48:45.183802+0000    23     128     37683     37555   6.37758       6.5    0.075232   0.0782945
2021-10-28T21:48:46.183880+0000    24     128     39347     39219   6.38266       6.5    0.125926   0.0781337
2021-10-28T21:48:47.183972+0000    25     128     41099     40971   6.40108   6.84375   0.0684853   0.0779921
2021-10-28T21:48:48.184048+0000    26     128     42763     42635   6.40486       6.5   0.0669526    0.078011
2021-10-28T21:48:49.184122+0000    27     128     44339     44211   6.39564   6.15625   0.0760749   0.0780054
2021-10-28T21:48:50.184199+0000    28     128     46131     46003    6.4172         7   0.0760077   0.0777786
2021-10-28T21:48:51.184314+0000    29     128     47627     47499   6.39741   5.84375   0.0695264    0.078076
2021-10-28T21:48:52.184428+0000    30     128     49331     49203   6.40601   6.65625    0.133618   0.0778136
2021-10-28T21:48:53.184508+0000    31     128     51211     51083   6.43624   7.34375   0.0792808   0.0776323
2021-10-28T21:48:54.184623+0000    32     127     52710     52583   6.41819   5.85938   0.0670872   0.0778253
2021-10-28T21:48:55.184734+0000    33     128     54323     54195   6.41449   6.29688   0.0856468   0.0778452
2021-10-28T21:48:56.184831+0000    34     128     55987     55859   6.41699       6.5    0.145036   0.0777075
2021-10-28T21:48:57.184919+0000    35     128     57523     57395   6.40506         6    0.078094   0.0779617
2021-10-28T21:48:58.185030+0000    36     128     59187     59059   6.40768       6.5   0.0768263   0.0779723
2021-10-28T21:48:59.185141+0000    37     128     60931     60803    6.4186    6.8125   0.0633981   0.0778549
2021-10-28T21:49:00.185253+0000    38     128     62387     62259   6.39934    5.6875   0.0751186   0.0780441
2021-10-28T21:49:01.185326+0000    39     128     64051     63923   6.40191       6.5   0.0799272   0.0779826
2021-10-28T21:49:02.185437+0000 min lat: 0.0331041 max lat: 0.321188 avg lat: 0.0783015
2021-10-28T21:49:02.185437+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-28T21:49:02.185437+0000    40     128     65459     65331   6.37935       5.5    0.118361   0.0783015
2021-10-28T21:49:03.185555+0000    41     128     67251     67123   6.39447         7   0.0727002   0.0780781
2021-10-28T21:49:04.185669+0000    42     128     68915     68787   6.39696       6.5   0.0585614   0.0781244
2021-10-28T21:49:05.185742+0000    43     128     70579     70451   6.39935       6.5   0.0671703   0.0780735
2021-10-28T21:49:06.185842+0000    44     128     72331     72203   6.40943   6.84375   0.0812093   0.0778754
2021-10-28T21:49:07.185961+0000    45     128     73907     73779   6.40379   6.15625   0.0673506   0.0778367
2021-10-28T21:49:08.186075+0000    46     128     75443     75315     6.395         6     0.06925   0.0780935
2021-10-28T21:49:09.186150+0000    47     128     77195     77067   6.40453   6.84375   0.0647841   0.0779736
2021-10-28T21:49:10.186260+0000    48     128     78899     78771   6.40976   6.65625   0.0726542   0.0779634
2021-10-28T21:49:11.186373+0000    49     128     80651     80523    6.4186   6.84375   0.0836073   0.0778604
2021-10-28T21:49:12.186487+0000    50     128     82246     82118   6.41483   6.23047    0.141497   0.0777807
2021-10-28T21:49:13.186562+0000    51     128     83763     83635   6.40523   5.92578   0.0742818    0.077964
2021-10-28T21:49:14.186684+0000    52     128     85387     85259   6.40403   6.34375   0.0713485   0.0780379
2021-10-28T21:49:15.186801+0000    53     128     87173     87045   6.41482   6.97656   0.0685095   0.0779163
2021-10-28T21:49:16.186927+0000    54     128     88883     88755   6.41971   6.67969   0.0830793   0.0777475
2021-10-28T21:49:17.187040+0000    55     128     90763     90635   6.43649   7.34375   0.0768866   0.0776394
2021-10-28T21:49:18.187155+0000    56     127     92049     91922   6.41132   5.02734   0.0730325   0.0779294
2021-10-28T21:49:19.187267+0000    57     128     93828     93700   6.42067   6.94531   0.0681674   0.0778449
2021-10-28T21:49:20.187387+0000    58     128     95411     95283   6.41657   6.18359    0.069504   0.0778547
2021-10-28T21:49:21.187470+0000    59     128     97163     97035    6.4238   6.84375   0.0581685   0.0777848
2021-10-28T21:49:22.187588+0000 min lat: 0.0331041 max lat: 0.321188 avg lat: 0.0777037
2021-10-28T21:49:22.187588+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-28T21:49:22.187588+0000    60     119     98858     98739   6.42767   6.65625    0.151419   0.0777037
2021-10-28T21:49:23.187749+0000 Total time run:         60.0482
Total writes made:      98858
Write size:             4096
Object size:            4096
Bandwidth (MB/sec):     6.4309
Stddev Bandwidth:       0.554259
Max bandwidth (MB/sec): 7.34375
Min bandwidth (MB/sec): 5.02734
Average IOPS:           1646
Stddev IOPS:            141.89
Max IOPS:               1880
Min IOPS:               1287
Average Latency(s):     0.0777283
Stddev Latency(s):      0.0282448
Max latency(s):         0.321188
Min latency(s):         0.0331041

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:49:23,937552101-07:00] INFO: > Stop system activity collection services[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:49:23,941942916-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:391 - rados_bench() > kill -INT 782007

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:49:23,946587229-07:00] INFO: >> for OSD host[0m
# ./benchmarks/bench-rados:396 - rados_bench() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.5 kill -INT 250725
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:49:23,953933915-07:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.5 kill -INT 250725
[1] 14:49:25 [SUCCESS] ljishen@10.10.2.5

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:49:25,031066948-07:00] INFO: >>> Copy system activity file from OSD host[0m
## ./benchmarks/bench-rados:403 - rados_bench() > basename -- /tmp/bench-rados/sys_activity_4KB_write.dat.osd_host.1
# ./benchmarks/bench-rados:403 - rados_bench() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.5:/tmp/bench-rados/sys_activity_4KB_write.dat.osd_host.1 /mnt/sda8/ceph-research/scripts/output/sys_activity_4KB_write.dat.osd_host.1


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:49:26,109965909-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:49:50,097936364-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 98.86k objects, 386 MiB
    usage:   1.3 GiB used, 399 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:49:50,103993319-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:49:59,123738074-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 98.86k objects, 386 MiB
    usage:   1.3 GiB used, 399 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:49:59,129713395-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:50:08,133943710-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 98.86k objects, 386 MiB
    usage:   1.3 GiB used, 399 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:50:08,139641599-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:50:17,090348600-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 98.86k objects, 386 MiB
    usage:   1.3 GiB used, 399 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:50:17,096775874-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:50:26,189300888-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 98.86k objects, 386 MiB
    usage:   1.3 GiB used, 399 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:50:26,195595992-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:50:26,200012506-07:00] INFO: > The cluster is idle now.[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:50:26,202755686-07:00] STAGE: Start benchmarking of mode seq ...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:50:26,207930078-07:00] INFO: > Start system activity collection services[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:50:26,212150673-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:341 - rados_bench() > rm --force /mnt/sda8/ceph-research/scripts/output/sys_activity_4KB_seq.dat.rados_bench_host.1
# ./benchmarks/bench-rados:342 - rados_bench() > sar_pid_rados_bench_host=783943
# ./benchmarks/bench-rados:342 - rados_bench() > S_TIME_FORMAT=ISO
# ./benchmarks/bench-rados:342 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/output/sys_activity_4KB_seq.dat.rados_bench_host.1 2

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:50:26,217966023-07:00] INFO: >> for OSD host[0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:50:26,226540693-07:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.5 bash
10.10.2.5: b'251272\n'
[1] 14:50:27 [SUCCESS] ljishen@10.10.2.5
251272

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:50:27,343183581-07:00] INFO: > Run rados bench[0m
# ./benchmarks/bench-rados:379 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/output/rados_bench_4KB_seq.log.1
## ./benchmarks/bench-rados:376 - rados_bench() > count_cpus 32-63,96-127
# ./benchmarks/bench-rados:376 - rados_bench() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 128 --show-time
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:50:27,362661501-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:50:27,365634824-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '06799c3a-3838-11ec-b51d-53e6e728d2d3', '--', 'rados', 'bench', '99999999', 'seq', '--pool', 'bench_rados', '--concurrent-ios', '128', '--show-time'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 06799c3a-3838-11ec-b51d-53e6e728d2d3 -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 128 --show-time
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
hints = 1
2021-10-28T21:50:30.387880+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-28T21:50:30.387880+0000     0       0         0         0         0         0           -           0
2021-10-28T21:50:31.388044+0000     1     127      4643      4516   17.6366   17.6406   0.0279527   0.0277102
2021-10-28T21:50:32.388175+0000     2     128      9842      9714   18.9692   20.3047  0.00878205   0.0260804
2021-10-28T21:50:33.388269+0000     3     128     14232     14104   18.3618   17.1484  0.00408127    0.027087
2021-10-28T21:50:34.388387+0000     4     127     18421     18294   17.8627   16.3672  0.00622317    0.027836
2021-10-28T21:50:35.388504+0000     5     128     22688     22560   17.6226   16.6641   0.0237854   0.0282601
2021-10-28T21:50:36.388622+0000     6     128     28318     28190   18.3504   21.9922   0.0715042    0.027101
2021-10-28T21:50:37.388714+0000     7     128     32802     32674   18.2309   17.5156  0.00610054   0.0273925
2021-10-28T21:50:38.388856+0000     8     128     37798     37670   18.3912   19.5156   0.0254819   0.0271086
2021-10-28T21:50:39.388978+0000     9     128     41741     41613   18.0589   15.4023    0.250308   0.0271609
2021-10-28T21:50:40.389101+0000    10     128     46638     46510   18.1656   19.1289   0.0587239   0.0274719
2021-10-28T21:50:41.389188+0000    11     128     51744     51616   18.3273   19.9453   0.0235591   0.0272236
2021-10-28T21:50:42.389306+0000    12     128     56798     56670    18.445   19.7422  0.00815511   0.0270724
2021-10-28T21:50:43.389387+0000    13     128     61252     61124   18.3644   17.3984   0.0192604   0.0271998
2021-10-28T21:50:44.389511+0000    14     128     65938     65810   18.3599   18.3047     0.07499   0.0271619
2021-10-28T21:50:45.389595+0000    15     128     70181     70053   18.2408   16.5742    0.019518   0.0273826
2021-10-28T21:50:46.389718+0000    16     128     74305     74177   18.1075   16.1094   0.0295511   0.0275635
2021-10-28T21:50:47.389840+0000    17     128     79572     79444   18.2524   20.5742  0.00803536   0.0273565
2021-10-28T21:50:48.389957+0000    18     128     83834     83706   18.1632   16.6484   0.0235194   0.0275067
2021-10-28T21:50:49.390054+0000    19     127     87820     87693   18.0269   15.5742    0.135695   0.0276805
2021-10-28T21:50:50.390174+0000 min lat: 0.000411285 max lat: 0.335728 avg lat: 0.0277867
2021-10-28T21:50:50.390174+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-28T21:50:50.390174+0000    20     128     92103     91975   17.9617   16.7266   0.0277755   0.0277867
2021-10-28T21:50:51.390307+0000    21     127     97064     96937   18.0293   19.3828   0.0281234    0.027705
2021-10-28T21:50:52.390459+0000 Total time run:       21.3701
Total reads made:     98858
Read size:            4096
Object size:          4096
Bandwidth (MB/sec):   18.0703
Average IOPS:         4625
Stddev IOPS:          471.487
Max IOPS:             5630
Min IOPS:             3943
Average Latency(s):   0.0276587
Max latency(s):       0.335728
Min latency(s):       0.000411285

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:50:52,940964297-07:00] INFO: > Stop system activity collection services[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:50:52,946205134-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:391 - rados_bench() > kill -INT 783943

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:50:52,951213223-07:00] INFO: >> for OSD host[0m
# ./benchmarks/bench-rados:396 - rados_bench() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.5 kill -INT 251272
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:50:52,958994468-07:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.5 kill -INT 251272
[1] 14:50:54 [SUCCESS] ljishen@10.10.2.5

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:50:54,043860306-07:00] INFO: >>> Copy system activity file from OSD host[0m
## ./benchmarks/bench-rados:403 - rados_bench() > basename -- /tmp/bench-rados/sys_activity_4KB_seq.dat.osd_host.1
# ./benchmarks/bench-rados:403 - rados_bench() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.5:/tmp/bench-rados/sys_activity_4KB_seq.dat.osd_host.1 /mnt/sda8/ceph-research/scripts/output/sys_activity_4KB_seq.dat.osd_host.1


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:50:55,117724198-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:51:19,079099079-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 98.86k objects, 386 MiB
    usage:   1.3 GiB used, 399 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:51:19,085800099-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:51:28,161079718-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 98.86k objects, 386 MiB
    usage:   1.3 GiB used, 399 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:51:28,167439714-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:51:37,111294598-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 98.86k objects, 386 MiB
    usage:   1.3 GiB used, 399 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:51:37,118333845-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:51:46,153037782-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 98.86k objects, 386 MiB
    usage:   1.3 GiB used, 399 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:51:46,159985046-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:51:55,143331380-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 98.86k objects, 386 MiB
    usage:   1.3 GiB used, 399 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:51:55,150902129-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:51:55,156094715-07:00] INFO: > The cluster is idle now.[0m


[1;7;39;49m[2021-10-28T14:51:55,158154588-07:00][RUNNING][ROUND 2/1/21] object_size=4KB[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:51:55,161379516-07:00] STAGE: Launch a Ceph cluster (CEPH_HOST: 10.10.2.1, OSD_HOST: 10.10.2.5) ...[0m
# ./benchmarks/bench-rados:183 - launch_ceph_cluster() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.1 bash -euo pipefail
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:51:55,170106173-07:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.1 bash -euo pipefail
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T14:51:55,851833526-07:00] INFO: > Remove existing clusters\x1b[0m\n'
10.10.2.1: b'## bash:17 -  > basename -- /var/lib/ceph/06799c3a-3838-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'# bash:17 -  > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm --host 10.10.2.1 --host 10.10.2.5 rm-cluster --force --fsid 06799c3a-3838-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T14:51:55,862801726-07:00] DEBUG: HOST_PARAMS: ['--host', '10.10.2.1', '--host', '10.10.2.5']\x1b[0m\n"
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T14:51:55,866525336-07:00] DEBUG: CEPHADM_PARAMS: ['rm-cluster', '--force', '--fsid', '06799c3a-3838-11ec-b51d-53e6e728d2d3']\x1b[0m\n"
10.10.2.1: b"# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:63 -  > /tmp/bench-rados/ceph-research/scripts/parallel-ssh --host 10.10.2.1 --host 10.10.2.5 'sudo python3 -u - rm-cluster --force --fsid 06799c3a-3838-11ec-b51d-53e6e728d2d3'\n"
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T14:51:55,875157735-07:00] DEBUG: command from stdin\x1b[0m\n'
10.10.2.1: b"# /tmp/bench-rados/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host 10.10.2.1 --host 10.10.2.5 'sudo python3 -u - rm-cluster --force --fsid 06799c3a-3838-11ec-b51d-53e6e728d2d3'\n"
10.10.2.1: b'[1] 14:52:01 [SUCCESS] 10.10.2.1\n[2] 14:52:03 [SUCCESS] 10.10.2.5\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T14:52:03,136472684-07:00] INFO: > Deploy a new cluster\x1b[0m\n'
10.10.2.1: b'# bash:24 -  > /tmp/bench-rados/ceph-research/scripts/ceph-deploy -m 10.10.2.1 -o sm1/10.10.2.5:/dev/nvme0n1\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T14:52:03,149045249-07:00] STAGE: Verify passwordless SSH to hosts, passwordless sudo via ljishen, and the existence of devices on hosts...\x1b[0m\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T14:52:03,154051832-07:00] INFO: > Check the host for the monitor\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:181 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@10.10.2.1 sudo --non-interactive --validate\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T14:52:03,302997060-07:00] INFO: > Check hosts for OSDs\x1b[0m\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T14:52:03,307693168-07:00] INFO: >> Check host 'sm1'\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:200 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 sudo --non-interactive --validate\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T14:52:04,381729214-07:00] INFO: >>> Check device '/dev/nvme0n1'\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:215 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 test -b /dev/nvme0n1\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:226 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 lsblk --nodeps --noheadings --output TYPE /dev/nvme0n1\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:227 - verify_parameters() > grep --word-regexp disk\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T14:52:06,515808930-07:00] STAGE: Reset devices on hosts...\x1b[0m\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T14:52:06,520831131-07:00] INFO: > Resetting device '/dev/nvme0n1' on host 'sm1'\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:288 - reset_host_devices() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 sudo sh\n'
10.10.2.1: b'  Removing ceph--983f6d47--e3f8--47d4--9244--9168d992754d-osd--block--2bfe4bb5--7e80--4c39--a13d--894098910afd (252:0)\n'
10.10.2.1: b'  Archiving volume group "ceph-983f6d47-e3f8-47d4-9244-9168d992754d" metadata (seqno 5).\n'
10.10.2.1: b'  Releasing logical volume "osd-block-2bfe4bb5-7e80-4c39-a13d-894098910afd"\n'
10.10.2.1: b'  Creating volume group backup "/etc/lvm/backup/ceph-983f6d47-e3f8-47d4-9244-9168d992754d" (seqno 6).\n'
10.10.2.1: b'  Logical volume "osd-block-2bfe4bb5-7e80-4c39-a13d-894098910afd" successfully removed\n'
10.10.2.1: b'  Removing physical volume "/dev/nvme0n1" from volume group "ceph-983f6d47-e3f8-47d4-9244-9168d992754d"\n'
10.10.2.1: b'  Volume group "ceph-983f6d47-e3f8-47d4-9244-9168d992754d" successfully removed\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T14:52:08,790655490-07:00] STAGE: Bootstrap a new cluster...\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:317 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.1 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T14:52:08,800558457-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T14:52:08,804357949-07:00] DEBUG: CEPHADM_PARAMS: ['bootstrap', '--config', '/tmp/bench-rados/ceph-research/scripts/ceph.conf', '--mon-ip', '10.10.2.1', '--output-dir', '/tmp/bench-rados/deployment_data_root/etc/ceph', '--ssh-user', 'ljishen', '--skip-dashboard', '--no-minimize-config', '--allow-overwrite', '--allow-fqdn-hostname', '--orphan-initial-daemons', '--skip-monitoring-stack', '--single-host-defaults']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.1 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults\n'
10.10.2.1: b'Verifying podman|docker is present...\nVerifying lvm2 is present...\n'
10.10.2.1: b'Verifying time synchronization is in place...\n'
10.10.2.1: b'Unit ntp.service is enabled and running\n'
10.10.2.1: b'Repeating the final host check...\npodman|docker (/usr/bin/docker) is present\nsystemctl is present\n'
10.10.2.1: b'lvcreate is present\n'
10.10.2.1: b'Unit ntp.service is enabled and running\n'
10.10.2.1: b'Host looks OK\n'
10.10.2.1: b'Cluster fsid: 4c8c8394-3839-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Verifying IP 10.10.2.1 port 3300 ...\n'
10.10.2.1: b'Verifying IP 10.10.2.1 port 6789 ...\n'
10.10.2.1: b'Mon IP `10.10.2.1` is in CIDR network `10.10.2.0/24`\n'
10.10.2.1: b'- internal network (--cluster-network) has not been provided, OSD replication will default to the public_network\n'
10.10.2.1: b'Adjusting default settings to suit single-host cluster...\n'
10.10.2.1: b'Pulling container image quay.io/ceph/ceph:v16...\n'
10.10.2.1: b'Ceph version: ceph version 16.2.6 (ee28fb57e47e9f88813e24bbf4c14496ca299d31) pacific (stable)\n'
10.10.2.1: b'Extracting ceph user uid/gid from container image...\n'
10.10.2.1: b'Creating initial keys...\n'
10.10.2.1: b'Creating initial monmap...\n'
10.10.2.1: b'Creating mon...\n'
10.10.2.1: b'Waiting for mon to start...\n'
10.10.2.1: b'Waiting for mon...\n'
10.10.2.1: b'mon is available\n'
10.10.2.1: b'Setting mon public_network to 10.10.2.0/24\n'
10.10.2.1: b'Wrote config to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf\n'
10.10.2.1: b'Wrote keyring to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring\n'
10.10.2.1: b'Creating mgr...\n'
10.10.2.1: b'Verifying port 9283 ...\n'
10.10.2.1: b'Waiting for mgr to start...\n'
10.10.2.1: b'Waiting for mgr...\n'
10.10.2.1: b'mgr not available, waiting (1/15)...\n'
10.10.2.1: b'mgr not available, waiting (2/15)...\n'
10.10.2.1: b'mgr is available\n'
10.10.2.1: b'Enabling cephadm module...\n'
10.10.2.1: b'Waiting for the mgr to restart...\n'
10.10.2.1: b'Waiting for mgr epoch 5...\n'
10.10.2.1: b'mgr epoch 5 is available\n'
10.10.2.1: b'Setting orchestrator backend to cephadm...\n'
10.10.2.1: b'Generating ssh key...\n'
10.10.2.1: b'Wrote public SSH key to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub\n'
10.10.2.1: b'Adding key to ljishen@localhost authorized_keys...\n'
10.10.2.1: b'Adding host node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us...\n'
10.10.2.1: b'Deploying unmanaged mon service...\n'
10.10.2.1: b'Deploying unmanaged mgr service...\n'
10.10.2.1: b'You can access the Ceph CLI with:\n\n\tsudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --fsid 4c8c8394-3839-11ec-b51d-53e6e728d2d3 -c /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf -k /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring\n\n'
10.10.2.1: b'Please consider enabling telemetry to help improve Ceph:\n\n\tceph telemetry on\n\nFor more information see:\n\n\thttps://docs.ceph.com/docs/pacific/mgr/telemetry/\n\nBootstrap complete.\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T14:53:10,697817931-07:00] INFO: Sleep for 20 seconds\x1b[0m\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T14:53:30,705107933-07:00] INFO: > Set the default number of monitor daemons to 1\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:337 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T14:53:30,715130976-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T14:53:30,718953562-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mon', '1']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1\n'
10.10.2.1: b'Inferring fsid 4c8c8394-3839-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/4c8c8394-3839-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b'Scheduled mon update...\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T14:53:39,768351600-07:00] INFO: > Set the default number of manager daemons to 1\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:342 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T14:53:39,778750860-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T14:53:39,782208601-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mgr', '1']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1\n'
10.10.2.1: b'Inferring fsid 4c8c8394-3839-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/4c8c8394-3839-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b'Scheduled mgr update...\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T14:53:48,869811603-07:00] STAGE: Add hosts to the cluster...\x1b[0m\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T14:53:48,875525956-07:00] INFO: > Adding host 'sm1'\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:361 - add_hosts() > ssh-copy-id -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -f -i /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub ljishen@sm1\n'
10.10.2.1: b'/usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: "/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub"\n'
10.10.2.1: b'\nNumber of key(s) added: 1\n\nNow try logging into the machine, with:   "ssh -o \'GlobalKnownHostsFile=/dev/null\' -o \'LogLevel=ERROR\' -o \'PasswordAuthentication=no\' -o \'StrictHostKeyChecking=no\' -o \'UserKnownHostsFile=/dev/null\' \'ljishen@sm1\'"\nand check to make sure that only the key(s) you wanted were added.\n\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:367 - add_hosts() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add sm1 10.10.2.5\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T14:53:50,072626718-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T14:53:50,075936681-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'host', 'add', 'sm1', '10.10.2.5']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add sm1 10.10.2.5\n'
10.10.2.1: b'Inferring fsid 4c8c8394-3839-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/4c8c8394-3839-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b"Added host 'sm1' with addr '10.10.2.5'\n"
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T14:54:00,931525619-07:00] INFO: Sleep for 20 seconds\x1b[0m\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T14:54:20,936405018-07:00] STAGE: Add OSDs to the cluster...\x1b[0m\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T14:54:20,943391923-07:00] INFO: > Adding devices ['/dev/nvme0n1'] on host 'sm1' as OSDs\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:388 - add_host_devices_as_osds() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd sm1:/dev/nvme0n1\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T14:54:20,952763080-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T14:54:20,956487562-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'daemon', 'add', 'osd', 'sm1:/dev/nvme0n1']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd sm1:/dev/nvme0n1\n'
10.10.2.1: b'Inferring fsid 4c8c8394-3839-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/4c8c8394-3839-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b"Created osd(s) 0 on host 'sm1'\n"
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T14:54:45,460401184-07:00] INFO: > Sleep for 20 seconds\x1b[0m\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T14:55:05,465702903-07:00] STAGE: Show Cluster Status\x1b[0m\n# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:408 - show_status() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T14:55:05,476540948-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T14:55:05,480399171-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', '--status']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status\n'
10.10.2.1: b'Inferring fsid 4c8c8394-3839-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/4c8c8394-3839-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b'  cluster:\n    id:     4c8c8394-3839-11ec-b51d-53e6e728d2d3\n    health: HEALTH_WARN\n            1 pool(s) have no replicas configured\n \n  services:\n    mon: 1 daemons, quorum node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us (age 2m)\n    mgr: node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us.krgvio(active, since 2m)\n    osd: 1 osds: 1 up (since 20s), 1 in (since 40s)\n \n  data:\n    pools:   1 pools, 1 pgs\n    objects: 0 objects, 0 B\n    usage:   4.7 MiB used, 400 GiB / 400 GiB avail\n    pgs:     1 active+clean\n \n'
[1] 14:55:14 [SUCCESS] ljishen@10.10.2.1

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:51:55,851833526-07:00] INFO: > Remove existing clusters[0m
## bash:17 -  > basename -- /var/lib/ceph/06799c3a-3838-11ec-b51d-53e6e728d2d3
# bash:17 -  > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm --host 10.10.2.1 --host 10.10.2.5 rm-cluster --force --fsid 06799c3a-3838-11ec-b51d-53e6e728d2d3
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:51:55,862801726-07:00] DEBUG: HOST_PARAMS: ['--host', '10.10.2.1', '--host', '10.10.2.5'][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:51:55,866525336-07:00] DEBUG: CEPHADM_PARAMS: ['rm-cluster', '--force', '--fsid', '06799c3a-3838-11ec-b51d-53e6e728d2d3'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:63 -  > /tmp/bench-rados/ceph-research/scripts/parallel-ssh --host 10.10.2.1 --host 10.10.2.5 'sudo python3 -u - rm-cluster --force --fsid 06799c3a-3838-11ec-b51d-53e6e728d2d3'
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:51:55,875157735-07:00] DEBUG: command from stdin[0m
# /tmp/bench-rados/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host 10.10.2.1 --host 10.10.2.5 'sudo python3 -u - rm-cluster --force --fsid 06799c3a-3838-11ec-b51d-53e6e728d2d3'
[1] 14:52:01 [SUCCESS] 10.10.2.1
[2] 14:52:03 [SUCCESS] 10.10.2.5

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:52:03,136472684-07:00] INFO: > Deploy a new cluster[0m
# bash:24 -  > /tmp/bench-rados/ceph-research/scripts/ceph-deploy -m 10.10.2.1 -o sm1/10.10.2.5:/dev/nvme0n1


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:52:03,149045249-07:00] STAGE: Verify passwordless SSH to hosts, passwordless sudo via ljishen, and the existence of devices on hosts...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:52:03,154051832-07:00] INFO: > Check the host for the monitor[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:181 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@10.10.2.1 sudo --non-interactive --validate

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:52:03,302997060-07:00] INFO: > Check hosts for OSDs[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:52:03,307693168-07:00] INFO: >> Check host 'sm1'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:200 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 sudo --non-interactive --validate

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:52:04,381729214-07:00] INFO: >>> Check device '/dev/nvme0n1'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:215 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 test -b /dev/nvme0n1
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:226 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 lsblk --nodeps --noheadings --output TYPE /dev/nvme0n1
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:227 - verify_parameters() > grep --word-regexp disk


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:52:06,515808930-07:00] STAGE: Reset devices on hosts...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:52:06,520831131-07:00] INFO: > Resetting device '/dev/nvme0n1' on host 'sm1'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:288 - reset_host_devices() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 sudo sh
  Removing ceph--983f6d47--e3f8--47d4--9244--9168d992754d-osd--block--2bfe4bb5--7e80--4c39--a13d--894098910afd (252:0)
  Archiving volume group "ceph-983f6d47-e3f8-47d4-9244-9168d992754d" metadata (seqno 5).
  Releasing logical volume "osd-block-2bfe4bb5-7e80-4c39-a13d-894098910afd"
  Creating volume group backup "/etc/lvm/backup/ceph-983f6d47-e3f8-47d4-9244-9168d992754d" (seqno 6).
  Logical volume "osd-block-2bfe4bb5-7e80-4c39-a13d-894098910afd" successfully removed
  Removing physical volume "/dev/nvme0n1" from volume group "ceph-983f6d47-e3f8-47d4-9244-9168d992754d"
  Volume group "ceph-983f6d47-e3f8-47d4-9244-9168d992754d" successfully removed


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:52:08,790655490-07:00] STAGE: Bootstrap a new cluster...[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:317 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.1 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:52:08,800558457-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:52:08,804357949-07:00] DEBUG: CEPHADM_PARAMS: ['bootstrap', '--config', '/tmp/bench-rados/ceph-research/scripts/ceph.conf', '--mon-ip', '10.10.2.1', '--output-dir', '/tmp/bench-rados/deployment_data_root/etc/ceph', '--ssh-user', 'ljishen', '--skip-dashboard', '--no-minimize-config', '--allow-overwrite', '--allow-fqdn-hostname', '--orphan-initial-daemons', '--skip-monitoring-stack', '--single-host-defaults'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.1 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults
Verifying podman|docker is present...
Verifying lvm2 is present...
Verifying time synchronization is in place...
Unit ntp.service is enabled and running
Repeating the final host check...
podman|docker (/usr/bin/docker) is present
systemctl is present
lvcreate is present
Unit ntp.service is enabled and running
Host looks OK
Cluster fsid: 4c8c8394-3839-11ec-b51d-53e6e728d2d3
Verifying IP 10.10.2.1 port 3300 ...
Verifying IP 10.10.2.1 port 6789 ...
Mon IP `10.10.2.1` is in CIDR network `10.10.2.0/24`
- internal network (--cluster-network) has not been provided, OSD replication will default to the public_network
Adjusting default settings to suit single-host cluster...
Pulling container image quay.io/ceph/ceph:v16...
Ceph version: ceph version 16.2.6 (ee28fb57e47e9f88813e24bbf4c14496ca299d31) pacific (stable)
Extracting ceph user uid/gid from container image...
Creating initial keys...
Creating initial monmap...
Creating mon...
Waiting for mon to start...
Waiting for mon...
mon is available
Setting mon public_network to 10.10.2.0/24
Wrote config to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf
Wrote keyring to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring
Creating mgr...
Verifying port 9283 ...
Waiting for mgr to start...
Waiting for mgr...
mgr not available, waiting (1/15)...
mgr not available, waiting (2/15)...
mgr is available
Enabling cephadm module...
Waiting for the mgr to restart...
Waiting for mgr epoch 5...
mgr epoch 5 is available
Setting orchestrator backend to cephadm...
Generating ssh key...
Wrote public SSH key to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub
Adding key to ljishen@localhost authorized_keys...
Adding host node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us...
Deploying unmanaged mon service...
Deploying unmanaged mgr service...
You can access the Ceph CLI with:

	sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --fsid 4c8c8394-3839-11ec-b51d-53e6e728d2d3 -c /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf -k /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring

Please consider enabling telemetry to help improve Ceph:

	ceph telemetry on

For more information see:

	https://docs.ceph.com/docs/pacific/mgr/telemetry/

Bootstrap complete.

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:53:10,697817931-07:00] INFO: Sleep for 20 seconds[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:53:30,705107933-07:00] INFO: > Set the default number of monitor daemons to 1[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:337 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:53:30,715130976-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:53:30,718953562-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mon', '1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1
Inferring fsid 4c8c8394-3839-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/4c8c8394-3839-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Scheduled mon update...

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:53:39,768351600-07:00] INFO: > Set the default number of manager daemons to 1[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:342 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:53:39,778750860-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:53:39,782208601-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mgr', '1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1
Inferring fsid 4c8c8394-3839-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/4c8c8394-3839-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Scheduled mgr update...


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:53:48,869811603-07:00] STAGE: Add hosts to the cluster...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:53:48,875525956-07:00] INFO: > Adding host 'sm1'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:361 - add_hosts() > ssh-copy-id -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -f -i /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub ljishen@sm1
/usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: "/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub"

Number of key(s) added: 1

Now try logging into the machine, with:   "ssh -o 'GlobalKnownHostsFile=/dev/null' -o 'LogLevel=ERROR' -o 'PasswordAuthentication=no' -o 'StrictHostKeyChecking=no' -o 'UserKnownHostsFile=/dev/null' 'ljishen@sm1'"
and check to make sure that only the key(s) you wanted were added.

# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:367 - add_hosts() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add sm1 10.10.2.5
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:53:50,072626718-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:53:50,075936681-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'host', 'add', 'sm1', '10.10.2.5'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add sm1 10.10.2.5
Inferring fsid 4c8c8394-3839-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/4c8c8394-3839-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Added host 'sm1' with addr '10.10.2.5'

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:54:00,931525619-07:00] INFO: Sleep for 20 seconds[0m


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:54:20,936405018-07:00] STAGE: Add OSDs to the cluster...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:54:20,943391923-07:00] INFO: > Adding devices ['/dev/nvme0n1'] on host 'sm1' as OSDs[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:388 - add_host_devices_as_osds() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd sm1:/dev/nvme0n1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:54:20,952763080-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:54:20,956487562-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'daemon', 'add', 'osd', 'sm1:/dev/nvme0n1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd sm1:/dev/nvme0n1
Inferring fsid 4c8c8394-3839-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/4c8c8394-3839-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Created osd(s) 0 on host 'sm1'

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:54:45,460401184-07:00] INFO: > Sleep for 20 seconds[0m


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:55:05,465702903-07:00] STAGE: Show Cluster Status[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:408 - show_status() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:55:05,476540948-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:55:05,480399171-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', '--status'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status
Inferring fsid 4c8c8394-3839-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/4c8c8394-3839-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
  cluster:
    id:     4c8c8394-3839-11ec-b51d-53e6e728d2d3
    health: HEALTH_WARN
            1 pool(s) have no replicas configured
 
  services:
    mon: 1 daemons, quorum node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us (age 2m)
    mgr: node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us.krgvio(active, since 2m)
    osd: 1 osds: 1 up (since 20s), 1 in (since 40s)
 
  data:
    pools:   1 pools, 1 pgs
    objects: 0 objects, 0 B
    usage:   4.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     1 active+clean
 

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:55:14,138410528-07:00] INFO: > Copy configuration files to local[0m
# ./benchmarks/bench-rados:220 - launch_ceph_cluster() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.1 sudo chown --changes --recursive ljishen /tmp/bench-rados/deployment_data_root/etc/ceph
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:55:14,146033726-07:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.1 sudo chown --changes --recursive ljishen /tmp/bench-rados/deployment_data_root/etc/ceph
[1] 14:55:14 [SUCCESS] ljishen@10.10.2.1
# ./benchmarks/bench-rados:223 - launch_ceph_cluster() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.1:/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf
# ./benchmarks/bench-rados:226 - launch_ceph_cluster() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.1:/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:55:14,620601982-07:00] STAGE: Update local ceph.conf as a client...[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:55:14,623682708-07:00] STAGE: Configure Ceph pool...[0m
# ./benchmarks/bench-rados:257 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_max_backfills 32
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:55:14,645144276-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:55:14,648007733-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '4c8c8394-3839-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'config', 'set', 'osd', 'osd_max_backfills', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 4c8c8394-3839-11ec-b51d-53e6e728d2d3 -- ceph config set osd osd_max_backfills 32
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:258 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_active 32
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:55:18,694479065-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:55:18,697520266-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '4c8c8394-3839-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_active', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 4c8c8394-3839-11ec-b51d-53e6e728d2d3 -- ceph config set osd osd_recovery_max_active 32
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:259 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_single_start 8
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:55:22,755457599-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:55:22,758471469-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '4c8c8394-3839-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_single_start', '8'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 4c8c8394-3839-11ec-b51d-53e6e728d2d3 -- ceph config set osd osd_recovery_max_single_start 8
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:260 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_op_priority 63
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:55:26,661006976-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:55:26,664108762-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '4c8c8394-3839-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_op_priority', '63'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 4c8c8394-3839-11ec-b51d-53e6e728d2d3 -- ceph config set osd osd_recovery_op_priority 63
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:262 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config show-with-defaults osd.0
# ./benchmarks/bench-rados:263 - configure_ceph_pool() > grep 'osd_max_backfills\|osd_recovery'
# ./benchmarks/bench-rados:263 - configure_ceph_pool() > column -t -s ' '
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
osd_max_backfills                        32         mon
osd_recovery_cost                        20971520   default
osd_recovery_delay_start                 0.000000   default
osd_recovery_max_active                  32         mon
osd_recovery_max_active_hdd              3          default
osd_recovery_max_active_ssd              10         default
osd_recovery_max_chunk                   8388608    default
osd_recovery_max_omap_entries_per_chunk  8096       default
osd_recovery_max_single_start            8          mon
osd_recovery_op_priority                 63         mon
osd_recovery_op_warn_multiple            16         default
osd_recovery_priority                    5          default
osd_recovery_retry_interval              30.000000  default
osd_recovery_sleep                       0.000000   default
osd_recovery_sleep_hdd                   0.100000   default
osd_recovery_sleep_hybrid                0.025000   default
osd_recovery_sleep_ssd                   0.000000   default
# ./benchmarks/bench-rados:265 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool create bench_rados 128 128 replicated --size 1 --pg-num-min 128
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:55:34,800625651-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:55:34,803756141-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '4c8c8394-3839-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'create', 'bench_rados', '128', '128', 'replicated', '--size', '1', '--pg-num-min', '128'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 4c8c8394-3839-11ec-b51d-53e6e728d2d3 -- ceph osd pool create bench_rados 128 128 replicated --size 1 --pg-num-min 128
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
pool 'bench_rados' created
# ./benchmarks/bench-rados:267 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados min_size 1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:55:38,957855610-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:55:38,960729236-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '4c8c8394-3839-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'min_size', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 4c8c8394-3839-11ec-b51d-53e6e728d2d3 -- ceph osd pool set bench_rados min_size 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 min_size to 1
# ./benchmarks/bench-rados:268 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados pg_autoscale_mode off
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:55:43,257673143-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:55:43,260727579-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '4c8c8394-3839-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'pg_autoscale_mode', 'off'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 4c8c8394-3839-11ec-b51d-53e6e728d2d3 -- ceph osd pool set bench_rados pg_autoscale_mode off
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 pg_autoscale_mode to off
# ./benchmarks/bench-rados:269 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool application enable bench_rados benchmark
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:55:47,269395945-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:55:47,272367526-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '4c8c8394-3839-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'application', 'enable', 'bench_rados', 'benchmark'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 4c8c8394-3839-11ec-b51d-53e6e728d2d3 -- ceph osd pool application enable bench_rados benchmark
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
enabled application 'benchmark' on pool 'bench_rados'
# ./benchmarks/bench-rados:270 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados noscrub 1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:55:52,217197160-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:55:52,220358318-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '4c8c8394-3839-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'noscrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 4c8c8394-3839-11ec-b51d-53e6e728d2d3 -- ceph osd pool set bench_rados noscrub 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 noscrub to 1
# ./benchmarks/bench-rados:271 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados nodeep-scrub 1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:55:57,135059910-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:55:57,138185390-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '4c8c8394-3839-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'nodeep-scrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 4c8c8394-3839-11ec-b51d-53e6e728d2d3 -- ceph osd pool set bench_rados nodeep-scrub 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 nodeep-scrub to 1
# ./benchmarks/bench-rados:272 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool ls detail
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:56:01,332775563-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:56:01,335972378-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '4c8c8394-3839-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'ls', 'detail'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 4c8c8394-3839-11ec-b51d-53e6e728d2d3 -- ceph osd pool ls detail
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
pool 1 'device_health_metrics' replicated size 1 min_size 1 crush_rule 0 object_hash rjenkins pg_num 64 pgp_num 64 autoscale_mode on last_change 18 lfor 0/0/16 flags hashpspool stripe_width 0 pg_num_min 1 application mgr_devicehealth
pool 2 'bench_rados' replicated size 1 min_size 1 crush_rule 0 object_hash rjenkins pg_num 128 pgp_num 128 autoscale_mode off last_change 20 flags hashpspool,noscrub,nodeep-scrub stripe_width 0 pg_num_min 128 application benchmark

# ./benchmarks/bench-rados:273 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd df tree
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:56:05,383909443-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:56:05,386805331-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '4c8c8394-3839-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'df', 'tree'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 4c8c8394-3839-11ec-b51d-53e6e728d2d3 -- ceph osd df tree
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
ID  CLASS  WEIGHT   REWEIGHT  SIZE     RAW USE  DATA     OMAP  META     AVAIL    %USE  VAR   PGS  STATUS  TYPE NAME    
-1         0.39059         -  400 GiB  5.7 MiB  168 KiB   0 B  5.6 MiB  400 GiB  0.00  1.00    -          root default 
-3         0.39059         -  400 GiB  5.7 MiB  168 KiB   0 B  5.6 MiB  400 GiB  0.00  1.00    -              host sm1 
 0    ssd  0.39059   1.00000  400 GiB  5.7 MiB  168 KiB   0 B  5.6 MiB  400 GiB  0.00  1.00  192      up          osd.0
                       TOTAL  400 GiB  5.7 MiB  168 KiB   0 B  5.6 MiB  400 GiB  0.00                                  
MIN/MAX VAR: 1.00/1.00  STDDEV: 0


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:56:09,225468490-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:56:33,188449020-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:56:42,212608960-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:56:51,145666762-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:57:00,218433092-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:57:00,225099146-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:57:09,308284192-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:57:09,315550136-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:57:18,431983697-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:57:18,439502578-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:57:27,372009826-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:57:27,378785776-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:57:36,395139168-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:57:36,401855787-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:57:36,407153271-07:00] INFO: > The cluster is idle now.[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:57:36,410244768-07:00] STAGE: Start benchmarking of mode write ...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:57:36,415972173-07:00] INFO: > Start system activity collection services[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:57:36,420926429-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:341 - rados_bench() > rm --force /mnt/sda8/ceph-research/scripts/output/sys_activity_4KB_write.dat.rados_bench_host.2
# ./benchmarks/bench-rados:342 - rados_bench() > sar_pid_rados_bench_host=791385
# ./benchmarks/bench-rados:342 - rados_bench() > S_TIME_FORMAT=ISO
# ./benchmarks/bench-rados:342 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/output/sys_activity_4KB_write.dat.rados_bench_host.2 2

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:57:36,426798146-07:00] INFO: >> for OSD host[0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:57:36,435591939-07:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.5 bash
10.10.2.5: b'256125\n'
[1] 14:57:37 [SUCCESS] ljishen@10.10.2.5
256125

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:57:37,555702747-07:00] INFO: > Run rados bench[0m
# ./benchmarks/bench-rados:370 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/output/rados_bench_4KB_write.log.2
## ./benchmarks/bench-rados:361 - rados_bench() > count_cpus 32-63,96-127
# ./benchmarks/bench-rados:361 - rados_bench() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 60 write --pool bench_rados -b 4096 -O 4096 --concurrent-ios 128 --show-time --write-object --write-omap --write-xattr --no-cleanup
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:57:37,575289172-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:57:37,578403271-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '4c8c8394-3839-11ec-b51d-53e6e728d2d3', '--', 'rados', 'bench', '60', 'write', '--pool', 'bench_rados', '-b', '4096', '-O', '4096', '--concurrent-ios', '128', '--show-time', '--write-object', '--write-omap', '--write-xattr', '--no-cleanup'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 4c8c8394-3839-11ec-b51d-53e6e728d2d3 -- rados bench 60 write --pool bench_rados -b 4096 -O 4096 --concurrent-ios 128 --show-time --write-object --write-omap --write-xattr --no-cleanup
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
hints = 1
2021-10-28T21:57:40.583245+0000 Maintaining 128 concurrent writes of 4096 bytes to objects of size 4096 for up to 60 seconds or 0 objects
2021-10-28T21:57:40.583256+0000 Object prefix: benchmark_data_node-2.bluefield2.ucsc-cmps10_7
2021-10-28T21:57:40.583793+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-28T21:57:40.583793+0000     0       0         0         0         0         0           -           0
2021-10-28T21:57:41.583916+0000     1     128      1722      1594    6.2261   6.22656   0.0749256   0.0773228
2021-10-28T21:57:42.584035+0000     2     128      3386      3258   6.36267       6.5   0.0675729   0.0766272
2021-10-28T21:57:43.584171+0000     3     128      5050      4922   6.40815       6.5   0.0782174   0.0773931
2021-10-28T21:57:44.584285+0000     4     128      6785      6657   6.50026   6.77734   0.0893062    0.075797
2021-10-28T21:57:45.584376+0000     5     128      8250      8122   6.34463   5.72266   0.0804709   0.0765975
2021-10-28T21:57:46.584492+0000     6     128      9914      9786    6.3704       6.5   0.0998438   0.0771753
2021-10-28T21:57:47.584609+0000     7     128     11450     11322   6.31739         6   0.0732359   0.0786898
2021-10-28T21:57:48.584722+0000     8     128     13057     12929   6.31229   6.27734   0.0807845   0.0783975
2021-10-28T21:57:49.584809+0000     9     128     14721     14593   6.33309       6.5   0.0754419   0.0787875
2021-10-28T21:57:50.584921+0000    10     128     16314     16186   6.32197   6.22266   0.0773505   0.0789184
2021-10-28T21:57:51.585033+0000    11     128     17921     17793   6.31785   6.27734   0.0650838    0.079019
2021-10-28T21:57:52.585145+0000    12     128     18945     18817   6.12466         4   0.0722388   0.0814669
2021-10-28T21:57:53.585231+0000    13     128     20481     20353   6.11503         6   0.0741187   0.0815916
2021-10-28T21:57:54.585341+0000    14     128     21889     21761   6.07106       5.5   0.0821094   0.0820756
2021-10-28T21:57:55.585453+0000    15     128     23425     23297   6.06628         6    0.154934   0.0815656
2021-10-28T21:57:56.585568+0000    16     128     25217     25089   6.12458         7   0.0702462   0.0814376
2021-10-28T21:57:57.585660+0000    17     128     26426     26298   6.04209   4.72266   0.0755474   0.0825667
2021-10-28T21:57:58.585775+0000    18     128     28033     27905   6.05512   6.27734   0.0870082   0.0822662
2021-10-28T21:57:59.585893+0000    19     128     29626     29498    6.0639   6.22266   0.0792714   0.0822262
2021-10-28T21:58:00.586008+0000 min lat: 0.0537641 max lat: 0.496403 avg lat: 0.0817373
2021-10-28T21:58:00.586008+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-28T21:58:00.586008+0000    20     128     31361     31233   6.09953   6.77734   0.0914952   0.0817373
2021-10-28T21:58:01.586106+0000    21     128     32954     32826   6.10537   6.22266    0.062751   0.0817006
2021-10-28T21:58:02.586233+0000    22     128     34618     34490   6.12327       6.5   0.0834722   0.0815482
2021-10-28T21:58:03.586335+0000    23     128     35841     35713   6.06473   4.77734    0.173128    0.081974
2021-10-28T21:58:04.586450+0000    24     128     37454     37326   6.07453   6.30078    0.162148   0.0819719
2021-10-28T21:58:05.586541+0000    25     128     39098     38970   6.08841   6.42188   0.0794026   0.0819028
2021-10-28T21:58:06.586654+0000    26     128     40762     40634   6.10421       6.5   0.0752046   0.0816262
2021-10-28T21:58:07.586765+0000    27     128     42170     42042   6.08181       5.5   0.0667826   0.0821176
2021-10-28T21:58:08.586875+0000    28     128     43834     43706   6.09672       6.5     0.10248   0.0818514
2021-10-28T21:58:09.586962+0000    29     128     45242     45114   6.07612       5.5    0.085511   0.0822069
2021-10-28T21:58:10.587074+0000    30     128     46849     46721   6.08281   6.27734   0.0859727   0.0820193
2021-10-28T21:58:11.587186+0000    31     128     48385     48257   6.08012         6   0.0748551   0.0821128
2021-10-28T21:58:12.587296+0000    32     128     50106     49978   6.10017   6.72266   0.0759055   0.0818984
2021-10-28T21:58:13.587383+0000    33     128     51642     51514   6.09712         6   0.0652967   0.0819557
2021-10-28T21:58:14.587496+0000    34     128     53249     53121    6.1024   6.27734   0.0905925   0.0817808
2021-10-28T21:58:15.587610+0000    35     128     54586     54458   6.07725   5.22266    0.165471   0.0820069
2021-10-28T21:58:16.587722+0000    36     128     56065     55937    6.0689   5.77734   0.0782933   0.0820529
2021-10-28T21:58:17.587818+0000    37     128     57601     57473   6.06702         6   0.0673865   0.0823357
2021-10-28T21:58:18.587935+0000    38     128     59322     59194   6.08425   6.72266   0.0759628   0.0820002
2021-10-28T21:58:19.588046+0000    39     128     60858     60730   6.08208         6   0.0662582   0.0820893
2021-10-28T21:58:20.588159+0000 min lat: 0.0537641 max lat: 0.496403 avg lat: 0.0817885
2021-10-28T21:58:20.588159+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-28T21:58:20.588159+0000    40     128     62593     62465   6.09944   6.77734   0.0702921   0.0817885
2021-10-28T21:58:21.588274+0000    41     128     64314     64186   6.11462   6.72266   0.0770367   0.0817223
2021-10-28T21:58:22.588392+0000    42     128     65537     65409   6.08277   4.77734   0.0798685   0.0821218
2021-10-28T21:58:23.588496+0000    43     128     67059     66931   6.07956   5.94531   0.0803367   0.0821924
2021-10-28T21:58:24.588611+0000    44     128     68737     68609   6.09034   6.55469   0.0603484   0.0819459
2021-10-28T21:58:25.588702+0000    45     128     70401     70273   6.09943       6.5   0.0687823   0.0819085
2021-10-28T21:58:26.588814+0000    46     128     71994     71866   6.10209   6.22266   0.0750437   0.0817784
2021-10-28T21:58:27.588895+0000    47     128     73089     72961   6.06326   4.27734   0.0755983   0.0823227
2021-10-28T21:58:28.589010+0000    48     128     74810     74682   6.07698   6.72266   0.0967187   0.0821833
2021-10-28T21:58:29.589104+0000    49     128     76474     76346    6.0856       6.5   0.0793681    0.082095
2021-10-28T21:58:30.589218+0000    50     128     78081     77953   6.08942   6.27734   0.0746286   0.0819591
2021-10-28T21:58:31.589339+0000    51     128     79674     79546   6.09202   6.22266   0.0673131   0.0819934
2021-10-28T21:58:32.589455+0000    52     128     81338     81210   6.09985       6.5   0.0701336   0.0819275
2021-10-28T21:58:33.589541+0000    53     128     82618     82490   6.07909         5    0.141892     0.08216
2021-10-28T21:58:34.589655+0000    54     128     84282     84154   6.08687       6.5     0.10207   0.0820402
2021-10-28T21:58:35.589773+0000    55     128     86017     85889   6.09941   6.77734   0.0668571   0.0819304
2021-10-28T21:58:36.589892+0000    56     128     87738     87610   6.11053   6.72266   0.0924359   0.0817495
2021-10-28T21:58:37.589981+0000    57     128     89217     89089   6.10467   5.77734    0.077585   0.0818411
2021-10-28T21:58:38.590100+0000    58     128     90625     90497   6.09424       5.5   0.0810667   0.0819515
2021-10-28T21:58:39.590219+0000    59     128     92289     92161    6.1011       6.5   0.0654582   0.0818988
2021-10-28T21:58:40.590329+0000 min lat: 0.0536061 max lat: 0.496403 avg lat: 0.0816979
2021-10-28T21:58:40.590329+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-28T21:58:40.590329+0000    60     128     94010     93882   6.11145   6.72266   0.0760462   0.0816979
2021-10-28T21:58:41.590459+0000 Total time run:         60.0624
Total writes made:      94010
Write size:             4096
Object size:            4096
Bandwidth (MB/sec):     6.11408
Stddev Bandwidth:       0.644474
Max bandwidth (MB/sec): 7
Min bandwidth (MB/sec): 4
Average IOPS:           1565
Stddev IOPS:            164.985
Max IOPS:               1792
Min IOPS:               1024
Average Latency(s):     0.0817437
Stddev Latency(s):      0.0320302
Max latency(s):         0.496403
Min latency(s):         0.0536061

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:58:42,247924918-07:00] INFO: > Stop system activity collection services[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:58:42,253194390-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:391 - rados_bench() > kill -INT 791385

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:58:42,258357942-07:00] INFO: >> for OSD host[0m
# ./benchmarks/bench-rados:396 - rados_bench() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.5 kill -INT 256125
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:58:42,265989295-07:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.5 kill -INT 256125
[1] 14:58:43 [SUCCESS] ljishen@10.10.2.5

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:58:43,371728437-07:00] INFO: >>> Copy system activity file from OSD host[0m
## ./benchmarks/bench-rados:403 - rados_bench() > basename -- /tmp/bench-rados/sys_activity_4KB_write.dat.osd_host.2
# ./benchmarks/bench-rados:403 - rados_bench() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.5:/tmp/bench-rados/sys_activity_4KB_write.dat.osd_host.2 /mnt/sda8/ceph-research/scripts/output/sys_activity_4KB_write.dat.osd_host.2


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:58:44,430404429-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:59:08,269483906-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 94.01k objects, 367 MiB
    usage:   1.3 GiB used, 399 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:59:08,276325971-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:59:17,265410678-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 94.01k objects, 367 MiB
    usage:   1.3 GiB used, 399 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:59:17,272081020-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:59:26,181635036-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 94.01k objects, 367 MiB
    usage:   1.3 GiB used, 399 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:59:26,188481079-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:59:35,253395654-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 94.01k objects, 367 MiB
    usage:   1.3 GiB used, 399 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:59:35,260471559-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:59:44,179378365-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 94.01k objects, 367 MiB
    usage:   1.3 GiB used, 399 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:59:44,186370753-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:59:44,191405893-07:00] INFO: > The cluster is idle now.[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:59:44,194790031-07:00] STAGE: Start benchmarking of mode seq ...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:59:44,200461249-07:00] INFO: > Start system activity collection services[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:59:44,205424824-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:341 - rados_bench() > rm --force /mnt/sda8/ceph-research/scripts/output/sys_activity_4KB_seq.dat.rados_bench_host.2
# ./benchmarks/bench-rados:342 - rados_bench() > sar_pid_rados_bench_host=792735
# ./benchmarks/bench-rados:342 - rados_bench() > S_TIME_FORMAT=ISO
# ./benchmarks/bench-rados:342 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/output/sys_activity_4KB_seq.dat.rados_bench_host.2 2

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:59:44,211597708-07:00] INFO: >> for OSD host[0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:59:44,220426939-07:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.5 bash
10.10.2.5: b'256807\n'
[1] 14:59:45 [SUCCESS] ljishen@10.10.2.5
256807

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:59:45,314347126-07:00] INFO: > Run rados bench[0m
# ./benchmarks/bench-rados:379 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/output/rados_bench_4KB_seq.log.2
## ./benchmarks/bench-rados:376 - rados_bench() > count_cpus 32-63,96-127
# ./benchmarks/bench-rados:376 - rados_bench() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 128 --show-time
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:59:45,333628416-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:59:45,336439834-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '4c8c8394-3839-11ec-b51d-53e6e728d2d3', '--', 'rados', 'bench', '99999999', 'seq', '--pool', 'bench_rados', '--concurrent-ios', '128', '--show-time'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 4c8c8394-3839-11ec-b51d-53e6e728d2d3 -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 128 --show-time
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
hints = 1
2021-10-28T21:59:48.336250+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-28T21:59:48.336250+0000     0       0         0         0         0         0           -           0
2021-10-28T21:59:49.336375+0000     1     128      3716      3588   14.0129   14.0156   0.0591165   0.0350903
2021-10-28T21:59:50.336490+0000     2     128      8273      8145   15.9057   17.8008   0.0256711   0.0312128
2021-10-28T21:59:51.336554+0000     3     128     13465     13337   17.3637   20.2812   0.0473366    0.027946
2021-10-28T21:59:52.336622+0000     4     127     18546     18419   17.9853   19.8516   0.0270781   0.0275983
2021-10-28T21:59:53.336686+0000     5     128     22756     22628   17.6763   16.4414   0.0048998   0.0281654
2021-10-28T21:59:54.336750+0000     6     128     27184     27056   17.6129   17.2969    0.042432   0.0283026
2021-10-28T21:59:55.336815+0000     7     128     30233     30105   16.7981   11.9102   0.0328879   0.0296951
2021-10-28T21:59:56.336887+0000     8     127     35353     35226   17.1987   20.0039   0.0312206   0.0289956
2021-10-28T21:59:57.336963+0000     9     128     40214     40086   17.3969   18.9844   0.0475027   0.0286411
2021-10-28T21:59:58.337029+0000    10     128     45324     45196   17.6532   19.9609   0.0527776   0.0282618
2021-10-28T21:59:59.337093+0000    11     128     49424     49296   17.5042   16.0156     0.02627   0.0285326
2021-10-28T22:00:00.337155+0000    12     128     53733     53605   17.4481    16.832    0.050098   0.0286063
2021-10-28T22:00:01.337219+0000    13     128     57347     57219   17.1918   14.1172  0.00279906   0.0289971
2021-10-28T22:00:02.337283+0000    14     128     62185     62057   17.3136   18.8984  0.00554507   0.0288314
2021-10-28T22:00:03.337348+0000    15     128     67010     66882   17.4158   18.8477   0.0399451   0.0286621
2021-10-28T22:00:04.337412+0000    16     128     71670     71542    17.465   18.2031   0.0106958   0.0286015
2021-10-28T22:00:05.337477+0000    17     128     75974     75846   17.4265   16.8125  0.00126497   0.0286425
2021-10-28T22:00:06.337540+0000    18     128     80552     80424   17.4518   17.8828   0.0208858   0.0286035
2021-10-28T22:00:07.337604+0000    19     128     83738     83610   17.1883   12.4453   0.0448871   0.0290537
2021-10-28T22:00:08.337668+0000 min lat: 0.000374596 max lat: 0.37356 avg lat: 0.0288108
2021-10-28T22:00:08.337668+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-28T22:00:08.337668+0000    20     128     88902     88774   17.3374   20.1719   0.0353472   0.0288108
2021-10-28T22:00:09.337742+0000    21     128     93813     93685   17.4252   19.1836   0.0324442   0.0286693
2021-10-28T22:00:10.337849+0000 Total time run:       21.0845
Total reads made:     94010
Read size:            4096
Object size:          4096
Bandwidth (MB/sec):   17.4169
Average IOPS:         4458
Stddev IOPS:          644.648
Max IOPS:             5192
Min IOPS:             3049
Average Latency(s):   0.0286785
Max latency(s):       0.37356
Min latency(s):       0.000374596

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:00:10,986718697-07:00] INFO: > Stop system activity collection services[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:00:10,991757864-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:391 - rados_bench() > kill -INT 792735

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:00:10,996798764-07:00] INFO: >> for OSD host[0m
# ./benchmarks/bench-rados:396 - rados_bench() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.5 kill -INT 256807
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:00:11,004564360-07:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.5 kill -INT 256807
[1] 15:00:12 [SUCCESS] ljishen@10.10.2.5

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:00:12,091489905-07:00] INFO: >>> Copy system activity file from OSD host[0m
## ./benchmarks/bench-rados:403 - rados_bench() > basename -- /tmp/bench-rados/sys_activity_4KB_seq.dat.osd_host.2
# ./benchmarks/bench-rados:403 - rados_bench() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.5:/tmp/bench-rados/sys_activity_4KB_seq.dat.osd_host.2 /mnt/sda8/ceph-research/scripts/output/sys_activity_4KB_seq.dat.osd_host.2


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:00:13,161947424-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:00:37,072286682-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 94.01k objects, 367 MiB
    usage:   1.3 GiB used, 399 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:00:37,078940533-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:00:46,072315833-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 94.01k objects, 367 MiB
    usage:   1.3 GiB used, 399 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:00:46,079174019-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:00:54,928317346-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 94.01k objects, 367 MiB
    usage:   1.3 GiB used, 399 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:00:54,935366471-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:01:03,810214775-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 94.01k objects, 367 MiB
    usage:   1.3 GiB used, 399 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:01:03,817287776-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:01:12,909473847-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 94.01k objects, 367 MiB
    usage:   1.3 GiB used, 399 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:01:12,916364664-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:01:12,921658772-07:00] INFO: > The cluster is idle now.[0m


[1;7;39;49m[2021-10-28T15:01:12,923761715-07:00][RUNNING][ROUND 3/1/21] object_size=4KB[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:01:12,926964241-07:00] STAGE: Launch a Ceph cluster (CEPH_HOST: 10.10.2.1, OSD_HOST: 10.10.2.5) ...[0m
# ./benchmarks/bench-rados:183 - launch_ceph_cluster() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.1 bash -euo pipefail
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:01:12,935836773-07:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.1 bash -euo pipefail
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T15:01:13,413287049-07:00] INFO: > Remove existing clusters\x1b[0m\n'
10.10.2.1: b'## bash:17 -  > basename -- /var/lib/ceph/4c8c8394-3839-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'# bash:17 -  > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm --host 10.10.2.1 --host 10.10.2.5 rm-cluster --force --fsid 4c8c8394-3839-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T15:01:13,424762082-07:00] DEBUG: HOST_PARAMS: ['--host', '10.10.2.1', '--host', '10.10.2.5']\x1b[0m\n"
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T15:01:13,428054081-07:00] DEBUG: CEPHADM_PARAMS: ['rm-cluster', '--force', '--fsid', '4c8c8394-3839-11ec-b51d-53e6e728d2d3']\x1b[0m\n"
10.10.2.1: b"# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:63 -  > /tmp/bench-rados/ceph-research/scripts/parallel-ssh --host 10.10.2.1 --host 10.10.2.5 'sudo python3 -u - rm-cluster --force --fsid 4c8c8394-3839-11ec-b51d-53e6e728d2d3'\n"
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T15:01:13,436298249-07:00] DEBUG: command from stdin\x1b[0m\n'
10.10.2.1: b"# /tmp/bench-rados/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host 10.10.2.1 --host 10.10.2.5 'sudo python3 -u - rm-cluster --force --fsid 4c8c8394-3839-11ec-b51d-53e6e728d2d3'\n"
10.10.2.1: b'[1] 15:01:19 [SUCCESS] 10.10.2.1\n[2] 15:01:20 [SUCCESS] 10.10.2.5\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T15:01:20,632926522-07:00] INFO: > Deploy a new cluster\x1b[0m\n'
10.10.2.1: b'# bash:24 -  > /tmp/bench-rados/ceph-research/scripts/ceph-deploy -m 10.10.2.1 -o sm1/10.10.2.5:/dev/nvme0n1\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T15:01:20,644670040-07:00] STAGE: Verify passwordless SSH to hosts, passwordless sudo via ljishen, and the existence of devices on hosts...\x1b[0m\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T15:01:20,649610347-07:00] INFO: > Check the host for the monitor\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:181 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@10.10.2.1 sudo --non-interactive --validate\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T15:01:20,799115169-07:00] INFO: > Check hosts for OSDs\x1b[0m\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T15:01:20,804675352-07:00] INFO: >> Check host 'sm1'\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:200 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 sudo --non-interactive --validate\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T15:01:21,891094057-07:00] INFO: >>> Check device '/dev/nvme0n1'\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:215 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 test -b /dev/nvme0n1\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:226 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 lsblk --nodeps --noheadings --output TYPE /dev/nvme0n1\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:227 - verify_parameters() > grep --word-regexp disk\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T15:01:24,015831410-07:00] STAGE: Reset devices on hosts...\x1b[0m\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T15:01:24,020823946-07:00] INFO: > Resetting device '/dev/nvme0n1' on host 'sm1'\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:288 - reset_host_devices() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 sudo sh\n'
10.10.2.1: b'  Removing ceph--83d22ead--fea1--4bbd--9d50--737b806861fe-osd--block--6f39e761--38ee--4a0e--922d--07a6701b55fb (252:0)\n'
10.10.2.1: b'  Archiving volume group "ceph-83d22ead-fea1-4bbd-9d50-737b806861fe" metadata (seqno 5).\n'
10.10.2.1: b'  Releasing logical volume "osd-block-6f39e761-38ee-4a0e-922d-07a6701b55fb"\n'
10.10.2.1: b'  Creating volume group backup "/etc/lvm/backup/ceph-83d22ead-fea1-4bbd-9d50-737b806861fe" (seqno 6).\n'
10.10.2.1: b'  Logical volume "osd-block-6f39e761-38ee-4a0e-922d-07a6701b55fb" successfully removed\n'
10.10.2.1: b'  Removing physical volume "/dev/nvme0n1" from volume group "ceph-83d22ead-fea1-4bbd-9d50-737b806861fe"\n'
10.10.2.1: b'  Volume group "ceph-83d22ead-fea1-4bbd-9d50-737b806861fe" successfully removed\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T15:01:26,354285311-07:00] STAGE: Bootstrap a new cluster...\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:317 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.1 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T15:01:26,364183569-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T15:01:26,368111854-07:00] DEBUG: CEPHADM_PARAMS: ['bootstrap', '--config', '/tmp/bench-rados/ceph-research/scripts/ceph.conf', '--mon-ip', '10.10.2.1', '--output-dir', '/tmp/bench-rados/deployment_data_root/etc/ceph', '--ssh-user', 'ljishen', '--skip-dashboard', '--no-minimize-config', '--allow-overwrite', '--allow-fqdn-hostname', '--orphan-initial-daemons', '--skip-monitoring-stack', '--single-host-defaults']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.1 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults\n'
10.10.2.1: b'Verifying podman|docker is present...\nVerifying lvm2 is present...\n'
10.10.2.1: b'Verifying time synchronization is in place...\n'
10.10.2.1: b'Unit ntp.service is enabled and running\n'
10.10.2.1: b'Repeating the final host check...\n'
10.10.2.1: b'podman|docker (/usr/bin/docker) is present\n'
10.10.2.1: b'systemctl is present\n'
10.10.2.1: b'lvcreate is present\n'
10.10.2.1: b'Unit ntp.service is enabled and running\n'
10.10.2.1: b'Host looks OK\n'
10.10.2.1: b'Cluster fsid: 98e1fa5c-383a-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Verifying IP 10.10.2.1 port 3300 ...\n'
10.10.2.1: b'Verifying IP 10.10.2.1 port 6789 ...\n'
10.10.2.1: b'Mon IP `10.10.2.1` is in CIDR network `10.10.2.0/24`\n- internal network (--cluster-network) has not been provided, OSD replication will default to the public_network\n'
10.10.2.1: b'Adjusting default settings to suit single-host cluster...\n'
10.10.2.1: b'Pulling container image quay.io/ceph/ceph:v16...\n'
10.10.2.1: b'Ceph version: ceph version 16.2.6 (ee28fb57e47e9f88813e24bbf4c14496ca299d31) pacific (stable)\n'
10.10.2.1: b'Extracting ceph user uid/gid from container image...\n'
10.10.2.1: b'Creating initial keys...\n'
10.10.2.1: b'Creating initial monmap...\n'
10.10.2.1: b'Creating mon...\n'
10.10.2.1: b'Waiting for mon to start...\n'
10.10.2.1: b'Waiting for mon...\n'
10.10.2.1: b'mon is available\nSetting mon public_network to 10.10.2.0/24\n'
10.10.2.1: b'Wrote config to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf\n'
10.10.2.1: b'Wrote keyring to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring\n'
10.10.2.1: b'Creating mgr...\n'
10.10.2.1: b'Verifying port 9283 ...\n'
10.10.2.1: b'Waiting for mgr to start...\nWaiting for mgr...\n'
10.10.2.1: b'mgr not available, waiting (1/15)...\n'
10.10.2.1: b'mgr not available, waiting (2/15)...\n'
10.10.2.1: b'mgr is available\n'
10.10.2.1: b'Enabling cephadm module...\n'
10.10.2.1: b'Waiting for the mgr to restart...\n'
10.10.2.1: b'Waiting for mgr epoch 5...\n'
10.10.2.1: b'mgr epoch 5 is available\nSetting orchestrator backend to cephadm...\n'
10.10.2.1: b'Generating ssh key...\n'
10.10.2.1: b'Wrote public SSH key to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub\n'
10.10.2.1: b'Adding key to ljishen@localhost authorized_keys...\n'
10.10.2.1: b'Adding host node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us...\n'
10.10.2.1: b'Deploying unmanaged mon service...\n'
10.10.2.1: b'Deploying unmanaged mgr service...\n'
10.10.2.1: b'You can access the Ceph CLI with:\n\n\tsudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --fsid 98e1fa5c-383a-11ec-b51d-53e6e728d2d3 -c /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf -k /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring\n\n'
10.10.2.1: b'Please consider enabling telemetry to help improve Ceph:\n\n\tceph telemetry on\n\nFor more information see:\n\n\thttps://docs.ceph.com/docs/pacific/mgr/telemetry/\n\n'
10.10.2.1: b'Bootstrap complete.\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T15:02:27,386610214-07:00] INFO: Sleep for 20 seconds\x1b[0m\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T15:02:47,394141591-07:00] INFO: > Set the default number of monitor daemons to 1\x1b[0m\n# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:337 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T15:02:47,404585585-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T15:02:47,408676185-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mon', '1']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1\n'
10.10.2.1: b'Inferring fsid 98e1fa5c-383a-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/98e1fa5c-383a-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b'Scheduled mon update...\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T15:02:56,376063113-07:00] INFO: > Set the default number of manager daemons to 1\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:342 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T15:02:56,386089142-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T15:02:56,389782896-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mgr', '1']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1\n'
10.10.2.1: b'Inferring fsid 98e1fa5c-383a-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/98e1fa5c-383a-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b'Scheduled mgr update...\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T15:03:05,391587740-07:00] STAGE: Add hosts to the cluster...\x1b[0m\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T15:03:05,397509753-07:00] INFO: > Adding host 'sm1'\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:361 - add_hosts() > ssh-copy-id -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -f -i /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub ljishen@sm1\n'
10.10.2.1: b'/usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: "/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub"\n'
10.10.2.1: b'\nNumber of key(s) added: 1\n\nNow try logging into the machine, with:   "ssh -o \'GlobalKnownHostsFile=/dev/null\' -o \'LogLevel=ERROR\' -o \'PasswordAuthentication=no\' -o \'StrictHostKeyChecking=no\' -o \'UserKnownHostsFile=/dev/null\' \'ljishen@sm1\'"\nand check to make sure that only the key(s) you wanted were added.\n\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:367 - add_hosts() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add sm1 10.10.2.5\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T15:03:06,596666963-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T15:03:06,600743657-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'host', 'add', 'sm1', '10.10.2.5']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add sm1 10.10.2.5\n'
10.10.2.1: b'Inferring fsid 98e1fa5c-383a-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/98e1fa5c-383a-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b"Added host 'sm1' with addr '10.10.2.5'\n"
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T15:03:17,542913988-07:00] INFO: Sleep for 20 seconds\x1b[0m\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T15:03:37,547548959-07:00] STAGE: Add OSDs to the cluster...\x1b[0m\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T15:03:37,554087260-07:00] INFO: > Adding devices ['/dev/nvme0n1'] on host 'sm1' as OSDs\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:388 - add_host_devices_as_osds() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd sm1:/dev/nvme0n1\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T15:03:37,564527197-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T15:03:37,568606456-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'daemon', 'add', 'osd', 'sm1:/dev/nvme0n1']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd sm1:/dev/nvme0n1\n'
10.10.2.1: b'Inferring fsid 98e1fa5c-383a-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/98e1fa5c-383a-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b"Created osd(s) 0 on host 'sm1'\n"
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T15:04:02,349116219-07:00] INFO: > Sleep for 20 seconds\x1b[0m\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T15:04:22,354777181-07:00] STAGE: Show Cluster Status\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:408 - show_status() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T15:04:22,364984651-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T15:04:22,368637017-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', '--status']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status\n'
10.10.2.1: b'Inferring fsid 98e1fa5c-383a-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/98e1fa5c-383a-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b'  cluster:\n    id:     98e1fa5c-383a-11ec-b51d-53e6e728d2d3\n    health: HEALTH_WARN\n            1 pool(s) have no replicas configured\n \n  services:\n    mon: 1 daemons, quorum node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us (age 2m)\n    mgr: node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us.guohhr(active, since 2m)\n    osd: 1 osds: 1 up (since 19s), 1 in (since 40s)\n \n  data:\n    pools:   1 pools, 1 pgs\n    objects: 0 objects, 0 B\n    usage:   4.7 MiB used, 400 GiB / 400 GiB avail\n    pgs:     1 active+clean\n \n'
[1] 15:04:30 [SUCCESS] ljishen@10.10.2.1

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:01:13,413287049-07:00] INFO: > Remove existing clusters[0m
## bash:17 -  > basename -- /var/lib/ceph/4c8c8394-3839-11ec-b51d-53e6e728d2d3
# bash:17 -  > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm --host 10.10.2.1 --host 10.10.2.5 rm-cluster --force --fsid 4c8c8394-3839-11ec-b51d-53e6e728d2d3
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:01:13,424762082-07:00] DEBUG: HOST_PARAMS: ['--host', '10.10.2.1', '--host', '10.10.2.5'][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:01:13,428054081-07:00] DEBUG: CEPHADM_PARAMS: ['rm-cluster', '--force', '--fsid', '4c8c8394-3839-11ec-b51d-53e6e728d2d3'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:63 -  > /tmp/bench-rados/ceph-research/scripts/parallel-ssh --host 10.10.2.1 --host 10.10.2.5 'sudo python3 -u - rm-cluster --force --fsid 4c8c8394-3839-11ec-b51d-53e6e728d2d3'
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:01:13,436298249-07:00] DEBUG: command from stdin[0m
# /tmp/bench-rados/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host 10.10.2.1 --host 10.10.2.5 'sudo python3 -u - rm-cluster --force --fsid 4c8c8394-3839-11ec-b51d-53e6e728d2d3'
[1] 15:01:19 [SUCCESS] 10.10.2.1
[2] 15:01:20 [SUCCESS] 10.10.2.5

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:01:20,632926522-07:00] INFO: > Deploy a new cluster[0m
# bash:24 -  > /tmp/bench-rados/ceph-research/scripts/ceph-deploy -m 10.10.2.1 -o sm1/10.10.2.5:/dev/nvme0n1


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:01:20,644670040-07:00] STAGE: Verify passwordless SSH to hosts, passwordless sudo via ljishen, and the existence of devices on hosts...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:01:20,649610347-07:00] INFO: > Check the host for the monitor[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:181 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@10.10.2.1 sudo --non-interactive --validate

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:01:20,799115169-07:00] INFO: > Check hosts for OSDs[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:01:20,804675352-07:00] INFO: >> Check host 'sm1'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:200 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 sudo --non-interactive --validate

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:01:21,891094057-07:00] INFO: >>> Check device '/dev/nvme0n1'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:215 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 test -b /dev/nvme0n1
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:226 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 lsblk --nodeps --noheadings --output TYPE /dev/nvme0n1
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:227 - verify_parameters() > grep --word-regexp disk


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:01:24,015831410-07:00] STAGE: Reset devices on hosts...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:01:24,020823946-07:00] INFO: > Resetting device '/dev/nvme0n1' on host 'sm1'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:288 - reset_host_devices() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 sudo sh
  Removing ceph--83d22ead--fea1--4bbd--9d50--737b806861fe-osd--block--6f39e761--38ee--4a0e--922d--07a6701b55fb (252:0)
  Archiving volume group "ceph-83d22ead-fea1-4bbd-9d50-737b806861fe" metadata (seqno 5).
  Releasing logical volume "osd-block-6f39e761-38ee-4a0e-922d-07a6701b55fb"
  Creating volume group backup "/etc/lvm/backup/ceph-83d22ead-fea1-4bbd-9d50-737b806861fe" (seqno 6).
  Logical volume "osd-block-6f39e761-38ee-4a0e-922d-07a6701b55fb" successfully removed
  Removing physical volume "/dev/nvme0n1" from volume group "ceph-83d22ead-fea1-4bbd-9d50-737b806861fe"
  Volume group "ceph-83d22ead-fea1-4bbd-9d50-737b806861fe" successfully removed


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:01:26,354285311-07:00] STAGE: Bootstrap a new cluster...[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:317 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.1 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:01:26,364183569-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:01:26,368111854-07:00] DEBUG: CEPHADM_PARAMS: ['bootstrap', '--config', '/tmp/bench-rados/ceph-research/scripts/ceph.conf', '--mon-ip', '10.10.2.1', '--output-dir', '/tmp/bench-rados/deployment_data_root/etc/ceph', '--ssh-user', 'ljishen', '--skip-dashboard', '--no-minimize-config', '--allow-overwrite', '--allow-fqdn-hostname', '--orphan-initial-daemons', '--skip-monitoring-stack', '--single-host-defaults'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.1 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults
Verifying podman|docker is present...
Verifying lvm2 is present...
Verifying time synchronization is in place...
Unit ntp.service is enabled and running
Repeating the final host check...
podman|docker (/usr/bin/docker) is present
systemctl is present
lvcreate is present
Unit ntp.service is enabled and running
Host looks OK
Cluster fsid: 98e1fa5c-383a-11ec-b51d-53e6e728d2d3
Verifying IP 10.10.2.1 port 3300 ...
Verifying IP 10.10.2.1 port 6789 ...
Mon IP `10.10.2.1` is in CIDR network `10.10.2.0/24`
- internal network (--cluster-network) has not been provided, OSD replication will default to the public_network
Adjusting default settings to suit single-host cluster...
Pulling container image quay.io/ceph/ceph:v16...
Ceph version: ceph version 16.2.6 (ee28fb57e47e9f88813e24bbf4c14496ca299d31) pacific (stable)
Extracting ceph user uid/gid from container image...
Creating initial keys...
Creating initial monmap...
Creating mon...
Waiting for mon to start...
Waiting for mon...
mon is available
Setting mon public_network to 10.10.2.0/24
Wrote config to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf
Wrote keyring to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring
Creating mgr...
Verifying port 9283 ...
Waiting for mgr to start...
Waiting for mgr...
mgr not available, waiting (1/15)...
mgr not available, waiting (2/15)...
mgr is available
Enabling cephadm module...
Waiting for the mgr to restart...
Waiting for mgr epoch 5...
mgr epoch 5 is available
Setting orchestrator backend to cephadm...
Generating ssh key...
Wrote public SSH key to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub
Adding key to ljishen@localhost authorized_keys...
Adding host node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us...
Deploying unmanaged mon service...
Deploying unmanaged mgr service...
You can access the Ceph CLI with:

	sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --fsid 98e1fa5c-383a-11ec-b51d-53e6e728d2d3 -c /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf -k /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring

Please consider enabling telemetry to help improve Ceph:

	ceph telemetry on

For more information see:

	https://docs.ceph.com/docs/pacific/mgr/telemetry/

Bootstrap complete.

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:02:27,386610214-07:00] INFO: Sleep for 20 seconds[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:02:47,394141591-07:00] INFO: > Set the default number of monitor daemons to 1[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:337 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:02:47,404585585-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:02:47,408676185-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mon', '1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1
Inferring fsid 98e1fa5c-383a-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/98e1fa5c-383a-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Scheduled mon update...

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:02:56,376063113-07:00] INFO: > Set the default number of manager daemons to 1[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:342 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:02:56,386089142-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:02:56,389782896-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mgr', '1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1
Inferring fsid 98e1fa5c-383a-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/98e1fa5c-383a-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Scheduled mgr update...


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:03:05,391587740-07:00] STAGE: Add hosts to the cluster...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:03:05,397509753-07:00] INFO: > Adding host 'sm1'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:361 - add_hosts() > ssh-copy-id -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -f -i /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub ljishen@sm1
/usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: "/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub"

Number of key(s) added: 1

Now try logging into the machine, with:   "ssh -o 'GlobalKnownHostsFile=/dev/null' -o 'LogLevel=ERROR' -o 'PasswordAuthentication=no' -o 'StrictHostKeyChecking=no' -o 'UserKnownHostsFile=/dev/null' 'ljishen@sm1'"
and check to make sure that only the key(s) you wanted were added.

# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:367 - add_hosts() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add sm1 10.10.2.5
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:03:06,596666963-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:03:06,600743657-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'host', 'add', 'sm1', '10.10.2.5'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add sm1 10.10.2.5
Inferring fsid 98e1fa5c-383a-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/98e1fa5c-383a-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Added host 'sm1' with addr '10.10.2.5'

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:03:17,542913988-07:00] INFO: Sleep for 20 seconds[0m


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:03:37,547548959-07:00] STAGE: Add OSDs to the cluster...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:03:37,554087260-07:00] INFO: > Adding devices ['/dev/nvme0n1'] on host 'sm1' as OSDs[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:388 - add_host_devices_as_osds() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd sm1:/dev/nvme0n1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:03:37,564527197-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:03:37,568606456-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'daemon', 'add', 'osd', 'sm1:/dev/nvme0n1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd sm1:/dev/nvme0n1
Inferring fsid 98e1fa5c-383a-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/98e1fa5c-383a-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Created osd(s) 0 on host 'sm1'

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:04:02,349116219-07:00] INFO: > Sleep for 20 seconds[0m


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:04:22,354777181-07:00] STAGE: Show Cluster Status[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:408 - show_status() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:04:22,364984651-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:04:22,368637017-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', '--status'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status
Inferring fsid 98e1fa5c-383a-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/98e1fa5c-383a-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
  cluster:
    id:     98e1fa5c-383a-11ec-b51d-53e6e728d2d3
    health: HEALTH_WARN
            1 pool(s) have no replicas configured
 
  services:
    mon: 1 daemons, quorum node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us (age 2m)
    mgr: node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us.guohhr(active, since 2m)
    osd: 1 osds: 1 up (since 19s), 1 in (since 40s)
 
  data:
    pools:   1 pools, 1 pgs
    objects: 0 objects, 0 B
    usage:   4.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     1 active+clean
 

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:04:30,701905819-07:00] INFO: > Copy configuration files to local[0m
# ./benchmarks/bench-rados:220 - launch_ceph_cluster() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.1 sudo chown --changes --recursive ljishen /tmp/bench-rados/deployment_data_root/etc/ceph
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:04:30,709631087-07:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.1 sudo chown --changes --recursive ljishen /tmp/bench-rados/deployment_data_root/etc/ceph
[1] 15:04:30 [SUCCESS] ljishen@10.10.2.1
# ./benchmarks/bench-rados:223 - launch_ceph_cluster() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.1:/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf
# ./benchmarks/bench-rados:226 - launch_ceph_cluster() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.1:/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:04:31,184372818-07:00] STAGE: Update local ceph.conf as a client...[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:04:31,187649573-07:00] STAGE: Configure Ceph pool...[0m
# ./benchmarks/bench-rados:257 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_max_backfills 32
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:04:31,209297760-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:04:31,212085133-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '98e1fa5c-383a-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'config', 'set', 'osd', 'osd_max_backfills', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 98e1fa5c-383a-11ec-b51d-53e6e728d2d3 -- ceph config set osd osd_max_backfills 32
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:258 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_active 32
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:04:35,053172416-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:04:35,056346198-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '98e1fa5c-383a-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_active', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 98e1fa5c-383a-11ec-b51d-53e6e728d2d3 -- ceph config set osd osd_recovery_max_active 32
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:259 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_single_start 8
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:04:39,143648378-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:04:39,146898533-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '98e1fa5c-383a-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_single_start', '8'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 98e1fa5c-383a-11ec-b51d-53e6e728d2d3 -- ceph config set osd osd_recovery_max_single_start 8
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:260 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_op_priority 63
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:04:43,096037446-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:04:43,099009097-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '98e1fa5c-383a-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_op_priority', '63'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 98e1fa5c-383a-11ec-b51d-53e6e728d2d3 -- ceph config set osd osd_recovery_op_priority 63
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:262 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config show-with-defaults osd.0
# ./benchmarks/bench-rados:263 - configure_ceph_pool() > grep 'osd_max_backfills\|osd_recovery'
# ./benchmarks/bench-rados:263 - configure_ceph_pool() > column -t -s ' '
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
osd_max_backfills                        32         mon
osd_recovery_cost                        20971520   default
osd_recovery_delay_start                 0.000000   default
osd_recovery_max_active                  32         mon
osd_recovery_max_active_hdd              3          default
osd_recovery_max_active_ssd              10         default
osd_recovery_max_chunk                   8388608    default
osd_recovery_max_omap_entries_per_chunk  8096       default
osd_recovery_max_single_start            8          mon
osd_recovery_op_priority                 3          default
osd_recovery_op_warn_multiple            16         default
osd_recovery_priority                    5          default
osd_recovery_retry_interval              30.000000  default
osd_recovery_sleep                       0.000000   default
osd_recovery_sleep_hdd                   0.100000   default
osd_recovery_sleep_hybrid                0.025000   default
osd_recovery_sleep_ssd                   0.000000   default
# ./benchmarks/bench-rados:265 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool create bench_rados 128 128 replicated --size 1 --pg-num-min 128
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:04:51,162382147-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:04:51,165382651-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '98e1fa5c-383a-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'create', 'bench_rados', '128', '128', 'replicated', '--size', '1', '--pg-num-min', '128'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 98e1fa5c-383a-11ec-b51d-53e6e728d2d3 -- ceph osd pool create bench_rados 128 128 replicated --size 1 --pg-num-min 128
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
pool 'bench_rados' created
# ./benchmarks/bench-rados:267 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados min_size 1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:04:55,561013487-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:04:55,564284011-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '98e1fa5c-383a-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'min_size', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 98e1fa5c-383a-11ec-b51d-53e6e728d2d3 -- ceph osd pool set bench_rados min_size 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 min_size to 1
# ./benchmarks/bench-rados:268 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados pg_autoscale_mode off
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:05:00,593962710-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:05:00,596893924-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '98e1fa5c-383a-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'pg_autoscale_mode', 'off'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 98e1fa5c-383a-11ec-b51d-53e6e728d2d3 -- ceph osd pool set bench_rados pg_autoscale_mode off
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 pg_autoscale_mode to off
# ./benchmarks/bench-rados:269 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool application enable bench_rados benchmark
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:05:05,627951512-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:05:05,630960152-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '98e1fa5c-383a-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'application', 'enable', 'bench_rados', 'benchmark'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 98e1fa5c-383a-11ec-b51d-53e6e728d2d3 -- ceph osd pool application enable bench_rados benchmark
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
enabled application 'benchmark' on pool 'bench_rados'
# ./benchmarks/bench-rados:270 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados noscrub 1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:05:09,892262329-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:05:09,895350659-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '98e1fa5c-383a-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'noscrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 98e1fa5c-383a-11ec-b51d-53e6e728d2d3 -- ceph osd pool set bench_rados noscrub 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 noscrub to 1
# ./benchmarks/bench-rados:271 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados nodeep-scrub 1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:05:14,783071382-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:05:14,786079250-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '98e1fa5c-383a-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'nodeep-scrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 98e1fa5c-383a-11ec-b51d-53e6e728d2d3 -- ceph osd pool set bench_rados nodeep-scrub 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 nodeep-scrub to 1
# ./benchmarks/bench-rados:272 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool ls detail
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:05:19,624040898-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:05:19,626960922-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '98e1fa5c-383a-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'ls', 'detail'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 98e1fa5c-383a-11ec-b51d-53e6e728d2d3 -- ceph osd pool ls detail
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
pool 1 'device_health_metrics' replicated size 1 min_size 1 crush_rule 0 object_hash rjenkins pg_num 64 pgp_num 64 autoscale_mode on last_change 18 lfor 0/0/16 flags hashpspool stripe_width 0 pg_num_min 1 application mgr_devicehealth
pool 2 'bench_rados' replicated size 1 min_size 1 crush_rule 0 object_hash rjenkins pg_num 128 pgp_num 128 autoscale_mode off last_change 20 flags hashpspool,noscrub,nodeep-scrub stripe_width 0 pg_num_min 128 application benchmark

# ./benchmarks/bench-rados:273 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd df tree
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:05:23,699379207-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:05:23,702456286-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '98e1fa5c-383a-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'df', 'tree'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 98e1fa5c-383a-11ec-b51d-53e6e728d2d3 -- ceph osd df tree
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
ID  CLASS  WEIGHT   REWEIGHT  SIZE     RAW USE  DATA     OMAP  META     AVAIL    %USE  VAR   PGS  STATUS  TYPE NAME    
-1         0.39059         -  400 GiB  5.7 MiB  168 KiB   0 B  5.6 MiB  400 GiB  0.00  1.00    -          root default 
-3         0.39059         -  400 GiB  5.7 MiB  168 KiB   0 B  5.6 MiB  400 GiB  0.00  1.00    -              host sm1 
 0    ssd  0.39059   1.00000  400 GiB  5.7 MiB  168 KiB   0 B  5.6 MiB  400 GiB  0.00  1.00  192      up          osd.0
                       TOTAL  400 GiB  5.7 MiB  168 KiB   0 B  5.6 MiB  400 GiB  0.00                                  
MIN/MAX VAR: 1.00/1.00  STDDEV: 0


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:05:27,700590248-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:05:51,647847504-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:06:00,665127917-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:06:09,634467796-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:06:09,641211876-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:06:18,674827178-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:06:18,682245378-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:06:27,499225009-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:06:27,506032648-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:06:36,414697978-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:06:36,422000340-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:06:45,352615327-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:06:45,359355479-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:06:45,364334643-07:00] INFO: > The cluster is idle now.[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:06:45,367538491-07:00] STAGE: Start benchmarking of mode write ...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:06:45,373394928-07:00] INFO: > Start system activity collection services[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:06:45,378110495-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:341 - rados_bench() > rm --force /mnt/sda8/ceph-research/scripts/output/sys_activity_4KB_write.dat.rados_bench_host.3
# ./benchmarks/bench-rados:342 - rados_bench() > sar_pid_rados_bench_host=798290
# ./benchmarks/bench-rados:342 - rados_bench() > S_TIME_FORMAT=ISO
# ./benchmarks/bench-rados:342 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/output/sys_activity_4KB_write.dat.rados_bench_host.3 2

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:06:45,384290822-07:00] INFO: >> for OSD host[0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:06:45,392844071-07:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.5 bash
10.10.2.5: b'261686\n'
[1] 15:06:46 [SUCCESS] ljishen@10.10.2.5
261686

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:06:46,530517380-07:00] INFO: > Run rados bench[0m
# ./benchmarks/bench-rados:370 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/output/rados_bench_4KB_write.log.3
## ./benchmarks/bench-rados:361 - rados_bench() > count_cpus 32-63,96-127
# ./benchmarks/bench-rados:361 - rados_bench() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 60 write --pool bench_rados -b 4096 -O 4096 --concurrent-ios 128 --show-time --write-object --write-omap --write-xattr --no-cleanup
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:06:46,550727978-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:06:46,553555046-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '98e1fa5c-383a-11ec-b51d-53e6e728d2d3', '--', 'rados', 'bench', '60', 'write', '--pool', 'bench_rados', '-b', '4096', '-O', '4096', '--concurrent-ios', '128', '--show-time', '--write-object', '--write-omap', '--write-xattr', '--no-cleanup'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 98e1fa5c-383a-11ec-b51d-53e6e728d2d3 -- rados bench 60 write --pool bench_rados -b 4096 -O 4096 --concurrent-ios 128 --show-time --write-object --write-omap --write-xattr --no-cleanup
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
hints = 1
2021-10-28T22:06:49.584601+0000 Maintaining 128 concurrent writes of 4096 bytes to objects of size 4096 for up to 60 seconds or 0 objects
2021-10-28T22:06:49.584612+0000 Object prefix: benchmark_data_node-2.bluefield2.ucsc-cmps10_7
2021-10-28T22:06:49.585147+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-28T22:06:49.585147+0000     0       0         0         0         0         0           -           0
2021-10-28T22:06:50.585275+0000     1     128      1537      1409   5.50347   5.50391    0.065772   0.0869477
2021-10-28T22:06:51.585396+0000     2     128      3457      3329    6.5013       7.5   0.0588901    0.074946
2021-10-28T22:06:52.585468+0000     3     128      5079      4951   6.44603   6.33594   0.0814567   0.0770881
2021-10-28T22:06:53.585551+0000     4     128      6615      6487    6.3344         6   0.0751364   0.0781375
2021-10-28T22:06:54.585661+0000     5     128      8193      8065    6.3002   6.16406   0.0749361   0.0786923
2021-10-28T22:06:55.585770+0000     6     128      9857      9729   6.33338       6.5    0.072833   0.0782595
2021-10-28T22:06:56.585881+0000     7     128     11521     11393   6.35708       6.5    0.067393   0.0782761
2021-10-28T22:06:57.585960+0000     8     128     13394     13266   6.47692   7.31641    0.074471   0.0770052
2021-10-28T22:06:58.586074+0000     9     128     15063     14935   6.48157   6.51953   0.0690099   0.0767971
2021-10-28T22:06:59.586151+0000    10     127     16708     16581   6.47633   6.42969   0.0687173   0.0770219
2021-10-28T22:07:00.586265+0000    11     128     18177     18049   6.40882   5.73438   0.0843414   0.0777435
2021-10-28T22:07:01.586340+0000    12     128     20055     19927   6.48604   7.33594   0.0629728   0.0769472
2021-10-28T22:07:02.586452+0000    13     128     21719     21591   6.48705       6.5   0.0784921   0.0769283
2021-10-28T22:07:03.586523+0000    14     128     23553     23425   6.53537   7.16406   0.0622009   0.0763735
2021-10-28T22:07:04.586643+0000    15     128     25047     24919    6.4887   5.83594   0.0769646   0.0768556
2021-10-28T22:07:05.586722+0000    16     128     26455     26327   6.42688       5.5   0.0625831   0.0777142
2021-10-28T22:07:06.586842+0000    17     127     27957     27830   6.39414   5.87109    0.114479   0.0780059
2021-10-28T22:07:07.586922+0000    18     128     29655     29527   6.40715   6.62891   0.0867749   0.0779116
2021-10-28T22:07:08.587055+0000    19     128     31191     31063   6.38568         6   0.0761926   0.0781207
2021-10-28T22:07:09.587134+0000 min lat: 0.0482426 max lat: 0.267519 avg lat: 0.0776169
2021-10-28T22:07:09.587134+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-28T22:07:09.587134+0000    20     128     33025     32897   6.42457   7.16406   0.0699584   0.0776169
2021-10-28T22:07:10.587268+0000    21     128     34433     34305   6.38051       5.5   0.0620569   0.0778447
2021-10-28T22:07:11.587353+0000    22     128     35927     35799   6.35573   5.83594   0.0667873   0.0786057
2021-10-28T22:07:12.587472+0000    23     128     37591     37463   6.36197       6.5   0.0664478   0.0783395
2021-10-28T22:07:13.587549+0000    24     128     39041     38913   6.33288   5.66406   0.0948513    0.078801
2021-10-28T22:07:14.587671+0000    25     128     40663     40535   6.33297   6.33594   0.0642053   0.0788316
2021-10-28T22:07:15.587751+0000    26     127     42273     42146   6.33141   6.29297   0.0762292   0.0788315
2021-10-28T22:07:16.587874+0000    27     128     43649     43521   6.29582   5.37109     0.12583   0.0791088
2021-10-28T22:07:17.587955+0000    28     128     45457     45329   6.32318    7.0625   0.0665684   0.0789386
2021-10-28T22:07:18.588074+0000    29     128     47105     46977   6.32709    6.4375   0.0693631   0.0788958
2021-10-28T22:07:19.588151+0000    30     128     48769     48641   6.33284       6.5    0.115528   0.0787753
2021-10-28T22:07:20.588277+0000    31     128     50305     50177   6.32208         6   0.0677158   0.0789933
2021-10-28T22:07:21.588358+0000    32     128     51799     51671   6.30687   5.83594   0.0730384    0.079156
2021-10-28T22:07:22.588486+0000    33     128     53335     53207   6.29755         6   0.0790065   0.0793194
2021-10-28T22:07:23.588563+0000    34     128     55127     54999   6.31819         7   0.0629391   0.0791031
2021-10-28T22:07:24.588683+0000    35     128     56653     56525   6.30797   5.96094   0.0610563   0.0792221
2021-10-28T22:07:25.588760+0000    36     128     58199     58071   6.30048   6.03906   0.0720397   0.0793044
2021-10-28T22:07:26.588881+0000    37     128     59905     59777   6.31029   6.66406   0.0986788   0.0790941
2021-10-28T22:07:27.588962+0000    38     128     61441     61313   6.30211         6   0.0687302   0.0792657
2021-10-28T22:07:28.589081+0000    39     128     63191     63063   6.31578   6.83594   0.0720779   0.0790649
2021-10-28T22:07:29.589157+0000 min lat: 0.0482426 max lat: 0.267519 avg lat: 0.0787815
2021-10-28T22:07:29.589157+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-28T22:07:29.589157+0000    40     128     65062     64934   6.34058   7.30859   0.0644913   0.0787815
2021-10-28T22:07:30.589284+0000    41     128     66433     66305   6.31654   5.35547   0.0767606   0.0788787
2021-10-28T22:07:31.589360+0000    42     128     68225     68097    6.3328         7   0.0821762   0.0788791
2021-10-28T22:07:32.589477+0000    43     128     69563     69435   6.30706   5.22656   0.0652185   0.0792183
2021-10-28T22:07:33.589554+0000    44     128     71255     71127   6.31392   6.60938   0.0696872   0.0791193
2021-10-28T22:07:34.589673+0000    45     128     72791     72663   6.30692         6   0.0762348   0.0791913
2021-10-28T22:07:35.589749+0000    46     127     74524     74397   6.31705   6.77344    0.100541   0.0790388
2021-10-28T22:07:36.589871+0000    47     128     76247     76119   6.32575   6.72656   0.0665003   0.0789561
2021-10-28T22:07:37.589953+0000    48     128     77569     77441   6.30154   5.16406    0.208412   0.0792755
2021-10-28T22:07:38.590072+0000    49     128     79233     79105   6.30557       6.5   0.0656863   0.0792426
2021-10-28T22:07:39.590151+0000    50     128     81111     80983   6.32617   7.33594   0.0721894   0.0790054
2021-10-28T22:07:40.590277+0000    51     128     82775     82647   6.32956       6.5   0.0685494   0.0789482
2021-10-28T22:07:41.590355+0000    52     128     84439     84311   6.33283       6.5   0.0727708   0.0788955
2021-10-28T22:07:42.590471+0000    53     128     85999     85871   6.32831   6.09375   0.0720056   0.0789625
2021-10-28T22:07:43.590549+0000    54     128     87553     87425   6.32352   6.07031   0.0744876   0.0790038
2021-10-28T22:07:44.590679+0000    55     128     89175     89047   6.32373   6.33594   0.0704827   0.0790104
2021-10-28T22:07:45.590776+0000    56     128     90967     90839   6.33579         7   0.0793341   0.0788641
2021-10-28T22:07:46.590906+0000    57     128     92631     92503   6.33866       6.5   0.0783008   0.0788293
2021-10-28T22:07:47.590982+0000    58     128     94337     94209   6.34426   6.66406   0.0835545   0.0787543
2021-10-28T22:07:48.591100+0000    59     128     95617     95489   6.32147         5    0.226577   0.0788703
2021-10-28T22:07:49.591178+0000 min lat: 0.0482426 max lat: 0.267519 avg lat: 0.0790272
2021-10-28T22:07:49.591178+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-28T22:07:49.591178+0000    60     128     97239     97111    6.3217   6.33594   0.0801889   0.0790272
2021-10-28T22:07:50.591347+0000 Total time run:         60.052
Total writes made:      97239
Write size:             4096
Object size:            4096
Bandwidth (MB/sec):     6.32518
Stddev Bandwidth:       0.595967
Max bandwidth (MB/sec): 7.5
Min bandwidth (MB/sec): 5
Average IOPS:           1619
Stddev IOPS:            152.567
Max IOPS:               1920
Min IOPS:               1280
Average Latency(s):     0.0790275
Stddev Latency(s):      0.0258924
Max latency(s):         0.267519
Min latency(s):         0.0482426

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:07:51,333343142-07:00] INFO: > Stop system activity collection services[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:07:51,338216076-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:391 - rados_bench() > kill -INT 798290

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:07:51,343552603-07:00] INFO: >> for OSD host[0m
# ./benchmarks/bench-rados:396 - rados_bench() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.5 kill -INT 261686
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:07:51,351176601-07:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.5 kill -INT 261686
[1] 15:07:52 [SUCCESS] ljishen@10.10.2.5

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:07:52,460284598-07:00] INFO: >>> Copy system activity file from OSD host[0m
## ./benchmarks/bench-rados:403 - rados_bench() > basename -- /tmp/bench-rados/sys_activity_4KB_write.dat.osd_host.3
# ./benchmarks/bench-rados:403 - rados_bench() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.5:/tmp/bench-rados/sys_activity_4KB_write.dat.osd_host.3 /mnt/sda8/ceph-research/scripts/output/sys_activity_4KB_write.dat.osd_host.3


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:07:53,542127573-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:08:17,365063649-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 97.24k objects, 380 MiB
    usage:   1.3 GiB used, 399 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:08:17,371851481-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:08:26,301722481-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 97.24k objects, 380 MiB
    usage:   1.3 GiB used, 399 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:08:26,309067292-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:08:35,200213183-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 97.24k objects, 380 MiB
    usage:   1.3 GiB used, 399 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:08:35,207170234-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:08:44,072630772-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 97.24k objects, 380 MiB
    usage:   1.3 GiB used, 399 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:08:44,079522940-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:08:53,134770596-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 97.24k objects, 380 MiB
    usage:   1.3 GiB used, 399 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:08:53,142206339-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:08:53,147403754-07:00] INFO: > The cluster is idle now.[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:08:53,150628271-07:00] STAGE: Start benchmarking of mode seq ...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:08:53,156553909-07:00] INFO: > Start system activity collection services[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:08:53,161452019-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:341 - rados_bench() > rm --force /mnt/sda8/ceph-research/scripts/output/sys_activity_4KB_seq.dat.rados_bench_host.3
# ./benchmarks/bench-rados:342 - rados_bench() > sar_pid_rados_bench_host=799658
# ./benchmarks/bench-rados:342 - rados_bench() > S_TIME_FORMAT=ISO
# ./benchmarks/bench-rados:342 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/output/sys_activity_4KB_seq.dat.rados_bench_host.3 2

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:08:53,167447969-07:00] INFO: >> for OSD host[0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:08:53,176361727-07:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.5 bash
10.10.2.5: b'262231\n'
[1] 15:08:54 [SUCCESS] ljishen@10.10.2.5
262231

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:08:54,290677270-07:00] INFO: > Run rados bench[0m
# ./benchmarks/bench-rados:379 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/output/rados_bench_4KB_seq.log.3
## ./benchmarks/bench-rados:376 - rados_bench() > count_cpus 32-63,96-127
# ./benchmarks/bench-rados:376 - rados_bench() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 128 --show-time
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:08:54,309612676-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:08:54,312620644-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '98e1fa5c-383a-11ec-b51d-53e6e728d2d3', '--', 'rados', 'bench', '99999999', 'seq', '--pool', 'bench_rados', '--concurrent-ios', '128', '--show-time'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 98e1fa5c-383a-11ec-b51d-53e6e728d2d3 -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 128 --show-time
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
hints = 1
2021-10-28T22:08:57.265168+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-28T22:08:57.265168+0000     0       0         0         0         0         0           -           0
2021-10-28T22:08:58.265304+0000     1     128      4309      4181   16.3287    16.332  0.00466174   0.0302402
2021-10-28T22:08:59.265419+0000     2     128     10026      9898    19.329    22.332   0.0383783   0.0255626
2021-10-28T22:09:00.265491+0000     3     128     14658     14530   18.9168   18.0938   0.0115381   0.0262864
2021-10-28T22:09:01.265569+0000     4     128     18404     18276   17.8456   14.6328    0.193629   0.0271765
2021-10-28T22:09:02.265680+0000     5     128     23354     23226   18.1432   19.3359   0.0250165   0.0274939
2021-10-28T22:09:03.265772+0000     6     128     27758     27630   17.9863   17.2031   0.0476538   0.0276406
2021-10-28T22:09:04.265892+0000     7     128     31941     31813   17.7508   16.3398  0.00254767   0.0280831
2021-10-28T22:09:05.265979+0000     8     128     37166     37038    18.083   20.4102   0.0165056    0.027585
2021-10-28T22:09:06.266091+0000     9     128     42353     42225   18.3248   20.2617   0.0154376   0.0272321
2021-10-28T22:09:07.266201+0000    10     128     46401     46273   18.0734   15.8125   0.0450114   0.0276097
2021-10-28T22:09:08.266314+0000    11     128     50797     50669   17.9913   17.1719   0.0248419   0.0277393
2021-10-28T22:09:09.266392+0000    12     128     55809     55681   18.1234   19.5781   0.0340221   0.0275334
2021-10-28T22:09:10.266504+0000    13     128     59714     59586   17.9025   15.2539   0.0149336   0.0279056
2021-10-28T22:09:11.266620+0000    14     128     65133     65005   18.1356    21.168   0.0179395   0.0275312
2021-10-28T22:09:12.266729+0000    15     128     68394     68266   17.7757   12.7383   0.0637743   0.0280592
2021-10-28T22:09:13.266801+0000    16     128     72586     72458   17.6881    16.375  0.00527318   0.0282337
2021-10-28T22:09:14.266912+0000    17     128     76691     76563   17.5907   16.0352   0.0478216   0.0283938
2021-10-28T22:09:15.267024+0000    18     128     81805     81677   17.7231   19.9766  0.00410636   0.0281807
2021-10-28T22:09:16.267137+0000    19     128     86722     86594   17.8011    19.207  0.00469416   0.0280733
2021-10-28T22:09:17.267212+0000 min lat: 0.000404062 max lat: 0.277219 avg lat: 0.0282049
2021-10-28T22:09:17.267212+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-28T22:09:17.267212+0000    20     128     90825     90697   17.7124   16.0273 0.000459356   0.0282049
2021-10-28T22:09:18.267336+0000    21     128     95748     95620   17.7846   19.2305   0.0343331   0.0280839
2021-10-28T22:09:19.267473+0000 Total time run:       21.3515
Total reads made:     97239
Read size:            4096
Object size:          4096
Bandwidth (MB/sec):   17.7898
Average IOPS:         4554
Stddev IOPS:          623.223
Max IOPS:             5717
Min IOPS:             3261
Average Latency(s):   0.028082
Max latency(s):       0.277219
Min latency(s):       0.000393241

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:09:19,913242237-07:00] INFO: > Stop system activity collection services[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:09:19,918377124-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:391 - rados_bench() > kill -INT 799658

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:09:19,923485611-07:00] INFO: >> for OSD host[0m
# ./benchmarks/bench-rados:396 - rados_bench() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.5 kill -INT 262231
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:09:19,931113617-07:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.5 kill -INT 262231
[1] 15:09:21 [SUCCESS] ljishen@10.10.2.5

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:09:21,027656878-07:00] INFO: >>> Copy system activity file from OSD host[0m
## ./benchmarks/bench-rados:403 - rados_bench() > basename -- /tmp/bench-rados/sys_activity_4KB_seq.dat.osd_host.3
# ./benchmarks/bench-rados:403 - rados_bench() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.5:/tmp/bench-rados/sys_activity_4KB_seq.dat.osd_host.3 /mnt/sda8/ceph-research/scripts/output/sys_activity_4KB_seq.dat.osd_host.3


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:09:22,110200765-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:09:46,099772051-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 97.24k objects, 380 MiB
    usage:   1.3 GiB used, 399 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:09:46,106939789-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:09:55,203345244-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 97.24k objects, 380 MiB
    usage:   1.3 GiB used, 399 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:09:55,210219459-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:10:04,490283463-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 97.24k objects, 380 MiB
    usage:   1.3 GiB used, 399 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:10:04,497264510-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:10:13,489247928-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 97.24k objects, 380 MiB
    usage:   1.3 GiB used, 399 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:10:13,496501277-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:10:22,538785623-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 97.24k objects, 380 MiB
    usage:   1.3 GiB used, 399 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:10:22,545963881-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:10:22,551292003-07:00] INFO: > The cluster is idle now.[0m


[1;7;39;49m[2021-10-28T15:10:22,554773544-07:00][RUNNING][ROUND 1/2/21] object_size=16KB[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:10:22,557973123-07:00] STAGE: Launch a Ceph cluster (CEPH_HOST: 10.10.2.1, OSD_HOST: 10.10.2.5) ...[0m
# ./benchmarks/bench-rados:183 - launch_ceph_cluster() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.1 bash -euo pipefail
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:10:22,566902111-07:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.1 bash -euo pipefail
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T15:10:22,988878369-07:00] INFO: > Remove existing clusters\x1b[0m\n'
10.10.2.1: b'## bash:17 -  > basename -- /var/lib/ceph/98e1fa5c-383a-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'# bash:17 -  > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm --host 10.10.2.1 --host 10.10.2.5 rm-cluster --force --fsid 98e1fa5c-383a-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T15:10:23,000286366-07:00] DEBUG: HOST_PARAMS: ['--host', '10.10.2.1', '--host', '10.10.2.5']\x1b[0m\n"
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T15:10:23,004233627-07:00] DEBUG: CEPHADM_PARAMS: ['rm-cluster', '--force', '--fsid', '98e1fa5c-383a-11ec-b51d-53e6e728d2d3']\x1b[0m\n"
10.10.2.1: b"# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:63 -  > /tmp/bench-rados/ceph-research/scripts/parallel-ssh --host 10.10.2.1 --host 10.10.2.5 'sudo python3 -u - rm-cluster --force --fsid 98e1fa5c-383a-11ec-b51d-53e6e728d2d3'\n"
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T15:10:23,012531306-07:00] DEBUG: command from stdin\x1b[0m\n'
10.10.2.1: b"# /tmp/bench-rados/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host 10.10.2.1 --host 10.10.2.5 'sudo python3 -u - rm-cluster --force --fsid 98e1fa5c-383a-11ec-b51d-53e6e728d2d3'\n"
10.10.2.1: b'[1] 15:10:28 [SUCCESS] 10.10.2.1\n[2] 15:10:30 [SUCCESS] 10.10.2.5\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T15:10:30,237390885-07:00] INFO: > Deploy a new cluster\x1b[0m\n'
10.10.2.1: b'# bash:24 -  > /tmp/bench-rados/ceph-research/scripts/ceph-deploy -m 10.10.2.1 -o sm1/10.10.2.5:/dev/nvme0n1\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T15:10:30,248418786-07:00] STAGE: Verify passwordless SSH to hosts, passwordless sudo via ljishen, and the existence of devices on hosts...\x1b[0m\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T15:10:30,253160040-07:00] INFO: > Check the host for the monitor\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:181 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@10.10.2.1 sudo --non-interactive --validate\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T15:10:30,402672878-07:00] INFO: > Check hosts for OSDs\x1b[0m\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T15:10:30,407866051-07:00] INFO: >> Check host 'sm1'\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:200 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 sudo --non-interactive --validate\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T15:10:31,475235032-07:00] INFO: >>> Check device '/dev/nvme0n1'\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:215 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 test -b /dev/nvme0n1\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:226 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 lsblk --nodeps --noheadings --output TYPE /dev/nvme0n1\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:227 - verify_parameters() > grep --word-regexp disk\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T15:10:33,611354512-07:00] STAGE: Reset devices on hosts...\x1b[0m\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T15:10:33,616304698-07:00] INFO: > Resetting device '/dev/nvme0n1' on host 'sm1'\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:288 - reset_host_devices() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 sudo sh\n'
10.10.2.1: b'  Removing ceph--66c1a39e--270e--4b35--b186--74f46c53b784-osd--block--79cc158e--dfb4--4c1d--8f8e--57cc9d77db5a (252:0)\n'
10.10.2.1: b'  Archiving volume group "ceph-66c1a39e-270e-4b35-b186-74f46c53b784" metadata (seqno 5).\n'
10.10.2.1: b'  Releasing logical volume "osd-block-79cc158e-dfb4-4c1d-8f8e-57cc9d77db5a"\n'
10.10.2.1: b'  Creating volume group backup "/etc/lvm/backup/ceph-66c1a39e-270e-4b35-b186-74f46c53b784" (seqno 6).\n'
10.10.2.1: b'  Logical volume "osd-block-79cc158e-dfb4-4c1d-8f8e-57cc9d77db5a" successfully removed\n'
10.10.2.1: b'  Removing physical volume "/dev/nvme0n1" from volume group "ceph-66c1a39e-270e-4b35-b186-74f46c53b784"\n'
10.10.2.1: b'  Volume group "ceph-66c1a39e-270e-4b35-b186-74f46c53b784" successfully removed\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T15:10:35,886837810-07:00] STAGE: Bootstrap a new cluster...\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:317 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.1 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T15:10:35,896905577-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T15:10:35,900872485-07:00] DEBUG: CEPHADM_PARAMS: ['bootstrap', '--config', '/tmp/bench-rados/ceph-research/scripts/ceph.conf', '--mon-ip', '10.10.2.1', '--output-dir', '/tmp/bench-rados/deployment_data_root/etc/ceph', '--ssh-user', 'ljishen', '--skip-dashboard', '--no-minimize-config', '--allow-overwrite', '--allow-fqdn-hostname', '--orphan-initial-daemons', '--skip-monitoring-stack', '--single-host-defaults']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.1 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults\n'
10.10.2.1: b'Verifying podman|docker is present...\n'
10.10.2.1: b'Verifying lvm2 is present...\n'
10.10.2.1: b'Verifying time synchronization is in place...\n'
10.10.2.1: b'Unit ntp.service is enabled and running\n'
10.10.2.1: b'Repeating the final host check...\n'
10.10.2.1: b'podman|docker (/usr/bin/docker) is present\n'
10.10.2.1: b'systemctl is present\n'
10.10.2.1: b'lvcreate is present\n'
10.10.2.1: b'Unit ntp.service is enabled and running\n'
10.10.2.1: b'Host looks OK\n'
10.10.2.1: b'Cluster fsid: e06df582-383b-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Verifying IP 10.10.2.1 port 3300 ...\n'
10.10.2.1: b'Verifying IP 10.10.2.1 port 6789 ...\n'
10.10.2.1: b'Mon IP `10.10.2.1` is in CIDR network `10.10.2.0/24`\n'
10.10.2.1: b'- internal network (--cluster-network) has not been provided, OSD replication will default to the public_network\n'
10.10.2.1: b'Adjusting default settings to suit single-host cluster...\n'
10.10.2.1: b'Pulling container image quay.io/ceph/ceph:v16...\n'
10.10.2.1: b'Ceph version: ceph version 16.2.6 (ee28fb57e47e9f88813e24bbf4c14496ca299d31) pacific (stable)\n'
10.10.2.1: b'Extracting ceph user uid/gid from container image...\n'
10.10.2.1: b'Creating initial keys...\n'
10.10.2.1: b'Creating initial monmap...\n'
10.10.2.1: b'Creating mon...\n'
10.10.2.1: b'Waiting for mon to start...\n'
10.10.2.1: b'Waiting for mon...\n'
10.10.2.1: b'mon is available\n'
10.10.2.1: b'Setting mon public_network to 10.10.2.0/24\n'
10.10.2.1: b'Wrote config to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf\n'
10.10.2.1: b'Wrote keyring to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring\n'
10.10.2.1: b'Creating mgr...\n'
10.10.2.1: b'Verifying port 9283 ...\n'
10.10.2.1: b'Waiting for mgr to start...\nWaiting for mgr...\n'
10.10.2.1: b'mgr not available, waiting (1/15)...\n'
10.10.2.1: b'mgr not available, waiting (2/15)...\n'
10.10.2.1: b'mgr is available\n'
10.10.2.1: b'Enabling cephadm module...\n'
10.10.2.1: b'Waiting for the mgr to restart...\n'
10.10.2.1: b'Waiting for mgr epoch 5...\n'
10.10.2.1: b'mgr epoch 5 is available\n'
10.10.2.1: b'Setting orchestrator backend to cephadm...\n'
10.10.2.1: b'Generating ssh key...\n'
10.10.2.1: b'Wrote public SSH key to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub\n'
10.10.2.1: b'Adding key to ljishen@localhost authorized_keys...\n'
10.10.2.1: b'Adding host node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us...\n'
10.10.2.1: b'Deploying unmanaged mon service...\n'
10.10.2.1: b'Deploying unmanaged mgr service...\n'
10.10.2.1: b'You can access the Ceph CLI with:\n\n\tsudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --fsid e06df582-383b-11ec-b51d-53e6e728d2d3 -c /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf -k /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring\n\n'
10.10.2.1: b'Please consider enabling telemetry to help improve Ceph:\n\n\tceph telemetry on\n\nFor more information see:\n\n\thttps://docs.ceph.com/docs/pacific/mgr/telemetry/\n\n'
10.10.2.1: b'Bootstrap complete.\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T15:11:37,505139761-07:00] INFO: Sleep for 20 seconds\x1b[0m\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T15:11:57,512759734-07:00] INFO: > Set the default number of monitor daemons to 1\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:337 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T15:11:57,522996599-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T15:11:57,526855213-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mon', '1']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1\n'
10.10.2.1: b'Inferring fsid e06df582-383b-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/e06df582-383b-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b'Scheduled mon update...\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T15:12:06,718087586-07:00] INFO: > Set the default number of manager daemons to 1\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:342 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T15:12:06,728432875-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T15:12:06,732279606-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mgr', '1']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1\n'
10.10.2.1: b'Inferring fsid e06df582-383b-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/e06df582-383b-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b'Scheduled mgr update...\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T15:12:16,015478655-07:00] STAGE: Add hosts to the cluster...\x1b[0m\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T15:12:16,021229847-07:00] INFO: > Adding host 'sm1'\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:361 - add_hosts() > ssh-copy-id -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -f -i /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub ljishen@sm1\n'
10.10.2.1: b'/usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: "/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub"\n'
10.10.2.1: b'\nNumber of key(s) added: 1\n\nNow try logging into the machine, with:   "ssh -o \'GlobalKnownHostsFile=/dev/null\' -o \'LogLevel=ERROR\' -o \'PasswordAuthentication=no\' -o \'StrictHostKeyChecking=no\' -o \'UserKnownHostsFile=/dev/null\' \'ljishen@sm1\'"\nand check to make sure that only the key(s) you wanted were added.\n\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:367 - add_hosts() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add sm1 10.10.2.5\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T15:12:17,172058190-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T15:12:17,176134414-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'host', 'add', 'sm1', '10.10.2.5']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add sm1 10.10.2.5\n'
10.10.2.1: b'Inferring fsid e06df582-383b-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/e06df582-383b-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b"Added host 'sm1' with addr '10.10.2.5'\n"
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T15:12:27,854120440-07:00] INFO: Sleep for 20 seconds\x1b[0m\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T15:12:47,859540381-07:00] STAGE: Add OSDs to the cluster...\x1b[0m\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T15:12:47,866073954-07:00] INFO: > Adding devices ['/dev/nvme0n1'] on host 'sm1' as OSDs\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:388 - add_host_devices_as_osds() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd sm1:/dev/nvme0n1\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T15:12:47,876188679-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T15:12:47,879757558-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'daemon', 'add', 'osd', 'sm1:/dev/nvme0n1']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd sm1:/dev/nvme0n1\n'
10.10.2.1: b'Inferring fsid e06df582-383b-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/e06df582-383b-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b"Created osd(s) 0 on host 'sm1'\n"
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T15:13:12,555990198-07:00] INFO: > Sleep for 20 seconds\x1b[0m\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T15:13:32,561190080-07:00] STAGE: Show Cluster Status\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:408 - show_status() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T15:13:32,571692164-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T15:13:32,575423648-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', '--status']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status\n'
10.10.2.1: b'Inferring fsid e06df582-383b-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/e06df582-383b-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b'  cluster:\n    id:     e06df582-383b-11ec-b51d-53e6e728d2d3\n    health: HEALTH_WARN\n            1 pool(s) have no replicas configured\n \n  services:\n    mon: 1 daemons, quorum node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us (age 2m)\n    mgr: node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us.cbxzon(active, since 2m)\n    osd: 1 osds: 1 up (since 20s), 1 in (since 41s)\n \n  data:\n    pools:   1 pools, 1 pgs\n    objects: 0 objects, 0 B\n    usage:   4.7 MiB used, 400 GiB / 400 GiB avail\n    pgs:     1 active+clean\n \n'
[1] 15:13:41 [SUCCESS] ljishen@10.10.2.1

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:10:22,988878369-07:00] INFO: > Remove existing clusters[0m
## bash:17 -  > basename -- /var/lib/ceph/98e1fa5c-383a-11ec-b51d-53e6e728d2d3
# bash:17 -  > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm --host 10.10.2.1 --host 10.10.2.5 rm-cluster --force --fsid 98e1fa5c-383a-11ec-b51d-53e6e728d2d3
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:10:23,000286366-07:00] DEBUG: HOST_PARAMS: ['--host', '10.10.2.1', '--host', '10.10.2.5'][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:10:23,004233627-07:00] DEBUG: CEPHADM_PARAMS: ['rm-cluster', '--force', '--fsid', '98e1fa5c-383a-11ec-b51d-53e6e728d2d3'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:63 -  > /tmp/bench-rados/ceph-research/scripts/parallel-ssh --host 10.10.2.1 --host 10.10.2.5 'sudo python3 -u - rm-cluster --force --fsid 98e1fa5c-383a-11ec-b51d-53e6e728d2d3'
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:10:23,012531306-07:00] DEBUG: command from stdin[0m
# /tmp/bench-rados/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host 10.10.2.1 --host 10.10.2.5 'sudo python3 -u - rm-cluster --force --fsid 98e1fa5c-383a-11ec-b51d-53e6e728d2d3'
[1] 15:10:28 [SUCCESS] 10.10.2.1
[2] 15:10:30 [SUCCESS] 10.10.2.5

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:10:30,237390885-07:00] INFO: > Deploy a new cluster[0m
# bash:24 -  > /tmp/bench-rados/ceph-research/scripts/ceph-deploy -m 10.10.2.1 -o sm1/10.10.2.5:/dev/nvme0n1


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:10:30,248418786-07:00] STAGE: Verify passwordless SSH to hosts, passwordless sudo via ljishen, and the existence of devices on hosts...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:10:30,253160040-07:00] INFO: > Check the host for the monitor[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:181 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@10.10.2.1 sudo --non-interactive --validate

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:10:30,402672878-07:00] INFO: > Check hosts for OSDs[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:10:30,407866051-07:00] INFO: >> Check host 'sm1'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:200 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 sudo --non-interactive --validate

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:10:31,475235032-07:00] INFO: >>> Check device '/dev/nvme0n1'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:215 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 test -b /dev/nvme0n1
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:226 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 lsblk --nodeps --noheadings --output TYPE /dev/nvme0n1
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:227 - verify_parameters() > grep --word-regexp disk


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:10:33,611354512-07:00] STAGE: Reset devices on hosts...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:10:33,616304698-07:00] INFO: > Resetting device '/dev/nvme0n1' on host 'sm1'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:288 - reset_host_devices() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 sudo sh
  Removing ceph--66c1a39e--270e--4b35--b186--74f46c53b784-osd--block--79cc158e--dfb4--4c1d--8f8e--57cc9d77db5a (252:0)
  Archiving volume group "ceph-66c1a39e-270e-4b35-b186-74f46c53b784" metadata (seqno 5).
  Releasing logical volume "osd-block-79cc158e-dfb4-4c1d-8f8e-57cc9d77db5a"
  Creating volume group backup "/etc/lvm/backup/ceph-66c1a39e-270e-4b35-b186-74f46c53b784" (seqno 6).
  Logical volume "osd-block-79cc158e-dfb4-4c1d-8f8e-57cc9d77db5a" successfully removed
  Removing physical volume "/dev/nvme0n1" from volume group "ceph-66c1a39e-270e-4b35-b186-74f46c53b784"
  Volume group "ceph-66c1a39e-270e-4b35-b186-74f46c53b784" successfully removed


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:10:35,886837810-07:00] STAGE: Bootstrap a new cluster...[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:317 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.1 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:10:35,896905577-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:10:35,900872485-07:00] DEBUG: CEPHADM_PARAMS: ['bootstrap', '--config', '/tmp/bench-rados/ceph-research/scripts/ceph.conf', '--mon-ip', '10.10.2.1', '--output-dir', '/tmp/bench-rados/deployment_data_root/etc/ceph', '--ssh-user', 'ljishen', '--skip-dashboard', '--no-minimize-config', '--allow-overwrite', '--allow-fqdn-hostname', '--orphan-initial-daemons', '--skip-monitoring-stack', '--single-host-defaults'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.1 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults
Verifying podman|docker is present...
Verifying lvm2 is present...
Verifying time synchronization is in place...
Unit ntp.service is enabled and running
Repeating the final host check...
podman|docker (/usr/bin/docker) is present
systemctl is present
lvcreate is present
Unit ntp.service is enabled and running
Host looks OK
Cluster fsid: e06df582-383b-11ec-b51d-53e6e728d2d3
Verifying IP 10.10.2.1 port 3300 ...
Verifying IP 10.10.2.1 port 6789 ...
Mon IP `10.10.2.1` is in CIDR network `10.10.2.0/24`
- internal network (--cluster-network) has not been provided, OSD replication will default to the public_network
Adjusting default settings to suit single-host cluster...
Pulling container image quay.io/ceph/ceph:v16...
Ceph version: ceph version 16.2.6 (ee28fb57e47e9f88813e24bbf4c14496ca299d31) pacific (stable)
Extracting ceph user uid/gid from container image...
Creating initial keys...
Creating initial monmap...
Creating mon...
Waiting for mon to start...
Waiting for mon...
mon is available
Setting mon public_network to 10.10.2.0/24
Wrote config to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf
Wrote keyring to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring
Creating mgr...
Verifying port 9283 ...
Waiting for mgr to start...
Waiting for mgr...
mgr not available, waiting (1/15)...
mgr not available, waiting (2/15)...
mgr is available
Enabling cephadm module...
Waiting for the mgr to restart...
Waiting for mgr epoch 5...
mgr epoch 5 is available
Setting orchestrator backend to cephadm...
Generating ssh key...
Wrote public SSH key to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub
Adding key to ljishen@localhost authorized_keys...
Adding host node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us...
Deploying unmanaged mon service...
Deploying unmanaged mgr service...
You can access the Ceph CLI with:

	sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --fsid e06df582-383b-11ec-b51d-53e6e728d2d3 -c /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf -k /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring

Please consider enabling telemetry to help improve Ceph:

	ceph telemetry on

For more information see:

	https://docs.ceph.com/docs/pacific/mgr/telemetry/

Bootstrap complete.

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:11:37,505139761-07:00] INFO: Sleep for 20 seconds[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:11:57,512759734-07:00] INFO: > Set the default number of monitor daemons to 1[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:337 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:11:57,522996599-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:11:57,526855213-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mon', '1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1
Inferring fsid e06df582-383b-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/e06df582-383b-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Scheduled mon update...

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:12:06,718087586-07:00] INFO: > Set the default number of manager daemons to 1[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:342 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:12:06,728432875-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:12:06,732279606-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mgr', '1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1
Inferring fsid e06df582-383b-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/e06df582-383b-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Scheduled mgr update...


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:12:16,015478655-07:00] STAGE: Add hosts to the cluster...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:12:16,021229847-07:00] INFO: > Adding host 'sm1'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:361 - add_hosts() > ssh-copy-id -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -f -i /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub ljishen@sm1
/usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: "/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub"

Number of key(s) added: 1

Now try logging into the machine, with:   "ssh -o 'GlobalKnownHostsFile=/dev/null' -o 'LogLevel=ERROR' -o 'PasswordAuthentication=no' -o 'StrictHostKeyChecking=no' -o 'UserKnownHostsFile=/dev/null' 'ljishen@sm1'"
and check to make sure that only the key(s) you wanted were added.

# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:367 - add_hosts() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add sm1 10.10.2.5
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:12:17,172058190-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:12:17,176134414-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'host', 'add', 'sm1', '10.10.2.5'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add sm1 10.10.2.5
Inferring fsid e06df582-383b-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/e06df582-383b-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Added host 'sm1' with addr '10.10.2.5'

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:12:27,854120440-07:00] INFO: Sleep for 20 seconds[0m


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:12:47,859540381-07:00] STAGE: Add OSDs to the cluster...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:12:47,866073954-07:00] INFO: > Adding devices ['/dev/nvme0n1'] on host 'sm1' as OSDs[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:388 - add_host_devices_as_osds() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd sm1:/dev/nvme0n1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:12:47,876188679-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:12:47,879757558-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'daemon', 'add', 'osd', 'sm1:/dev/nvme0n1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd sm1:/dev/nvme0n1
Inferring fsid e06df582-383b-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/e06df582-383b-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Created osd(s) 0 on host 'sm1'

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:13:12,555990198-07:00] INFO: > Sleep for 20 seconds[0m


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:13:32,561190080-07:00] STAGE: Show Cluster Status[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:408 - show_status() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:13:32,571692164-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:13:32,575423648-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', '--status'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status
Inferring fsid e06df582-383b-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/e06df582-383b-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
  cluster:
    id:     e06df582-383b-11ec-b51d-53e6e728d2d3
    health: HEALTH_WARN
            1 pool(s) have no replicas configured
 
  services:
    mon: 1 daemons, quorum node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us (age 2m)
    mgr: node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us.cbxzon(active, since 2m)
    osd: 1 osds: 1 up (since 20s), 1 in (since 41s)
 
  data:
    pools:   1 pools, 1 pgs
    objects: 0 objects, 0 B
    usage:   4.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     1 active+clean
 

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:13:41,616567335-07:00] INFO: > Copy configuration files to local[0m
# ./benchmarks/bench-rados:220 - launch_ceph_cluster() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.1 sudo chown --changes --recursive ljishen /tmp/bench-rados/deployment_data_root/etc/ceph
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:13:41,624030640-07:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.1 sudo chown --changes --recursive ljishen /tmp/bench-rados/deployment_data_root/etc/ceph
[1] 15:13:41 [SUCCESS] ljishen@10.10.2.1
# ./benchmarks/bench-rados:223 - launch_ceph_cluster() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.1:/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf
# ./benchmarks/bench-rados:226 - launch_ceph_cluster() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.1:/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:13:42,100609378-07:00] STAGE: Update local ceph.conf as a client...[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:13:42,103679885-07:00] STAGE: Configure Ceph pool...[0m
# ./benchmarks/bench-rados:257 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_max_backfills 32
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:13:42,125757682-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:13:42,128613404-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'e06df582-383b-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'config', 'set', 'osd', 'osd_max_backfills', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid e06df582-383b-11ec-b51d-53e6e728d2d3 -- ceph config set osd osd_max_backfills 32
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:258 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_active 32
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:13:46,174817629-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:13:46,177754343-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'e06df582-383b-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_active', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid e06df582-383b-11ec-b51d-53e6e728d2d3 -- ceph config set osd osd_recovery_max_active 32
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:259 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_single_start 8
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:13:50,408034363-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:13:50,410935821-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'e06df582-383b-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_single_start', '8'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid e06df582-383b-11ec-b51d-53e6e728d2d3 -- ceph config set osd osd_recovery_max_single_start 8
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:260 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_op_priority 63
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:13:54,477469118-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:13:54,480627059-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'e06df582-383b-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_op_priority', '63'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid e06df582-383b-11ec-b51d-53e6e728d2d3 -- ceph config set osd osd_recovery_op_priority 63
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:262 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config show-with-defaults osd.0
# ./benchmarks/bench-rados:263 - configure_ceph_pool() > grep 'osd_max_backfills\|osd_recovery'
# ./benchmarks/bench-rados:263 - configure_ceph_pool() > column -t -s ' '
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
osd_max_backfills                        32         mon
osd_recovery_cost                        20971520   default
osd_recovery_delay_start                 0.000000   default
osd_recovery_max_active                  32         mon
osd_recovery_max_active_hdd              3          default
osd_recovery_max_active_ssd              10         default
osd_recovery_max_chunk                   8388608    default
osd_recovery_max_omap_entries_per_chunk  8096       default
osd_recovery_max_single_start            8          mon
osd_recovery_op_priority                 63         mon
osd_recovery_op_warn_multiple            16         default
osd_recovery_priority                    5          default
osd_recovery_retry_interval              30.000000  default
osd_recovery_sleep                       0.000000   default
osd_recovery_sleep_hdd                   0.100000   default
osd_recovery_sleep_hybrid                0.025000   default
osd_recovery_sleep_ssd                   0.000000   default
# ./benchmarks/bench-rados:265 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool create bench_rados 128 128 replicated --size 1 --pg-num-min 128
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:14:02,619882328-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:14:02,623014771-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'e06df582-383b-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'create', 'bench_rados', '128', '128', 'replicated', '--size', '1', '--pg-num-min', '128'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid e06df582-383b-11ec-b51d-53e6e728d2d3 -- ceph osd pool create bench_rados 128 128 replicated --size 1 --pg-num-min 128
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
pool 'bench_rados' created
# ./benchmarks/bench-rados:267 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados min_size 1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:14:06,844710605-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:14:06,847808223-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'e06df582-383b-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'min_size', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid e06df582-383b-11ec-b51d-53e6e728d2d3 -- ceph osd pool set bench_rados min_size 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 min_size to 1
# ./benchmarks/bench-rados:268 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados pg_autoscale_mode off
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:14:11,022176194-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:14:11,025185776-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'e06df582-383b-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'pg_autoscale_mode', 'off'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid e06df582-383b-11ec-b51d-53e6e728d2d3 -- ceph osd pool set bench_rados pg_autoscale_mode off
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 pg_autoscale_mode to off
# ./benchmarks/bench-rados:269 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool application enable bench_rados benchmark
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:14:15,690153049-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:14:15,693320598-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'e06df582-383b-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'application', 'enable', 'bench_rados', 'benchmark'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid e06df582-383b-11ec-b51d-53e6e728d2d3 -- ceph osd pool application enable bench_rados benchmark
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
enabled application 'benchmark' on pool 'bench_rados'
# ./benchmarks/bench-rados:270 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados noscrub 1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:14:20,104823479-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:14:20,107689140-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'e06df582-383b-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'noscrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid e06df582-383b-11ec-b51d-53e6e728d2d3 -- ceph osd pool set bench_rados noscrub 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 noscrub to 1
# ./benchmarks/bench-rados:271 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados nodeep-scrub 1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:14:24,713569240-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:14:24,716731229-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'e06df582-383b-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'nodeep-scrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid e06df582-383b-11ec-b51d-53e6e728d2d3 -- ceph osd pool set bench_rados nodeep-scrub 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 nodeep-scrub to 1
# ./benchmarks/bench-rados:272 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool ls detail
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:14:29,689706007-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:14:29,692647140-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'e06df582-383b-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'ls', 'detail'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid e06df582-383b-11ec-b51d-53e6e728d2d3 -- ceph osd pool ls detail
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
pool 1 'device_health_metrics' replicated size 1 min_size 1 crush_rule 0 object_hash rjenkins pg_num 64 pgp_num 64 autoscale_mode on last_change 18 lfor 0/0/16 flags hashpspool stripe_width 0 pg_num_min 1 application mgr_devicehealth
pool 2 'bench_rados' replicated size 1 min_size 1 crush_rule 0 object_hash rjenkins pg_num 128 pgp_num 128 autoscale_mode off last_change 20 flags hashpspool,noscrub,nodeep-scrub stripe_width 0 pg_num_min 128 application benchmark

# ./benchmarks/bench-rados:273 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd df tree
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:14:33,607210903-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:14:33,610461208-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'e06df582-383b-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'df', 'tree'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid e06df582-383b-11ec-b51d-53e6e728d2d3 -- ceph osd df tree
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
ID  CLASS  WEIGHT   REWEIGHT  SIZE     RAW USE  DATA     OMAP  META     AVAIL    %USE  VAR   PGS  STATUS  TYPE NAME    
-1         0.39059         -  400 GiB  5.7 MiB  168 KiB   0 B  5.6 MiB  400 GiB  0.00  1.00    -          root default 
-3         0.39059         -  400 GiB  5.7 MiB  168 KiB   0 B  5.6 MiB  400 GiB  0.00  1.00    -              host sm1 
 0    ssd  0.39059   1.00000  400 GiB  5.7 MiB  168 KiB   0 B  5.6 MiB  400 GiB  0.00  1.00  192      up          osd.0
                       TOTAL  400 GiB  5.7 MiB  168 KiB   0 B  5.6 MiB  400 GiB  0.00                                  
MIN/MAX VAR: 1.00/1.00  STDDEV: 0


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:14:37,608980722-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:15:01,706525015-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:15:10,813964624-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:15:19,888079683-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:15:19,895373558-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:15:28,696737358-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:15:28,703610100-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:15:37,654661419-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:15:37,661511078-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:15:46,520148327-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:15:46,527008345-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:15:55,618874514-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:15:55,625655895-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:15:55,631159466-07:00] INFO: > The cluster is idle now.[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:15:55,634618785-07:00] STAGE: Start benchmarking of mode write ...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:15:55,640418927-07:00] INFO: > Start system activity collection services[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:15:55,645295587-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:341 - rados_bench() > rm --force /mnt/sda8/ceph-research/scripts/output/sys_activity_16KB_write.dat.rados_bench_host.1
# ./benchmarks/bench-rados:342 - rados_bench() > sar_pid_rados_bench_host=805218
# ./benchmarks/bench-rados:342 - rados_bench() > S_TIME_FORMAT=ISO
# ./benchmarks/bench-rados:342 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/output/sys_activity_16KB_write.dat.rados_bench_host.1 2

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:15:55,651475213-07:00] INFO: >> for OSD host[0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:15:55,660444466-07:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.5 bash
10.10.2.5: b'267057\n'
[1] 15:15:56 [SUCCESS] ljishen@10.10.2.5
267057

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:15:56,766754564-07:00] INFO: > Run rados bench[0m
# ./benchmarks/bench-rados:370 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/output/rados_bench_16KB_write.log.1
## ./benchmarks/bench-rados:361 - rados_bench() > count_cpus 32-63,96-127
# ./benchmarks/bench-rados:361 - rados_bench() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 60 write --pool bench_rados -b 16384 -O 16384 --concurrent-ios 128 --show-time --write-object --write-omap --write-xattr --no-cleanup
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:15:56,786746069-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:15:56,789653258-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'e06df582-383b-11ec-b51d-53e6e728d2d3', '--', 'rados', 'bench', '60', 'write', '--pool', 'bench_rados', '-b', '16384', '-O', '16384', '--concurrent-ios', '128', '--show-time', '--write-object', '--write-omap', '--write-xattr', '--no-cleanup'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid e06df582-383b-11ec-b51d-53e6e728d2d3 -- rados bench 60 write --pool bench_rados -b 16384 -O 16384 --concurrent-ios 128 --show-time --write-object --write-omap --write-xattr --no-cleanup
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
hints = 1
2021-10-28T22:15:59.726214+0000 Maintaining 128 concurrent writes of 16384 bytes to objects of size 16384 for up to 60 seconds or 0 objects
2021-10-28T22:15:59.726223+0000 Object prefix: benchmark_data_node-2.bluefield2.ucsc-cmps10_7
2021-10-28T22:15:59.727288+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-28T22:15:59.727288+0000     0       0         0         0         0         0           -           0
2021-10-28T22:16:00.727428+0000     1     128       805       677   10.5771   10.5781    0.181907    0.169405
2021-10-28T22:16:01.727538+0000     2     128      1957      1829   14.2876        18    0.100037     0.13675
2021-10-28T22:16:02.727608+0000     3     128      2981      2853    14.858        16    0.108262    0.130785
2021-10-28T22:16:03.727675+0000     4     128      3969      3841   15.0026   15.4375     0.13213    0.131265
2021-10-28T22:16:04.727738+0000     5     128      4901      4773   14.9144   14.5625    0.219705    0.131267
2021-10-28T22:16:05.727800+0000     6     128      5889      5761   15.0014   15.4375    0.248755    0.131153
2021-10-28T22:16:06.727865+0000     7     128      6913      6785   15.1439        16     0.11989    0.130924
2021-10-28T22:16:07.727933+0000     8     128      7937      7809   15.2508        16    0.119492    0.129885
2021-10-28T22:16:08.727998+0000     9     128      8833      8705   15.1117        14    0.129515    0.131335
2021-10-28T22:16:09.728060+0000    10     128      9893      9765   15.2567   16.5625    0.112596     0.12975
2021-10-28T22:16:10.728125+0000    11     128     11009     10881   15.4549   17.4375    0.101726    0.128976
2021-10-28T22:16:11.728191+0000    12     128     11813     11685   15.2138   12.5625    0.128632    0.130829
2021-10-28T22:16:12.728255+0000    13     128     12709     12581   15.1203        14    0.116614    0.131411
2021-10-28T22:16:13.728317+0000    14     128     13733     13605   15.1831        16    0.188486    0.131193
2021-10-28T22:16:14.728381+0000    15     128     14757     14629   15.2375        16    0.159177     0.13033
2021-10-28T22:16:15.728445+0000    16     128     15745     15617   15.2499   15.4375    0.125519    0.130846
2021-10-28T22:16:16.728508+0000    17     128     16513     16385   15.0587        12    0.118935    0.132289
2021-10-28T22:16:17.728571+0000    18     128     17537     17409   15.1109        16    0.116351     0.13185
2021-10-28T22:16:18.728634+0000    19     128     18469     18341    15.082   14.5625    0.158671    0.132124
2021-10-28T22:16:19.728697+0000 min lat: 0.065404 max lat: 0.355905 avg lat: 0.131554
2021-10-28T22:16:19.728697+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-28T22:16:19.728697+0000    20     128     19493     19365   15.1279        16     0.11859    0.131554
2021-10-28T22:16:20.728769+0000    21     128     20517     20389   15.1693        16    0.125076    0.131301
2021-10-28T22:16:21.728837+0000    22     128     21413     21285   15.1162        14    0.109844    0.131417
2021-10-28T22:16:22.728902+0000    23     128     22181     22053   14.9806        12    0.126061    0.133097
2021-10-28T22:16:23.728969+0000    24     128     23205     23077   15.0231        16    0.137593    0.132518
2021-10-28T22:16:24.729040+0000    25     128     24193     24065   15.0396   15.4375    0.126759    0.132722
2021-10-28T22:16:25.729103+0000    26     128     25217     25089   15.0765        16    0.118482    0.132378
2021-10-28T22:16:26.729167+0000    27     128     26113     25985   15.0366        14    0.111761    0.132897
2021-10-28T22:16:27.729234+0000    28     128     27009     26881   14.9995        14    0.130602    0.133151
2021-10-28T22:16:28.729308+0000    29     128     27941     27813   14.9844   14.5625    0.133173    0.133203
2021-10-28T22:16:29.729375+0000    30     128     28929     28801   14.9995   15.4375    0.109763    0.133256
2021-10-28T22:16:30.729439+0000    31     128     29861     29733   14.9854   14.5625    0.120624     0.13324
2021-10-28T22:16:31.729505+0000    32     128     30885     30757    15.017        16    0.136171    0.132799
2021-10-28T22:16:32.729571+0000    33     128     31397     31269   14.8044         8    0.323923    0.134699
2021-10-28T22:16:33.729634+0000    34     128     31781     31653   14.5454         6    0.280679    0.136986
2021-10-28T22:16:34.729698+0000    35     128     32165     32037   14.3013         6    0.266214    0.139276
2021-10-28T22:16:35.729760+0000    36     128     32549     32421   14.0707         6    0.333787    0.141392
2021-10-28T22:16:36.729825+0000    37     128     32933     32805   13.8525         6    0.247849    0.143699
2021-10-28T22:16:37.729887+0000    38     128     33317     33189   13.6459         6    0.458505    0.145737
2021-10-28T22:16:38.729953+0000    39     128     33701     33573   13.4498         6    0.275843    0.148246
2021-10-28T22:16:39.730020+0000 min lat: 0.065404 max lat: 0.557291 avg lat: 0.150326
2021-10-28T22:16:39.730020+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-28T22:16:39.730020+0000    40     128     34085     33957   13.2636         6    0.318727    0.150326
2021-10-28T22:16:40.730098+0000    41     128     34469     34341   13.0864         6    0.258509    0.152353
2021-10-28T22:16:41.730164+0000    42     128     34945     34817   12.9519    7.4375    0.291284    0.154127
2021-10-28T22:16:42.730231+0000    43     128     35329     35201   12.7902         6    0.263952    0.155842
2021-10-28T22:16:43.730296+0000    44     128     35621     35493   12.6032    4.5625    0.332173    0.157821
2021-10-28T22:16:44.730365+0000    45     128     35969     35841    12.444    5.4375    0.389071    0.160303
2021-10-28T22:16:45.730435+0000    46     128     36389     36261   12.3161    6.5625    0.353886    0.161967
2021-10-28T22:16:46.730504+0000    47     128     36865     36737   12.2123    7.4375    0.277394    0.163549
2021-10-28T22:16:47.730570+0000    48     128     37249     37121   12.0828         6    0.332839    0.165192
2021-10-28T22:16:48.730632+0000    49     128     37541     37413   11.9294    4.5625    0.433854    0.167179
2021-10-28T22:16:49.730694+0000    50     128     37925     37797   11.8108         6    0.319826    0.168654
2021-10-28T22:16:50.730758+0000    51     128     38309     38181   11.6968         6    0.274155    0.170324
2021-10-28T22:16:51.730823+0000    52     128     38785     38657   11.6149    7.4375    0.323396    0.171806
2021-10-28T22:16:52.730886+0000    53     128     39169     39041    11.509         6    0.282612    0.173444
2021-10-28T22:16:53.730952+0000    54     128     39553     39425   11.4069         6    0.327094    0.174803
2021-10-28T22:16:54.731017+0000    55     128     39845     39717   11.2825    4.5625    0.259928    0.177041
2021-10-28T22:16:55.731080+0000    56     128     40229     40101   11.1881         6    0.296579    0.177946
2021-10-28T22:16:56.731141+0000    57     128     40577     40449   11.0873    5.4375    0.356598    0.179912
2021-10-28T22:16:57.731202+0000    58     128     40997     40869   11.0092    6.5625    0.311904    0.181143
2021-10-28T22:16:58.731268+0000    59     128     41381     41253   10.9243         6    0.320519    0.182641
2021-10-28T22:16:59.731334+0000 min lat: 0.065404 max lat: 0.701051 avg lat: 0.18422
2021-10-28T22:16:59.731334+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-28T22:16:59.731334+0000    60     128     41765     41637   10.8422         6    0.124221     0.18422
2021-10-28T22:17:00.731446+0000 Total time run:         60.1132
Total writes made:      41765
Write size:             16384
Object size:            16384
Bandwidth (MB/sec):     10.8558
Stddev Bandwidth:       4.67849
Max bandwidth (MB/sec): 18
Min bandwidth (MB/sec): 4.5625
Average IOPS:           694
Stddev IOPS:            299.424
Max IOPS:               1152
Min IOPS:               292
Average Latency(s):     0.184046
Stddev Latency(s):      0.101067
Max latency(s):         0.701051
Min latency(s):         0.065404

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:17:01,454945630-07:00] INFO: > Stop system activity collection services[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:17:01,459816619-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:391 - rados_bench() > kill -INT 805218

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:17:01,465110676-07:00] INFO: >> for OSD host[0m
# ./benchmarks/bench-rados:396 - rados_bench() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.5 kill -INT 267057
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:17:01,472758980-07:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.5 kill -INT 267057
[1] 15:17:02 [SUCCESS] ljishen@10.10.2.5

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:17:02,636397319-07:00] INFO: >>> Copy system activity file from OSD host[0m
## ./benchmarks/bench-rados:403 - rados_bench() > basename -- /tmp/bench-rados/sys_activity_16KB_write.dat.osd_host.1
# ./benchmarks/bench-rados:403 - rados_bench() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.5:/tmp/bench-rados/sys_activity_16KB_write.dat.osd_host.1 /mnt/sda8/ceph-research/scripts/output/sys_activity_16KB_write.dat.osd_host.1


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:17:03,746492732-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:17:27,670683547-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 41.77k objects, 653 MiB
    usage:   2.0 GiB used, 398 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:17:27,677442966-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:17:36,687299231-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 41.77k objects, 653 MiB
    usage:   2.0 GiB used, 398 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:17:36,694758960-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:17:45,653841641-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 41.77k objects, 653 MiB
    usage:   2.0 GiB used, 398 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:17:45,660577164-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:17:54,753877862-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 41.77k objects, 653 MiB
    usage:   2.0 GiB used, 398 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:17:54,761258120-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:18:03,603635404-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 41.77k objects, 653 MiB
    usage:   2.0 GiB used, 398 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:18:03,610520970-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:18:03,616093101-07:00] INFO: > The cluster is idle now.[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:18:03,619377751-07:00] STAGE: Start benchmarking of mode seq ...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:18:03,625579009-07:00] INFO: > Start system activity collection services[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:18:03,630553754-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:341 - rados_bench() > rm --force /mnt/sda8/ceph-research/scripts/output/sys_activity_16KB_seq.dat.rados_bench_host.1
# ./benchmarks/bench-rados:342 - rados_bench() > sar_pid_rados_bench_host=806587
# ./benchmarks/bench-rados:342 - rados_bench() > S_TIME_FORMAT=ISO
# ./benchmarks/bench-rados:342 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/output/sys_activity_16KB_seq.dat.rados_bench_host.1 2

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:18:03,636863555-07:00] INFO: >> for OSD host[0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:18:03,645641237-07:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.5 bash
10.10.2.5: b'267603\n'
[1] 15:18:04 [SUCCESS] ljishen@10.10.2.5
267603

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:18:04,790678482-07:00] INFO: > Run rados bench[0m
# ./benchmarks/bench-rados:379 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/output/rados_bench_16KB_seq.log.1
## ./benchmarks/bench-rados:376 - rados_bench() > count_cpus 32-63,96-127
# ./benchmarks/bench-rados:376 - rados_bench() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 128 --show-time
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:18:04,810260877-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:18:04,813192312-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'e06df582-383b-11ec-b51d-53e6e728d2d3', '--', 'rados', 'bench', '99999999', 'seq', '--pool', 'bench_rados', '--concurrent-ios', '128', '--show-time'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid e06df582-383b-11ec-b51d-53e6e728d2d3 -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 128 --show-time
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
hints = 1
2021-10-28T22:18:07.739096+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-28T22:18:07.739096+0000     0       0         0         0         0         0           -           0
2021-10-28T22:18:08.739207+0000     1     128      2908      2780   43.4295   43.4375   0.0514215   0.0446134
2021-10-28T22:18:09.739297+0000     2     128      5152      5024   39.2446   35.0625   0.0292129    0.050309
2021-10-28T22:18:10.739409+0000     3     128      7813      7685   40.0209   41.5781   0.0649518   0.0489737
2021-10-28T22:18:11.739518+0000     4     128     11208     11080   43.2759   53.0469   0.0533372   0.0460574
2021-10-28T22:18:12.739596+0000     5     128     13914     13786   43.0763   42.2812   0.0537749   0.0461696
2021-10-28T22:18:13.739686+0000     6     128     16611     16483   42.9197   42.1406    0.154738    0.046159
2021-10-28T22:18:14.739804+0000     7     128     18901     18773   41.8993   35.7812    0.112131   0.0475104
2021-10-28T22:18:15.739913+0000     8     128     21529     21401   41.7942   41.0625   0.0112746   0.0477406
2021-10-28T22:18:16.740023+0000     9     128     24560     24432    42.412   47.3594   0.0237152   0.0470089
2021-10-28T22:18:17.740112+0000    10     128     27693     27565   43.0656   48.9531   0.0342105   0.0461992
2021-10-28T22:18:18.740221+0000    11     128     30702     30574   43.4242   47.0156   0.0285883   0.0459592
2021-10-28T22:18:19.740332+0000    12     128     33596     33468   43.5734   45.2188   0.0207799   0.0458149
2021-10-28T22:18:20.740442+0000    13     128     35865     35737   42.9484   35.4531   0.0447611    0.046474
2021-10-28T22:18:21.740527+0000    14     128     39438     39310   43.8681   55.8281   0.0103457   0.0455008
2021-10-28T22:18:22.740640+0000 Total time run:       14.825
Total reads made:     41765
Read size:            16384
Object size:          16384
Bandwidth (MB/sec):   44.0188
Average IOPS:         2817
Stddev IOPS:          400.711
Max IOPS:             3573
Min IOPS:             2244
Average Latency(s):   0.0453078
Max latency(s):       0.39198
Min latency(s):       0.000598809

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:18:23,388695062-07:00] INFO: > Stop system activity collection services[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:18:23,394228801-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:391 - rados_bench() > kill -INT 806587

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:18:23,399489795-07:00] INFO: >> for OSD host[0m
# ./benchmarks/bench-rados:396 - rados_bench() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.5 kill -INT 267603
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:18:23,407535257-07:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.5 kill -INT 267603
[1] 15:18:24 [SUCCESS] ljishen@10.10.2.5

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:18:24,504207476-07:00] INFO: >>> Copy system activity file from OSD host[0m
## ./benchmarks/bench-rados:403 - rados_bench() > basename -- /tmp/bench-rados/sys_activity_16KB_seq.dat.osd_host.1
# ./benchmarks/bench-rados:403 - rados_bench() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.5:/tmp/bench-rados/sys_activity_16KB_seq.dat.osd_host.1 /mnt/sda8/ceph-research/scripts/output/sys_activity_16KB_seq.dat.osd_host.1


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:18:25,557466087-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:18:49,475996145-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 41.77k objects, 653 MiB
    usage:   2.0 GiB used, 398 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:18:49,483509995-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:18:58,475339947-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 41.77k objects, 653 MiB
    usage:   2.0 GiB used, 398 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:18:58,482357492-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:19:07,523152672-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 41.77k objects, 653 MiB
    usage:   2.0 GiB used, 398 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:19:07,530140201-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:19:16,595359533-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 41.77k objects, 653 MiB
    usage:   2.0 GiB used, 398 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:19:16,602806258-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:19:25,560827808-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 41.77k objects, 653 MiB
    usage:   2.0 GiB used, 398 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:19:25,567872144-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:19:25,573661555-07:00] INFO: > The cluster is idle now.[0m


[1;7;39;49m[2021-10-28T15:19:25,575661824-07:00][RUNNING][ROUND 2/2/21] object_size=16KB[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:19:25,578653372-07:00] STAGE: Launch a Ceph cluster (CEPH_HOST: 10.10.2.1, OSD_HOST: 10.10.2.5) ...[0m
# ./benchmarks/bench-rados:183 - launch_ceph_cluster() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.1 bash -euo pipefail
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:19:25,588089215-07:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.1 bash -euo pipefail
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T15:19:26,005426784-07:00] INFO: > Remove existing clusters\x1b[0m\n'
10.10.2.1: b'## bash:17 -  > basename -- /var/lib/ceph/e06df582-383b-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'# bash:17 -  > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm --host 10.10.2.1 --host 10.10.2.5 rm-cluster --force --fsid e06df582-383b-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T15:19:26,016985765-07:00] DEBUG: HOST_PARAMS: ['--host', '10.10.2.1', '--host', '10.10.2.5']\x1b[0m\n"
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T15:19:26,020962871-07:00] DEBUG: CEPHADM_PARAMS: ['rm-cluster', '--force', '--fsid', 'e06df582-383b-11ec-b51d-53e6e728d2d3']\x1b[0m\n"
10.10.2.1: b"# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:63 -  > /tmp/bench-rados/ceph-research/scripts/parallel-ssh --host 10.10.2.1 --host 10.10.2.5 'sudo python3 -u - rm-cluster --force --fsid e06df582-383b-11ec-b51d-53e6e728d2d3'\n"
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T15:19:26,030411495-07:00] DEBUG: command from stdin\x1b[0m\n'
10.10.2.1: b"# /tmp/bench-rados/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host 10.10.2.1 --host 10.10.2.5 'sudo python3 -u - rm-cluster --force --fsid e06df582-383b-11ec-b51d-53e6e728d2d3'\n"
10.10.2.1: b'[1] 15:19:31 [SUCCESS] 10.10.2.1\n[2] 15:19:33 [SUCCESS] 10.10.2.5\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T15:19:33,613071749-07:00] INFO: > Deploy a new cluster\x1b[0m\n'
10.10.2.1: b'# bash:24 -  > /tmp/bench-rados/ceph-research/scripts/ceph-deploy -m 10.10.2.1 -o sm1/10.10.2.5:/dev/nvme0n1\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T15:19:33,624298936-07:00] STAGE: Verify passwordless SSH to hosts, passwordless sudo via ljishen, and the existence of devices on hosts...\x1b[0m\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T15:19:33,629090594-07:00] INFO: > Check the host for the monitor\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:181 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@10.10.2.1 sudo --non-interactive --validate\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T15:19:33,778992305-07:00] INFO: > Check hosts for OSDs\x1b[0m\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T15:19:33,783016640-07:00] INFO: >> Check host 'sm1'\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:200 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 sudo --non-interactive --validate\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T15:19:34,865077750-07:00] INFO: >>> Check device '/dev/nvme0n1'\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:215 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 test -b /dev/nvme0n1\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:226 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 lsblk --nodeps --noheadings --output TYPE /dev/nvme0n1\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:227 - verify_parameters() > grep --word-regexp disk\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T15:19:36,995750526-07:00] STAGE: Reset devices on hosts...\x1b[0m\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T15:19:37,000622365-07:00] INFO: > Resetting device '/dev/nvme0n1' on host 'sm1'\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:288 - reset_host_devices() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 sudo sh\n'
10.10.2.1: b'  Removing ceph--80198a05--cc04--4969--8a85--26d2c836166a-osd--block--0992b562--27c9--45de--af29--abda6ff37289 (252:0)\n'
10.10.2.1: b'  Archiving volume group "ceph-80198a05-cc04-4969-8a85-26d2c836166a" metadata (seqno 5).\n'
10.10.2.1: b'  Releasing logical volume "osd-block-0992b562-27c9-45de-af29-abda6ff37289"\n'
10.10.2.1: b'  Creating volume group backup "/etc/lvm/backup/ceph-80198a05-cc04-4969-8a85-26d2c836166a" (seqno 6).\n'
10.10.2.1: b'  Logical volume "osd-block-0992b562-27c9-45de-af29-abda6ff37289" successfully removed\n'
10.10.2.1: b'  Removing physical volume "/dev/nvme0n1" from volume group "ceph-80198a05-cc04-4969-8a85-26d2c836166a"\n'
10.10.2.1: b'  Volume group "ceph-80198a05-cc04-4969-8a85-26d2c836166a" successfully removed\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T15:19:39,350002113-07:00] STAGE: Bootstrap a new cluster...\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:317 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.1 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T15:19:39,360846600-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T15:19:39,364857731-07:00] DEBUG: CEPHADM_PARAMS: ['bootstrap', '--config', '/tmp/bench-rados/ceph-research/scripts/ceph.conf', '--mon-ip', '10.10.2.1', '--output-dir', '/tmp/bench-rados/deployment_data_root/etc/ceph', '--ssh-user', 'ljishen', '--skip-dashboard', '--no-minimize-config', '--allow-overwrite', '--allow-fqdn-hostname', '--orphan-initial-daemons', '--skip-monitoring-stack', '--single-host-defaults']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.1 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults\n'
10.10.2.1: b'Verifying podman|docker is present...\n'
10.10.2.1: b'Verifying lvm2 is present...\nVerifying time synchronization is in place...\n'
10.10.2.1: b'Unit ntp.service is enabled and running\n'
10.10.2.1: b'Repeating the final host check...\n'
10.10.2.1: b'podman|docker (/usr/bin/docker) is present\n'
10.10.2.1: b'systemctl is present\n'
10.10.2.1: b'lvcreate is present\n'
10.10.2.1: b'Unit ntp.service is enabled and running\n'
10.10.2.1: b'Host looks OK\n'
10.10.2.1: b'Cluster fsid: 245c5c7e-383d-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Verifying IP 10.10.2.1 port 3300 ...\n'
10.10.2.1: b'Verifying IP 10.10.2.1 port 6789 ...\n'
10.10.2.1: b'Mon IP `10.10.2.1` is in CIDR network `10.10.2.0/24`\n'
10.10.2.1: b'- internal network (--cluster-network) has not been provided, OSD replication will default to the public_network\n'
10.10.2.1: b'Adjusting default settings to suit single-host cluster...\n'
10.10.2.1: b'Pulling container image quay.io/ceph/ceph:v16...\n'
10.10.2.1: b'Ceph version: ceph version 16.2.6 (ee28fb57e47e9f88813e24bbf4c14496ca299d31) pacific (stable)\n'
10.10.2.1: b'Extracting ceph user uid/gid from container image...\n'
10.10.2.1: b'Creating initial keys...\n'
10.10.2.1: b'Creating initial monmap...\n'
10.10.2.1: b'Creating mon...\n'
10.10.2.1: b'Waiting for mon to start...\n'
10.10.2.1: b'Waiting for mon...\n'
10.10.2.1: b'mon is available\nSetting mon public_network to 10.10.2.0/24\n'
10.10.2.1: b'Wrote config to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf\n'
10.10.2.1: b'Wrote keyring to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring\n'
10.10.2.1: b'Creating mgr...\n'
10.10.2.1: b'Verifying port 9283 ...\n'
10.10.2.1: b'Waiting for mgr to start...\n'
10.10.2.1: b'Waiting for mgr...\n'
10.10.2.1: b'mgr not available, waiting (1/15)...\n'
10.10.2.1: b'mgr not available, waiting (2/15)...\n'
10.10.2.1: b'mgr is available\n'
10.10.2.1: b'Enabling cephadm module...\n'
10.10.2.1: b'Waiting for the mgr to restart...\n'
10.10.2.1: b'Waiting for mgr epoch 5...\n'
10.10.2.1: b'mgr epoch 5 is available\n'
10.10.2.1: b'Setting orchestrator backend to cephadm...\n'
10.10.2.1: b'Generating ssh key...\n'
10.10.2.1: b'Wrote public SSH key to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub\n'
10.10.2.1: b'Adding key to ljishen@localhost authorized_keys...\n'
10.10.2.1: b'Adding host node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us...\n'
10.10.2.1: b'Deploying unmanaged mon service...\n'
10.10.2.1: b'Deploying unmanaged mgr service...\n'
10.10.2.1: b'You can access the Ceph CLI with:\n\n\tsudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --fsid 245c5c7e-383d-11ec-b51d-53e6e728d2d3 -c /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf -k /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring\n\nPlease consider enabling telemetry to help improve Ceph:\n\n\tceph telemetry on\n\nFor more information see:\n\n\thttps://docs.ceph.com/docs/pacific/mgr/telemetry/\n\nBootstrap complete.\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T15:20:39,373960377-07:00] INFO: Sleep for 20 seconds\x1b[0m\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T15:20:59,380984753-07:00] INFO: > Set the default number of monitor daemons to 1\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:337 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T15:20:59,390508818-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T15:20:59,394413288-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mon', '1']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1\n'
10.10.2.1: b'Inferring fsid 245c5c7e-383d-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/245c5c7e-383d-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b'Scheduled mon update...\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T15:21:08,361427454-07:00] INFO: > Set the default number of manager daemons to 1\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:342 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T15:21:08,371695327-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T15:21:08,375462640-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mgr', '1']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1\n'
10.10.2.1: b'Inferring fsid 245c5c7e-383d-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/245c5c7e-383d-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b'Scheduled mgr update...\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T15:21:17,421097812-07:00] STAGE: Add hosts to the cluster...\x1b[0m\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T15:21:17,427280946-07:00] INFO: > Adding host 'sm1'\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:361 - add_hosts() > ssh-copy-id -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -f -i /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub ljishen@sm1\n'
10.10.2.1: b'/usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: "/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub"\n'
10.10.2.1: b'\nNumber of key(s) added: 1\n\nNow try logging into the machine, with:   "ssh -o \'GlobalKnownHostsFile=/dev/null\' -o \'LogLevel=ERROR\' -o \'PasswordAuthentication=no\' -o \'StrictHostKeyChecking=no\' -o \'UserKnownHostsFile=/dev/null\' \'ljishen@sm1\'"\nand check to make sure that only the key(s) you wanted were added.\n\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:367 - add_hosts() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add sm1 10.10.2.5\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T15:21:18,560487749-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T15:21:18,564443276-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'host', 'add', 'sm1', '10.10.2.5']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add sm1 10.10.2.5\n'
10.10.2.1: b'Inferring fsid 245c5c7e-383d-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/245c5c7e-383d-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b"Added host 'sm1' with addr '10.10.2.5'\n"
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T15:21:29,598731855-07:00] INFO: Sleep for 20 seconds\x1b[0m\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T15:21:49,603575784-07:00] STAGE: Add OSDs to the cluster...\x1b[0m\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T15:21:49,610097114-07:00] INFO: > Adding devices ['/dev/nvme0n1'] on host 'sm1' as OSDs\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:388 - add_host_devices_as_osds() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd sm1:/dev/nvme0n1\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T15:21:49,620207902-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T15:21:49,624112804-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'daemon', 'add', 'osd', 'sm1:/dev/nvme0n1']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd sm1:/dev/nvme0n1\n'
10.10.2.1: b'Inferring fsid 245c5c7e-383d-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/245c5c7e-383d-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b"Created osd(s) 0 on host 'sm1'\n"
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T15:22:14,880106300-07:00] INFO: > Sleep for 20 seconds\x1b[0m\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T15:22:34,885071688-07:00] STAGE: Show Cluster Status\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:408 - show_status() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T15:22:34,895457512-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T15:22:34,898939399-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', '--status']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status\n'
10.10.2.1: b'Inferring fsid 245c5c7e-383d-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/245c5c7e-383d-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b'  cluster:\n    id:     245c5c7e-383d-11ec-b51d-53e6e728d2d3\n    health: HEALTH_WARN\n            1 pool(s) have no replicas configured\n \n  services:\n    mon: 1 daemons, quorum node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us (age 2m)\n    mgr: node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us.qrsquo(active, since 2m)\n    osd: 1 osds: 1 up (since 19s), 1 in (since 41s)\n \n  data:\n    pools:   1 pools, 1 pgs\n    objects: 0 objects, 0 B\n    usage:   4.7 MiB used, 400 GiB / 400 GiB avail\n    pgs:     1 active+clean\n \n'
[1] 15:22:43 [SUCCESS] ljishen@10.10.2.1

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:19:26,005426784-07:00] INFO: > Remove existing clusters[0m
## bash:17 -  > basename -- /var/lib/ceph/e06df582-383b-11ec-b51d-53e6e728d2d3
# bash:17 -  > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm --host 10.10.2.1 --host 10.10.2.5 rm-cluster --force --fsid e06df582-383b-11ec-b51d-53e6e728d2d3
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:19:26,016985765-07:00] DEBUG: HOST_PARAMS: ['--host', '10.10.2.1', '--host', '10.10.2.5'][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:19:26,020962871-07:00] DEBUG: CEPHADM_PARAMS: ['rm-cluster', '--force', '--fsid', 'e06df582-383b-11ec-b51d-53e6e728d2d3'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:63 -  > /tmp/bench-rados/ceph-research/scripts/parallel-ssh --host 10.10.2.1 --host 10.10.2.5 'sudo python3 -u - rm-cluster --force --fsid e06df582-383b-11ec-b51d-53e6e728d2d3'
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:19:26,030411495-07:00] DEBUG: command from stdin[0m
# /tmp/bench-rados/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host 10.10.2.1 --host 10.10.2.5 'sudo python3 -u - rm-cluster --force --fsid e06df582-383b-11ec-b51d-53e6e728d2d3'
[1] 15:19:31 [SUCCESS] 10.10.2.1
[2] 15:19:33 [SUCCESS] 10.10.2.5

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:19:33,613071749-07:00] INFO: > Deploy a new cluster[0m
# bash:24 -  > /tmp/bench-rados/ceph-research/scripts/ceph-deploy -m 10.10.2.1 -o sm1/10.10.2.5:/dev/nvme0n1


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:19:33,624298936-07:00] STAGE: Verify passwordless SSH to hosts, passwordless sudo via ljishen, and the existence of devices on hosts...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:19:33,629090594-07:00] INFO: > Check the host for the monitor[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:181 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@10.10.2.1 sudo --non-interactive --validate

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:19:33,778992305-07:00] INFO: > Check hosts for OSDs[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:19:33,783016640-07:00] INFO: >> Check host 'sm1'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:200 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 sudo --non-interactive --validate

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:19:34,865077750-07:00] INFO: >>> Check device '/dev/nvme0n1'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:215 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 test -b /dev/nvme0n1
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:226 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 lsblk --nodeps --noheadings --output TYPE /dev/nvme0n1
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:227 - verify_parameters() > grep --word-regexp disk


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:19:36,995750526-07:00] STAGE: Reset devices on hosts...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:19:37,000622365-07:00] INFO: > Resetting device '/dev/nvme0n1' on host 'sm1'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:288 - reset_host_devices() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 sudo sh
  Removing ceph--80198a05--cc04--4969--8a85--26d2c836166a-osd--block--0992b562--27c9--45de--af29--abda6ff37289 (252:0)
  Archiving volume group "ceph-80198a05-cc04-4969-8a85-26d2c836166a" metadata (seqno 5).
  Releasing logical volume "osd-block-0992b562-27c9-45de-af29-abda6ff37289"
  Creating volume group backup "/etc/lvm/backup/ceph-80198a05-cc04-4969-8a85-26d2c836166a" (seqno 6).
  Logical volume "osd-block-0992b562-27c9-45de-af29-abda6ff37289" successfully removed
  Removing physical volume "/dev/nvme0n1" from volume group "ceph-80198a05-cc04-4969-8a85-26d2c836166a"
  Volume group "ceph-80198a05-cc04-4969-8a85-26d2c836166a" successfully removed


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:19:39,350002113-07:00] STAGE: Bootstrap a new cluster...[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:317 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.1 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:19:39,360846600-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:19:39,364857731-07:00] DEBUG: CEPHADM_PARAMS: ['bootstrap', '--config', '/tmp/bench-rados/ceph-research/scripts/ceph.conf', '--mon-ip', '10.10.2.1', '--output-dir', '/tmp/bench-rados/deployment_data_root/etc/ceph', '--ssh-user', 'ljishen', '--skip-dashboard', '--no-minimize-config', '--allow-overwrite', '--allow-fqdn-hostname', '--orphan-initial-daemons', '--skip-monitoring-stack', '--single-host-defaults'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.1 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults
Verifying podman|docker is present...
Verifying lvm2 is present...
Verifying time synchronization is in place...
Unit ntp.service is enabled and running
Repeating the final host check...
podman|docker (/usr/bin/docker) is present
systemctl is present
lvcreate is present
Unit ntp.service is enabled and running
Host looks OK
Cluster fsid: 245c5c7e-383d-11ec-b51d-53e6e728d2d3
Verifying IP 10.10.2.1 port 3300 ...
Verifying IP 10.10.2.1 port 6789 ...
Mon IP `10.10.2.1` is in CIDR network `10.10.2.0/24`
- internal network (--cluster-network) has not been provided, OSD replication will default to the public_network
Adjusting default settings to suit single-host cluster...
Pulling container image quay.io/ceph/ceph:v16...
Ceph version: ceph version 16.2.6 (ee28fb57e47e9f88813e24bbf4c14496ca299d31) pacific (stable)
Extracting ceph user uid/gid from container image...
Creating initial keys...
Creating initial monmap...
Creating mon...
Waiting for mon to start...
Waiting for mon...
mon is available
Setting mon public_network to 10.10.2.0/24
Wrote config to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf
Wrote keyring to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring
Creating mgr...
Verifying port 9283 ...
Waiting for mgr to start...
Waiting for mgr...
mgr not available, waiting (1/15)...
mgr not available, waiting (2/15)...
mgr is available
Enabling cephadm module...
Waiting for the mgr to restart...
Waiting for mgr epoch 5...
mgr epoch 5 is available
Setting orchestrator backend to cephadm...
Generating ssh key...
Wrote public SSH key to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub
Adding key to ljishen@localhost authorized_keys...
Adding host node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us...
Deploying unmanaged mon service...
Deploying unmanaged mgr service...
You can access the Ceph CLI with:

	sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --fsid 245c5c7e-383d-11ec-b51d-53e6e728d2d3 -c /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf -k /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring

Please consider enabling telemetry to help improve Ceph:

	ceph telemetry on

For more information see:

	https://docs.ceph.com/docs/pacific/mgr/telemetry/

Bootstrap complete.

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:20:39,373960377-07:00] INFO: Sleep for 20 seconds[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:20:59,380984753-07:00] INFO: > Set the default number of monitor daemons to 1[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:337 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:20:59,390508818-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:20:59,394413288-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mon', '1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1
Inferring fsid 245c5c7e-383d-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/245c5c7e-383d-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Scheduled mon update...

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:21:08,361427454-07:00] INFO: > Set the default number of manager daemons to 1[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:342 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:21:08,371695327-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:21:08,375462640-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mgr', '1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1
Inferring fsid 245c5c7e-383d-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/245c5c7e-383d-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Scheduled mgr update...


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:21:17,421097812-07:00] STAGE: Add hosts to the cluster...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:21:17,427280946-07:00] INFO: > Adding host 'sm1'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:361 - add_hosts() > ssh-copy-id -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -f -i /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub ljishen@sm1
/usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: "/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub"

Number of key(s) added: 1

Now try logging into the machine, with:   "ssh -o 'GlobalKnownHostsFile=/dev/null' -o 'LogLevel=ERROR' -o 'PasswordAuthentication=no' -o 'StrictHostKeyChecking=no' -o 'UserKnownHostsFile=/dev/null' 'ljishen@sm1'"
and check to make sure that only the key(s) you wanted were added.

# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:367 - add_hosts() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add sm1 10.10.2.5
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:21:18,560487749-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:21:18,564443276-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'host', 'add', 'sm1', '10.10.2.5'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add sm1 10.10.2.5
Inferring fsid 245c5c7e-383d-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/245c5c7e-383d-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Added host 'sm1' with addr '10.10.2.5'

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:21:29,598731855-07:00] INFO: Sleep for 20 seconds[0m


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:21:49,603575784-07:00] STAGE: Add OSDs to the cluster...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:21:49,610097114-07:00] INFO: > Adding devices ['/dev/nvme0n1'] on host 'sm1' as OSDs[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:388 - add_host_devices_as_osds() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd sm1:/dev/nvme0n1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:21:49,620207902-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:21:49,624112804-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'daemon', 'add', 'osd', 'sm1:/dev/nvme0n1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd sm1:/dev/nvme0n1
Inferring fsid 245c5c7e-383d-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/245c5c7e-383d-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Created osd(s) 0 on host 'sm1'

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:22:14,880106300-07:00] INFO: > Sleep for 20 seconds[0m


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:22:34,885071688-07:00] STAGE: Show Cluster Status[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:408 - show_status() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:22:34,895457512-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:22:34,898939399-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', '--status'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status
Inferring fsid 245c5c7e-383d-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/245c5c7e-383d-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
  cluster:
    id:     245c5c7e-383d-11ec-b51d-53e6e728d2d3
    health: HEALTH_WARN
            1 pool(s) have no replicas configured
 
  services:
    mon: 1 daemons, quorum node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us (age 2m)
    mgr: node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us.qrsquo(active, since 2m)
    osd: 1 osds: 1 up (since 19s), 1 in (since 41s)
 
  data:
    pools:   1 pools, 1 pgs
    objects: 0 objects, 0 B
    usage:   4.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     1 active+clean
 

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:22:43,459079029-07:00] INFO: > Copy configuration files to local[0m
# ./benchmarks/bench-rados:220 - launch_ceph_cluster() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.1 sudo chown --changes --recursive ljishen /tmp/bench-rados/deployment_data_root/etc/ceph
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:22:43,466690732-07:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.1 sudo chown --changes --recursive ljishen /tmp/bench-rados/deployment_data_root/etc/ceph
[1] 15:22:43 [SUCCESS] ljishen@10.10.2.1
# ./benchmarks/bench-rados:223 - launch_ceph_cluster() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.1:/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf
# ./benchmarks/bench-rados:226 - launch_ceph_cluster() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.1:/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:22:43,944762119-07:00] STAGE: Update local ceph.conf as a client...[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:22:43,947957029-07:00] STAGE: Configure Ceph pool...[0m
# ./benchmarks/bench-rados:257 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_max_backfills 32
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:22:43,969727194-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:22:43,972485862-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '245c5c7e-383d-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'config', 'set', 'osd', 'osd_max_backfills', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 245c5c7e-383d-11ec-b51d-53e6e728d2d3 -- ceph config set osd osd_max_backfills 32
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:258 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_active 32
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:22:47,863477170-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:22:47,866497201-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '245c5c7e-383d-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_active', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 245c5c7e-383d-11ec-b51d-53e6e728d2d3 -- ceph config set osd osd_recovery_max_active 32
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:259 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_single_start 8
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:22:51,923109561-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:22:51,926192360-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '245c5c7e-383d-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_single_start', '8'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 245c5c7e-383d-11ec-b51d-53e6e728d2d3 -- ceph config set osd osd_recovery_max_single_start 8
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:260 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_op_priority 63
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:22:55,906186189-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:22:55,909284277-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '245c5c7e-383d-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_op_priority', '63'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 245c5c7e-383d-11ec-b51d-53e6e728d2d3 -- ceph config set osd osd_recovery_op_priority 63
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:262 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config show-with-defaults osd.0
# ./benchmarks/bench-rados:263 - configure_ceph_pool() > grep 'osd_max_backfills\|osd_recovery'
# ./benchmarks/bench-rados:263 - configure_ceph_pool() > column -t -s ' '
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
osd_max_backfills                        32         mon
osd_recovery_cost                        20971520   default
osd_recovery_delay_start                 0.000000   default
osd_recovery_max_active                  32         mon
osd_recovery_max_active_hdd              3          default
osd_recovery_max_active_ssd              10         default
osd_recovery_max_chunk                   8388608    default
osd_recovery_max_omap_entries_per_chunk  8096       default
osd_recovery_max_single_start            8          mon
osd_recovery_op_priority                 3          default
osd_recovery_op_warn_multiple            16         default
osd_recovery_priority                    5          default
osd_recovery_retry_interval              30.000000  default
osd_recovery_sleep                       0.000000   default
osd_recovery_sleep_hdd                   0.100000   default
osd_recovery_sleep_hybrid                0.025000   default
osd_recovery_sleep_ssd                   0.000000   default
# ./benchmarks/bench-rados:265 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool create bench_rados 128 128 replicated --size 1 --pg-num-min 128
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:23:03,821609630-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:23:03,824510847-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '245c5c7e-383d-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'create', 'bench_rados', '128', '128', 'replicated', '--size', '1', '--pg-num-min', '128'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 245c5c7e-383d-11ec-b51d-53e6e728d2d3 -- ceph osd pool create bench_rados 128 128 replicated --size 1 --pg-num-min 128
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
pool 'bench_rados' created
# ./benchmarks/bench-rados:267 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados min_size 1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:23:08,677722295-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:23:08,680631007-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '245c5c7e-383d-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'min_size', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 245c5c7e-383d-11ec-b51d-53e6e728d2d3 -- ceph osd pool set bench_rados min_size 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 min_size to 1
# ./benchmarks/bench-rados:268 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados pg_autoscale_mode off
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:23:13,163331269-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:23:13,166462731-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '245c5c7e-383d-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'pg_autoscale_mode', 'off'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 245c5c7e-383d-11ec-b51d-53e6e728d2d3 -- ceph osd pool set bench_rados pg_autoscale_mode off
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 pg_autoscale_mode to off
# ./benchmarks/bench-rados:269 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool application enable bench_rados benchmark
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:23:17,801091685-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:23:17,804197809-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '245c5c7e-383d-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'application', 'enable', 'bench_rados', 'benchmark'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 245c5c7e-383d-11ec-b51d-53e6e728d2d3 -- ceph osd pool application enable bench_rados benchmark
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
enabled application 'benchmark' on pool 'bench_rados'
# ./benchmarks/bench-rados:270 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados noscrub 1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:23:22,026138678-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:23:22,029315414-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '245c5c7e-383d-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'noscrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 245c5c7e-383d-11ec-b51d-53e6e728d2d3 -- ceph osd pool set bench_rados noscrub 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 noscrub to 1
# ./benchmarks/bench-rados:271 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados nodeep-scrub 1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:23:26,359396408-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:23:26,362477785-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '245c5c7e-383d-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'nodeep-scrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 245c5c7e-383d-11ec-b51d-53e6e728d2d3 -- ceph osd pool set bench_rados nodeep-scrub 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 nodeep-scrub to 1
# ./benchmarks/bench-rados:272 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool ls detail
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:23:31,367617930-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:23:31,370630717-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '245c5c7e-383d-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'ls', 'detail'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 245c5c7e-383d-11ec-b51d-53e6e728d2d3 -- ceph osd pool ls detail
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
pool 1 'device_health_metrics' replicated size 1 min_size 1 crush_rule 0 object_hash rjenkins pg_num 64 pgp_num 64 autoscale_mode on last_change 18 lfor 0/0/16 flags hashpspool stripe_width 0 pg_num_min 1 application mgr_devicehealth
pool 2 'bench_rados' replicated size 1 min_size 1 crush_rule 0 object_hash rjenkins pg_num 128 pgp_num 128 autoscale_mode off last_change 20 flags hashpspool,noscrub,nodeep-scrub stripe_width 0 pg_num_min 128 application benchmark

# ./benchmarks/bench-rados:273 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd df tree
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:23:35,269420651-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:23:35,272583491-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '245c5c7e-383d-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'df', 'tree'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 245c5c7e-383d-11ec-b51d-53e6e728d2d3 -- ceph osd df tree
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
ID  CLASS  WEIGHT   REWEIGHT  SIZE     RAW USE  DATA     OMAP  META     AVAIL    %USE  VAR   PGS  STATUS  TYPE NAME    
-1         0.39059         -  400 GiB  5.7 MiB  168 KiB   0 B  5.6 MiB  400 GiB  0.00  1.00    -          root default 
-3         0.39059         -  400 GiB  5.7 MiB  168 KiB   0 B  5.6 MiB  400 GiB  0.00  1.00    -              host sm1 
 0    ssd  0.39059   1.00000  400 GiB  5.7 MiB  168 KiB   0 B  5.6 MiB  400 GiB  0.00  1.00  192      up          osd.0
                       TOTAL  400 GiB  5.7 MiB  168 KiB   0 B  5.6 MiB  400 GiB  0.00                                  
MIN/MAX VAR: 1.00/1.00  STDDEV: 0


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:23:39,125992858-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:24:03,089636386-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:24:12,055652109-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:24:21,003788464-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:24:30,037040760-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:24:30,044183440-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:24:38,891999956-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:24:38,898699691-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:24:47,906994955-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:24:47,914162642-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:24:56,898533456-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:24:56,905695412-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:25:06,070671241-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:25:06,077526490-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:25:06,082888524-07:00] INFO: > The cluster is idle now.[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:25:06,086478409-07:00] STAGE: Start benchmarking of mode write ...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:25:06,092611427-07:00] INFO: > Start system activity collection services[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:25:06,097834619-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:341 - rados_bench() > rm --force /mnt/sda8/ceph-research/scripts/output/sys_activity_16KB_write.dat.rados_bench_host.2
# ./benchmarks/bench-rados:342 - rados_bench() > sar_pid_rados_bench_host=812404
# ./benchmarks/bench-rados:342 - rados_bench() > S_TIME_FORMAT=ISO
# ./benchmarks/bench-rados:342 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/output/sys_activity_16KB_write.dat.rados_bench_host.2 2

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:25:06,104448853-07:00] INFO: >> for OSD host[0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:25:06,113195095-07:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.5 bash
10.10.2.5: b'272448\n'
[1] 15:25:07 [SUCCESS] ljishen@10.10.2.5
272448

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:25:07,215565284-07:00] INFO: > Run rados bench[0m
# ./benchmarks/bench-rados:370 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/output/rados_bench_16KB_write.log.2
## ./benchmarks/bench-rados:361 - rados_bench() > count_cpus 32-63,96-127
# ./benchmarks/bench-rados:361 - rados_bench() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 60 write --pool bench_rados -b 16384 -O 16384 --concurrent-ios 128 --show-time --write-object --write-omap --write-xattr --no-cleanup
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:25:07,235392747-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:25:07,238230465-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '245c5c7e-383d-11ec-b51d-53e6e728d2d3', '--', 'rados', 'bench', '60', 'write', '--pool', 'bench_rados', '-b', '16384', '-O', '16384', '--concurrent-ios', '128', '--show-time', '--write-object', '--write-omap', '--write-xattr', '--no-cleanup'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 245c5c7e-383d-11ec-b51d-53e6e728d2d3 -- rados bench 60 write --pool bench_rados -b 16384 -O 16384 --concurrent-ios 128 --show-time --write-object --write-omap --write-xattr --no-cleanup
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
hints = 1
2021-10-28T22:25:10.202706+0000 Maintaining 128 concurrent writes of 16384 bytes to objects of size 16384 for up to 60 seconds or 0 objects
2021-10-28T22:25:10.202717+0000 Object prefix: benchmark_data_node-2.bluefield2.ucsc-cmps10_7
2021-10-28T22:25:10.203757+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-28T22:25:10.203757+0000     0       0         0         0         0         0           -           0
2021-10-28T22:25:11.203880+0000     1     128      1025       897   14.0148   14.0156    0.120222    0.132233
2021-10-28T22:25:12.203972+0000     2     128      2049      1921   15.0067        16    0.124693    0.130882
2021-10-28T22:25:13.204061+0000     3     128      2945      2817   14.6707        14    0.124549    0.134821
2021-10-28T22:25:14.204139+0000     4     128      3841      3713   14.5027        14    0.118799    0.135701
2021-10-28T22:25:15.204217+0000     5     128      4865      4737   14.8019        16     0.13338    0.134365
2021-10-28T22:25:16.204293+0000     6     128      5807      5679   14.7879   14.7188    0.116849    0.133352
2021-10-28T22:25:17.204371+0000     7     128      6785      6657   14.8582   15.2812   0.0999075    0.133638
2021-10-28T22:25:18.204455+0000     8     128      7855      7727   15.0906   16.7188    0.127361     0.13124
2021-10-28T22:25:19.204538+0000     9     128      8879      8751   15.1915        16    0.119074    0.130711
2021-10-28T22:25:20.204623+0000    10     128      9729      9601   15.0004   13.2812    0.115045     0.13259
2021-10-28T22:25:21.204714+0000    11     128     10625     10497   14.9093        14    0.125929    0.133334
2021-10-28T22:25:22.204801+0000    12     128     11695     11567     15.06   16.7188    0.107957    0.132244
2021-10-28T22:25:23.204889+0000    13     128     12673     12545   15.0769   15.2812    0.111304    0.132346
2021-10-28T22:25:24.204970+0000    14     128     13615     13487   15.0512   14.7188    0.107874    0.132487
2021-10-28T22:25:25.205055+0000    15     128     14383     14255   14.8477        12    0.275731    0.132309
2021-10-28T22:25:26.205132+0000    16     128     15489     15361   14.9997   17.2812    0.122136     0.13272
2021-10-28T22:25:27.205211+0000    17     128     16414     16286   14.9675   14.4531    0.116006    0.133193
2021-10-28T22:25:28.205286+0000    18     128     17409     17281   14.9996   15.5469     0.12862    0.132876
2021-10-28T22:25:29.205369+0000    19     128     18351     18223   14.9848   14.7188    0.125171    0.132876
2021-10-28T22:25:30.205453+0000 min lat: 0.0659281 max lat: 0.380023 avg lat: 0.131974
2021-10-28T22:25:30.205453+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-28T22:25:30.205453+0000    20     128     19457     19329   15.0995   17.2812    0.114721    0.131974
2021-10-28T22:25:31.205542+0000    21     128     20353     20225   15.0471        14    0.109319     0.13263
2021-10-28T22:25:32.205628+0000    22     128     21249     21121   14.9995        14     0.12736     0.13301
2021-10-28T22:25:33.205712+0000    23     128     22145     22017    14.956        14    0.121365    0.133552
2021-10-28T22:25:34.205791+0000    24     128     23169     23041   14.9994        16     0.12531    0.132901
2021-10-28T22:25:35.205869+0000    25     128     24111     23983   14.9881   14.7188    0.117149    0.132942
2021-10-28T22:25:36.205945+0000    26     128     25089     24961   14.9994   15.2812    0.110218    0.133051
2021-10-28T22:25:37.206023+0000    27     128     25985     25857   14.9623        14    0.100004     0.13339
2021-10-28T22:25:38.206103+0000    28     128     27009     26881   14.9993        16    0.113505    0.132985
2021-10-28T22:25:39.206195+0000    29     128     28033     27905   15.0338        16    0.122144    0.132649
2021-10-28T22:25:40.206280+0000    30     128     29103     28975   15.0899   16.7188    0.117111    0.132269
2021-10-28T22:25:41.206374+0000    31     128     29871     29743   14.9902        12    0.127478    0.133223
2021-10-28T22:25:42.206459+0000    32     128     30849     30721   14.9993   15.2812    0.118848     0.13316
2021-10-28T22:25:43.206539+0000    33     128     31361     31233   14.7871         8    0.350433    0.134293
2021-10-28T22:25:44.206620+0000    34     128     31791     31663   14.5498   6.71875    0.249773    0.136529
2021-10-28T22:25:45.206706+0000    35     128     32257     32129   14.3421   7.28125    0.233235    0.139038
2021-10-28T22:25:46.206785+0000    36     128     32559     32431   14.0748   4.71875    0.516582    0.141036
2021-10-28T22:25:47.206864+0000    37     128     32943     32815   13.8565         6    0.452954    0.143853
2021-10-28T22:25:48.206948+0000    38     128     33409     33281   13.6835   7.28125    0.336167    0.145719
2021-10-28T22:25:49.207036+0000    39     128     33839     33711   13.5049   6.71875    0.296185    0.147708
2021-10-28T22:25:50.207126+0000 min lat: 0.0659281 max lat: 0.523068 avg lat: 0.14997
2021-10-28T22:25:50.207126+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-28T22:25:50.207126+0000    40     128     34177     34049   13.2993   5.28125    0.314435     0.14997
2021-10-28T22:25:51.207216+0000    41     128     34479     34351     13.09   4.71875    0.466601     0.15182
2021-10-28T22:25:52.207301+0000    42     128     34863     34735   12.9212         6    0.327317    0.154152
2021-10-28T22:25:53.207398+0000    43     128     35247     35119   12.7602         6    0.287034    0.155813
2021-10-28T22:25:54.207491+0000    44     128     35713     35585   12.6357   7.28125    0.285109    0.157978
2021-10-28T22:25:55.207576+0000    45     128     36097     35969   12.4882         6     0.31305     0.15974
2021-10-28T22:25:56.207654+0000    46     128     36527     36399   12.3628   6.71875    0.284607    0.161405
2021-10-28T22:25:57.207732+0000    47     128     36783     36655   12.1848         4     0.39916    0.162938
2021-10-28T22:25:58.207820+0000    48     128     37249     37121   12.0827   7.28125    0.266924     0.16527
2021-10-28T22:25:59.207919+0000    49     128     37633     37505   11.9585         6    0.335891    0.166957
2021-10-28T22:26:00.208007+0000    50     128     38017     37889   11.8393         6    0.313796    0.168451
2021-10-28T22:26:01.208092+0000    51     128     38401     38273   11.7248         6      0.4398    0.170133
2021-10-28T22:26:02.208181+0000    52     127     38803     38676   11.6204   6.29688    0.333815    0.171475
2021-10-28T22:26:03.208269+0000    53     128     39087     38959   11.4846   4.42188    0.492548    0.173532
2021-10-28T22:26:04.208362+0000    54     128     39471     39343    11.383         6    0.271503    0.175142
2021-10-28T22:26:05.208450+0000    55     128     39855     39727   11.2851         6    0.309909    0.176527
2021-10-28T22:26:06.208528+0000    56     128     40321     40193   11.2136   7.28125    0.307533    0.178157
2021-10-28T22:26:07.208603+0000    57     128     40623     40495   11.0997   4.71875    0.463563    0.179336
2021-10-28T22:26:08.208680+0000    58     128     40961     40833   10.9993   5.28125    0.489188    0.181372
2021-10-28T22:26:09.208766+0000    59     128     41345     41217   10.9146         6    0.346625    0.182701
2021-10-28T22:26:10.208849+0000 min lat: 0.0659281 max lat: 0.572471 avg lat: 0.183865
2021-10-28T22:26:10.208849+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-28T22:26:10.208849+0000    60     128     41857     41729    10.866         8    0.171807    0.183865
2021-10-28T22:26:11.208981+0000 Total time run:         60.0952
Total writes made:      41857
Write size:             16384
Object size:            16384
Bandwidth (MB/sec):     10.883
Stddev Bandwidth:       4.61534
Max bandwidth (MB/sec): 17.2812
Min bandwidth (MB/sec): 4
Average IOPS:           696
Stddev IOPS:            295.382
Max IOPS:               1106
Min IOPS:               256
Average Latency(s):     0.183675
Stddev Latency(s):      0.101228
Max latency(s):         0.572471
Min latency(s):         0.0659281

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:26:11,936026762-07:00] INFO: > Stop system activity collection services[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:26:11,941326499-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:391 - rados_bench() > kill -INT 812404

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:26:11,946633229-07:00] INFO: >> for OSD host[0m
# ./benchmarks/bench-rados:396 - rados_bench() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.5 kill -INT 272448
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:26:11,953994171-07:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.5 kill -INT 272448
[1] 15:26:13 [SUCCESS] ljishen@10.10.2.5

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:26:13,084432876-07:00] INFO: >>> Copy system activity file from OSD host[0m
## ./benchmarks/bench-rados:403 - rados_bench() > basename -- /tmp/bench-rados/sys_activity_16KB_write.dat.osd_host.2
# ./benchmarks/bench-rados:403 - rados_bench() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.5:/tmp/bench-rados/sys_activity_16KB_write.dat.osd_host.2 /mnt/sda8/ceph-research/scripts/output/sys_activity_16KB_write.dat.osd_host.2


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:26:14,214535248-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:26:38,198313992-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 41.86k objects, 654 MiB
    usage:   2.0 GiB used, 398 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:26:38,205788838-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:26:47,154897855-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 41.86k objects, 654 MiB
    usage:   2.0 GiB used, 398 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:26:47,162255430-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:26:56,187446916-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 41.86k objects, 654 MiB
    usage:   2.0 GiB used, 398 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:26:56,194309899-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:27:05,194902374-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 41.86k objects, 654 MiB
    usage:   2.0 GiB used, 398 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:27:05,202029595-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:27:14,099423420-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 41.86k objects, 654 MiB
    usage:   2.0 GiB used, 398 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:27:14,106775265-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:27:14,112006322-07:00] INFO: > The cluster is idle now.[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:27:14,115445563-07:00] STAGE: Start benchmarking of mode seq ...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:27:14,121575745-07:00] INFO: > Start system activity collection services[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:27:14,126506497-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:341 - rados_bench() > rm --force /mnt/sda8/ceph-research/scripts/output/sys_activity_16KB_seq.dat.rados_bench_host.2
# ./benchmarks/bench-rados:342 - rados_bench() > sar_pid_rados_bench_host=813807
# ./benchmarks/bench-rados:342 - rados_bench() > S_TIME_FORMAT=ISO
# ./benchmarks/bench-rados:342 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/output/sys_activity_16KB_seq.dat.rados_bench_host.2 2

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:27:14,132878054-07:00] INFO: >> for OSD host[0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:27:14,141654603-07:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.5 bash
10.10.2.5: b'272995\n'
[1] 15:27:15 [SUCCESS] ljishen@10.10.2.5
272995

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:27:15,303237743-07:00] INFO: > Run rados bench[0m
# ./benchmarks/bench-rados:379 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/output/rados_bench_16KB_seq.log.2
## ./benchmarks/bench-rados:376 - rados_bench() > count_cpus 32-63,96-127
# ./benchmarks/bench-rados:376 - rados_bench() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 128 --show-time
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:27:15,322635678-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:27:15,325676758-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '245c5c7e-383d-11ec-b51d-53e6e728d2d3', '--', 'rados', 'bench', '99999999', 'seq', '--pool', 'bench_rados', '--concurrent-ios', '128', '--show-time'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 245c5c7e-383d-11ec-b51d-53e6e728d2d3 -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 128 --show-time
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
hints = 1
2021-10-28T22:27:18.278430+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-28T22:27:18.278430+0000     0       0         0         0         0         0           -           0
2021-10-28T22:27:19.278568+0000     1     128      2913      2785   43.5068   43.5156   0.0066761   0.0425682
2021-10-28T22:27:20.278682+0000     2     128      6073      5945    46.438    49.375   0.0256987   0.0426912
2021-10-28T22:27:21.278755+0000     3     128      8674      8546   44.5046   40.6406   0.0246013   0.0443917
2021-10-28T22:27:22.278870+0000     4     128     11394     11266   44.0023      42.5   0.0387501   0.0449789
2021-10-28T22:27:23.278986+0000     5     128     13013     12885   40.2606   25.2969   0.0517626   0.0492714
2021-10-28T22:27:24.279095+0000     6     128     15970     15842   41.2502   46.2031   0.0752325   0.0481566
2021-10-28T22:27:25.279168+0000     7     128     18216     18088   40.3704   35.0938   0.0200709   0.0493776
2021-10-28T22:27:26.279290+0000     8     128     20754     20626   40.2805   39.6562    0.111014   0.0494734
2021-10-28T22:27:27.279415+0000     9     128     23282     23154   40.1932      39.5   0.0612833   0.0495165
2021-10-28T22:27:28.279528+0000    10     128     25460     25332   39.5766   34.0312   0.0174894   0.0503629
2021-10-28T22:27:29.279603+0000    11     128     27985     27857   39.5652   39.4531    0.081496   0.0496348
2021-10-28T22:27:30.279713+0000    12     128     30706     30578   39.8106   42.5156  0.00713658    0.050042
2021-10-28T22:27:31.279824+0000    13     128     34097     33969   40.8235   52.9844   0.0215678   0.0489248
2021-10-28T22:27:32.279935+0000    14     128     37567     37439   41.7799   54.2188   0.0347498   0.0478139
2021-10-28T22:27:33.280010+0000    15     128     40580     40452   42.1329   47.0781   0.0445694   0.0473869
2021-10-28T22:27:34.280159+0000 Total time run:       15.5296
Total reads made:     41857
Read size:            16384
Object size:          16384
Bandwidth (MB/sec):   42.1142
Average IOPS:         2695
Stddev IOPS:          477.892
Max IOPS:             3470
Min IOPS:             1619
Average Latency(s):   0.0473081
Max latency(s):       0.516428
Min latency(s):       0.000614378

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:27:34,760610735-07:00] INFO: > Stop system activity collection services[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:27:34,766015530-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:391 - rados_bench() > kill -INT 813807

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:27:34,771548386-07:00] INFO: >> for OSD host[0m
# ./benchmarks/bench-rados:396 - rados_bench() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.5 kill -INT 272995
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:27:34,779315423-07:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.5 kill -INT 272995
[1] 15:27:35 [SUCCESS] ljishen@10.10.2.5

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:27:35,871835198-07:00] INFO: >>> Copy system activity file from OSD host[0m
## ./benchmarks/bench-rados:403 - rados_bench() > basename -- /tmp/bench-rados/sys_activity_16KB_seq.dat.osd_host.2
# ./benchmarks/bench-rados:403 - rados_bench() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.5:/tmp/bench-rados/sys_activity_16KB_seq.dat.osd_host.2 /mnt/sda8/ceph-research/scripts/output/sys_activity_16KB_seq.dat.osd_host.2


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:27:36,961641288-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:28:00,838692582-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 41.86k objects, 654 MiB
    usage:   2.0 GiB used, 398 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:28:00,845635786-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:28:09,868883444-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 41.86k objects, 654 MiB
    usage:   2.0 GiB used, 398 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:28:09,876075116-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:28:18,877505785-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 41.86k objects, 654 MiB
    usage:   2.0 GiB used, 398 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:28:18,884477733-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:28:27,966498305-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 41.86k objects, 654 MiB
    usage:   2.0 GiB used, 398 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:28:27,973760470-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:28:37,021851847-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 41.86k objects, 654 MiB
    usage:   2.0 GiB used, 398 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:28:37,029099194-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:28:37,034455858-07:00] INFO: > The cluster is idle now.[0m


[1;7;39;49m[2021-10-28T15:28:37,036602183-07:00][RUNNING][ROUND 3/2/21] object_size=16KB[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:28:37,039767408-07:00] STAGE: Launch a Ceph cluster (CEPH_HOST: 10.10.2.1, OSD_HOST: 10.10.2.5) ...[0m
# ./benchmarks/bench-rados:183 - launch_ceph_cluster() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.1 bash -euo pipefail
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:28:37,048738053-07:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.1 bash -euo pipefail
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T15:28:37,490093116-07:00] INFO: > Remove existing clusters\x1b[0m\n'
10.10.2.1: b'## bash:17 -  > basename -- /var/lib/ceph/245c5c7e-383d-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'# bash:17 -  > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm --host 10.10.2.1 --host 10.10.2.5 rm-cluster --force --fsid 245c5c7e-383d-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T15:28:37,501201217-07:00] DEBUG: HOST_PARAMS: ['--host', '10.10.2.1', '--host', '10.10.2.5']\x1b[0m\n"
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T15:28:37,504988346-07:00] DEBUG: CEPHADM_PARAMS: ['rm-cluster', '--force', '--fsid', '245c5c7e-383d-11ec-b51d-53e6e728d2d3']\x1b[0m\n"
10.10.2.1: b"# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:63 -  > /tmp/bench-rados/ceph-research/scripts/parallel-ssh --host 10.10.2.1 --host 10.10.2.5 'sudo python3 -u - rm-cluster --force --fsid 245c5c7e-383d-11ec-b51d-53e6e728d2d3'\n"
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T15:28:37,513403996-07:00] DEBUG: command from stdin\x1b[0m\n'
10.10.2.1: b"# /tmp/bench-rados/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host 10.10.2.1 --host 10.10.2.5 'sudo python3 -u - rm-cluster --force --fsid 245c5c7e-383d-11ec-b51d-53e6e728d2d3'\n"
10.10.2.1: b'[1] 15:28:43 [SUCCESS] 10.10.2.1\n[2] 15:28:45 [SUCCESS] 10.10.2.5\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T15:28:45,139784331-07:00] INFO: > Deploy a new cluster\x1b[0m\n'
10.10.2.1: b'# bash:24 -  > /tmp/bench-rados/ceph-research/scripts/ceph-deploy -m 10.10.2.1 -o sm1/10.10.2.5:/dev/nvme0n1\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T15:28:45,151738733-07:00] STAGE: Verify passwordless SSH to hosts, passwordless sudo via ljishen, and the existence of devices on hosts...\x1b[0m\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T15:28:45,156584722-07:00] INFO: > Check the host for the monitor\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:181 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@10.10.2.1 sudo --non-interactive --validate\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T15:28:45,307353355-07:00] INFO: > Check hosts for OSDs\x1b[0m\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T15:28:45,312176301-07:00] INFO: >> Check host 'sm1'\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:200 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 sudo --non-interactive --validate\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T15:28:46,379302953-07:00] INFO: >>> Check device '/dev/nvme0n1'\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:215 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 test -b /dev/nvme0n1\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:226 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 lsblk --nodeps --noheadings --output TYPE /dev/nvme0n1\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:227 - verify_parameters() > grep --word-regexp disk\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T15:28:48,523907795-07:00] STAGE: Reset devices on hosts...\x1b[0m\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T15:28:48,528844875-07:00] INFO: > Resetting device '/dev/nvme0n1' on host 'sm1'\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:288 - reset_host_devices() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 sudo sh\n'
10.10.2.1: b'  Removing ceph--6405681f--4e34--4b63--bf95--6fc0f88e58f3-osd--block--1fd9aa80--25ac--4525--8c49--50be83943cd7 (252:0)\n'
10.10.2.1: b'  Archiving volume group "ceph-6405681f-4e34-4b63-bf95-6fc0f88e58f3" metadata (seqno 5).\n'
10.10.2.1: b'  Releasing logical volume "osd-block-1fd9aa80-25ac-4525-8c49-50be83943cd7"\n'
10.10.2.1: b'  Creating volume group backup "/etc/lvm/backup/ceph-6405681f-4e34-4b63-bf95-6fc0f88e58f3" (seqno 6).\n'
10.10.2.1: b'  Logical volume "osd-block-1fd9aa80-25ac-4525-8c49-50be83943cd7" successfully removed\n'
10.10.2.1: b'  Removing physical volume "/dev/nvme0n1" from volume group "ceph-6405681f-4e34-4b63-bf95-6fc0f88e58f3"\n'
10.10.2.1: b'  Volume group "ceph-6405681f-4e34-4b63-bf95-6fc0f88e58f3" successfully removed\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T15:28:50,885802165-07:00] STAGE: Bootstrap a new cluster...\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:317 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.1 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T15:28:50,896290681-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T15:28:50,900133254-07:00] DEBUG: CEPHADM_PARAMS: ['bootstrap', '--config', '/tmp/bench-rados/ceph-research/scripts/ceph.conf', '--mon-ip', '10.10.2.1', '--output-dir', '/tmp/bench-rados/deployment_data_root/etc/ceph', '--ssh-user', 'ljishen', '--skip-dashboard', '--no-minimize-config', '--allow-overwrite', '--allow-fqdn-hostname', '--orphan-initial-daemons', '--skip-monitoring-stack', '--single-host-defaults']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.1 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults\n'
10.10.2.1: b'Verifying podman|docker is present...\n'
10.10.2.1: b'Verifying lvm2 is present...\n'
10.10.2.1: b'Verifying time synchronization is in place...\n'
10.10.2.1: b'Unit ntp.service is enabled and running\nRepeating the final host check...\npodman|docker (/usr/bin/docker) is present\nsystemctl is present\nlvcreate is present\n'
10.10.2.1: b'Unit ntp.service is enabled and running\n'
10.10.2.1: b'Host looks OK\n'
10.10.2.1: b'Cluster fsid: 6d1996f6-383e-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Verifying IP 10.10.2.1 port 3300 ...\n'
10.10.2.1: b'Verifying IP 10.10.2.1 port 6789 ...\n'
10.10.2.1: b'Mon IP `10.10.2.1` is in CIDR network `10.10.2.0/24`\n'
10.10.2.1: b'- internal network (--cluster-network) has not been provided, OSD replication will default to the public_network\n'
10.10.2.1: b'Adjusting default settings to suit single-host cluster...\n'
10.10.2.1: b'Pulling container image quay.io/ceph/ceph:v16...\n'
10.10.2.1: b'Ceph version: ceph version 16.2.6 (ee28fb57e47e9f88813e24bbf4c14496ca299d31) pacific (stable)\n'
10.10.2.1: b'Extracting ceph user uid/gid from container image...\n'
10.10.2.1: b'Creating initial keys...\n'
10.10.2.1: b'Creating initial monmap...\n'
10.10.2.1: b'Creating mon...\n'
10.10.2.1: b'Waiting for mon to start...\n'
10.10.2.1: b'Waiting for mon...\n'
10.10.2.1: b'mon is available\n'
10.10.2.1: b'Setting mon public_network to 10.10.2.0/24\n'
10.10.2.1: b'Wrote config to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf\n'
10.10.2.1: b'Wrote keyring to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring\n'
10.10.2.1: b'Creating mgr...\n'
10.10.2.1: b'Verifying port 9283 ...\n'
10.10.2.1: b'Waiting for mgr to start...\nWaiting for mgr...\n'
10.10.2.1: b'mgr not available, waiting (1/15)...\n'
10.10.2.1: b'mgr not available, waiting (2/15)...\n'
10.10.2.1: b'mgr is available\n'
10.10.2.1: b'Enabling cephadm module...\n'
10.10.2.1: b'Waiting for the mgr to restart...\n'
10.10.2.1: b'Waiting for mgr epoch 5...\n'
10.10.2.1: b'mgr epoch 5 is available\n'
10.10.2.1: b'Setting orchestrator backend to cephadm...\n'
10.10.2.1: b'Generating ssh key...\n'
10.10.2.1: b'Wrote public SSH key to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub\n'
10.10.2.1: b'Adding key to ljishen@localhost authorized_keys...\n'
10.10.2.1: b'Adding host node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us...\n'
10.10.2.1: b'Deploying unmanaged mon service...\n'
10.10.2.1: b'Deploying unmanaged mgr service...\n'
10.10.2.1: b'You can access the Ceph CLI with:\n\n\tsudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --fsid 6d1996f6-383e-11ec-b51d-53e6e728d2d3 -c /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf -k /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring\n\n'
10.10.2.1: b'Please consider enabling telemetry to help improve Ceph:\n\n\tceph telemetry on\n\nFor more information see:\n\n\thttps://docs.ceph.com/docs/pacific/mgr/telemetry/\n\n'
10.10.2.1: b'Bootstrap complete.\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T15:29:52,238460247-07:00] INFO: Sleep for 20 seconds\x1b[0m\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T15:30:12,245727266-07:00] INFO: > Set the default number of monitor daemons to 1\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:337 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T15:30:12,256345487-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T15:30:12,260219499-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mon', '1']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1\n'
10.10.2.1: b'Inferring fsid 6d1996f6-383e-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/6d1996f6-383e-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b'Scheduled mon update...\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T15:30:21,312357757-07:00] INFO: > Set the default number of manager daemons to 1\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:342 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T15:30:21,321656877-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T15:30:21,325519237-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mgr', '1']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1\n'
10.10.2.1: b'Inferring fsid 6d1996f6-383e-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/6d1996f6-383e-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b'Scheduled mgr update...\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T15:30:30,452627767-07:00] STAGE: Add hosts to the cluster...\x1b[0m\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T15:30:30,458702366-07:00] INFO: > Adding host 'sm1'\x1b[0m\n# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:361 - add_hosts() > ssh-copy-id -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -f -i /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub ljishen@sm1\n"
10.10.2.1: b'/usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: "/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub"\n'
10.10.2.1: b'\nNumber of key(s) added: 1\n\nNow try logging into the machine, with:   "ssh -o \'GlobalKnownHostsFile=/dev/null\' -o \'LogLevel=ERROR\' -o \'PasswordAuthentication=no\' -o \'StrictHostKeyChecking=no\' -o \'UserKnownHostsFile=/dev/null\' \'ljishen@sm1\'"\nand check to make sure that only the key(s) you wanted were added.\n\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:367 - add_hosts() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add sm1 10.10.2.5\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T15:30:31,620418499-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T15:30:31,624405725-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'host', 'add', 'sm1', '10.10.2.5']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add sm1 10.10.2.5\n'
10.10.2.1: b'Inferring fsid 6d1996f6-383e-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/6d1996f6-383e-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b"Added host 'sm1' with addr '10.10.2.5'\n"
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T15:30:42,529105108-07:00] INFO: Sleep for 20 seconds\x1b[0m\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T15:31:02,533670605-07:00] STAGE: Add OSDs to the cluster...\x1b[0m\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T15:31:02,540220758-07:00] INFO: > Adding devices ['/dev/nvme0n1'] on host 'sm1' as OSDs\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:388 - add_host_devices_as_osds() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd sm1:/dev/nvme0n1\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T15:31:02,549661865-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T15:31:02,553296768-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'daemon', 'add', 'osd', 'sm1:/dev/nvme0n1']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd sm1:/dev/nvme0n1\n'
10.10.2.1: b'Inferring fsid 6d1996f6-383e-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/6d1996f6-383e-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b"Created osd(s) 0 on host 'sm1'\n"
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T15:31:26,657677375-07:00] INFO: > Sleep for 20 seconds\x1b[0m\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T15:31:46,663014492-07:00] STAGE: Show Cluster Status\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:408 - show_status() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T15:31:46,673115941-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T15:31:46,676738691-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', '--status']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status\n'
10.10.2.1: b'Inferring fsid 6d1996f6-383e-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/6d1996f6-383e-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b'  cluster:\n    id:     6d1996f6-383e-11ec-b51d-53e6e728d2d3\n    health: HEALTH_WARN\n            1 pool(s) have no replicas configured\n \n  services:\n    mon: 1 daemons, quorum node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us (age 2m)\n    mgr: node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us.ndwuea(active, since 2m)\n    osd: 1 osds: 1 up (since 20s), 1 in (since 40s)\n \n  data:\n    pools:   1 pools, 1 pgs\n    objects: 0 objects, 0 B\n    usage:   4.7 MiB used, 400 GiB / 400 GiB avail\n    pgs:     1 active+clean\n \n'
[1] 15:31:55 [SUCCESS] ljishen@10.10.2.1

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:28:37,490093116-07:00] INFO: > Remove existing clusters[0m
## bash:17 -  > basename -- /var/lib/ceph/245c5c7e-383d-11ec-b51d-53e6e728d2d3
# bash:17 -  > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm --host 10.10.2.1 --host 10.10.2.5 rm-cluster --force --fsid 245c5c7e-383d-11ec-b51d-53e6e728d2d3
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:28:37,501201217-07:00] DEBUG: HOST_PARAMS: ['--host', '10.10.2.1', '--host', '10.10.2.5'][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:28:37,504988346-07:00] DEBUG: CEPHADM_PARAMS: ['rm-cluster', '--force', '--fsid', '245c5c7e-383d-11ec-b51d-53e6e728d2d3'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:63 -  > /tmp/bench-rados/ceph-research/scripts/parallel-ssh --host 10.10.2.1 --host 10.10.2.5 'sudo python3 -u - rm-cluster --force --fsid 245c5c7e-383d-11ec-b51d-53e6e728d2d3'
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:28:37,513403996-07:00] DEBUG: command from stdin[0m
# /tmp/bench-rados/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host 10.10.2.1 --host 10.10.2.5 'sudo python3 -u - rm-cluster --force --fsid 245c5c7e-383d-11ec-b51d-53e6e728d2d3'
[1] 15:28:43 [SUCCESS] 10.10.2.1
[2] 15:28:45 [SUCCESS] 10.10.2.5

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:28:45,139784331-07:00] INFO: > Deploy a new cluster[0m
# bash:24 -  > /tmp/bench-rados/ceph-research/scripts/ceph-deploy -m 10.10.2.1 -o sm1/10.10.2.5:/dev/nvme0n1


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:28:45,151738733-07:00] STAGE: Verify passwordless SSH to hosts, passwordless sudo via ljishen, and the existence of devices on hosts...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:28:45,156584722-07:00] INFO: > Check the host for the monitor[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:181 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@10.10.2.1 sudo --non-interactive --validate

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:28:45,307353355-07:00] INFO: > Check hosts for OSDs[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:28:45,312176301-07:00] INFO: >> Check host 'sm1'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:200 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 sudo --non-interactive --validate

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:28:46,379302953-07:00] INFO: >>> Check device '/dev/nvme0n1'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:215 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 test -b /dev/nvme0n1
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:226 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 lsblk --nodeps --noheadings --output TYPE /dev/nvme0n1
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:227 - verify_parameters() > grep --word-regexp disk


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:28:48,523907795-07:00] STAGE: Reset devices on hosts...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:28:48,528844875-07:00] INFO: > Resetting device '/dev/nvme0n1' on host 'sm1'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:288 - reset_host_devices() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 sudo sh
  Removing ceph--6405681f--4e34--4b63--bf95--6fc0f88e58f3-osd--block--1fd9aa80--25ac--4525--8c49--50be83943cd7 (252:0)
  Archiving volume group "ceph-6405681f-4e34-4b63-bf95-6fc0f88e58f3" metadata (seqno 5).
  Releasing logical volume "osd-block-1fd9aa80-25ac-4525-8c49-50be83943cd7"
  Creating volume group backup "/etc/lvm/backup/ceph-6405681f-4e34-4b63-bf95-6fc0f88e58f3" (seqno 6).
  Logical volume "osd-block-1fd9aa80-25ac-4525-8c49-50be83943cd7" successfully removed
  Removing physical volume "/dev/nvme0n1" from volume group "ceph-6405681f-4e34-4b63-bf95-6fc0f88e58f3"
  Volume group "ceph-6405681f-4e34-4b63-bf95-6fc0f88e58f3" successfully removed


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:28:50,885802165-07:00] STAGE: Bootstrap a new cluster...[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:317 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.1 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:28:50,896290681-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:28:50,900133254-07:00] DEBUG: CEPHADM_PARAMS: ['bootstrap', '--config', '/tmp/bench-rados/ceph-research/scripts/ceph.conf', '--mon-ip', '10.10.2.1', '--output-dir', '/tmp/bench-rados/deployment_data_root/etc/ceph', '--ssh-user', 'ljishen', '--skip-dashboard', '--no-minimize-config', '--allow-overwrite', '--allow-fqdn-hostname', '--orphan-initial-daemons', '--skip-monitoring-stack', '--single-host-defaults'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.1 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults
Verifying podman|docker is present...
Verifying lvm2 is present...
Verifying time synchronization is in place...
Unit ntp.service is enabled and running
Repeating the final host check...
podman|docker (/usr/bin/docker) is present
systemctl is present
lvcreate is present
Unit ntp.service is enabled and running
Host looks OK
Cluster fsid: 6d1996f6-383e-11ec-b51d-53e6e728d2d3
Verifying IP 10.10.2.1 port 3300 ...
Verifying IP 10.10.2.1 port 6789 ...
Mon IP `10.10.2.1` is in CIDR network `10.10.2.0/24`
- internal network (--cluster-network) has not been provided, OSD replication will default to the public_network
Adjusting default settings to suit single-host cluster...
Pulling container image quay.io/ceph/ceph:v16...
Ceph version: ceph version 16.2.6 (ee28fb57e47e9f88813e24bbf4c14496ca299d31) pacific (stable)
Extracting ceph user uid/gid from container image...
Creating initial keys...
Creating initial monmap...
Creating mon...
Waiting for mon to start...
Waiting for mon...
mon is available
Setting mon public_network to 10.10.2.0/24
Wrote config to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf
Wrote keyring to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring
Creating mgr...
Verifying port 9283 ...
Waiting for mgr to start...
Waiting for mgr...
mgr not available, waiting (1/15)...
mgr not available, waiting (2/15)...
mgr is available
Enabling cephadm module...
Waiting for the mgr to restart...
Waiting for mgr epoch 5...
mgr epoch 5 is available
Setting orchestrator backend to cephadm...
Generating ssh key...
Wrote public SSH key to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub
Adding key to ljishen@localhost authorized_keys...
Adding host node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us...
Deploying unmanaged mon service...
Deploying unmanaged mgr service...
You can access the Ceph CLI with:

	sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --fsid 6d1996f6-383e-11ec-b51d-53e6e728d2d3 -c /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf -k /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring

Please consider enabling telemetry to help improve Ceph:

	ceph telemetry on

For more information see:

	https://docs.ceph.com/docs/pacific/mgr/telemetry/

Bootstrap complete.

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:29:52,238460247-07:00] INFO: Sleep for 20 seconds[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:30:12,245727266-07:00] INFO: > Set the default number of monitor daemons to 1[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:337 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:30:12,256345487-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:30:12,260219499-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mon', '1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1
Inferring fsid 6d1996f6-383e-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/6d1996f6-383e-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Scheduled mon update...

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:30:21,312357757-07:00] INFO: > Set the default number of manager daemons to 1[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:342 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:30:21,321656877-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:30:21,325519237-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mgr', '1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1
Inferring fsid 6d1996f6-383e-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/6d1996f6-383e-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Scheduled mgr update...


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:30:30,452627767-07:00] STAGE: Add hosts to the cluster...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:30:30,458702366-07:00] INFO: > Adding host 'sm1'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:361 - add_hosts() > ssh-copy-id -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -f -i /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub ljishen@sm1
/usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: "/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub"

Number of key(s) added: 1

Now try logging into the machine, with:   "ssh -o 'GlobalKnownHostsFile=/dev/null' -o 'LogLevel=ERROR' -o 'PasswordAuthentication=no' -o 'StrictHostKeyChecking=no' -o 'UserKnownHostsFile=/dev/null' 'ljishen@sm1'"
and check to make sure that only the key(s) you wanted were added.

# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:367 - add_hosts() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add sm1 10.10.2.5
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:30:31,620418499-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:30:31,624405725-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'host', 'add', 'sm1', '10.10.2.5'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add sm1 10.10.2.5
Inferring fsid 6d1996f6-383e-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/6d1996f6-383e-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Added host 'sm1' with addr '10.10.2.5'

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:30:42,529105108-07:00] INFO: Sleep for 20 seconds[0m


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:31:02,533670605-07:00] STAGE: Add OSDs to the cluster...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:31:02,540220758-07:00] INFO: > Adding devices ['/dev/nvme0n1'] on host 'sm1' as OSDs[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:388 - add_host_devices_as_osds() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd sm1:/dev/nvme0n1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:31:02,549661865-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:31:02,553296768-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'daemon', 'add', 'osd', 'sm1:/dev/nvme0n1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd sm1:/dev/nvme0n1
Inferring fsid 6d1996f6-383e-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/6d1996f6-383e-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Created osd(s) 0 on host 'sm1'

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:31:26,657677375-07:00] INFO: > Sleep for 20 seconds[0m


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:31:46,663014492-07:00] STAGE: Show Cluster Status[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:408 - show_status() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:31:46,673115941-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:31:46,676738691-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', '--status'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status
Inferring fsid 6d1996f6-383e-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/6d1996f6-383e-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
  cluster:
    id:     6d1996f6-383e-11ec-b51d-53e6e728d2d3
    health: HEALTH_WARN
            1 pool(s) have no replicas configured
 
  services:
    mon: 1 daemons, quorum node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us (age 2m)
    mgr: node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us.ndwuea(active, since 2m)
    osd: 1 osds: 1 up (since 20s), 1 in (since 40s)
 
  data:
    pools:   1 pools, 1 pgs
    objects: 0 objects, 0 B
    usage:   4.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     1 active+clean
 

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:31:55,322133145-07:00] INFO: > Copy configuration files to local[0m
# ./benchmarks/bench-rados:220 - launch_ceph_cluster() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.1 sudo chown --changes --recursive ljishen /tmp/bench-rados/deployment_data_root/etc/ceph
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:31:55,329820512-07:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.1 sudo chown --changes --recursive ljishen /tmp/bench-rados/deployment_data_root/etc/ceph
[1] 15:31:55 [SUCCESS] ljishen@10.10.2.1
# ./benchmarks/bench-rados:223 - launch_ceph_cluster() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.1:/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf
# ./benchmarks/bench-rados:226 - launch_ceph_cluster() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.1:/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:31:55,809097784-07:00] STAGE: Update local ceph.conf as a client...[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:31:55,812269662-07:00] STAGE: Configure Ceph pool...[0m
# ./benchmarks/bench-rados:257 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_max_backfills 32
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:31:55,833889684-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:31:55,836841356-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '6d1996f6-383e-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'config', 'set', 'osd', 'osd_max_backfills', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 6d1996f6-383e-11ec-b51d-53e6e728d2d3 -- ceph config set osd osd_max_backfills 32
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:258 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_active 32
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:32:00,199581832-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:32:00,202539375-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '6d1996f6-383e-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_active', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 6d1996f6-383e-11ec-b51d-53e6e728d2d3 -- ceph config set osd osd_recovery_max_active 32
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:259 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_single_start 8
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:32:04,332428060-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:32:04,335329037-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '6d1996f6-383e-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_single_start', '8'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 6d1996f6-383e-11ec-b51d-53e6e728d2d3 -- ceph config set osd osd_recovery_max_single_start 8
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:260 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_op_priority 63
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:32:08,407289148-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:32:08,410441148-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '6d1996f6-383e-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_op_priority', '63'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 6d1996f6-383e-11ec-b51d-53e6e728d2d3 -- ceph config set osd osd_recovery_op_priority 63
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:262 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config show-with-defaults osd.0
# ./benchmarks/bench-rados:263 - configure_ceph_pool() > grep 'osd_max_backfills\|osd_recovery'
# ./benchmarks/bench-rados:263 - configure_ceph_pool() > column -t -s ' '
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
osd_max_backfills                        32         mon
osd_recovery_cost                        20971520   default
osd_recovery_delay_start                 0.000000   default
osd_recovery_max_active                  32         mon
osd_recovery_max_active_hdd              3          default
osd_recovery_max_active_ssd              10         default
osd_recovery_max_chunk                   8388608    default
osd_recovery_max_omap_entries_per_chunk  8096       default
osd_recovery_max_single_start            8          mon
osd_recovery_op_priority                 63         mon
osd_recovery_op_warn_multiple            16         default
osd_recovery_priority                    5          default
osd_recovery_retry_interval              30.000000  default
osd_recovery_sleep                       0.000000   default
osd_recovery_sleep_hdd                   0.100000   default
osd_recovery_sleep_hybrid                0.025000   default
osd_recovery_sleep_ssd                   0.000000   default
# ./benchmarks/bench-rados:265 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool create bench_rados 128 128 replicated --size 1 --pg-num-min 128
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:32:16,374112438-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:32:16,377124194-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '6d1996f6-383e-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'create', 'bench_rados', '128', '128', 'replicated', '--size', '1', '--pg-num-min', '128'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 6d1996f6-383e-11ec-b51d-53e6e728d2d3 -- ceph osd pool create bench_rados 128 128 replicated --size 1 --pg-num-min 128
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
pool 'bench_rados' created
# ./benchmarks/bench-rados:267 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados min_size 1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:32:20,624952215-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:32:20,627908235-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '6d1996f6-383e-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'min_size', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 6d1996f6-383e-11ec-b51d-53e6e728d2d3 -- ceph osd pool set bench_rados min_size 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 min_size to 1
# ./benchmarks/bench-rados:268 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados pg_autoscale_mode off
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:32:24,908214251-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:32:24,911188526-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '6d1996f6-383e-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'pg_autoscale_mode', 'off'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 6d1996f6-383e-11ec-b51d-53e6e728d2d3 -- ceph osd pool set bench_rados pg_autoscale_mode off
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 pg_autoscale_mode to off
# ./benchmarks/bench-rados:269 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool application enable bench_rados benchmark
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:32:29,630587270-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:32:29,633889813-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '6d1996f6-383e-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'application', 'enable', 'bench_rados', 'benchmark'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 6d1996f6-383e-11ec-b51d-53e6e728d2d3 -- ceph osd pool application enable bench_rados benchmark
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
enabled application 'benchmark' on pool 'bench_rados'
# ./benchmarks/bench-rados:270 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados noscrub 1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:32:33,963984236-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:32:33,967126006-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '6d1996f6-383e-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'noscrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 6d1996f6-383e-11ec-b51d-53e6e728d2d3 -- ceph osd pool set bench_rados noscrub 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 noscrub to 1
# ./benchmarks/bench-rados:271 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados nodeep-scrub 1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:32:38,536811372-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:32:38,540090060-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '6d1996f6-383e-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'nodeep-scrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 6d1996f6-383e-11ec-b51d-53e6e728d2d3 -- ceph osd pool set bench_rados nodeep-scrub 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 nodeep-scrub to 1
# ./benchmarks/bench-rados:272 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool ls detail
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:32:43,393212445-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:32:43,396389502-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '6d1996f6-383e-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'ls', 'detail'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 6d1996f6-383e-11ec-b51d-53e6e728d2d3 -- ceph osd pool ls detail
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
pool 1 'device_health_metrics' replicated size 1 min_size 1 crush_rule 0 object_hash rjenkins pg_num 64 pgp_num 64 autoscale_mode on last_change 18 lfor 0/0/16 flags hashpspool stripe_width 0 pg_num_min 1 application mgr_devicehealth
pool 2 'bench_rados' replicated size 1 min_size 1 crush_rule 0 object_hash rjenkins pg_num 128 pgp_num 128 autoscale_mode off last_change 20 flags hashpspool,noscrub,nodeep-scrub stripe_width 0 pg_num_min 128 application benchmark

# ./benchmarks/bench-rados:273 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd df tree
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:32:47,221665051-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:32:47,224686705-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '6d1996f6-383e-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'df', 'tree'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 6d1996f6-383e-11ec-b51d-53e6e728d2d3 -- ceph osd df tree
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
ID  CLASS  WEIGHT   REWEIGHT  SIZE     RAW USE  DATA     OMAP  META     AVAIL    %USE  VAR   PGS  STATUS  TYPE NAME    
-1         0.39059         -  400 GiB  5.7 MiB  168 KiB   0 B  5.6 MiB  400 GiB  0.00  1.00    -          root default 
-3         0.39059         -  400 GiB  5.7 MiB  168 KiB   0 B  5.6 MiB  400 GiB  0.00  1.00    -              host sm1 
 0    ssd  0.39059   1.00000  400 GiB  5.7 MiB  168 KiB   0 B  5.6 MiB  400 GiB  0.00  1.00  192      up          osd.0
                       TOTAL  400 GiB  5.7 MiB  168 KiB   0 B  5.6 MiB  400 GiB  0.00                                  
MIN/MAX VAR: 1.00/1.00  STDDEV: 0


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:32:51,188865343-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:33:15,207832950-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:33:24,091365729-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:33:32,905276792-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:33:41,954715956-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:33:41,962023637-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:33:50,971845066-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:33:50,979157797-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:33:59,912601719-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:33:59,919718400-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:34:09,071466413-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:34:09,078871829-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:34:17,993629023-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:34:18,000998962-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:34:18,006568988-07:00] INFO: > The cluster is idle now.[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:34:18,010049888-07:00] STAGE: Start benchmarking of mode write ...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:34:18,016225316-07:00] INFO: > Start system activity collection services[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:34:18,021286954-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:341 - rados_bench() > rm --force /mnt/sda8/ceph-research/scripts/output/sys_activity_16KB_write.dat.rados_bench_host.3
# ./benchmarks/bench-rados:342 - rados_bench() > sar_pid_rados_bench_host=819605
# ./benchmarks/bench-rados:342 - rados_bench() > S_TIME_FORMAT=ISO
# ./benchmarks/bench-rados:342 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/output/sys_activity_16KB_write.dat.rados_bench_host.3 2

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:34:18,027831798-07:00] INFO: >> for OSD host[0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:34:18,036876722-07:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.5 bash
10.10.2.5: b'277841\n'
[1] 15:34:19 [SUCCESS] ljishen@10.10.2.5
277841

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:34:19,158829705-07:00] INFO: > Run rados bench[0m
# ./benchmarks/bench-rados:370 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/output/rados_bench_16KB_write.log.3
## ./benchmarks/bench-rados:361 - rados_bench() > count_cpus 32-63,96-127
# ./benchmarks/bench-rados:361 - rados_bench() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 60 write --pool bench_rados -b 16384 -O 16384 --concurrent-ios 128 --show-time --write-object --write-omap --write-xattr --no-cleanup
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:34:19,178766495-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:34:19,181720111-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '6d1996f6-383e-11ec-b51d-53e6e728d2d3', '--', 'rados', 'bench', '60', 'write', '--pool', 'bench_rados', '-b', '16384', '-O', '16384', '--concurrent-ios', '128', '--show-time', '--write-object', '--write-omap', '--write-xattr', '--no-cleanup'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 6d1996f6-383e-11ec-b51d-53e6e728d2d3 -- rados bench 60 write --pool bench_rados -b 16384 -O 16384 --concurrent-ios 128 --show-time --write-object --write-omap --write-xattr --no-cleanup
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
hints = 1
2021-10-28T22:34:22.278270+0000 Maintaining 128 concurrent writes of 16384 bytes to objects of size 16384 for up to 60 seconds or 0 objects
2021-10-28T22:34:22.278281+0000 Object prefix: benchmark_data_node-2.bluefield2.ucsc-cmps10_7
2021-10-28T22:34:22.279316+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-28T22:34:22.279316+0000     0       0         0         0         0         0           -           0
2021-10-28T22:34:23.279463+0000     1     128       960       832    12.999        13    0.127175    0.140884
2021-10-28T22:34:24.279579+0000     2     127      1779      1652    12.905   12.8125    0.109891    0.153097
2021-10-28T22:34:25.279644+0000     3     128      2710      2582   13.4467   14.5312    0.137083    0.141387
2021-10-28T22:34:26.279713+0000     4     128      3841      3713   14.5027   17.6719    0.119881    0.136294
2021-10-28T22:34:27.279780+0000     5     128      4865      4737   14.8019        16     0.13083    0.133704
2021-10-28T22:34:28.279846+0000     6     128      5889      5761   15.0014        16      0.1285    0.132153
2021-10-28T22:34:29.279919+0000     7     128      6654      6526   14.5658   11.9531    0.163498    0.134084
2021-10-28T22:34:30.279987+0000     8     128      7678      7550    14.745        16    0.124547    0.134808
2021-10-28T22:34:31.280064+0000     9     128      8702      8574   14.8843        16    0.128505     0.13396
2021-10-28T22:34:32.280150+0000    10     128      9750      9622   15.0332    16.375     0.12516    0.132402
2021-10-28T22:34:33.280214+0000    11     128     10750     10622   15.0869    15.625    0.103388    0.132092
2021-10-28T22:34:34.280279+0000    12     128     11774     11646   15.1629        16    0.164215    0.131412
2021-10-28T22:34:35.280345+0000    13     128     12670     12542   15.0734        14    0.304978    0.132053
2021-10-28T22:34:36.280418+0000    14     128     13590     13462   15.0234    14.375    0.130319     0.13244
2021-10-28T22:34:37.280482+0000    15     128     14614     14486   15.0885        16    0.128596    0.131997
2021-10-28T22:34:38.280546+0000    16     128     15510     15382   15.0204        14    0.196197    0.132412
2021-10-28T22:34:39.280618+0000    17     128     16534     16406   15.0779        16     0.17898    0.131718
2021-10-28T22:34:40.280682+0000    18     128     17662     17534   15.2194    17.625    0.118651    0.131252
2021-10-28T22:34:41.280759+0000    19     128     18430     18302   15.0499        12    0.120831    0.132651
2021-10-28T22:34:42.280828+0000 min lat: 0.0635691 max lat: 0.45016 avg lat: 0.131956
2021-10-28T22:34:42.280828+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-28T22:34:42.280828+0000    20     128     19478     19350   15.1161    16.375    0.120274    0.131956
2021-10-28T22:34:43.280902+0000    21     128     20350     20222    15.045    13.625    0.136719    0.132647
2021-10-28T22:34:44.280974+0000    22     128     21398     21270   15.1054    16.375    0.167194    0.132087
2021-10-28T22:34:45.281037+0000    23     128     22422     22294   15.1443        16    0.143289    0.131576
2021-10-28T22:34:46.281105+0000    24     128     23190     23062   15.0132        12    0.119074    0.132727
2021-10-28T22:34:47.281174+0000    25     128     24214     24086   15.0527        16    0.127034    0.132496
2021-10-28T22:34:48.281238+0000    26     128     25238     25110   15.0891        16    0.136327    0.132262
2021-10-28T22:34:49.281303+0000    27     128     26134     26006   15.0487        14    0.121843    0.132366
2021-10-28T22:34:50.281367+0000    28     128     27158     27030   15.0826        16    0.121891     0.13208
2021-10-28T22:34:51.281435+0000    29     128     28030     27902   15.0323    13.625     0.15001    0.132854
2021-10-28T22:34:52.281497+0000    30     128     29078     28950   15.0771    16.375    0.119117    0.132236
2021-10-28T22:34:53.281561+0000    31     128     30078     29950   15.0947    15.625    0.118796    0.132297
2021-10-28T22:34:54.281633+0000    32     128     30998     30870   15.0722    14.375    0.132721    0.132047
2021-10-28T22:34:55.281706+0000    33     128     31510     31382   14.8579         8    0.258322    0.133799
2021-10-28T22:34:56.281771+0000    34     128     31870     31742   14.5863     5.625    0.402588     0.13661
2021-10-28T22:34:57.281833+0000    35     128     32254     32126    14.341         6    0.249263    0.138981
2021-10-28T22:34:58.281895+0000    36     128     32662     32534   14.1197     6.375    0.337428    0.141069
2021-10-28T22:34:59.281962+0000    37     128     33150     33022   13.9441     7.625    0.210308    0.142998
2021-10-28T22:35:00.282025+0000    38     128     33430     33302   13.6923     4.375    0.314613    0.145053
2021-10-28T22:35:01.282091+0000    39     128     33814     33686   13.4951         6    0.416872    0.147261
2021-10-28T22:35:02.282154+0000 min lat: 0.0635691 max lat: 0.563514 avg lat: 0.149928
2021-10-28T22:35:02.282154+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-28T22:35:02.282154+0000    40     128     34198     34070   13.3077         6    0.469958    0.149928
2021-10-28T22:35:03.282224+0000    41     128     34582     34454   13.1294         6    0.278335    0.151842
2021-10-28T22:35:04.282289+0000    42     128     35070     34942   12.9984     7.625    0.289662    0.153691
2021-10-28T22:35:05.282353+0000    43     128     35454     35326   12.8356         6    0.316685    0.155684
2021-10-28T22:35:06.282417+0000    44     128     35838     35710   12.6802         6    0.274161    0.157334
2021-10-28T22:35:07.282482+0000    45     128     36246     36118   12.5401     6.375    0.251118    0.158858
2021-10-28T22:35:08.282549+0000    46     128     36630     36502   12.3979         6    0.336002    0.160912
2021-10-28T22:35:09.282622+0000    47     128     37014     36886   12.2618         6    0.241955    0.162661
2021-10-28T22:35:10.282687+0000    48     128     37398     37270   12.1313         6    0.305329    0.164195
2021-10-28T22:35:11.282801+0000    49     128     37782     37654   12.0062         6    0.277355    0.165962
2021-10-28T22:35:12.282908+0000    50     128     38166     38038    11.886         6    0.317227    0.167424
2021-10-28T22:35:13.282977+0000    51     128     38678     38550   11.8098         8    0.283869    0.168966
2021-10-28T22:35:14.283086+0000    52     128     39038     38910   11.6909     5.625    0.307747     0.17084
2021-10-28T22:35:15.283154+0000    53     128     39318     39190   11.5528     4.375    0.441551    0.172285
2021-10-28T22:35:16.283249+0000    54     128     39806     39678   11.4801     7.625     0.31975    0.173973
2021-10-28T22:35:17.283321+0000    55     128     40190     40062   11.3804         6    0.273363    0.175387
2021-10-28T22:35:18.283411+0000    56     128     40574     40446   11.2843         6    0.333863      0.1768
2021-10-28T22:35:19.283502+0000    57     128     40854     40726   11.1631     4.375    0.410698    0.178386
2021-10-28T22:35:20.283568+0000    58     128     41238     41110   11.0741         6    0.322509    0.180158
2021-10-28T22:35:21.283645+0000    59     128     41598     41470   10.9817     5.625    0.365599    0.181741
2021-10-28T22:35:22.283755+0000 min lat: 0.0635691 max lat: 0.563514 avg lat: 0.181792
2021-10-28T22:35:22.283755+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-28T22:35:22.283755+0000    60     128     42277     42149   10.9755   10.6094    0.121329    0.181792
2021-10-28T22:35:23.283882+0000 Total time run:         60.1944
Total writes made:      42277
Write size:             16384
Object size:            16384
Bandwidth (MB/sec):     10.9741
Stddev Bandwidth:       4.64209
Max bandwidth (MB/sec): 17.6719
Min bandwidth (MB/sec): 4.375
Average IOPS:           702
Stddev IOPS:            297.094
Max IOPS:               1131
Min IOPS:               280
Average Latency(s):     0.18212
Stddev Latency(s):      0.0979541
Max latency(s):         0.563514
Min latency(s):         0.0635691

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:35:24,000168233-07:00] INFO: > Stop system activity collection services[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:35:24,005609787-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:391 - rados_bench() > kill -INT 819605

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:35:24,010935544-07:00] INFO: >> for OSD host[0m
# ./benchmarks/bench-rados:396 - rados_bench() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.5 kill -INT 277841
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:35:24,018631757-07:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.5 kill -INT 277841
[1] 15:35:25 [SUCCESS] ljishen@10.10.2.5

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:35:25,180092795-07:00] INFO: >>> Copy system activity file from OSD host[0m
## ./benchmarks/bench-rados:403 - rados_bench() > basename -- /tmp/bench-rados/sys_activity_16KB_write.dat.osd_host.3
# ./benchmarks/bench-rados:403 - rados_bench() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.5:/tmp/bench-rados/sys_activity_16KB_write.dat.osd_host.3 /mnt/sda8/ceph-research/scripts/output/sys_activity_16KB_write.dat.osd_host.3


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:35:26,294568439-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:35:50,212550937-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 42.28k objects, 661 MiB
    usage:   2.0 GiB used, 398 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:35:50,219950361-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:35:59,120007426-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 42.28k objects, 661 MiB
    usage:   2.0 GiB used, 398 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:35:59,126972030-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:36:08,119492003-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 42.28k objects, 661 MiB
    usage:   2.0 GiB used, 398 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:36:08,126589267-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:36:17,003135373-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 42.28k objects, 661 MiB
    usage:   2.0 GiB used, 398 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:36:17,010726598-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:36:26,036108065-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 42.28k objects, 661 MiB
    usage:   2.0 GiB used, 398 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:36:26,043405647-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:36:26,048791767-07:00] INFO: > The cluster is idle now.[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:36:26,052563956-07:00] STAGE: Start benchmarking of mode seq ...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:36:26,058959077-07:00] INFO: > Start system activity collection services[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:36:26,063983927-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:341 - rados_bench() > rm --force /mnt/sda8/ceph-research/scripts/output/sys_activity_16KB_seq.dat.rados_bench_host.3
# ./benchmarks/bench-rados:342 - rados_bench() > sar_pid_rados_bench_host=820965
# ./benchmarks/bench-rados:342 - rados_bench() > S_TIME_FORMAT=ISO
# ./benchmarks/bench-rados:342 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/output/sys_activity_16KB_seq.dat.rados_bench_host.3 2

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:36:26,070761799-07:00] INFO: >> for OSD host[0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:36:26,079379810-07:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.5 bash
10.10.2.5: b'278530\n'
[1] 15:36:27 [SUCCESS] ljishen@10.10.2.5
278530

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:36:27,223861273-07:00] INFO: > Run rados bench[0m
# ./benchmarks/bench-rados:379 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/output/rados_bench_16KB_seq.log.3
## ./benchmarks/bench-rados:376 - rados_bench() > count_cpus 32-63,96-127
# ./benchmarks/bench-rados:376 - rados_bench() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 128 --show-time
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:36:27,243458053-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:36:27,246177117-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '6d1996f6-383e-11ec-b51d-53e6e728d2d3', '--', 'rados', 'bench', '99999999', 'seq', '--pool', 'bench_rados', '--concurrent-ios', '128', '--show-time'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 6d1996f6-383e-11ec-b51d-53e6e728d2d3 -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 128 --show-time
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
hints = 1
2021-10-28T22:36:30.235787+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-28T22:36:30.235787+0000     0       0         0         0         0         0           -           0
2021-10-28T22:36:31.235903+0000     1     128      3778      3650   57.0206   57.0312   0.0406087   0.0341376
2021-10-28T22:36:32.235989+0000     2     128      6035      5907   46.1421   35.2656   0.0333602   0.0430464
2021-10-28T22:36:33.236077+0000     3     128      9145      9017   46.9579   48.5938   0.0306214   0.0421117
2021-10-28T22:36:34.236188+0000     4     128     12334     12206   47.6741   49.8281    0.134209   0.0413891
2021-10-28T22:36:35.236263+0000     5     128     15234     15106   47.2011   45.3125   0.0366991   0.0420186
2021-10-28T22:36:36.236345+0000     6     128     18709     18581   48.3829   54.2969  0.00155995   0.0410381
2021-10-28T22:36:37.236433+0000     7     128     21042     20914   46.6783   36.4531   0.0812878   0.0426323
2021-10-28T22:36:38.236516+0000     8     128     24181     24053   46.9738   49.0469   0.0791873   0.0424404
2021-10-28T22:36:39.236592+0000     9     127     27413     27286   47.3669   50.5156   0.0721343   0.0421009
2021-10-28T22:36:40.236662+0000    10     128     30056     29928   46.7581   41.2812   0.0129133   0.0426806
2021-10-28T22:36:41.236732+0000    11     128     33160     33032   46.9161      48.5   0.0114771   0.0424424
2021-10-28T22:36:42.236801+0000    12     128     35927     35799   46.6091   43.2344   0.0402305   0.0428277
2021-10-28T22:36:43.236880+0000    13     128     38766     38638   46.4357   44.3594   0.0229984   0.0430146
2021-10-28T22:36:44.236994+0000    14     128     41567     41439   46.2447   43.7656   0.0510673   0.0430776
2021-10-28T22:36:45.237130+0000 Total time run:       14.3881
Total reads made:     42277
Read size:            16384
Object size:          16384
Bandwidth (MB/sec):   45.9113
Average IOPS:         2938
Stddev IOPS:          393.486
Max IOPS:             3650
Min IOPS:             2257
Average Latency(s):   0.04331
Max latency(s):       0.363318
Min latency(s):       0.000569033

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:36:45,880922638-07:00] INFO: > Stop system activity collection services[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:36:45,886553729-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:391 - rados_bench() > kill -INT 820965

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:36:45,892133194-07:00] INFO: >> for OSD host[0m
# ./benchmarks/bench-rados:396 - rados_bench() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.5 kill -INT 278530
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:36:45,899830038-07:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.5 kill -INT 278530
[1] 15:36:46 [SUCCESS] ljishen@10.10.2.5

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:36:46,996407208-07:00] INFO: >>> Copy system activity file from OSD host[0m
## ./benchmarks/bench-rados:403 - rados_bench() > basename -- /tmp/bench-rados/sys_activity_16KB_seq.dat.osd_host.3
# ./benchmarks/bench-rados:403 - rados_bench() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.5:/tmp/bench-rados/sys_activity_16KB_seq.dat.osd_host.3 /mnt/sda8/ceph-research/scripts/output/sys_activity_16KB_seq.dat.osd_host.3


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:36:48,049524348-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:37:11,925649597-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 42.28k objects, 661 MiB
    usage:   2.0 GiB used, 398 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:37:11,932943261-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:37:20,910953064-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 42.28k objects, 661 MiB
    usage:   2.0 GiB used, 398 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:37:20,917756784-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:37:30,074990742-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 42.28k objects, 661 MiB
    usage:   2.0 GiB used, 398 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:37:30,082136938-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:37:39,132612716-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 42.28k objects, 661 MiB
    usage:   2.0 GiB used, 398 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:37:39,140584328-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:37:48,174363158-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 42.28k objects, 661 MiB
    usage:   2.0 GiB used, 398 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:37:48,181494205-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:37:48,186951158-07:00] INFO: > The cluster is idle now.[0m


[1;7;39;49m[2021-10-28T15:37:48,190361324-07:00][RUNNING][ROUND 1/3/21] object_size=64KB[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:37:48,193380814-07:00] STAGE: Launch a Ceph cluster (CEPH_HOST: 10.10.2.1, OSD_HOST: 10.10.2.5) ...[0m
# ./benchmarks/bench-rados:183 - launch_ceph_cluster() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.1 bash -euo pipefail
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:37:48,202785717-07:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.1 bash -euo pipefail
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T15:37:48,610965300-07:00] INFO: > Remove existing clusters\x1b[0m\n'
10.10.2.1: b'## bash:17 -  > basename -- /var/lib/ceph/6d1996f6-383e-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'# bash:17 -  > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm --host 10.10.2.1 --host 10.10.2.5 rm-cluster --force --fsid 6d1996f6-383e-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T15:37:48,621912007-07:00] DEBUG: HOST_PARAMS: ['--host', '10.10.2.1', '--host', '10.10.2.5']\x1b[0m\n"
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T15:37:48,625698966-07:00] DEBUG: CEPHADM_PARAMS: ['rm-cluster', '--force', '--fsid', '6d1996f6-383e-11ec-b51d-53e6e728d2d3']\x1b[0m\n"
10.10.2.1: b"# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:63 -  > /tmp/bench-rados/ceph-research/scripts/parallel-ssh --host 10.10.2.1 --host 10.10.2.5 'sudo python3 -u - rm-cluster --force --fsid 6d1996f6-383e-11ec-b51d-53e6e728d2d3'\n"
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T15:37:48,633977749-07:00] DEBUG: command from stdin\x1b[0m\n'
10.10.2.1: b"# /tmp/bench-rados/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host 10.10.2.1 --host 10.10.2.5 'sudo python3 -u - rm-cluster --force --fsid 6d1996f6-383e-11ec-b51d-53e6e728d2d3'\n"
10.10.2.1: b'[1] 15:37:54 [SUCCESS] 10.10.2.1\n[2] 15:37:56 [SUCCESS] 10.10.2.5\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T15:37:56,349859783-07:00] INFO: > Deploy a new cluster\x1b[0m\n'
10.10.2.1: b'# bash:24 -  > /tmp/bench-rados/ceph-research/scripts/ceph-deploy -m 10.10.2.1 -o sm1/10.10.2.5:/dev/nvme0n1\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T15:37:56,362145107-07:00] STAGE: Verify passwordless SSH to hosts, passwordless sudo via ljishen, and the existence of devices on hosts...\x1b[0m\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T15:37:56,367143433-07:00] INFO: > Check the host for the monitor\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:181 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@10.10.2.1 sudo --non-interactive --validate\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T15:37:56,515156419-07:00] INFO: > Check hosts for OSDs\x1b[0m\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T15:37:56,519713836-07:00] INFO: >> Check host 'sm1'\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:200 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 sudo --non-interactive --validate\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T15:37:57,603112105-07:00] INFO: >>> Check device '/dev/nvme0n1'\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:215 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 test -b /dev/nvme0n1\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:226 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 lsblk --nodeps --noheadings --output TYPE /dev/nvme0n1\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:227 - verify_parameters() > grep --word-regexp disk\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T15:37:59,743926171-07:00] STAGE: Reset devices on hosts...\x1b[0m\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T15:37:59,748977196-07:00] INFO: > Resetting device '/dev/nvme0n1' on host 'sm1'\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:288 - reset_host_devices() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 sudo sh\n'
10.10.2.1: b'  Removing ceph--be94cdf8--15c4--4014--be0c--14823681cc16-osd--block--c52db3e5--ddae--4c65--9dbc--16ff26e3b47f (252:0)\n'
10.10.2.1: b'  Archiving volume group "ceph-be94cdf8-15c4-4014-be0c-14823681cc16" metadata (seqno 5).\n'
10.10.2.1: b'  Releasing logical volume "osd-block-c52db3e5-ddae-4c65-9dbc-16ff26e3b47f"\n'
10.10.2.1: b'  Creating volume group backup "/etc/lvm/backup/ceph-be94cdf8-15c4-4014-be0c-14823681cc16" (seqno 6).\n'
10.10.2.1: b'  Logical volume "osd-block-c52db3e5-ddae-4c65-9dbc-16ff26e3b47f" successfully removed\n'
10.10.2.1: b'  Removing physical volume "/dev/nvme0n1" from volume group "ceph-be94cdf8-15c4-4014-be0c-14823681cc16"\n'
10.10.2.1: b'  Volume group "ceph-be94cdf8-15c4-4014-be0c-14823681cc16" successfully removed\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T15:38:02,085479849-07:00] STAGE: Bootstrap a new cluster...\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:317 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.1 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T15:38:02,096007600-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T15:38:02,099694380-07:00] DEBUG: CEPHADM_PARAMS: ['bootstrap', '--config', '/tmp/bench-rados/ceph-research/scripts/ceph.conf', '--mon-ip', '10.10.2.1', '--output-dir', '/tmp/bench-rados/deployment_data_root/etc/ceph', '--ssh-user', 'ljishen', '--skip-dashboard', '--no-minimize-config', '--allow-overwrite', '--allow-fqdn-hostname', '--orphan-initial-daemons', '--skip-monitoring-stack', '--single-host-defaults']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.1 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults\n'
10.10.2.1: b'Verifying podman|docker is present...\nVerifying lvm2 is present...\n'
10.10.2.1: b'Verifying time synchronization is in place...\n'
10.10.2.1: b'Unit ntp.service is enabled and running\n'
10.10.2.1: b'Repeating the final host check...\n'
10.10.2.1: b'podman|docker (/usr/bin/docker) is present\n'
10.10.2.1: b'systemctl is present\n'
10.10.2.1: b'lvcreate is present\n'
10.10.2.1: b'Unit ntp.service is enabled and running\n'
10.10.2.1: b'Host looks OK\nCluster fsid: b5a3cfa8-383f-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Verifying IP 10.10.2.1 port 3300 ...\n'
10.10.2.1: b'Verifying IP 10.10.2.1 port 6789 ...\n'
10.10.2.1: b'Mon IP `10.10.2.1` is in CIDR network `10.10.2.0/24`\n- internal network (--cluster-network) has not been provided, OSD replication will default to the public_network\n'
10.10.2.1: b'Adjusting default settings to suit single-host cluster...\n'
10.10.2.1: b'Pulling container image quay.io/ceph/ceph:v16...\n'
10.10.2.1: b'Ceph version: ceph version 16.2.6 (ee28fb57e47e9f88813e24bbf4c14496ca299d31) pacific (stable)\nExtracting ceph user uid/gid from container image...\n'
10.10.2.1: b'Creating initial keys...\n'
10.10.2.1: b'Creating initial monmap...\n'
10.10.2.1: b'Creating mon...\n'
10.10.2.1: b'Waiting for mon to start...\n'
10.10.2.1: b'Waiting for mon...\n'
10.10.2.1: b'mon is available\n'
10.10.2.1: b'Setting mon public_network to 10.10.2.0/24\n'
10.10.2.1: b'Wrote config to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf\n'
10.10.2.1: b'Wrote keyring to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring\n'
10.10.2.1: b'Creating mgr...\n'
10.10.2.1: b'Verifying port 9283 ...\n'
10.10.2.1: b'Waiting for mgr to start...\nWaiting for mgr...\n'
10.10.2.1: b'mgr not available, waiting (1/15)...\n'
10.10.2.1: b'mgr not available, waiting (2/15)...\n'
10.10.2.1: b'mgr is available\n'
10.10.2.1: b'Enabling cephadm module...\n'
10.10.2.1: b'Waiting for the mgr to restart...\n'
10.10.2.1: b'Waiting for mgr epoch 5...\n'
10.10.2.1: b'mgr epoch 5 is available\nSetting orchestrator backend to cephadm...\n'
10.10.2.1: b'Generating ssh key...\n'
10.10.2.1: b'Wrote public SSH key to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub\nAdding key to ljishen@localhost authorized_keys...\n'
10.10.2.1: b'Adding host node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us...\n'
10.10.2.1: b'Deploying unmanaged mon service...\n'
10.10.2.1: b'Deploying unmanaged mgr service...\n'
10.10.2.1: b'You can access the Ceph CLI with:\n\n\tsudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --fsid b5a3cfa8-383f-11ec-b51d-53e6e728d2d3 -c /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf -k /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring\n\n'
10.10.2.1: b'Please consider enabling telemetry to help improve Ceph:\n\n\tceph telemetry on\n\nFor more information see:\n\n\thttps://docs.ceph.com/docs/pacific/mgr/telemetry/\n\n'
10.10.2.1: b'Bootstrap complete.\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T15:39:02,426598806-07:00] INFO: Sleep for 20 seconds\x1b[0m\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T15:39:22,433720801-07:00] INFO: > Set the default number of monitor daemons to 1\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:337 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T15:39:22,443585866-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T15:39:22,447081737-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mon', '1']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1\n'
10.10.2.1: b'Inferring fsid b5a3cfa8-383f-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/b5a3cfa8-383f-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b'Scheduled mon update...\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T15:39:31,522910886-07:00] INFO: > Set the default number of manager daemons to 1\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:342 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T15:39:31,532897058-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T15:39:31,536860910-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mgr', '1']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1\n'
10.10.2.1: b'Inferring fsid b5a3cfa8-383f-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/b5a3cfa8-383f-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b'Scheduled mgr update...\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T15:39:40,619300837-07:00] STAGE: Add hosts to the cluster...\x1b[0m\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T15:39:40,625516632-07:00] INFO: > Adding host 'sm1'\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:361 - add_hosts() > ssh-copy-id -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -f -i /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub ljishen@sm1\n'
10.10.2.1: b'/usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: "/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub"\n'
10.10.2.1: b'\nNumber of key(s) added: 1\n\nNow try logging into the machine, with:   "ssh -o \'GlobalKnownHostsFile=/dev/null\' -o \'LogLevel=ERROR\' -o \'PasswordAuthentication=no\' -o \'StrictHostKeyChecking=no\' -o \'UserKnownHostsFile=/dev/null\' \'ljishen@sm1\'"\nand check to make sure that only the key(s) you wanted were added.\n\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:367 - add_hosts() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add sm1 10.10.2.5\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T15:39:41,794447730-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T15:39:41,798022089-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'host', 'add', 'sm1', '10.10.2.5']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add sm1 10.10.2.5\n'
10.10.2.1: b'Inferring fsid b5a3cfa8-383f-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/b5a3cfa8-383f-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b"Added host 'sm1' with addr '10.10.2.5'\n"
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T15:39:52,571044904-07:00] INFO: Sleep for 20 seconds\x1b[0m\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T15:40:12,576714641-07:00] STAGE: Add OSDs to the cluster...\x1b[0m\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T15:40:12,583346197-07:00] INFO: > Adding devices ['/dev/nvme0n1'] on host 'sm1' as OSDs\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:388 - add_host_devices_as_osds() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd sm1:/dev/nvme0n1\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T15:40:12,592971331-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T15:40:12,596664934-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'daemon', 'add', 'osd', 'sm1:/dev/nvme0n1']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd sm1:/dev/nvme0n1\n'
10.10.2.1: b'Inferring fsid b5a3cfa8-383f-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/b5a3cfa8-383f-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b"Created osd(s) 0 on host 'sm1'\n"
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T15:40:37,424013861-07:00] INFO: > Sleep for 20 seconds\x1b[0m\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T15:40:57,429454054-07:00] STAGE: Show Cluster Status\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:408 - show_status() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T15:40:57,439277490-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T15:40:57,443005388-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', '--status']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status\n'
10.10.2.1: b'Inferring fsid b5a3cfa8-383f-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/b5a3cfa8-383f-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b'  cluster:\n    id:     b5a3cfa8-383f-11ec-b51d-53e6e728d2d3\n    health: HEALTH_WARN\n            1 pool(s) have no replicas configured\n \n  services:\n    mon: 1 daemons, quorum node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us (age 2m)\n    mgr: node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us.wgssnc(active, since 2m)\n    osd: 1 osds: 1 up (since 19s), 1 in (since 40s)\n \n  data:\n    pools:   1 pools, 1 pgs\n    objects: 0 objects, 0 B\n    usage:   4.7 MiB used, 400 GiB / 400 GiB avail\n    pgs:     1 active+clean\n \n'
[1] 15:41:05 [SUCCESS] ljishen@10.10.2.1

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:37:48,610965300-07:00] INFO: > Remove existing clusters[0m
## bash:17 -  > basename -- /var/lib/ceph/6d1996f6-383e-11ec-b51d-53e6e728d2d3
# bash:17 -  > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm --host 10.10.2.1 --host 10.10.2.5 rm-cluster --force --fsid 6d1996f6-383e-11ec-b51d-53e6e728d2d3
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:37:48,621912007-07:00] DEBUG: HOST_PARAMS: ['--host', '10.10.2.1', '--host', '10.10.2.5'][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:37:48,625698966-07:00] DEBUG: CEPHADM_PARAMS: ['rm-cluster', '--force', '--fsid', '6d1996f6-383e-11ec-b51d-53e6e728d2d3'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:63 -  > /tmp/bench-rados/ceph-research/scripts/parallel-ssh --host 10.10.2.1 --host 10.10.2.5 'sudo python3 -u - rm-cluster --force --fsid 6d1996f6-383e-11ec-b51d-53e6e728d2d3'
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:37:48,633977749-07:00] DEBUG: command from stdin[0m
# /tmp/bench-rados/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host 10.10.2.1 --host 10.10.2.5 'sudo python3 -u - rm-cluster --force --fsid 6d1996f6-383e-11ec-b51d-53e6e728d2d3'
[1] 15:37:54 [SUCCESS] 10.10.2.1
[2] 15:37:56 [SUCCESS] 10.10.2.5

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:37:56,349859783-07:00] INFO: > Deploy a new cluster[0m
# bash:24 -  > /tmp/bench-rados/ceph-research/scripts/ceph-deploy -m 10.10.2.1 -o sm1/10.10.2.5:/dev/nvme0n1


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:37:56,362145107-07:00] STAGE: Verify passwordless SSH to hosts, passwordless sudo via ljishen, and the existence of devices on hosts...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:37:56,367143433-07:00] INFO: > Check the host for the monitor[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:181 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@10.10.2.1 sudo --non-interactive --validate

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:37:56,515156419-07:00] INFO: > Check hosts for OSDs[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:37:56,519713836-07:00] INFO: >> Check host 'sm1'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:200 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 sudo --non-interactive --validate

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:37:57,603112105-07:00] INFO: >>> Check device '/dev/nvme0n1'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:215 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 test -b /dev/nvme0n1
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:226 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 lsblk --nodeps --noheadings --output TYPE /dev/nvme0n1
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:227 - verify_parameters() > grep --word-regexp disk


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:37:59,743926171-07:00] STAGE: Reset devices on hosts...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:37:59,748977196-07:00] INFO: > Resetting device '/dev/nvme0n1' on host 'sm1'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:288 - reset_host_devices() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 sudo sh
  Removing ceph--be94cdf8--15c4--4014--be0c--14823681cc16-osd--block--c52db3e5--ddae--4c65--9dbc--16ff26e3b47f (252:0)
  Archiving volume group "ceph-be94cdf8-15c4-4014-be0c-14823681cc16" metadata (seqno 5).
  Releasing logical volume "osd-block-c52db3e5-ddae-4c65-9dbc-16ff26e3b47f"
  Creating volume group backup "/etc/lvm/backup/ceph-be94cdf8-15c4-4014-be0c-14823681cc16" (seqno 6).
  Logical volume "osd-block-c52db3e5-ddae-4c65-9dbc-16ff26e3b47f" successfully removed
  Removing physical volume "/dev/nvme0n1" from volume group "ceph-be94cdf8-15c4-4014-be0c-14823681cc16"
  Volume group "ceph-be94cdf8-15c4-4014-be0c-14823681cc16" successfully removed


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:38:02,085479849-07:00] STAGE: Bootstrap a new cluster...[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:317 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.1 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:38:02,096007600-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:38:02,099694380-07:00] DEBUG: CEPHADM_PARAMS: ['bootstrap', '--config', '/tmp/bench-rados/ceph-research/scripts/ceph.conf', '--mon-ip', '10.10.2.1', '--output-dir', '/tmp/bench-rados/deployment_data_root/etc/ceph', '--ssh-user', 'ljishen', '--skip-dashboard', '--no-minimize-config', '--allow-overwrite', '--allow-fqdn-hostname', '--orphan-initial-daemons', '--skip-monitoring-stack', '--single-host-defaults'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.1 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults
Verifying podman|docker is present...
Verifying lvm2 is present...
Verifying time synchronization is in place...
Unit ntp.service is enabled and running
Repeating the final host check...
podman|docker (/usr/bin/docker) is present
systemctl is present
lvcreate is present
Unit ntp.service is enabled and running
Host looks OK
Cluster fsid: b5a3cfa8-383f-11ec-b51d-53e6e728d2d3
Verifying IP 10.10.2.1 port 3300 ...
Verifying IP 10.10.2.1 port 6789 ...
Mon IP `10.10.2.1` is in CIDR network `10.10.2.0/24`
- internal network (--cluster-network) has not been provided, OSD replication will default to the public_network
Adjusting default settings to suit single-host cluster...
Pulling container image quay.io/ceph/ceph:v16...
Ceph version: ceph version 16.2.6 (ee28fb57e47e9f88813e24bbf4c14496ca299d31) pacific (stable)
Extracting ceph user uid/gid from container image...
Creating initial keys...
Creating initial monmap...
Creating mon...
Waiting for mon to start...
Waiting for mon...
mon is available
Setting mon public_network to 10.10.2.0/24
Wrote config to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf
Wrote keyring to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring
Creating mgr...
Verifying port 9283 ...
Waiting for mgr to start...
Waiting for mgr...
mgr not available, waiting (1/15)...
mgr not available, waiting (2/15)...
mgr is available
Enabling cephadm module...
Waiting for the mgr to restart...
Waiting for mgr epoch 5...
mgr epoch 5 is available
Setting orchestrator backend to cephadm...
Generating ssh key...
Wrote public SSH key to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub
Adding key to ljishen@localhost authorized_keys...
Adding host node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us...
Deploying unmanaged mon service...
Deploying unmanaged mgr service...
You can access the Ceph CLI with:

	sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --fsid b5a3cfa8-383f-11ec-b51d-53e6e728d2d3 -c /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf -k /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring

Please consider enabling telemetry to help improve Ceph:

	ceph telemetry on

For more information see:

	https://docs.ceph.com/docs/pacific/mgr/telemetry/

Bootstrap complete.

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:39:02,426598806-07:00] INFO: Sleep for 20 seconds[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:39:22,433720801-07:00] INFO: > Set the default number of monitor daemons to 1[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:337 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:39:22,443585866-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:39:22,447081737-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mon', '1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1
Inferring fsid b5a3cfa8-383f-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/b5a3cfa8-383f-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Scheduled mon update...

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:39:31,522910886-07:00] INFO: > Set the default number of manager daemons to 1[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:342 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:39:31,532897058-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:39:31,536860910-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mgr', '1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1
Inferring fsid b5a3cfa8-383f-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/b5a3cfa8-383f-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Scheduled mgr update...


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:39:40,619300837-07:00] STAGE: Add hosts to the cluster...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:39:40,625516632-07:00] INFO: > Adding host 'sm1'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:361 - add_hosts() > ssh-copy-id -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -f -i /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub ljishen@sm1
/usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: "/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub"

Number of key(s) added: 1

Now try logging into the machine, with:   "ssh -o 'GlobalKnownHostsFile=/dev/null' -o 'LogLevel=ERROR' -o 'PasswordAuthentication=no' -o 'StrictHostKeyChecking=no' -o 'UserKnownHostsFile=/dev/null' 'ljishen@sm1'"
and check to make sure that only the key(s) you wanted were added.

# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:367 - add_hosts() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add sm1 10.10.2.5
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:39:41,794447730-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:39:41,798022089-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'host', 'add', 'sm1', '10.10.2.5'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add sm1 10.10.2.5
Inferring fsid b5a3cfa8-383f-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/b5a3cfa8-383f-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Added host 'sm1' with addr '10.10.2.5'

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:39:52,571044904-07:00] INFO: Sleep for 20 seconds[0m


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:40:12,576714641-07:00] STAGE: Add OSDs to the cluster...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:40:12,583346197-07:00] INFO: > Adding devices ['/dev/nvme0n1'] on host 'sm1' as OSDs[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:388 - add_host_devices_as_osds() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd sm1:/dev/nvme0n1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:40:12,592971331-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:40:12,596664934-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'daemon', 'add', 'osd', 'sm1:/dev/nvme0n1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd sm1:/dev/nvme0n1
Inferring fsid b5a3cfa8-383f-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/b5a3cfa8-383f-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Created osd(s) 0 on host 'sm1'

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:40:37,424013861-07:00] INFO: > Sleep for 20 seconds[0m


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:40:57,429454054-07:00] STAGE: Show Cluster Status[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:408 - show_status() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:40:57,439277490-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:40:57,443005388-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', '--status'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status
Inferring fsid b5a3cfa8-383f-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/b5a3cfa8-383f-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
  cluster:
    id:     b5a3cfa8-383f-11ec-b51d-53e6e728d2d3
    health: HEALTH_WARN
            1 pool(s) have no replicas configured
 
  services:
    mon: 1 daemons, quorum node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us (age 2m)
    mgr: node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us.wgssnc(active, since 2m)
    osd: 1 osds: 1 up (since 19s), 1 in (since 40s)
 
  data:
    pools:   1 pools, 1 pgs
    objects: 0 objects, 0 B
    usage:   4.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     1 active+clean
 

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:41:05,978186532-07:00] INFO: > Copy configuration files to local[0m
# ./benchmarks/bench-rados:220 - launch_ceph_cluster() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.1 sudo chown --changes --recursive ljishen /tmp/bench-rados/deployment_data_root/etc/ceph
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:41:05,985926798-07:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.1 sudo chown --changes --recursive ljishen /tmp/bench-rados/deployment_data_root/etc/ceph
[1] 15:41:06 [SUCCESS] ljishen@10.10.2.1
# ./benchmarks/bench-rados:223 - launch_ceph_cluster() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.1:/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf
# ./benchmarks/bench-rados:226 - launch_ceph_cluster() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.1:/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:41:06,460892087-07:00] STAGE: Update local ceph.conf as a client...[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:41:06,464142351-07:00] STAGE: Configure Ceph pool...[0m
# ./benchmarks/bench-rados:257 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_max_backfills 32
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:41:06,485885033-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:41:06,488839341-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'b5a3cfa8-383f-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'config', 'set', 'osd', 'osd_max_backfills', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid b5a3cfa8-383f-11ec-b51d-53e6e728d2d3 -- ceph config set osd osd_max_backfills 32
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:258 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_active 32
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:41:10,568345310-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:41:10,571411748-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'b5a3cfa8-383f-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_active', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid b5a3cfa8-383f-11ec-b51d-53e6e728d2d3 -- ceph config set osd osd_recovery_max_active 32
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:259 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_single_start 8
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:41:14,686937815-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:41:14,689973986-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'b5a3cfa8-383f-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_single_start', '8'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid b5a3cfa8-383f-11ec-b51d-53e6e728d2d3 -- ceph config set osd osd_recovery_max_single_start 8
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:260 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_op_priority 63
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:41:18,521157016-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:41:18,524276394-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'b5a3cfa8-383f-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_op_priority', '63'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid b5a3cfa8-383f-11ec-b51d-53e6e728d2d3 -- ceph config set osd osd_recovery_op_priority 63
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:262 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config show-with-defaults osd.0
# ./benchmarks/bench-rados:263 - configure_ceph_pool() > grep 'osd_max_backfills\|osd_recovery'
# ./benchmarks/bench-rados:263 - configure_ceph_pool() > column -t -s ' '
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
osd_max_backfills                        32         mon
osd_recovery_cost                        20971520   default
osd_recovery_delay_start                 0.000000   default
osd_recovery_max_active                  32         mon
osd_recovery_max_active_hdd              3          default
osd_recovery_max_active_ssd              10         default
osd_recovery_max_chunk                   8388608    default
osd_recovery_max_omap_entries_per_chunk  8096       default
osd_recovery_max_single_start            8          mon
osd_recovery_op_priority                 3          default
osd_recovery_op_warn_multiple            16         default
osd_recovery_priority                    5          default
osd_recovery_retry_interval              30.000000  default
osd_recovery_sleep                       0.000000   default
osd_recovery_sleep_hdd                   0.100000   default
osd_recovery_sleep_hybrid                0.025000   default
osd_recovery_sleep_ssd                   0.000000   default
# ./benchmarks/bench-rados:265 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool create bench_rados 128 128 replicated --size 1 --pg-num-min 128
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:41:26,507758720-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:41:26,511038671-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'b5a3cfa8-383f-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'create', 'bench_rados', '128', '128', 'replicated', '--size', '1', '--pg-num-min', '128'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid b5a3cfa8-383f-11ec-b51d-53e6e728d2d3 -- ceph osd pool create bench_rados 128 128 replicated --size 1 --pg-num-min 128
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
pool 'bench_rados' created
# ./benchmarks/bench-rados:267 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados min_size 1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:41:31,230257447-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:41:31,233067532-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'b5a3cfa8-383f-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'min_size', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid b5a3cfa8-383f-11ec-b51d-53e6e728d2d3 -- ceph osd pool set bench_rados min_size 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 min_size to 1
# ./benchmarks/bench-rados:268 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados pg_autoscale_mode off
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:41:35,439329618-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:41:35,442158820-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'b5a3cfa8-383f-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'pg_autoscale_mode', 'off'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid b5a3cfa8-383f-11ec-b51d-53e6e728d2d3 -- ceph osd pool set bench_rados pg_autoscale_mode off
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 pg_autoscale_mode to off
# ./benchmarks/bench-rados:269 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool application enable bench_rados benchmark
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:41:40,191375356-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:41:40,194356163-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'b5a3cfa8-383f-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'application', 'enable', 'bench_rados', 'benchmark'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid b5a3cfa8-383f-11ec-b51d-53e6e728d2d3 -- ceph osd pool application enable bench_rados benchmark
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
enabled application 'benchmark' on pool 'bench_rados'
# ./benchmarks/bench-rados:270 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados noscrub 1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:41:44,624520616-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:41:44,627689808-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'b5a3cfa8-383f-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'noscrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid b5a3cfa8-383f-11ec-b51d-53e6e728d2d3 -- ceph osd pool set bench_rados noscrub 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 noscrub to 1
# ./benchmarks/bench-rados:271 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados nodeep-scrub 1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:41:49,066532205-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:41:49,069685828-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'b5a3cfa8-383f-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'nodeep-scrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid b5a3cfa8-383f-11ec-b51d-53e6e728d2d3 -- ceph osd pool set bench_rados nodeep-scrub 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 nodeep-scrub to 1
# ./benchmarks/bench-rados:272 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool ls detail
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:41:53,974790526-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:41:53,977730677-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'b5a3cfa8-383f-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'ls', 'detail'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid b5a3cfa8-383f-11ec-b51d-53e6e728d2d3 -- ceph osd pool ls detail
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
pool 1 'device_health_metrics' replicated size 1 min_size 1 crush_rule 0 object_hash rjenkins pg_num 64 pgp_num 64 autoscale_mode on last_change 18 lfor 0/0/16 flags hashpspool stripe_width 0 pg_num_min 1 application mgr_devicehealth
pool 2 'bench_rados' replicated size 1 min_size 1 crush_rule 0 object_hash rjenkins pg_num 128 pgp_num 128 autoscale_mode off last_change 20 flags hashpspool,noscrub,nodeep-scrub stripe_width 0 pg_num_min 128 application benchmark

# ./benchmarks/bench-rados:273 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd df tree
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:41:57,800098403-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:41:57,802909971-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'b5a3cfa8-383f-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'df', 'tree'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid b5a3cfa8-383f-11ec-b51d-53e6e728d2d3 -- ceph osd df tree
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
ID  CLASS  WEIGHT   REWEIGHT  SIZE     RAW USE  DATA     OMAP  META     AVAIL    %USE  VAR   PGS  STATUS  TYPE NAME    
-1         0.39059         -  400 GiB  5.7 MiB  168 KiB   0 B  5.6 MiB  400 GiB  0.00  1.00    -          root default 
-3         0.39059         -  400 GiB  5.7 MiB  168 KiB   0 B  5.6 MiB  400 GiB  0.00  1.00    -              host sm1 
 0    ssd  0.39059   1.00000  400 GiB  5.7 MiB  168 KiB   0 B  5.6 MiB  400 GiB  0.00  1.00  192      up          osd.0
                       TOTAL  400 GiB  5.7 MiB  168 KiB   0 B  5.6 MiB  400 GiB  0.00                                  
MIN/MAX VAR: 1.00/1.00  STDDEV: 0


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:42:01,753021562-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:42:25,707584831-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:42:34,734749981-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:42:43,612865665-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:42:52,693827931-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:42:52,700960942-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:43:01,652234146-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:43:01,659300441-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:43:10,774003318-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:43:10,781396931-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:43:19,863431403-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:43:19,870587848-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:43:28,872661946-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:43:28,880533759-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:43:28,885792409-07:00] INFO: > The cluster is idle now.[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:43:28,889370802-07:00] STAGE: Start benchmarking of mode write ...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:43:28,895522124-07:00] INFO: > Start system activity collection services[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:43:28,900591978-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:341 - rados_bench() > rm --force /mnt/sda8/ceph-research/scripts/output/sys_activity_64KB_write.dat.rados_bench_host.1
# ./benchmarks/bench-rados:342 - rados_bench() > sar_pid_rados_bench_host=826752
# ./benchmarks/bench-rados:342 - rados_bench() > S_TIME_FORMAT=ISO
# ./benchmarks/bench-rados:342 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/output/sys_activity_64KB_write.dat.rados_bench_host.1 2

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:43:28,907246167-07:00] INFO: >> for OSD host[0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:43:28,915998149-07:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.5 bash
10.10.2.5: b'283402\n'
[1] 15:43:30 [SUCCESS] ljishen@10.10.2.5
283402

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:43:30,083620638-07:00] INFO: > Run rados bench[0m
# ./benchmarks/bench-rados:370 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/output/rados_bench_64KB_write.log.1
## ./benchmarks/bench-rados:361 - rados_bench() > count_cpus 32-63,96-127
# ./benchmarks/bench-rados:361 - rados_bench() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 60 write --pool bench_rados -b 65536 -O 65536 --concurrent-ios 128 --show-time --write-object --write-omap --write-xattr --no-cleanup
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:43:30,103719643-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:43:30,106560787-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'b5a3cfa8-383f-11ec-b51d-53e6e728d2d3', '--', 'rados', 'bench', '60', 'write', '--pool', 'bench_rados', '-b', '65536', '-O', '65536', '--concurrent-ios', '128', '--show-time', '--write-object', '--write-omap', '--write-xattr', '--no-cleanup'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid b5a3cfa8-383f-11ec-b51d-53e6e728d2d3 -- rados bench 60 write --pool bench_rados -b 65536 -O 65536 --concurrent-ios 128 --show-time --write-object --write-omap --write-xattr --no-cleanup
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
hints = 1
2021-10-28T22:43:33.051650+0000 Maintaining 128 concurrent writes of 65536 bytes to objects of size 65536 for up to 60 seconds or 0 objects
2021-10-28T22:43:33.051663+0000 Object prefix: benchmark_data_node-2.bluefield2.ucsc-cmps10_7
2021-10-28T22:43:33.054348+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-28T22:43:33.054348+0000     0       0         0         0         0         0           -           0
2021-10-28T22:43:34.054468+0000     1     128       513       385   24.0611   24.0625    0.238072    0.290635
2021-10-28T22:43:35.054551+0000     2     128       854       726   22.6859   21.3125    0.254343    0.332314
2021-10-28T22:43:36.054617+0000     3     128      1238      1110   23.1234        24    0.300262    0.331534
2021-10-28T22:43:37.054683+0000     4     128      1665      1537    24.014   26.6875    0.278571    0.322137
2021-10-28T22:43:38.054753+0000     5     128      2049      1921   24.0108        24    0.276342    0.322777
2021-10-28T22:43:39.054823+0000     6     128      2518      2390   24.8941   29.3125    0.299706    0.312245
2021-10-28T22:43:40.054890+0000     7     128      2817      2689   24.0073   18.6875    0.269425    0.323976
2021-10-28T22:43:41.054960+0000     8     128      3286      3158   24.6702   29.3125     0.27177    0.315032
2021-10-28T22:43:42.055034+0000     9     128      3713      3585   24.8941   26.6875     0.27474    0.317576
2021-10-28T22:43:43.055104+0000    10     128      4097      3969   24.8045        24    0.314164    0.315375
2021-10-28T22:43:44.055170+0000    11     128      4566      4438   25.2142   29.3125    0.221436    0.315335
2021-10-28T22:43:45.055243+0000    12     128      4822      4694   24.4462        16    0.680845    0.320637
2021-10-28T22:43:46.055316+0000    13     128      5249      5121   24.6185   26.6875    0.358624    0.320236
2021-10-28T22:43:47.055385+0000    14     128      5633      5505   24.5742        24    0.239463    0.321859
2021-10-28T22:43:48.055456+0000    15     128      6017      5889   24.5358        24    0.424853    0.322761
2021-10-28T22:43:49.055526+0000    16     128      6401      6273   24.5022        24    0.289532    0.321336
2021-10-28T22:43:50.055591+0000    17     128      6870      6742    24.785   29.3125    0.374961    0.321391
2021-10-28T22:43:51.055656+0000    18     128      7169      7041   24.4462   18.6875    0.269102     0.32383
2021-10-28T22:43:52.055724+0000    19     128      7638      7510   24.7022   29.3125    0.265789     0.32241
2021-10-28T22:43:53.055791+0000 min lat: 0.0861499 max lat: 0.680938 avg lat: 0.320111
2021-10-28T22:43:53.055791+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-28T22:43:53.055791+0000    20     128      8065      7937   24.8014   26.6875    0.312981    0.320111
2021-10-28T22:43:54.055872+0000    21     128      8321      8193   24.3822        16    0.713025    0.325178
2021-10-28T22:43:55.055939+0000    22     128      8449      8321   23.6376         8    0.744228    0.331562
2021-10-28T22:43:56.056004+0000    23     128      8577      8449   22.9576         8     1.05287    0.341522
2021-10-28T22:43:57.056075+0000    24     128      8705      8577   22.3344         8    0.863183    0.349256
2021-10-28T22:43:58.056141+0000    25     128      8833      8705    21.761         8     0.96861    0.358206
2021-10-28T22:43:59.056209+0000    26     128      8961      8833   21.2317         8    0.876788    0.366585
2021-10-28T22:44:00.056277+0000    27     128      9089      8961   20.7416         8     1.00314     0.37551
2021-10-28T22:44:01.056348+0000    28     128      9302      9174   20.4763   13.3125    0.859759    0.388686
2021-10-28T22:44:02.056411+0000    29     128      9430      9302    20.046         8    0.858383    0.394664
2021-10-28T22:44:03.056478+0000    30     128      9558      9430   19.6445         8    0.783721    0.400353
2021-10-28T22:44:04.056549+0000    31     128      9729      9601   19.3555   10.6875    0.833984    0.408719
2021-10-28T22:44:05.056617+0000    32     128      9857      9729   19.0006         8    0.934643     0.41572
2021-10-28T22:44:06.056681+0000    33     128      9985      9857   18.6673         8      1.1167    0.425321
2021-10-28T22:44:07.056747+0000    34     128     10113      9985   18.3535         8    0.775004    0.429568
2021-10-28T22:44:08.056819+0000    35     128     10241     10113   18.0577         8    0.962299    0.436418
2021-10-28T22:44:09.056887+0000    36     128     10454     10326   17.9258   13.3125    0.761995    0.442302
2021-10-28T22:44:10.056952+0000    37     128     10625     10497   17.7302   10.6875    0.737638     0.44853
2021-10-28T22:44:11.057021+0000    38     128     10753     10625   17.4741         8    0.973451    0.453591
2021-10-28T22:44:12.057109+0000    39     128     10881     10753   17.2312         8    0.870063    0.460036
2021-10-28T22:44:13.057183+0000 min lat: 0.0861499 max lat: 1.18335 avg lat: 0.46429
2021-10-28T22:44:13.057183+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-28T22:44:13.057183+0000    40     128     11009     10881   17.0004         8    0.800608     0.46429
2021-10-28T22:44:14.057254+0000    41     128     11222     11094   16.9104   13.3125    0.793601    0.470802
2021-10-28T22:44:15.057319+0000    42     128     11350     11222   16.6982         8    0.753314    0.474025
2021-10-28T22:44:16.057382+0000    43     128     11521     11393   16.5584   10.6875    0.830989    0.479387
2021-10-28T22:44:17.057449+0000    44     128     11649     11521   16.3639         8     0.89513    0.484571
2021-10-28T22:44:18.057513+0000    45     128     11777     11649   16.1781         8     0.84783     0.48732
2021-10-28T22:44:19.057576+0000    46     128     11990     11862   16.1157   13.3125    0.831858    0.493576
2021-10-28T22:44:20.057640+0000    47     128     12289     12161   16.1704   18.6875    0.276257    0.492542
2021-10-28T22:44:21.057706+0000    48     128     12673     12545   16.3335        24    0.264313    0.488205
2021-10-28T22:44:22.057769+0000    49     128     13014     12886   16.4351   21.3125    0.618903    0.485972
2021-10-28T22:44:23.057834+0000    50     128     13441     13313   16.6401   26.6875    0.312892    0.479225
2021-10-28T22:44:24.057901+0000    51     128     13825     13697   16.7844        24    0.314594    0.474675
2021-10-28T22:44:25.057970+0000    52     128     14294     14166   17.0253   29.3125    0.283664    0.468609
2021-10-28T22:44:26.058036+0000    53     128     14611     14483   17.0778   19.8125    0.291965    0.466684
2021-10-28T22:44:27.058099+0000    54     128     15062     14934   17.2835   28.1875    0.304977    0.461032
2021-10-28T22:44:28.058162+0000    55     128     15446     15318   17.4056        24    0.280028    0.458753
2021-10-28T22:44:29.058230+0000    56     128     15873     15745   17.5713   26.6875    0.282243     0.45373
2021-10-28T22:44:30.058307+0000    57     128     16257     16129   17.6841        24    0.246008     0.45084
2021-10-28T22:44:31.058374+0000    58     128     16385     16257   17.5171         8    0.794717    0.451942
2021-10-28T22:44:32.058439+0000    59     128     16598     16470   17.4458   13.3125    0.820789    0.456749
2021-10-28T22:44:33.058508+0000 min lat: 0.0861499 max lat: 1.18335 avg lat: 0.461352
2021-10-28T22:44:33.058508+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-28T22:44:33.058508+0000    60      59     16657     16598   17.2884         8     1.05475    0.461352
2021-10-28T22:44:34.058619+0000 Total time run:         60.4615
Total writes made:      16657
Write size:             65536
Object size:            65536
Bandwidth (MB/sec):     17.2186
Stddev Bandwidth:       8.22961
Max bandwidth (MB/sec): 29.3125
Min bandwidth (MB/sec): 8
Average IOPS:           275
Stddev IOPS:            131.674
Max IOPS:               469
Min IOPS:               128
Average Latency(s):     0.462015
Stddev Latency(s):      0.254436
Max latency(s):         1.18335
Min latency(s):         0.0861499

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:44:35,000632982-07:00] INFO: > Stop system activity collection services[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:44:35,006134580-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:391 - rados_bench() > kill -INT 826752

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:44:35,011394281-07:00] INFO: >> for OSD host[0m
# ./benchmarks/bench-rados:396 - rados_bench() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.5 kill -INT 283402
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:44:35,019207785-07:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.5 kill -INT 283402
[1] 15:44:36 [SUCCESS] ljishen@10.10.2.5

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:44:36,124779489-07:00] INFO: >>> Copy system activity file from OSD host[0m
## ./benchmarks/bench-rados:403 - rados_bench() > basename -- /tmp/bench-rados/sys_activity_64KB_write.dat.osd_host.1
# ./benchmarks/bench-rados:403 - rados_bench() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.5:/tmp/bench-rados/sys_activity_64KB_write.dat.osd_host.1 /mnt/sda8/ceph-research/scripts/output/sys_activity_64KB_write.dat.osd_host.1


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:44:37,319917914-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:45:02,479166014-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 16.66k objects, 1.0 GiB
    usage:   4.0 GiB used, 396 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:45:02,487340107-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:45:11,976681157-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 16.66k objects, 1.0 GiB
    usage:   4.5 GiB used, 396 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:45:11,983834477-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:45:20,883370750-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 16.66k objects, 1.0 GiB
    usage:   3.2 GiB used, 397 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:45:20,890888386-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:45:29,756661920-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 16.66k objects, 1.0 GiB
    usage:   3.3 GiB used, 397 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:45:29,764183875-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:45:38,822846889-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 16.66k objects, 1.0 GiB
    usage:   3.1 GiB used, 397 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:45:38,830244319-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:45:38,836245709-07:00] INFO: > The cluster is idle now.[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:45:38,839735936-07:00] STAGE: Start benchmarking of mode seq ...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:45:38,846277633-07:00] INFO: > Start system activity collection services[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:45:38,851530081-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:341 - rados_bench() > rm --force /mnt/sda8/ceph-research/scripts/output/sys_activity_64KB_seq.dat.rados_bench_host.1
# ./benchmarks/bench-rados:342 - rados_bench() > sar_pid_rados_bench_host=828102
# ./benchmarks/bench-rados:342 - rados_bench() > S_TIME_FORMAT=ISO
# ./benchmarks/bench-rados:342 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/output/sys_activity_64KB_seq.dat.rados_bench_host.1 2

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:45:38,858371833-07:00] INFO: >> for OSD host[0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:45:38,866984293-07:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.5 bash
10.10.2.5: b'283955\n'
[1] 15:45:39 [SUCCESS] ljishen@10.10.2.5
283955

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:45:40,012116139-07:00] INFO: > Run rados bench[0m
# ./benchmarks/bench-rados:379 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/output/rados_bench_64KB_seq.log.1
## ./benchmarks/bench-rados:376 - rados_bench() > count_cpus 32-63,96-127
# ./benchmarks/bench-rados:376 - rados_bench() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 128 --show-time
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:45:40,031836059-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:45:40,034622591-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'b5a3cfa8-383f-11ec-b51d-53e6e728d2d3', '--', 'rados', 'bench', '99999999', 'seq', '--pool', 'bench_rados', '--concurrent-ios', '128', '--show-time'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid b5a3cfa8-383f-11ec-b51d-53e6e728d2d3 -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 128 --show-time
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
hints = 1
2021-10-28T22:45:43.092400+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-28T22:45:43.092400+0000     0       0         0         0         0         0           -           0
2021-10-28T22:45:44.092499+0000     1     128      1194      1066   66.6132    66.625   0.0849171    0.111668
2021-10-28T22:45:45.092570+0000     2     128      2314      2186   68.3041        70     0.23169    0.113055
2021-10-28T22:45:46.092637+0000     3     128      3202      3074    64.035      55.5    0.120886    0.122455
2021-10-28T22:45:47.092703+0000     4     128      4520      4392   68.6185    82.375    0.114617    0.113658
2021-10-28T22:45:48.092770+0000     5     128      5565      5437   67.9564   65.3125     0.13332    0.116265
2021-10-28T22:45:49.092836+0000     6     128      6556      6428   66.9526   61.9375   0.0652809    0.118193
2021-10-28T22:45:50.092904+0000     7     128      7625      7497   66.9319   66.8125   0.0911928    0.118282
2021-10-28T22:45:51.092970+0000     8     128      8929      8801   68.7522      81.5    0.316343    0.114507
2021-10-28T22:45:52.093038+0000     9     128      9956      9828   68.2446   64.1875    0.177797    0.116381
2021-10-28T22:45:53.093114+0000    10     128     10982     10854   67.8321    64.125    0.265131    0.115873
2021-10-28T22:45:54.093179+0000    11     128     11877     11749   66.7505   55.9375   0.0371618    0.118872
2021-10-28T22:45:55.093215+0000    12     128     13077     12949   67.4377        75    0.106869    0.118228
2021-10-28T22:45:56.093279+0000    13     128     13902     13774   66.2163   51.5625    0.159796    0.119351
2021-10-28T22:45:57.093344+0000    14     128     15057     14929   66.6425   72.1875  0.00568824    0.118984
2021-10-28T22:45:58.093410+0000    15     128     16128     16000   66.6618   66.9375    0.131646    0.119522
2021-10-28T22:45:59.093500+0000 Total time run:       15.453
Total reads made:     16657
Read size:            65536
Object size:          65536
Bandwidth (MB/sec):   67.3694
Average IOPS:         1077
Stddev IOPS:          140.703
Max IOPS:             1318
Min IOPS:             825
Average Latency(s):   0.118505
Max latency(s):       0.583858
Min latency(s):       0.00198313

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:45:59,705997527-07:00] INFO: > Stop system activity collection services[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:45:59,711675907-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:391 - rados_bench() > kill -INT 828102

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:45:59,717521022-07:00] INFO: >> for OSD host[0m
# ./benchmarks/bench-rados:396 - rados_bench() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.5 kill -INT 283955
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:45:59,725326220-07:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.5 kill -INT 283955
[1] 15:46:00 [SUCCESS] ljishen@10.10.2.5

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:46:00,820007216-07:00] INFO: >>> Copy system activity file from OSD host[0m
## ./benchmarks/bench-rados:403 - rados_bench() > basename -- /tmp/bench-rados/sys_activity_64KB_seq.dat.osd_host.1
# ./benchmarks/bench-rados:403 - rados_bench() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.5:/tmp/bench-rados/sys_activity_64KB_seq.dat.osd_host.1 /mnt/sda8/ceph-research/scripts/output/sys_activity_64KB_seq.dat.osd_host.1


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:46:01,901808522-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:46:25,825705124-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 16.66k objects, 1.0 GiB
    usage:   3.1 GiB used, 397 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:46:25,832997556-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:46:34,858378402-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 16.66k objects, 1.0 GiB
    usage:   3.1 GiB used, 397 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:46:34,865715368-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:46:43,780261929-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 16.66k objects, 1.0 GiB
    usage:   3.1 GiB used, 397 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:46:43,787373351-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:46:52,861978666-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 16.66k objects, 1.0 GiB
    usage:   3.1 GiB used, 397 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:46:52,869409519-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:47:01,776469842-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 16.66k objects, 1.0 GiB
    usage:   3.1 GiB used, 397 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:47:01,783784466-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:47:01,789626034-07:00] INFO: > The cluster is idle now.[0m


[1;7;39;49m[2021-10-28T15:47:01,791879591-07:00][RUNNING][ROUND 2/3/21] object_size=64KB[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:47:01,795119186-07:00] STAGE: Launch a Ceph cluster (CEPH_HOST: 10.10.2.1, OSD_HOST: 10.10.2.5) ...[0m
# ./benchmarks/bench-rados:183 - launch_ceph_cluster() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.1 bash -euo pipefail
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:47:01,804624448-07:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.1 bash -euo pipefail
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T15:47:02,255837808-07:00] INFO: > Remove existing clusters\x1b[0m\n'
10.10.2.1: b'## bash:17 -  > basename -- /var/lib/ceph/b5a3cfa8-383f-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'# bash:17 -  > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm --host 10.10.2.1 --host 10.10.2.5 rm-cluster --force --fsid b5a3cfa8-383f-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T15:47:02,267240724-07:00] DEBUG: HOST_PARAMS: ['--host', '10.10.2.1', '--host', '10.10.2.5']\x1b[0m\n"
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T15:47:02,270870236-07:00] DEBUG: CEPHADM_PARAMS: ['rm-cluster', '--force', '--fsid', 'b5a3cfa8-383f-11ec-b51d-53e6e728d2d3']\x1b[0m\n"
10.10.2.1: b"# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:63 -  > /tmp/bench-rados/ceph-research/scripts/parallel-ssh --host 10.10.2.1 --host 10.10.2.5 'sudo python3 -u - rm-cluster --force --fsid b5a3cfa8-383f-11ec-b51d-53e6e728d2d3'\n"
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T15:47:02,280254958-07:00] DEBUG: command from stdin\x1b[0m\n'
10.10.2.1: b"# /tmp/bench-rados/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host 10.10.2.1 --host 10.10.2.5 'sudo python3 -u - rm-cluster --force --fsid b5a3cfa8-383f-11ec-b51d-53e6e728d2d3'\n"
10.10.2.1: b'[1] 15:47:08 [SUCCESS] 10.10.2.1\n[2] 15:47:11 [SUCCESS] 10.10.2.5\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T15:47:11,214205944-07:00] INFO: > Deploy a new cluster\x1b[0m\n'
10.10.2.1: b'# bash:24 -  > /tmp/bench-rados/ceph-research/scripts/ceph-deploy -m 10.10.2.1 -o sm1/10.10.2.5:/dev/nvme0n1\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T15:47:11,226713366-07:00] STAGE: Verify passwordless SSH to hosts, passwordless sudo via ljishen, and the existence of devices on hosts...\x1b[0m\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T15:47:11,231771143-07:00] INFO: > Check the host for the monitor\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:181 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@10.10.2.1 sudo --non-interactive --validate\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T15:47:11,383262340-07:00] INFO: > Check hosts for OSDs\x1b[0m\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T15:47:11,388281665-07:00] INFO: >> Check host 'sm1'\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:200 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 sudo --non-interactive --validate\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T15:47:12,479254651-07:00] INFO: >>> Check device '/dev/nvme0n1'\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:215 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 test -b /dev/nvme0n1\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:226 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 lsblk --nodeps --noheadings --output TYPE /dev/nvme0n1\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:227 - verify_parameters() > grep --word-regexp disk\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T15:47:14,636057599-07:00] STAGE: Reset devices on hosts...\x1b[0m\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T15:47:14,641303030-07:00] INFO: > Resetting device '/dev/nvme0n1' on host 'sm1'\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:288 - reset_host_devices() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 sudo sh\n'
10.10.2.1: b'  Removing ceph--dc59b67b--bd5f--4734--898d--5f10f4c8cddd-osd--block--91d94981--2c3c--4b3c--b370--5fbce4387c11 (252:0)\n'
10.10.2.1: b'  Archiving volume group "ceph-dc59b67b-bd5f-4734-898d-5f10f4c8cddd" metadata (seqno 5).\n'
10.10.2.1: b'  Releasing logical volume "osd-block-91d94981-2c3c-4b3c-b370-5fbce4387c11"\n'
10.10.2.1: b'  Creating volume group backup "/etc/lvm/backup/ceph-dc59b67b-bd5f-4734-898d-5f10f4c8cddd" (seqno 6).\n'
10.10.2.1: b'  Logical volume "osd-block-91d94981-2c3c-4b3c-b370-5fbce4387c11" successfully removed\n'
10.10.2.1: b'  Removing physical volume "/dev/nvme0n1" from volume group "ceph-dc59b67b-bd5f-4734-898d-5f10f4c8cddd"\n'
10.10.2.1: b'  Volume group "ceph-dc59b67b-bd5f-4734-898d-5f10f4c8cddd" successfully removed\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T15:47:16,965315582-07:00] STAGE: Bootstrap a new cluster...\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:317 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.1 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T15:47:16,974623178-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T15:47:16,978116505-07:00] DEBUG: CEPHADM_PARAMS: ['bootstrap', '--config', '/tmp/bench-rados/ceph-research/scripts/ceph.conf', '--mon-ip', '10.10.2.1', '--output-dir', '/tmp/bench-rados/deployment_data_root/etc/ceph', '--ssh-user', 'ljishen', '--skip-dashboard', '--no-minimize-config', '--allow-overwrite', '--allow-fqdn-hostname', '--orphan-initial-daemons', '--skip-monitoring-stack', '--single-host-defaults']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.1 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults\n'
10.10.2.1: b'Verifying podman|docker is present...\nVerifying lvm2 is present...\n'
10.10.2.1: b'Verifying time synchronization is in place...\n'
10.10.2.1: b'Unit ntp.service is enabled and running\n'
10.10.2.1: b'Repeating the final host check...\n'
10.10.2.1: b'podman|docker (/usr/bin/docker) is present\n'
10.10.2.1: b'systemctl is present\n'
10.10.2.1: b'lvcreate is present\n'
10.10.2.1: b'Unit ntp.service is enabled and running\n'
10.10.2.1: b'Host looks OK\n'
10.10.2.1: b'Cluster fsid: 005f8e00-3841-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Verifying IP 10.10.2.1 port 3300 ...\n'
10.10.2.1: b'Verifying IP 10.10.2.1 port 6789 ...\n'
10.10.2.1: b'Mon IP `10.10.2.1` is in CIDR network `10.10.2.0/24`\n'
10.10.2.1: b'- internal network (--cluster-network) has not been provided, OSD replication will default to the public_network\n'
10.10.2.1: b'Adjusting default settings to suit single-host cluster...\n'
10.10.2.1: b'Pulling container image quay.io/ceph/ceph:v16...\n'
10.10.2.1: b'Ceph version: ceph version 16.2.6 (ee28fb57e47e9f88813e24bbf4c14496ca299d31) pacific (stable)\n'
10.10.2.1: b'Extracting ceph user uid/gid from container image...\n'
10.10.2.1: b'Creating initial keys...\n'
10.10.2.1: b'Creating initial monmap...\n'
10.10.2.1: b'Creating mon...\n'
10.10.2.1: b'Waiting for mon to start...\n'
10.10.2.1: b'Waiting for mon...\n'
10.10.2.1: b'mon is available\n'
10.10.2.1: b'Setting mon public_network to 10.10.2.0/24\n'
10.10.2.1: b'Wrote config to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf\n'
10.10.2.1: b'Wrote keyring to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring\n'
10.10.2.1: b'Creating mgr...\n'
10.10.2.1: b'Verifying port 9283 ...\n'
10.10.2.1: b'Waiting for mgr to start...\nWaiting for mgr...\n'
10.10.2.1: b'mgr not available, waiting (1/15)...\n'
10.10.2.1: b'mgr not available, waiting (2/15)...\n'
10.10.2.1: b'mgr is available\n'
10.10.2.1: b'Enabling cephadm module...\n'
10.10.2.1: b'Waiting for the mgr to restart...\n'
10.10.2.1: b'Waiting for mgr epoch 5...\n'
10.10.2.1: b'mgr epoch 5 is available\n'
10.10.2.1: b'Setting orchestrator backend to cephadm...\n'
10.10.2.1: b'Generating ssh key...\n'
10.10.2.1: b'Wrote public SSH key to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub\n'
10.10.2.1: b'Adding key to ljishen@localhost authorized_keys...\n'
10.10.2.1: b'Adding host node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us...\n'
10.10.2.1: b'Deploying unmanaged mon service...\n'
10.10.2.1: b'Deploying unmanaged mgr service...\n'
10.10.2.1: b'You can access the Ceph CLI with:\n\n\tsudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --fsid 005f8e00-3841-11ec-b51d-53e6e728d2d3 -c /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf -k /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring\n\nPlease consider enabling telemetry to help improve Ceph:\n\n\tceph telemetry on\n\nFor more information see:\n\n\thttps://docs.ceph.com/docs/pacific/mgr/telemetry/\n\nBootstrap complete.\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T15:48:19,437083413-07:00] INFO: Sleep for 20 seconds\x1b[0m\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T15:48:39,444671972-07:00] INFO: > Set the default number of monitor daemons to 1\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:337 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T15:48:39,455166470-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T15:48:39,459005206-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mon', '1']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1\n'
10.10.2.1: b'Inferring fsid 005f8e00-3841-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/005f8e00-3841-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b'Scheduled mon update...\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T15:48:48,389783652-07:00] INFO: > Set the default number of manager daemons to 1\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:342 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T15:48:48,400438791-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T15:48:48,404356787-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mgr', '1']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1\n'
10.10.2.1: b'Inferring fsid 005f8e00-3841-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/005f8e00-3841-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b'Scheduled mgr update...\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T15:48:57,660644736-07:00] STAGE: Add hosts to the cluster...\x1b[0m\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T15:48:57,666811077-07:00] INFO: > Adding host 'sm1'\x1b[0m\n# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:361 - add_hosts() > ssh-copy-id -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -f -i /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub ljishen@sm1\n"
10.10.2.1: b'/usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: "/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub"\n'
10.10.2.1: b'\nNumber of key(s) added: 1\n\nNow try logging into the machine, with:   "ssh -o \'GlobalKnownHostsFile=/dev/null\' -o \'LogLevel=ERROR\' -o \'PasswordAuthentication=no\' -o \'StrictHostKeyChecking=no\' -o \'UserKnownHostsFile=/dev/null\' \'ljishen@sm1\'"\nand check to make sure that only the key(s) you wanted were added.\n\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:367 - add_hosts() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add sm1 10.10.2.5\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T15:48:58,828341372-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T15:48:58,832337845-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'host', 'add', 'sm1', '10.10.2.5']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add sm1 10.10.2.5\n'
10.10.2.1: b'Inferring fsid 005f8e00-3841-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/005f8e00-3841-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b"Added host 'sm1' with addr '10.10.2.5'\n"
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T15:49:09,454927316-07:00] INFO: Sleep for 20 seconds\x1b[0m\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T15:49:29,459484043-07:00] STAGE: Add OSDs to the cluster...\x1b[0m\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T15:49:29,466264499-07:00] INFO: > Adding devices ['/dev/nvme0n1'] on host 'sm1' as OSDs\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:388 - add_host_devices_as_osds() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd sm1:/dev/nvme0n1\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T15:49:29,476007845-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T15:49:29,479656393-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'daemon', 'add', 'osd', 'sm1:/dev/nvme0n1']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd sm1:/dev/nvme0n1\n'
10.10.2.1: b'Inferring fsid 005f8e00-3841-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/005f8e00-3841-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b"Created osd(s) 0 on host 'sm1'\n"
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T15:49:54,332315155-07:00] INFO: > Sleep for 20 seconds\x1b[0m\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T15:50:14,337963878-07:00] STAGE: Show Cluster Status\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:408 - show_status() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T15:50:14,347882293-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T15:50:14,351758740-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', '--status']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status\n'
10.10.2.1: b'Inferring fsid 005f8e00-3841-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/005f8e00-3841-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b'  cluster:\n    id:     005f8e00-3841-11ec-b51d-53e6e728d2d3\n    health: HEALTH_WARN\n            1 pool(s) have no replicas configured\n \n  services:\n    mon: 1 daemons, quorum node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us (age 2m)\n    mgr: node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us.yjjrno(active, since 2m)\n    osd: 1 osds: 1 up (since 20s), 1 in (since 41s)\n \n  data:\n    pools:   1 pools, 1 pgs\n    objects: 0 objects, 0 B\n    usage:   4.7 MiB used, 400 GiB / 400 GiB avail\n    pgs:     1 active+clean\n \n'
[1] 15:50:22 [SUCCESS] ljishen@10.10.2.1

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:47:02,255837808-07:00] INFO: > Remove existing clusters[0m
## bash:17 -  > basename -- /var/lib/ceph/b5a3cfa8-383f-11ec-b51d-53e6e728d2d3
# bash:17 -  > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm --host 10.10.2.1 --host 10.10.2.5 rm-cluster --force --fsid b5a3cfa8-383f-11ec-b51d-53e6e728d2d3
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:47:02,267240724-07:00] DEBUG: HOST_PARAMS: ['--host', '10.10.2.1', '--host', '10.10.2.5'][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:47:02,270870236-07:00] DEBUG: CEPHADM_PARAMS: ['rm-cluster', '--force', '--fsid', 'b5a3cfa8-383f-11ec-b51d-53e6e728d2d3'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:63 -  > /tmp/bench-rados/ceph-research/scripts/parallel-ssh --host 10.10.2.1 --host 10.10.2.5 'sudo python3 -u - rm-cluster --force --fsid b5a3cfa8-383f-11ec-b51d-53e6e728d2d3'
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:47:02,280254958-07:00] DEBUG: command from stdin[0m
# /tmp/bench-rados/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host 10.10.2.1 --host 10.10.2.5 'sudo python3 -u - rm-cluster --force --fsid b5a3cfa8-383f-11ec-b51d-53e6e728d2d3'
[1] 15:47:08 [SUCCESS] 10.10.2.1
[2] 15:47:11 [SUCCESS] 10.10.2.5

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:47:11,214205944-07:00] INFO: > Deploy a new cluster[0m
# bash:24 -  > /tmp/bench-rados/ceph-research/scripts/ceph-deploy -m 10.10.2.1 -o sm1/10.10.2.5:/dev/nvme0n1


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:47:11,226713366-07:00] STAGE: Verify passwordless SSH to hosts, passwordless sudo via ljishen, and the existence of devices on hosts...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:47:11,231771143-07:00] INFO: > Check the host for the monitor[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:181 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@10.10.2.1 sudo --non-interactive --validate

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:47:11,383262340-07:00] INFO: > Check hosts for OSDs[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:47:11,388281665-07:00] INFO: >> Check host 'sm1'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:200 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 sudo --non-interactive --validate

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:47:12,479254651-07:00] INFO: >>> Check device '/dev/nvme0n1'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:215 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 test -b /dev/nvme0n1
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:226 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 lsblk --nodeps --noheadings --output TYPE /dev/nvme0n1
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:227 - verify_parameters() > grep --word-regexp disk


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:47:14,636057599-07:00] STAGE: Reset devices on hosts...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:47:14,641303030-07:00] INFO: > Resetting device '/dev/nvme0n1' on host 'sm1'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:288 - reset_host_devices() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 sudo sh
  Removing ceph--dc59b67b--bd5f--4734--898d--5f10f4c8cddd-osd--block--91d94981--2c3c--4b3c--b370--5fbce4387c11 (252:0)
  Archiving volume group "ceph-dc59b67b-bd5f-4734-898d-5f10f4c8cddd" metadata (seqno 5).
  Releasing logical volume "osd-block-91d94981-2c3c-4b3c-b370-5fbce4387c11"
  Creating volume group backup "/etc/lvm/backup/ceph-dc59b67b-bd5f-4734-898d-5f10f4c8cddd" (seqno 6).
  Logical volume "osd-block-91d94981-2c3c-4b3c-b370-5fbce4387c11" successfully removed
  Removing physical volume "/dev/nvme0n1" from volume group "ceph-dc59b67b-bd5f-4734-898d-5f10f4c8cddd"
  Volume group "ceph-dc59b67b-bd5f-4734-898d-5f10f4c8cddd" successfully removed


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:47:16,965315582-07:00] STAGE: Bootstrap a new cluster...[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:317 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.1 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:47:16,974623178-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:47:16,978116505-07:00] DEBUG: CEPHADM_PARAMS: ['bootstrap', '--config', '/tmp/bench-rados/ceph-research/scripts/ceph.conf', '--mon-ip', '10.10.2.1', '--output-dir', '/tmp/bench-rados/deployment_data_root/etc/ceph', '--ssh-user', 'ljishen', '--skip-dashboard', '--no-minimize-config', '--allow-overwrite', '--allow-fqdn-hostname', '--orphan-initial-daemons', '--skip-monitoring-stack', '--single-host-defaults'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.1 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults
Verifying podman|docker is present...
Verifying lvm2 is present...
Verifying time synchronization is in place...
Unit ntp.service is enabled and running
Repeating the final host check...
podman|docker (/usr/bin/docker) is present
systemctl is present
lvcreate is present
Unit ntp.service is enabled and running
Host looks OK
Cluster fsid: 005f8e00-3841-11ec-b51d-53e6e728d2d3
Verifying IP 10.10.2.1 port 3300 ...
Verifying IP 10.10.2.1 port 6789 ...
Mon IP `10.10.2.1` is in CIDR network `10.10.2.0/24`
- internal network (--cluster-network) has not been provided, OSD replication will default to the public_network
Adjusting default settings to suit single-host cluster...
Pulling container image quay.io/ceph/ceph:v16...
Ceph version: ceph version 16.2.6 (ee28fb57e47e9f88813e24bbf4c14496ca299d31) pacific (stable)
Extracting ceph user uid/gid from container image...
Creating initial keys...
Creating initial monmap...
Creating mon...
Waiting for mon to start...
Waiting for mon...
mon is available
Setting mon public_network to 10.10.2.0/24
Wrote config to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf
Wrote keyring to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring
Creating mgr...
Verifying port 9283 ...
Waiting for mgr to start...
Waiting for mgr...
mgr not available, waiting (1/15)...
mgr not available, waiting (2/15)...
mgr is available
Enabling cephadm module...
Waiting for the mgr to restart...
Waiting for mgr epoch 5...
mgr epoch 5 is available
Setting orchestrator backend to cephadm...
Generating ssh key...
Wrote public SSH key to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub
Adding key to ljishen@localhost authorized_keys...
Adding host node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us...
Deploying unmanaged mon service...
Deploying unmanaged mgr service...
You can access the Ceph CLI with:

	sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --fsid 005f8e00-3841-11ec-b51d-53e6e728d2d3 -c /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf -k /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring

Please consider enabling telemetry to help improve Ceph:

	ceph telemetry on

For more information see:

	https://docs.ceph.com/docs/pacific/mgr/telemetry/

Bootstrap complete.

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:48:19,437083413-07:00] INFO: Sleep for 20 seconds[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:48:39,444671972-07:00] INFO: > Set the default number of monitor daemons to 1[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:337 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:48:39,455166470-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:48:39,459005206-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mon', '1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1
Inferring fsid 005f8e00-3841-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/005f8e00-3841-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Scheduled mon update...

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:48:48,389783652-07:00] INFO: > Set the default number of manager daemons to 1[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:342 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:48:48,400438791-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:48:48,404356787-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mgr', '1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1
Inferring fsid 005f8e00-3841-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/005f8e00-3841-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Scheduled mgr update...


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:48:57,660644736-07:00] STAGE: Add hosts to the cluster...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:48:57,666811077-07:00] INFO: > Adding host 'sm1'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:361 - add_hosts() > ssh-copy-id -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -f -i /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub ljishen@sm1
/usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: "/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub"

Number of key(s) added: 1

Now try logging into the machine, with:   "ssh -o 'GlobalKnownHostsFile=/dev/null' -o 'LogLevel=ERROR' -o 'PasswordAuthentication=no' -o 'StrictHostKeyChecking=no' -o 'UserKnownHostsFile=/dev/null' 'ljishen@sm1'"
and check to make sure that only the key(s) you wanted were added.

# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:367 - add_hosts() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add sm1 10.10.2.5
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:48:58,828341372-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:48:58,832337845-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'host', 'add', 'sm1', '10.10.2.5'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add sm1 10.10.2.5
Inferring fsid 005f8e00-3841-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/005f8e00-3841-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Added host 'sm1' with addr '10.10.2.5'

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:49:09,454927316-07:00] INFO: Sleep for 20 seconds[0m


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:49:29,459484043-07:00] STAGE: Add OSDs to the cluster...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:49:29,466264499-07:00] INFO: > Adding devices ['/dev/nvme0n1'] on host 'sm1' as OSDs[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:388 - add_host_devices_as_osds() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd sm1:/dev/nvme0n1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:49:29,476007845-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:49:29,479656393-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'daemon', 'add', 'osd', 'sm1:/dev/nvme0n1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd sm1:/dev/nvme0n1
Inferring fsid 005f8e00-3841-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/005f8e00-3841-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Created osd(s) 0 on host 'sm1'

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:49:54,332315155-07:00] INFO: > Sleep for 20 seconds[0m


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:50:14,337963878-07:00] STAGE: Show Cluster Status[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:408 - show_status() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:50:14,347882293-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:50:14,351758740-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', '--status'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status
Inferring fsid 005f8e00-3841-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/005f8e00-3841-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
  cluster:
    id:     005f8e00-3841-11ec-b51d-53e6e728d2d3
    health: HEALTH_WARN
            1 pool(s) have no replicas configured
 
  services:
    mon: 1 daemons, quorum node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us (age 2m)
    mgr: node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us.yjjrno(active, since 2m)
    osd: 1 osds: 1 up (since 20s), 1 in (since 41s)
 
  data:
    pools:   1 pools, 1 pgs
    objects: 0 objects, 0 B
    usage:   4.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     1 active+clean
 

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:50:22,886430586-07:00] INFO: > Copy configuration files to local[0m
# ./benchmarks/bench-rados:220 - launch_ceph_cluster() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.1 sudo chown --changes --recursive ljishen /tmp/bench-rados/deployment_data_root/etc/ceph
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:50:22,893829769-07:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.1 sudo chown --changes --recursive ljishen /tmp/bench-rados/deployment_data_root/etc/ceph
[1] 15:50:23 [SUCCESS] ljishen@10.10.2.1
# ./benchmarks/bench-rados:223 - launch_ceph_cluster() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.1:/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf
# ./benchmarks/bench-rados:226 - launch_ceph_cluster() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.1:/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:50:23,372834456-07:00] STAGE: Update local ceph.conf as a client...[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:50:23,376263858-07:00] STAGE: Configure Ceph pool...[0m
# ./benchmarks/bench-rados:257 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_max_backfills 32
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:50:23,398156073-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:50:23,401025570-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '005f8e00-3841-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'config', 'set', 'osd', 'osd_max_backfills', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 005f8e00-3841-11ec-b51d-53e6e728d2d3 -- ceph config set osd osd_max_backfills 32
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:258 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_active 32
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:50:27,469126380-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:50:27,472148234-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '005f8e00-3841-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_active', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 005f8e00-3841-11ec-b51d-53e6e728d2d3 -- ceph config set osd osd_recovery_max_active 32
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:259 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_single_start 8
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:50:31,520493696-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:50:31,523409511-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '005f8e00-3841-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_single_start', '8'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 005f8e00-3841-11ec-b51d-53e6e728d2d3 -- ceph config set osd osd_recovery_max_single_start 8
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:260 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_op_priority 63
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:50:35,526260081-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:50:35,529156840-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '005f8e00-3841-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_op_priority', '63'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 005f8e00-3841-11ec-b51d-53e6e728d2d3 -- ceph config set osd osd_recovery_op_priority 63
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:262 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config show-with-defaults osd.0
# ./benchmarks/bench-rados:263 - configure_ceph_pool() > grep 'osd_max_backfills\|osd_recovery'
# ./benchmarks/bench-rados:263 - configure_ceph_pool() > column -t -s ' '
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
osd_max_backfills                        32         mon
osd_recovery_cost                        20971520   default
osd_recovery_delay_start                 0.000000   default
osd_recovery_max_active                  32         mon
osd_recovery_max_active_hdd              3          default
osd_recovery_max_active_ssd              10         default
osd_recovery_max_chunk                   8388608    default
osd_recovery_max_omap_entries_per_chunk  8096       default
osd_recovery_max_single_start            8          mon
osd_recovery_op_priority                 3          default
osd_recovery_op_warn_multiple            16         default
osd_recovery_priority                    5          default
osd_recovery_retry_interval              30.000000  default
osd_recovery_sleep                       0.000000   default
osd_recovery_sleep_hdd                   0.100000   default
osd_recovery_sleep_hybrid                0.025000   default
osd_recovery_sleep_ssd                   0.000000   default
# ./benchmarks/bench-rados:265 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool create bench_rados 128 128 replicated --size 1 --pg-num-min 128
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:50:43,502344023-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:50:43,505649361-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '005f8e00-3841-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'create', 'bench_rados', '128', '128', 'replicated', '--size', '1', '--pg-num-min', '128'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 005f8e00-3841-11ec-b51d-53e6e728d2d3 -- ceph osd pool create bench_rados 128 128 replicated --size 1 --pg-num-min 128
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
pool 'bench_rados' created
# ./benchmarks/bench-rados:267 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados min_size 1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:50:48,015427401-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:50:48,018500662-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '005f8e00-3841-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'min_size', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 005f8e00-3841-11ec-b51d-53e6e728d2d3 -- ceph osd pool set bench_rados min_size 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 min_size to 1
# ./benchmarks/bench-rados:268 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados pg_autoscale_mode off
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:50:52,441286891-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:50:52,444393906-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '005f8e00-3841-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'pg_autoscale_mode', 'off'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 005f8e00-3841-11ec-b51d-53e6e728d2d3 -- ceph osd pool set bench_rados pg_autoscale_mode off
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 pg_autoscale_mode to off
# ./benchmarks/bench-rados:269 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool application enable bench_rados benchmark
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:50:56,850131278-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:50:56,853178661-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '005f8e00-3841-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'application', 'enable', 'bench_rados', 'benchmark'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 005f8e00-3841-11ec-b51d-53e6e728d2d3 -- ceph osd pool application enable bench_rados benchmark
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
enabled application 'benchmark' on pool 'bench_rados'
# ./benchmarks/bench-rados:270 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados noscrub 1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:51:01,408916963-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:51:01,411862443-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '005f8e00-3841-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'noscrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 005f8e00-3841-11ec-b51d-53e6e728d2d3 -- ceph osd pool set bench_rados noscrub 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 noscrub to 1
# ./benchmarks/bench-rados:271 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados nodeep-scrub 1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:51:06,100911533-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:51:06,103778686-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '005f8e00-3841-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'nodeep-scrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 005f8e00-3841-11ec-b51d-53e6e728d2d3 -- ceph osd pool set bench_rados nodeep-scrub 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 nodeep-scrub to 1
# ./benchmarks/bench-rados:272 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool ls detail
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:51:11,190252271-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:51:11,193394472-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '005f8e00-3841-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'ls', 'detail'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 005f8e00-3841-11ec-b51d-53e6e728d2d3 -- ceph osd pool ls detail
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
pool 1 'device_health_metrics' replicated size 1 min_size 1 crush_rule 0 object_hash rjenkins pg_num 64 pgp_num 64 autoscale_mode on last_change 18 lfor 0/0/16 flags hashpspool stripe_width 0 pg_num_min 1 application mgr_devicehealth
pool 2 'bench_rados' replicated size 1 min_size 1 crush_rule 0 object_hash rjenkins pg_num 128 pgp_num 128 autoscale_mode off last_change 20 flags hashpspool,noscrub,nodeep-scrub stripe_width 0 pg_num_min 128 application benchmark

# ./benchmarks/bench-rados:273 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd df tree
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:51:15,339035512-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:51:15,342049161-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '005f8e00-3841-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'df', 'tree'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 005f8e00-3841-11ec-b51d-53e6e728d2d3 -- ceph osd df tree
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
ID  CLASS  WEIGHT   REWEIGHT  SIZE     RAW USE  DATA     OMAP  META     AVAIL    %USE  VAR   PGS  STATUS  TYPE NAME    
-1         0.39059         -  400 GiB  5.7 MiB  168 KiB   0 B  5.6 MiB  400 GiB  0.00  1.00    -          root default 
-3         0.39059         -  400 GiB  5.7 MiB  168 KiB   0 B  5.6 MiB  400 GiB  0.00  1.00    -              host sm1 
 0    ssd  0.39059   1.00000  400 GiB  5.7 MiB  168 KiB   0 B  5.6 MiB  400 GiB  0.00  1.00  192      up          osd.0
                       TOTAL  400 GiB  5.7 MiB  168 KiB   0 B  5.6 MiB  400 GiB  0.00                                  
MIN/MAX VAR: 1.00/1.00  STDDEV: 0


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:51:19,257314712-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:51:43,120824200-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:51:52,085267000-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:52:01,132886275-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:52:01,140507617-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:52:10,265379075-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:52:10,272557352-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:52:19,130065772-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:52:19,137690160-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:52:28,237637278-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:52:28,244947995-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:52:37,242430868-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:52:37,249537300-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:52:37,254814224-07:00] INFO: > The cluster is idle now.[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:52:37,258546427-07:00] STAGE: Start benchmarking of mode write ...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:52:37,264738436-07:00] INFO: > Start system activity collection services[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:52:37,269945538-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:341 - rados_bench() > rm --force /mnt/sda8/ceph-research/scripts/output/sys_activity_64KB_write.dat.rados_bench_host.2
# ./benchmarks/bench-rados:342 - rados_bench() > sar_pid_rados_bench_host=833659
# ./benchmarks/bench-rados:342 - rados_bench() > S_TIME_FORMAT=ISO
# ./benchmarks/bench-rados:342 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/output/sys_activity_64KB_write.dat.rados_bench_host.2 2

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:52:37,276587194-07:00] INFO: >> for OSD host[0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:52:37,285487096-07:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.5 bash
10.10.2.5: b'288810\n'
[1] 15:52:38 [SUCCESS] ljishen@10.10.2.5
288810

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:52:38,380039063-07:00] INFO: > Run rados bench[0m
# ./benchmarks/bench-rados:370 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/output/rados_bench_64KB_write.log.2
## ./benchmarks/bench-rados:361 - rados_bench() > count_cpus 32-63,96-127
# ./benchmarks/bench-rados:361 - rados_bench() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 60 write --pool bench_rados -b 65536 -O 65536 --concurrent-ios 128 --show-time --write-object --write-omap --write-xattr --no-cleanup
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:52:38,401113496-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:52:38,403998433-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '005f8e00-3841-11ec-b51d-53e6e728d2d3', '--', 'rados', 'bench', '60', 'write', '--pool', 'bench_rados', '-b', '65536', '-O', '65536', '--concurrent-ios', '128', '--show-time', '--write-object', '--write-omap', '--write-xattr', '--no-cleanup'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 005f8e00-3841-11ec-b51d-53e6e728d2d3 -- rados bench 60 write --pool bench_rados -b 65536 -O 65536 --concurrent-ios 128 --show-time --write-object --write-omap --write-xattr --no-cleanup
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
hints = 1
2021-10-28T22:52:41.515560+0000 Maintaining 128 concurrent writes of 65536 bytes to objects of size 65536 for up to 60 seconds or 0 objects
2021-10-28T22:52:41.515573+0000 Object prefix: benchmark_data_node-2.bluefield2.ucsc-cmps10_7
2021-10-28T22:52:41.518434+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-28T22:52:41.518434+0000     0       0         0         0         0         0           -           0
2021-10-28T22:52:42.518550+0000     1     128       465       337   21.0613   21.0625    0.339912     0.31859
2021-10-28T22:52:43.518633+0000     2     128       897       769   24.0296        27    0.290375    0.292049
2021-10-28T22:52:44.518716+0000     3     128      1361      1233   25.6856        29    0.393949    0.299429
2021-10-28T22:52:45.518792+0000     4     128      1745      1617   25.2637        24    0.256509     0.30239
2021-10-28T22:52:46.518864+0000     5     128      2177      2049   25.6106        27    0.288279    0.302463
2021-10-28T22:52:47.518936+0000     6     128      2561      2433   25.3419        24    0.235901    0.301471
2021-10-28T22:52:48.519015+0000     7     128      3025      2897   25.8641        29    0.265611    0.306358
2021-10-28T22:52:49.519090+0000     8     128      3409      3281   25.6309        24     0.30342    0.305548
2021-10-28T22:52:50.519172+0000     9     128      3793      3665   25.4495        24    0.340541    0.308485
2021-10-28T22:52:51.519253+0000    10     128      4225      4097   25.6043        27    0.289749    0.304434
2021-10-28T22:52:52.519326+0000    11     128      4737      4609   26.1855        32    0.279117    0.302117
2021-10-28T22:52:53.519397+0000    12     128      5073      4945   25.7533        21    0.284391    0.305844
2021-10-28T22:52:54.519466+0000    13     128      5505      5377    25.849        27    0.278102     0.30643
2021-10-28T22:52:55.519539+0000    14     128      5889      5761   25.7168        24    0.281768    0.307578
2021-10-28T22:52:56.519621+0000    15     128      6273      6145   25.6022        24    0.385828    0.306781
2021-10-28T22:52:57.519703+0000    16     128      6737      6609   25.8145        29    0.281698    0.305492
2021-10-28T22:52:58.519778+0000    17     128      7169      7041   25.8841        27    0.276667    0.307178
2021-10-28T22:52:59.519861+0000    18     128      7553      7425   25.7793        24    0.275112    0.307781
2021-10-28T22:53:00.519938+0000    19     128      7937      7809   25.6855        24    0.263436    0.309436
2021-10-28T22:53:01.520010+0000 min lat: 0.186872 max lat: 0.507489 avg lat: 0.30947
2021-10-28T22:53:01.520010+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-28T22:53:01.520010+0000    20     128      8193      8065   25.2012        16    0.323851     0.30947
2021-10-28T22:53:02.520089+0000    21     128      8401      8273   24.6202        13    0.825842     0.32056
2021-10-28T22:53:03.520159+0000    22     128      8529      8401   23.8647         8    0.716154    0.326991
2021-10-28T22:53:04.520237+0000    23     128      8657      8529   23.1749         8     1.08379    0.337326
2021-10-28T22:53:05.520313+0000    24     128      8785      8657   22.5426         8    0.928715    0.346179
2021-10-28T22:53:06.520385+0000    25     128      8961      8833   22.0808        11    0.936748    0.358865
2021-10-28T22:53:07.520462+0000    26     128      9089      8961   21.5392         8    0.974337     0.36768
2021-10-28T22:53:08.520536+0000    27     128      9217      9089   21.0378         8    0.983463    0.375531
2021-10-28T22:53:09.520613+0000    28     128      9345      9217   20.5721         8    0.785136    0.381275
2021-10-28T22:53:10.520691+0000    29     128      9473      9345   20.1386         8    0.980639    0.389507
2021-10-28T22:53:11.520769+0000    30     128      9601      9473   19.7339         8    0.929208    0.397213
2021-10-28T22:53:12.520845+0000    31     128      9729      9601   19.3554         8    0.919101    0.402996
2021-10-28T22:53:13.520914+0000    32     128      9894      9766   19.0728   10.3125    0.902639    0.412464
2021-10-28T22:53:14.520986+0000    33     128      9985      9857   18.6672    5.6875    0.880226    0.416953
2021-10-28T22:53:15.521056+0000    34     128     10122      9994   18.3699    8.5625    0.921893    0.426076
2021-10-28T22:53:16.521129+0000    35     128     10321     10193   18.2004   12.4375     0.74433    0.434479
2021-10-28T22:53:17.521209+0000    36     128     10449     10321   17.9171         8    0.883394    0.439979
2021-10-28T22:53:18.521293+0000    37     128     10625     10497   17.7301        11    0.861593    0.447427
2021-10-28T22:53:19.521365+0000    38     128     10753     10625    17.474         8    0.847365    0.451768
2021-10-28T22:53:20.521445+0000    39     128     10881     10753   17.2311         8     1.02236     0.45921
2021-10-28T22:53:21.521519+0000 min lat: 0.186872 max lat: 1.12838 avg lat: 0.464407
2021-10-28T22:53:21.521519+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-28T22:53:21.521519+0000    40     128     11009     10881   17.0003         8    0.893542    0.464407
2021-10-28T22:53:22.521605+0000    41     128     11137     11009   16.7807         8    0.917129    0.468499
2021-10-28T22:53:23.521675+0000    42     128     11345     11217   16.6907        13     0.74925    0.475649
2021-10-28T22:53:24.521747+0000    43     128     11521     11393   16.5583        11    0.727994    0.480536
2021-10-28T22:53:25.521821+0000    44     128     11649     11521   16.3638         8    0.974897    0.485032
2021-10-28T22:53:26.521891+0000    45     128     11777     11649   16.1779         8    0.877272    0.489668
2021-10-28T22:53:27.521961+0000    46     128     11905     11777   16.0002         8    0.745736    0.493133
2021-10-28T22:53:28.522030+0000    47     128     12241     12113   16.1065        21    0.299711    0.495708
2021-10-28T22:53:29.522102+0000    48     128     12625     12497   16.2709        24     0.25389    0.490134
2021-10-28T22:53:30.522175+0000    49     128     13009     12881   16.4286        24    0.411339    0.486242
2021-10-28T22:53:31.522244+0000    50     128     13393     13265     16.58        24    0.282663    0.481646
2021-10-28T22:53:32.522316+0000    51     128     13825     13697   16.7843        27    0.342676    0.475325
2021-10-28T22:53:33.522389+0000    52     128     14209     14081    16.923        24    0.307765    0.470509
2021-10-28T22:53:34.522460+0000    53     128     14673     14545   17.1508        29     0.30577     0.46479
2021-10-28T22:53:35.522532+0000    54     128     15105     14977   17.3332        27    0.261832    0.459535
2021-10-28T22:53:36.522601+0000    55     128     15441     15313   17.3998        21    0.260403    0.458366
2021-10-28T22:53:37.522670+0000    56     128     15873     15745   17.5712        27    0.271153    0.454087
2021-10-28T22:53:38.522741+0000    57     128     16257     16129    17.684        24     0.25631    0.450786
2021-10-28T22:53:39.522813+0000    58     128     16385     16257    17.517         8    0.812305    0.452181
2021-10-28T22:53:40.522898+0000    59     128     16593     16465   17.4404        13    0.863773    0.457032
2021-10-28T22:53:41.522978+0000 min lat: 0.186872 max lat: 1.12838 avg lat: 0.459966
2021-10-28T22:53:41.522978+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-28T22:53:41.522978+0000    60     128     16721     16593   17.2831         8    0.860522    0.459966
2021-10-28T22:53:42.523088+0000 Total time run:         60.7267
Total writes made:      16721
Write size:             65536
Object size:            65536
Bandwidth (MB/sec):     17.2093
Stddev Bandwidth:       8.49562
Max bandwidth (MB/sec): 32
Min bandwidth (MB/sec): 5.6875
Average IOPS:           275
Stddev IOPS:            135.93
Max IOPS:               512
Min IOPS:               91
Average Latency(s):     0.463318
Stddev Latency(s):      0.261656
Max latency(s):         1.12838
Min latency(s):         0.186872

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:53:43,289401536-07:00] INFO: > Stop system activity collection services[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:53:43,294941926-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:391 - rados_bench() > kill -INT 833659

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:53:43,300628642-07:00] INFO: >> for OSD host[0m
# ./benchmarks/bench-rados:396 - rados_bench() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.5 kill -INT 288810
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:53:43,308571400-07:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.5 kill -INT 288810
[1] 15:53:44 [SUCCESS] ljishen@10.10.2.5

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:53:44,436354284-07:00] INFO: >>> Copy system activity file from OSD host[0m
## ./benchmarks/bench-rados:403 - rados_bench() > basename -- /tmp/bench-rados/sys_activity_64KB_write.dat.osd_host.2
# ./benchmarks/bench-rados:403 - rados_bench() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.5:/tmp/bench-rados/sys_activity_64KB_write.dat.osd_host.2 /mnt/sda8/ceph-research/scripts/output/sys_activity_64KB_write.dat.osd_host.2


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:53:45,520460710-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:54:10,417028671-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 16.72k objects, 1.0 GiB
    usage:   4.0 GiB used, 396 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:54:10,424449406-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:54:19,714789994-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 16.72k objects, 1.0 GiB
    usage:   4.4 GiB used, 396 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:54:19,722382842-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:54:28,599702957-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 16.72k objects, 1.0 GiB
    usage:   3.4 GiB used, 397 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:54:28,607277100-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:54:37,594918872-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 16.72k objects, 1.0 GiB
    usage:   3.2 GiB used, 397 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:54:37,602589867-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:54:46,626749898-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 16.72k objects, 1.0 GiB
    usage:   3.1 GiB used, 397 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:54:46,633757143-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:54:46,639628496-07:00] INFO: > The cluster is idle now.[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:54:46,642882859-07:00] STAGE: Start benchmarking of mode seq ...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:54:46,649609354-07:00] INFO: > Start system activity collection services[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:54:46,654809162-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:341 - rados_bench() > rm --force /mnt/sda8/ceph-research/scripts/output/sys_activity_64KB_seq.dat.rados_bench_host.2
# ./benchmarks/bench-rados:342 - rados_bench() > sar_pid_rados_bench_host=835034
# ./benchmarks/bench-rados:342 - rados_bench() > S_TIME_FORMAT=ISO
# ./benchmarks/bench-rados:342 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/output/sys_activity_64KB_seq.dat.rados_bench_host.2 2

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:54:46,661386937-07:00] INFO: >> for OSD host[0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:54:46,670238487-07:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.5 bash
10.10.2.5: b'289355\n'
[1] 15:54:47 [SUCCESS] ljishen@10.10.2.5
289355

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:54:47,803492144-07:00] INFO: > Run rados bench[0m
# ./benchmarks/bench-rados:379 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/output/rados_bench_64KB_seq.log.2
## ./benchmarks/bench-rados:376 - rados_bench() > count_cpus 32-63,96-127
# ./benchmarks/bench-rados:376 - rados_bench() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 128 --show-time
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:54:47,823334004-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:54:47,826108302-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '005f8e00-3841-11ec-b51d-53e6e728d2d3', '--', 'rados', 'bench', '99999999', 'seq', '--pool', 'bench_rados', '--concurrent-ios', '128', '--show-time'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 005f8e00-3841-11ec-b51d-53e6e728d2d3 -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 128 --show-time
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
hints = 1
2021-10-28T22:54:50.886912+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-28T22:54:50.886912+0000     0       0         0         0         0         0           -           0
2021-10-28T22:54:51.887084+0000     1     128      1097       969   60.5471   60.5625    0.183909    0.111059
2021-10-28T22:54:52.887220+0000     2     127      2327      2200   68.7366   76.9375   0.0537062    0.113243
2021-10-28T22:54:53.887325+0000     3     128      3517      3389   70.5925   74.3125    0.126542    0.111261
2021-10-28T22:54:54.887436+0000     4     128      4418      4290   67.0211   56.3125     0.13702    0.117615
2021-10-28T22:54:55.887547+0000     5     128      5517      5389   67.3528   68.6875   0.0620361    0.117864
2021-10-28T22:54:56.887661+0000     6     128      6458      6330   65.9284   58.8125   0.0971311    0.120618
2021-10-28T22:54:57.887761+0000     7     128      7741      7613   67.9642   80.1875    0.113396     0.11581
2021-10-28T22:54:58.887875+0000     8     128      8878      8750   68.3504   71.0625   0.0474618    0.116169
2021-10-28T22:54:59.887985+0000     9     128      9863      9735   67.5955   61.5625   0.0794534    0.117699
2021-10-28T22:55:00.888101+0000    10     128     10922     10794   67.4539   66.1875    0.106519    0.118131
2021-10-28T22:55:01.888216+0000    11     128     11823     11695   66.4405   56.3125    0.648002    0.116638
2021-10-28T22:55:02.888332+0000    12     128     12908     12780   66.5542   67.8125    0.249011    0.119334
2021-10-28T22:55:03.888443+0000    13     128     14038     13910   66.8667    70.625   0.0944783    0.119101
2021-10-28T22:55:04.888557+0000    14     128     15137     15009   66.9962   68.6875   0.0549396    0.118736
2021-10-28T22:55:05.888656+0000    15     128     16310     16182   67.4168   73.3125   0.0785208    0.118198
2021-10-28T22:55:06.888802+0000 Total time run:       15.4948
Total reads made:     16721
Read size:            65536
Object size:          65536
Bandwidth (MB/sec):   67.4462
Average IOPS:         1079
Stddev IOPS:          118.383
Max IOPS:             1283
Min IOPS:             901
Average Latency(s):   0.118196
Max latency(s):       0.837474
Min latency(s):       0.00333984

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:55:07,619953213-07:00] INFO: > Stop system activity collection services[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:55:07,625528488-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:391 - rados_bench() > kill -INT 835034

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:55:07,631054922-07:00] INFO: >> for OSD host[0m
# ./benchmarks/bench-rados:396 - rados_bench() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.5 kill -INT 289355
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:55:07,639097467-07:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.5 kill -INT 289355
[1] 15:55:08 [SUCCESS] ljishen@10.10.2.5

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:55:08,744383586-07:00] INFO: >>> Copy system activity file from OSD host[0m
## ./benchmarks/bench-rados:403 - rados_bench() > basename -- /tmp/bench-rados/sys_activity_64KB_seq.dat.osd_host.2
# ./benchmarks/bench-rados:403 - rados_bench() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.5:/tmp/bench-rados/sys_activity_64KB_seq.dat.osd_host.2 /mnt/sda8/ceph-research/scripts/output/sys_activity_64KB_seq.dat.osd_host.2


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:55:09,817959120-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:55:33,847514079-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 16.72k objects, 1.0 GiB
    usage:   3.1 GiB used, 397 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:55:33,854902452-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:55:42,903516070-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 16.72k objects, 1.0 GiB
    usage:   3.1 GiB used, 397 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:55:42,910611931-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:55:51,927079018-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 16.72k objects, 1.0 GiB
    usage:   3.1 GiB used, 397 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:55:51,935003962-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:56:00,884494047-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 16.72k objects, 1.0 GiB
    usage:   3.1 GiB used, 397 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:56:00,891873713-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:56:09,941696186-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 16.72k objects, 1.0 GiB
    usage:   3.1 GiB used, 397 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:56:09,949234551-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:56:09,955347880-07:00] INFO: > The cluster is idle now.[0m


[1;7;39;49m[2021-10-28T15:56:09,957572713-07:00][RUNNING][ROUND 3/3/21] object_size=64KB[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:56:09,960890555-07:00] STAGE: Launch a Ceph cluster (CEPH_HOST: 10.10.2.1, OSD_HOST: 10.10.2.5) ...[0m
# ./benchmarks/bench-rados:183 - launch_ceph_cluster() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.1 bash -euo pipefail
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:56:09,970032492-07:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.1 bash -euo pipefail
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T15:56:10,406423354-07:00] INFO: > Remove existing clusters\x1b[0m\n'
10.10.2.1: b'## bash:17 -  > basename -- /var/lib/ceph/005f8e00-3841-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'# bash:17 -  > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm --host 10.10.2.1 --host 10.10.2.5 rm-cluster --force --fsid 005f8e00-3841-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T15:56:10,417982864-07:00] DEBUG: HOST_PARAMS: ['--host', '10.10.2.1', '--host', '10.10.2.5']\x1b[0m\n"
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T15:56:10,421726091-07:00] DEBUG: CEPHADM_PARAMS: ['rm-cluster', '--force', '--fsid', '005f8e00-3841-11ec-b51d-53e6e728d2d3']\x1b[0m\n"
10.10.2.1: b"# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:63 -  > /tmp/bench-rados/ceph-research/scripts/parallel-ssh --host 10.10.2.1 --host 10.10.2.5 'sudo python3 -u - rm-cluster --force --fsid 005f8e00-3841-11ec-b51d-53e6e728d2d3'\n"
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T15:56:10,429851094-07:00] DEBUG: command from stdin\x1b[0m\n'
10.10.2.1: b"# /tmp/bench-rados/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host 10.10.2.1 --host 10.10.2.5 'sudo python3 -u - rm-cluster --force --fsid 005f8e00-3841-11ec-b51d-53e6e728d2d3'\n"
10.10.2.1: b'[1] 15:56:16 [SUCCESS] 10.10.2.1\n[2] 15:56:19 [SUCCESS] 10.10.2.5\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T15:56:19,260728736-07:00] INFO: > Deploy a new cluster\x1b[0m\n'
10.10.2.1: b'# bash:24 -  > /tmp/bench-rados/ceph-research/scripts/ceph-deploy -m 10.10.2.1 -o sm1/10.10.2.5:/dev/nvme0n1\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T15:56:19,272363147-07:00] STAGE: Verify passwordless SSH to hosts, passwordless sudo via ljishen, and the existence of devices on hosts...\x1b[0m\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T15:56:19,276955921-07:00] INFO: > Check the host for the monitor\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:181 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@10.10.2.1 sudo --non-interactive --validate\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T15:56:19,428206317-07:00] INFO: > Check hosts for OSDs\x1b[0m\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T15:56:19,432996833-07:00] INFO: >> Check host 'sm1'\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:200 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 sudo --non-interactive --validate\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T15:56:20,519493850-07:00] INFO: >>> Check device '/dev/nvme0n1'\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:215 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 test -b /dev/nvme0n1\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:226 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 lsblk --nodeps --noheadings --output TYPE /dev/nvme0n1\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:227 - verify_parameters() > grep --word-regexp disk\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T15:56:22,636006610-07:00] STAGE: Reset devices on hosts...\x1b[0m\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T15:56:22,641048077-07:00] INFO: > Resetting device '/dev/nvme0n1' on host 'sm1'\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:288 - reset_host_devices() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 sudo sh\n'
10.10.2.1: b'  Removing ceph--29bdb88a--c7b2--4ca5--8dbf--99105e01ddad-osd--block--f87db7c8--75f6--407b--af44--37ba629baa39 (252:0)\n'
10.10.2.1: b'  Archiving volume group "ceph-29bdb88a-c7b2-4ca5-8dbf-99105e01ddad" metadata (seqno 5).\n'
10.10.2.1: b'  Releasing logical volume "osd-block-f87db7c8-75f6-407b-af44-37ba629baa39"\n'
10.10.2.1: b'  Creating volume group backup "/etc/lvm/backup/ceph-29bdb88a-c7b2-4ca5-8dbf-99105e01ddad" (seqno 6).\n'
10.10.2.1: b'  Logical volume "osd-block-f87db7c8-75f6-407b-af44-37ba629baa39" successfully removed\n'
10.10.2.1: b'  Removing physical volume "/dev/nvme0n1" from volume group "ceph-29bdb88a-c7b2-4ca5-8dbf-99105e01ddad"\n'
10.10.2.1: b'  Volume group "ceph-29bdb88a-c7b2-4ca5-8dbf-99105e01ddad" successfully removed\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T15:56:24,997845373-07:00] STAGE: Bootstrap a new cluster...\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:317 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.1 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T15:56:25,008326265-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T15:56:25,012178557-07:00] DEBUG: CEPHADM_PARAMS: ['bootstrap', '--config', '/tmp/bench-rados/ceph-research/scripts/ceph.conf', '--mon-ip', '10.10.2.1', '--output-dir', '/tmp/bench-rados/deployment_data_root/etc/ceph', '--ssh-user', 'ljishen', '--skip-dashboard', '--no-minimize-config', '--allow-overwrite', '--allow-fqdn-hostname', '--orphan-initial-daemons', '--skip-monitoring-stack', '--single-host-defaults']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.1 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults\n'
10.10.2.1: b'Verifying podman|docker is present...\nVerifying lvm2 is present...\n'
10.10.2.1: b'Verifying time synchronization is in place...\n'
10.10.2.1: b'Unit ntp.service is enabled and running\n'
10.10.2.1: b'Repeating the final host check...\n'
10.10.2.1: b'podman|docker (/usr/bin/docker) is present\n'
10.10.2.1: b'systemctl is present\n'
10.10.2.1: b'lvcreate is present\n'
10.10.2.1: b'Unit ntp.service is enabled and running\n'
10.10.2.1: b'Host looks OK\n'
10.10.2.1: b'Cluster fsid: 4706f4c8-3842-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Verifying IP 10.10.2.1 port 3300 ...\n'
10.10.2.1: b'Verifying IP 10.10.2.1 port 6789 ...\n'
10.10.2.1: b'Mon IP `10.10.2.1` is in CIDR network `10.10.2.0/24`\n'
10.10.2.1: b'- internal network (--cluster-network) has not been provided, OSD replication will default to the public_network\n'
10.10.2.1: b'Adjusting default settings to suit single-host cluster...\nPulling container image quay.io/ceph/ceph:v16...\n'
10.10.2.1: b'Ceph version: ceph version 16.2.6 (ee28fb57e47e9f88813e24bbf4c14496ca299d31) pacific (stable)\n'
10.10.2.1: b'Extracting ceph user uid/gid from container image...\n'
10.10.2.1: b'Creating initial keys...\n'
10.10.2.1: b'Creating initial monmap...\n'
10.10.2.1: b'Creating mon...\n'
10.10.2.1: b'Waiting for mon to start...\n'
10.10.2.1: b'Waiting for mon...\n'
10.10.2.1: b'mon is available\nSetting mon public_network to 10.10.2.0/24\n'
10.10.2.1: b'Wrote config to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf\n'
10.10.2.1: b'Wrote keyring to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring\n'
10.10.2.1: b'Creating mgr...\n'
10.10.2.1: b'Verifying port 9283 ...\n'
10.10.2.1: b'Waiting for mgr to start...\nWaiting for mgr...\n'
10.10.2.1: b'mgr not available, waiting (1/15)...\n'
10.10.2.1: b'mgr not available, waiting (2/15)...\n'
10.10.2.1: b'mgr is available\n'
10.10.2.1: b'Enabling cephadm module...\n'
10.10.2.1: b'Waiting for the mgr to restart...\n'
10.10.2.1: b'Waiting for mgr epoch 5...\n'
10.10.2.1: b'mgr epoch 5 is available\nSetting orchestrator backend to cephadm...\n'
10.10.2.1: b'Generating ssh key...\n'
10.10.2.1: b'Wrote public SSH key to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub\n'
10.10.2.1: b'Adding key to ljishen@localhost authorized_keys...\n'
10.10.2.1: b'Adding host node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us...\n'
10.10.2.1: b'Deploying unmanaged mon service...\n'
10.10.2.1: b'Deploying unmanaged mgr service...\n'
10.10.2.1: b'You can access the Ceph CLI with:\n\n\tsudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --fsid 4706f4c8-3842-11ec-b51d-53e6e728d2d3 -c /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf -k /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring\n\nPlease consider enabling telemetry to help improve Ceph:\n\n\tceph telemetry on\n\nFor more information see:\n\n\thttps://docs.ceph.com/docs/pacific/mgr/telemetry/\n\n'
10.10.2.1: b'Bootstrap complete.\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T15:57:27,610909145-07:00] INFO: Sleep for 20 seconds\x1b[0m\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T15:57:47,618202821-07:00] INFO: > Set the default number of monitor daemons to 1\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:337 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T15:57:47,628602591-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T15:57:47,632468919-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mon', '1']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1\n'
10.10.2.1: b'Inferring fsid 4706f4c8-3842-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/4706f4c8-3842-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b'Scheduled mon update...\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T15:57:56,964503677-07:00] INFO: > Set the default number of manager daemons to 1\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:342 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T15:57:56,974150541-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T15:57:56,978141314-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mgr', '1']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1\n'
10.10.2.1: b'Inferring fsid 4706f4c8-3842-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/4706f4c8-3842-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b'Scheduled mgr update...\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T15:58:06,341945043-07:00] STAGE: Add hosts to the cluster...\x1b[0m\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T15:58:06,347862447-07:00] INFO: > Adding host 'sm1'\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:361 - add_hosts() > ssh-copy-id -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -f -i /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub ljishen@sm1\n'
10.10.2.1: b'/usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: "/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub"\n'
10.10.2.1: b'\nNumber of key(s) added: 1\n\nNow try logging into the machine, with:   "ssh -o \'GlobalKnownHostsFile=/dev/null\' -o \'LogLevel=ERROR\' -o \'PasswordAuthentication=no\' -o \'StrictHostKeyChecking=no\' -o \'UserKnownHostsFile=/dev/null\' \'ljishen@sm1\'"\nand check to make sure that only the key(s) you wanted were added.\n\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:367 - add_hosts() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add sm1 10.10.2.5\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T15:58:07,488715588-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T15:58:07,492718613-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'host', 'add', 'sm1', '10.10.2.5']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add sm1 10.10.2.5\n'
10.10.2.1: b'Inferring fsid 4706f4c8-3842-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/4706f4c8-3842-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b"Added host 'sm1' with addr '10.10.2.5'\n"
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T15:58:18,237306549-07:00] INFO: Sleep for 20 seconds\x1b[0m\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T15:58:38,241866848-07:00] STAGE: Add OSDs to the cluster...\x1b[0m\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T15:58:38,248811413-07:00] INFO: > Adding devices ['/dev/nvme0n1'] on host 'sm1' as OSDs\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:388 - add_host_devices_as_osds() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd sm1:/dev/nvme0n1\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T15:58:38,258978966-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T15:58:38,262784810-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'daemon', 'add', 'osd', 'sm1:/dev/nvme0n1']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd sm1:/dev/nvme0n1\n'
10.10.2.1: b'Inferring fsid 4706f4c8-3842-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/4706f4c8-3842-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b"Created osd(s) 0 on host 'sm1'\n"
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T15:59:03,507642217-07:00] INFO: > Sleep for 20 seconds\x1b[0m\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T15:59:23,512926060-07:00] STAGE: Show Cluster Status\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:408 - show_status() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T15:59:23,523271465-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T15:59:23,527071468-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', '--status']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status\n'
10.10.2.1: b'Inferring fsid 4706f4c8-3842-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/4706f4c8-3842-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b'  cluster:\n    id:     4706f4c8-3842-11ec-b51d-53e6e728d2d3\n    health: HEALTH_WARN\n            1 pool(s) have no replicas configured\n \n  services:\n    mon: 1 daemons, quorum node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us (age 2m)\n    mgr: node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us.xpxzrm(active, since 2m)\n    osd: 1 osds: 1 up (since 20s), 1 in (since 41s)\n \n  data:\n    pools:   1 pools, 1 pgs\n    objects: 0 objects, 0 B\n    usage:   4.7 MiB used, 400 GiB / 400 GiB avail\n    pgs:     1 active+clean\n \n'
[1] 15:59:32 [SUCCESS] ljishen@10.10.2.1

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:56:10,406423354-07:00] INFO: > Remove existing clusters[0m
## bash:17 -  > basename -- /var/lib/ceph/005f8e00-3841-11ec-b51d-53e6e728d2d3
# bash:17 -  > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm --host 10.10.2.1 --host 10.10.2.5 rm-cluster --force --fsid 005f8e00-3841-11ec-b51d-53e6e728d2d3
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:56:10,417982864-07:00] DEBUG: HOST_PARAMS: ['--host', '10.10.2.1', '--host', '10.10.2.5'][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:56:10,421726091-07:00] DEBUG: CEPHADM_PARAMS: ['rm-cluster', '--force', '--fsid', '005f8e00-3841-11ec-b51d-53e6e728d2d3'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:63 -  > /tmp/bench-rados/ceph-research/scripts/parallel-ssh --host 10.10.2.1 --host 10.10.2.5 'sudo python3 -u - rm-cluster --force --fsid 005f8e00-3841-11ec-b51d-53e6e728d2d3'
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:56:10,429851094-07:00] DEBUG: command from stdin[0m
# /tmp/bench-rados/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host 10.10.2.1 --host 10.10.2.5 'sudo python3 -u - rm-cluster --force --fsid 005f8e00-3841-11ec-b51d-53e6e728d2d3'
[1] 15:56:16 [SUCCESS] 10.10.2.1
[2] 15:56:19 [SUCCESS] 10.10.2.5

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:56:19,260728736-07:00] INFO: > Deploy a new cluster[0m
# bash:24 -  > /tmp/bench-rados/ceph-research/scripts/ceph-deploy -m 10.10.2.1 -o sm1/10.10.2.5:/dev/nvme0n1


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:56:19,272363147-07:00] STAGE: Verify passwordless SSH to hosts, passwordless sudo via ljishen, and the existence of devices on hosts...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:56:19,276955921-07:00] INFO: > Check the host for the monitor[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:181 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@10.10.2.1 sudo --non-interactive --validate

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:56:19,428206317-07:00] INFO: > Check hosts for OSDs[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:56:19,432996833-07:00] INFO: >> Check host 'sm1'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:200 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 sudo --non-interactive --validate

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:56:20,519493850-07:00] INFO: >>> Check device '/dev/nvme0n1'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:215 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 test -b /dev/nvme0n1
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:226 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 lsblk --nodeps --noheadings --output TYPE /dev/nvme0n1
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:227 - verify_parameters() > grep --word-regexp disk


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:56:22,636006610-07:00] STAGE: Reset devices on hosts...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:56:22,641048077-07:00] INFO: > Resetting device '/dev/nvme0n1' on host 'sm1'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:288 - reset_host_devices() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 sudo sh
  Removing ceph--29bdb88a--c7b2--4ca5--8dbf--99105e01ddad-osd--block--f87db7c8--75f6--407b--af44--37ba629baa39 (252:0)
  Archiving volume group "ceph-29bdb88a-c7b2-4ca5-8dbf-99105e01ddad" metadata (seqno 5).
  Releasing logical volume "osd-block-f87db7c8-75f6-407b-af44-37ba629baa39"
  Creating volume group backup "/etc/lvm/backup/ceph-29bdb88a-c7b2-4ca5-8dbf-99105e01ddad" (seqno 6).
  Logical volume "osd-block-f87db7c8-75f6-407b-af44-37ba629baa39" successfully removed
  Removing physical volume "/dev/nvme0n1" from volume group "ceph-29bdb88a-c7b2-4ca5-8dbf-99105e01ddad"
  Volume group "ceph-29bdb88a-c7b2-4ca5-8dbf-99105e01ddad" successfully removed


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:56:24,997845373-07:00] STAGE: Bootstrap a new cluster...[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:317 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.1 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:56:25,008326265-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:56:25,012178557-07:00] DEBUG: CEPHADM_PARAMS: ['bootstrap', '--config', '/tmp/bench-rados/ceph-research/scripts/ceph.conf', '--mon-ip', '10.10.2.1', '--output-dir', '/tmp/bench-rados/deployment_data_root/etc/ceph', '--ssh-user', 'ljishen', '--skip-dashboard', '--no-minimize-config', '--allow-overwrite', '--allow-fqdn-hostname', '--orphan-initial-daemons', '--skip-monitoring-stack', '--single-host-defaults'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.1 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults
Verifying podman|docker is present...
Verifying lvm2 is present...
Verifying time synchronization is in place...
Unit ntp.service is enabled and running
Repeating the final host check...
podman|docker (/usr/bin/docker) is present
systemctl is present
lvcreate is present
Unit ntp.service is enabled and running
Host looks OK
Cluster fsid: 4706f4c8-3842-11ec-b51d-53e6e728d2d3
Verifying IP 10.10.2.1 port 3300 ...
Verifying IP 10.10.2.1 port 6789 ...
Mon IP `10.10.2.1` is in CIDR network `10.10.2.0/24`
- internal network (--cluster-network) has not been provided, OSD replication will default to the public_network
Adjusting default settings to suit single-host cluster...
Pulling container image quay.io/ceph/ceph:v16...
Ceph version: ceph version 16.2.6 (ee28fb57e47e9f88813e24bbf4c14496ca299d31) pacific (stable)
Extracting ceph user uid/gid from container image...
Creating initial keys...
Creating initial monmap...
Creating mon...
Waiting for mon to start...
Waiting for mon...
mon is available
Setting mon public_network to 10.10.2.0/24
Wrote config to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf
Wrote keyring to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring
Creating mgr...
Verifying port 9283 ...
Waiting for mgr to start...
Waiting for mgr...
mgr not available, waiting (1/15)...
mgr not available, waiting (2/15)...
mgr is available
Enabling cephadm module...
Waiting for the mgr to restart...
Waiting for mgr epoch 5...
mgr epoch 5 is available
Setting orchestrator backend to cephadm...
Generating ssh key...
Wrote public SSH key to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub
Adding key to ljishen@localhost authorized_keys...
Adding host node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us...
Deploying unmanaged mon service...
Deploying unmanaged mgr service...
You can access the Ceph CLI with:

	sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --fsid 4706f4c8-3842-11ec-b51d-53e6e728d2d3 -c /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf -k /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring

Please consider enabling telemetry to help improve Ceph:

	ceph telemetry on

For more information see:

	https://docs.ceph.com/docs/pacific/mgr/telemetry/

Bootstrap complete.

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:57:27,610909145-07:00] INFO: Sleep for 20 seconds[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:57:47,618202821-07:00] INFO: > Set the default number of monitor daemons to 1[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:337 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:57:47,628602591-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:57:47,632468919-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mon', '1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1
Inferring fsid 4706f4c8-3842-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/4706f4c8-3842-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Scheduled mon update...

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:57:56,964503677-07:00] INFO: > Set the default number of manager daemons to 1[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:342 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:57:56,974150541-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:57:56,978141314-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mgr', '1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1
Inferring fsid 4706f4c8-3842-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/4706f4c8-3842-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Scheduled mgr update...


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:58:06,341945043-07:00] STAGE: Add hosts to the cluster...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:58:06,347862447-07:00] INFO: > Adding host 'sm1'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:361 - add_hosts() > ssh-copy-id -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -f -i /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub ljishen@sm1
/usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: "/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub"

Number of key(s) added: 1

Now try logging into the machine, with:   "ssh -o 'GlobalKnownHostsFile=/dev/null' -o 'LogLevel=ERROR' -o 'PasswordAuthentication=no' -o 'StrictHostKeyChecking=no' -o 'UserKnownHostsFile=/dev/null' 'ljishen@sm1'"
and check to make sure that only the key(s) you wanted were added.

# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:367 - add_hosts() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add sm1 10.10.2.5
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:58:07,488715588-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:58:07,492718613-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'host', 'add', 'sm1', '10.10.2.5'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add sm1 10.10.2.5
Inferring fsid 4706f4c8-3842-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/4706f4c8-3842-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Added host 'sm1' with addr '10.10.2.5'

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:58:18,237306549-07:00] INFO: Sleep for 20 seconds[0m


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:58:38,241866848-07:00] STAGE: Add OSDs to the cluster...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:58:38,248811413-07:00] INFO: > Adding devices ['/dev/nvme0n1'] on host 'sm1' as OSDs[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:388 - add_host_devices_as_osds() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd sm1:/dev/nvme0n1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:58:38,258978966-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:58:38,262784810-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'daemon', 'add', 'osd', 'sm1:/dev/nvme0n1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd sm1:/dev/nvme0n1
Inferring fsid 4706f4c8-3842-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/4706f4c8-3842-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Created osd(s) 0 on host 'sm1'

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:59:03,507642217-07:00] INFO: > Sleep for 20 seconds[0m


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:59:23,512926060-07:00] STAGE: Show Cluster Status[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:408 - show_status() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:59:23,523271465-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:59:23,527071468-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', '--status'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status
Inferring fsid 4706f4c8-3842-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/4706f4c8-3842-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
  cluster:
    id:     4706f4c8-3842-11ec-b51d-53e6e728d2d3
    health: HEALTH_WARN
            1 pool(s) have no replicas configured
 
  services:
    mon: 1 daemons, quorum node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us (age 2m)
    mgr: node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us.xpxzrm(active, since 2m)
    osd: 1 osds: 1 up (since 20s), 1 in (since 41s)
 
  data:
    pools:   1 pools, 1 pgs
    objects: 0 objects, 0 B
    usage:   4.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     1 active+clean
 

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:59:32,285099493-07:00] INFO: > Copy configuration files to local[0m
# ./benchmarks/bench-rados:220 - launch_ceph_cluster() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.1 sudo chown --changes --recursive ljishen /tmp/bench-rados/deployment_data_root/etc/ceph
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:59:32,292938486-07:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.1 sudo chown --changes --recursive ljishen /tmp/bench-rados/deployment_data_root/etc/ceph
[1] 15:59:32 [SUCCESS] ljishen@10.10.2.1
# ./benchmarks/bench-rados:223 - launch_ceph_cluster() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.1:/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf
# ./benchmarks/bench-rados:226 - launch_ceph_cluster() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.1:/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:59:32,765283362-07:00] STAGE: Update local ceph.conf as a client...[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:59:32,768671627-07:00] STAGE: Configure Ceph pool...[0m
# ./benchmarks/bench-rados:257 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_max_backfills 32
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:59:32,790663839-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:59:32,793540690-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '4706f4c8-3842-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'config', 'set', 'osd', 'osd_max_backfills', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 4706f4c8-3842-11ec-b51d-53e6e728d2d3 -- ceph config set osd osd_max_backfills 32
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:258 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_active 32
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:59:36,833399229-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:59:36,836549135-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '4706f4c8-3842-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_active', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 4706f4c8-3842-11ec-b51d-53e6e728d2d3 -- ceph config set osd osd_recovery_max_active 32
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:259 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_single_start 8
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:59:40,940317886-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:59:40,943498339-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '4706f4c8-3842-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_single_start', '8'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 4706f4c8-3842-11ec-b51d-53e6e728d2d3 -- ceph config set osd osd_recovery_max_single_start 8
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:260 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_op_priority 63
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:59:44,958165481-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:59:44,961261174-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '4706f4c8-3842-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_op_priority', '63'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 4706f4c8-3842-11ec-b51d-53e6e728d2d3 -- ceph config set osd osd_recovery_op_priority 63
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:262 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config show-with-defaults osd.0
# ./benchmarks/bench-rados:263 - configure_ceph_pool() > grep 'osd_max_backfills\|osd_recovery'
# ./benchmarks/bench-rados:263 - configure_ceph_pool() > column -t -s ' '
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
osd_max_backfills                        32         mon
osd_recovery_cost                        20971520   default
osd_recovery_delay_start                 0.000000   default
osd_recovery_max_active                  32         mon
osd_recovery_max_active_hdd              3          default
osd_recovery_max_active_ssd              10         default
osd_recovery_max_chunk                   8388608    default
osd_recovery_max_omap_entries_per_chunk  8096       default
osd_recovery_max_single_start            8          mon
osd_recovery_op_priority                 63         mon
osd_recovery_op_warn_multiple            16         default
osd_recovery_priority                    5          default
osd_recovery_retry_interval              30.000000  default
osd_recovery_sleep                       0.000000   default
osd_recovery_sleep_hdd                   0.100000   default
osd_recovery_sleep_hybrid                0.025000   default
osd_recovery_sleep_ssd                   0.000000   default
# ./benchmarks/bench-rados:265 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool create bench_rados 128 128 replicated --size 1 --pg-num-min 128
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:59:53,084732841-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:59:53,087811933-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '4706f4c8-3842-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'create', 'bench_rados', '128', '128', 'replicated', '--size', '1', '--pg-num-min', '128'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 4706f4c8-3842-11ec-b51d-53e6e728d2d3 -- ceph osd pool create bench_rados 128 128 replicated --size 1 --pg-num-min 128
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
pool 'bench_rados' created
# ./benchmarks/bench-rados:267 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados min_size 1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:59:57,434236546-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T15:59:57,437555409-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '4706f4c8-3842-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'min_size', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 4706f4c8-3842-11ec-b51d-53e6e728d2d3 -- ceph osd pool set bench_rados min_size 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 min_size to 1
# ./benchmarks/bench-rados:268 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados pg_autoscale_mode off
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:00:01,885010924-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:00:01,887893516-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '4706f4c8-3842-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'pg_autoscale_mode', 'off'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 4706f4c8-3842-11ec-b51d-53e6e728d2d3 -- ceph osd pool set bench_rados pg_autoscale_mode off
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 pg_autoscale_mode to off
# ./benchmarks/bench-rados:269 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool application enable bench_rados benchmark
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:00:06,125033076-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:00:06,128183993-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '4706f4c8-3842-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'application', 'enable', 'bench_rados', 'benchmark'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 4706f4c8-3842-11ec-b51d-53e6e728d2d3 -- ceph osd pool application enable bench_rados benchmark
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
enabled application 'benchmark' on pool 'bench_rados'
# ./benchmarks/bench-rados:270 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados noscrub 1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:00:11,151427334-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:00:11,154351164-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '4706f4c8-3842-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'noscrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 4706f4c8-3842-11ec-b51d-53e6e728d2d3 -- ceph osd pool set bench_rados noscrub 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 noscrub to 1
# ./benchmarks/bench-rados:271 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados nodeep-scrub 1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:00:15,250904869-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:00:15,254008728-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '4706f4c8-3842-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'nodeep-scrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 4706f4c8-3842-11ec-b51d-53e6e728d2d3 -- ceph osd pool set bench_rados nodeep-scrub 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 nodeep-scrub to 1
# ./benchmarks/bench-rados:272 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool ls detail
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:00:19,968310239-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:00:19,971364684-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '4706f4c8-3842-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'ls', 'detail'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 4706f4c8-3842-11ec-b51d-53e6e728d2d3 -- ceph osd pool ls detail
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
pool 1 'device_health_metrics' replicated size 1 min_size 1 crush_rule 0 object_hash rjenkins pg_num 64 pgp_num 64 autoscale_mode on last_change 19 lfor 0/0/16 flags hashpspool stripe_width 0 pg_num_min 1 application mgr_devicehealth
pool 2 'bench_rados' replicated size 1 min_size 1 crush_rule 0 object_hash rjenkins pg_num 128 pgp_num 128 autoscale_mode off last_change 21 flags hashpspool,noscrub,nodeep-scrub stripe_width 0 pg_num_min 128 application benchmark

# ./benchmarks/bench-rados:273 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd df tree
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:00:24,004699242-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:00:24,007849329-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '4706f4c8-3842-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'df', 'tree'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 4706f4c8-3842-11ec-b51d-53e6e728d2d3 -- ceph osd df tree
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
ID  CLASS  WEIGHT   REWEIGHT  SIZE     RAW USE  DATA     OMAP  META     AVAIL    %USE  VAR   PGS  STATUS  TYPE NAME    
-1         0.39059         -  400 GiB  5.7 MiB  176 KiB   0 B  5.6 MiB  400 GiB  0.00  1.00    -          root default 
-3         0.39059         -  400 GiB  5.7 MiB  176 KiB   0 B  5.6 MiB  400 GiB  0.00  1.00    -              host sm1 
 0    ssd  0.39059   1.00000  400 GiB  5.7 MiB  176 KiB   0 B  5.6 MiB  400 GiB  0.00  1.00  192      up          osd.0
                       TOTAL  400 GiB  5.7 MiB  176 KiB   0 B  5.6 MiB  400 GiB  0.00                                  
MIN/MAX VAR: 1.00/1.00  STDDEV: 0


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:00:27,904930334-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:00:51,818625451-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:01:00,912289151-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:01:09,918442272-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:01:09,925793534-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:01:18,784957096-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:01:18,791998825-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:01:27,925363923-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:01:27,932975396-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:01:36,947974903-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:01:36,955356173-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:01:45,839413529-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:01:45,846807593-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:01:45,852413817-07:00] INFO: > The cluster is idle now.[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:01:45,856142443-07:00] STAGE: Start benchmarking of mode write ...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:01:45,862572800-07:00] INFO: > Start system activity collection services[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:01:45,867926429-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:341 - rados_bench() > rm --force /mnt/sda8/ceph-research/scripts/output/sys_activity_64KB_write.dat.rados_bench_host.3
# ./benchmarks/bench-rados:342 - rados_bench() > sar_pid_rados_bench_host=840626
# ./benchmarks/bench-rados:342 - rados_bench() > S_TIME_FORMAT=ISO
# ./benchmarks/bench-rados:342 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/output/sys_activity_64KB_write.dat.rados_bench_host.3 2

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:01:45,874560229-07:00] INFO: >> for OSD host[0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:01:45,883380140-07:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.5 bash
10.10.2.5: b'294333\n'
[1] 16:01:46 [SUCCESS] ljishen@10.10.2.5
294333

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:01:47,013056840-07:00] INFO: > Run rados bench[0m
# ./benchmarks/bench-rados:370 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/output/rados_bench_64KB_write.log.3
## ./benchmarks/bench-rados:361 - rados_bench() > count_cpus 32-63,96-127
# ./benchmarks/bench-rados:361 - rados_bench() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 60 write --pool bench_rados -b 65536 -O 65536 --concurrent-ios 128 --show-time --write-object --write-omap --write-xattr --no-cleanup
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:01:47,033464375-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:01:47,036529080-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '4706f4c8-3842-11ec-b51d-53e6e728d2d3', '--', 'rados', 'bench', '60', 'write', '--pool', 'bench_rados', '-b', '65536', '-O', '65536', '--concurrent-ios', '128', '--show-time', '--write-object', '--write-omap', '--write-xattr', '--no-cleanup'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 4706f4c8-3842-11ec-b51d-53e6e728d2d3 -- rados bench 60 write --pool bench_rados -b 65536 -O 65536 --concurrent-ios 128 --show-time --write-object --write-omap --write-xattr --no-cleanup
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
hints = 1
2021-10-28T23:01:50.157887+0000 Maintaining 128 concurrent writes of 65536 bytes to objects of size 65536 for up to 60 seconds or 0 objects
2021-10-28T23:01:50.157899+0000 Object prefix: benchmark_data_node-2.bluefield2.ucsc-cmps10_7
2021-10-28T23:01:50.160686+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-28T23:01:50.160686+0000     0       0         0         0         0         0           -           0
2021-10-28T23:01:51.160815+0000     1     128       567       439   27.4362   27.4375    0.233353    0.251623
2021-10-28T23:01:52.160890+0000     2     128       897       769   24.0298    20.625    0.345951    0.301838
2021-10-28T23:01:53.160978+0000     3     128      1335      1207   25.1441    27.375    0.312475    0.308876
2021-10-28T23:01:54.161098+0000     4     128      1719      1591   24.8573        24    0.285787    0.298961
2021-10-28T23:01:55.161211+0000     5     128      2177      2049   25.6102    28.625    0.280454    0.302651
2021-10-28T23:01:56.161323+0000     6     128      2615      2487   25.9038    27.375    0.267943    0.302567
2021-10-28T23:01:57.161401+0000     7     128      2999      2871   25.6316        24    0.397827    0.305616
2021-10-28T23:01:58.161477+0000     8     128      3383      3255   25.4274        24    0.333411    0.310933
2021-10-28T23:01:59.161553+0000     9     128      3841      3713   25.7825    28.625    0.267301    0.305534
2021-10-28T23:02:00.161633+0000    10     128      4279      4151   25.9415    27.375     0.25662    0.304897
2021-10-28T23:02:01.161708+0000    11     128      4663      4535   25.7648        24    0.254834    0.304989
2021-10-28T23:02:02.161787+0000    12     128      5047      4919   25.6176        24    0.428789    0.306639
2021-10-28T23:02:03.161859+0000    13     128      5431      5303   25.4931        24    0.268674    0.306148
2021-10-28T23:02:04.161933+0000    14     128      5917      5789   25.8416    30.375    0.295538    0.306257
2021-10-28T23:02:05.162016+0000    15     128      6327      6199    25.827    25.625     0.28032    0.307316
2021-10-28T23:02:06.162093+0000    16     128      6785      6657   26.0018    28.625    0.249833    0.305506
2021-10-28T23:02:07.162170+0000    17     128      7223      7095   26.0824    27.375    0.297024    0.304791
2021-10-28T23:02:08.162252+0000    18     128      7479      7351   25.5222        16    0.427422    0.308767
2021-10-28T23:02:09.162338+0000    19     128      7991      7863    25.863        32    0.229468    0.306932
2021-10-28T23:02:10.162454+0000 min lat: 0.0820041 max lat: 0.538696 avg lat: 0.308102
2021-10-28T23:02:10.162454+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-28T23:02:10.162454+0000    20     128      8247      8119   25.3697        16    0.398261    0.308102
2021-10-28T23:02:11.162532+0000    21     128      8449      8321   24.7628    12.625    0.731649    0.317675
2021-10-28T23:02:12.162648+0000    22     128      8577      8449   24.0008         8    0.901753     0.32479
2021-10-28T23:02:13.162764+0000    23     128      8705      8577    23.305         8     1.00676    0.334773
2021-10-28T23:02:14.162875+0000    24     128      8833      8705   22.6673         8     1.13391    0.347554
2021-10-28T23:02:15.162953+0000    25     128      8961      8833   22.0806         8    0.866349    0.355885
2021-10-28T23:02:16.163032+0000    26     128      9089      8961    21.539         8    0.967887    0.363153
2021-10-28T23:02:17.163112+0000    27     128      9271      9143   21.1625    11.375    0.626214    0.371435
2021-10-28T23:02:18.163197+0000    28     128      9399      9271   20.6924         8    0.781703     0.37577
2021-10-28T23:02:19.163278+0000    29     128      9601      9473   20.4142    12.625    0.899807    0.388802
2021-10-28T23:02:20.163393+0000    30     128      9729      9601   20.0003         8    0.947706    0.395558
2021-10-28T23:02:21.163508+0000    31     128      9857      9729   19.6132         8    0.977408    0.403651
2021-10-28T23:02:22.163616+0000    32     128      9985      9857   19.2502         8    0.887045    0.410259
2021-10-28T23:02:23.163730+0000    33     128     10167     10039   19.0116    11.375    0.699653    0.416764
2021-10-28T23:02:24.163841+0000    34     128     10295     10167   18.6876         8     1.02417    0.424973
2021-10-28T23:02:25.163954+0000    35     128     10423     10295   18.3823         8    0.772421    0.428736
2021-10-28T23:02:26.164062+0000    36     128     10625     10497   18.2223    12.625    0.847702     0.43571
2021-10-28T23:02:27.164174+0000    37     128     10753     10625    17.946         8    0.930969     0.44146
2021-10-28T23:02:28.164285+0000    38     128     10881     10753   17.6842         8    0.942264     0.44702
2021-10-28T23:02:29.164399+0000    39     128     11009     10881   17.4359         8     0.97726    0.453255
2021-10-28T23:02:30.164471+0000 min lat: 0.0820041 max lat: 1.30059 avg lat: 0.458209
2021-10-28T23:02:30.164471+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-28T23:02:30.164471+0000    40     128     11137     11009      17.2         8    0.814713    0.458209
2021-10-28T23:02:31.164550+0000    41     128     11319     11191   17.0579    11.375    0.652369    0.463124
2021-10-28T23:02:32.164665+0000    42     128     11521     11393   16.9523    12.625    0.770527    0.467711
2021-10-28T23:02:33.164732+0000    43     128     11703     11575   16.8226    11.375    0.884794    0.472918
2021-10-28T23:02:34.164819+0000    44     128     11777     11649   16.5454     4.625     0.89073    0.475528
2021-10-28T23:02:35.164929+0000    45     128     11959     11831   16.4304    11.375    0.785661    0.483478
2021-10-28T23:02:36.165047+0000    46     128     12215     12087    16.421        16    0.291604    0.485424
2021-10-28T23:02:37.165165+0000    47     128     12599     12471   16.5822        24    0.297671     0.48053
2021-10-28T23:02:38.165279+0000    48     128     13057     12929   16.8331    28.625    0.226815    0.473978
2021-10-28T23:02:39.165395+0000    49     128     13495     13367   17.0481    27.375    0.263062    0.467302
2021-10-28T23:02:40.165507+0000    50     128     13858     13730   17.1609   22.6875    0.442009    0.464642
2021-10-28T23:02:41.165619+0000    51     128     14337     14209   17.4113   29.9375    0.274106    0.458066
2021-10-28T23:02:42.165729+0000    52     128     14721     14593    17.538        24     0.27532    0.454621
2021-10-28T23:02:43.165841+0000    53     128     15159     15031   17.7235    27.375    0.259899    0.449991
2021-10-28T23:02:44.165953+0000    54     128     15543     15415   17.8397        24    0.357677    0.446889
2021-10-28T23:02:45.166064+0000    55     128     15927     15799   17.9517        24    0.422956       0.444
2021-10-28T23:02:46.166172+0000    56     128     16311     16183   18.0596        24    0.382845    0.440819
2021-10-28T23:02:47.166285+0000    57     128     16439     16311   17.8831         8    0.882652     0.44388
2021-10-28T23:02:48.166402+0000    58     128     16567     16439   17.7127         8      1.0064    0.447543
2021-10-28T23:02:49.166520+0000    59     128     16695     16567   17.5481         8    0.925988    0.451611
2021-10-28T23:02:50.166632+0000 min lat: 0.0820041 max lat: 1.30059 avg lat: 0.455035
2021-10-28T23:02:50.166632+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-28T23:02:50.166632+0000    60     128     16823     16695   17.3889         8    0.891486    0.455035
2021-10-28T23:02:51.166782+0000 Total time run:         60.6362
Total writes made:      16823
Write size:             65536
Object size:            65536
Bandwidth (MB/sec):     17.3401
Stddev Bandwidth:       8.68102
Max bandwidth (MB/sec): 32
Min bandwidth (MB/sec): 4.625
Average IOPS:           277
Stddev IOPS:            138.896
Max IOPS:               512
Min IOPS:               74
Average Latency(s):     0.45973
Stddev Latency(s):      0.265945
Max latency(s):         1.30059
Min latency(s):         0.0820041

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:02:52,644125315-07:00] INFO: > Stop system activity collection services[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:02:52,649702114-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:391 - rados_bench() > kill -INT 840626

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:02:52,655116296-07:00] INFO: >> for OSD host[0m
# ./benchmarks/bench-rados:396 - rados_bench() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.5 kill -INT 294333
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:02:52,662608395-07:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.5 kill -INT 294333
[1] 16:02:53 [SUCCESS] ljishen@10.10.2.5

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:02:53,752333155-07:00] INFO: >>> Copy system activity file from OSD host[0m
## ./benchmarks/bench-rados:403 - rados_bench() > basename -- /tmp/bench-rados/sys_activity_64KB_write.dat.osd_host.3
# ./benchmarks/bench-rados:403 - rados_bench() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.5:/tmp/bench-rados/sys_activity_64KB_write.dat.osd_host.3 /mnt/sda8/ceph-research/scripts/output/sys_activity_64KB_write.dat.osd_host.3


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:02:54,939563884-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:03:19,016590935-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 16.82k objects, 1.0 GiB
    usage:   4.0 GiB used, 396 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:03:19,024255568-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:03:28,182305695-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 16.82k objects, 1.0 GiB
    usage:   4.4 GiB used, 396 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:03:28,189988663-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:03:37,289858190-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 16.82k objects, 1.0 GiB
    usage:   3.1 GiB used, 397 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:03:37,296900290-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:03:46,396616773-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 16.82k objects, 1.0 GiB
    usage:   3.3 GiB used, 397 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:03:46,404099564-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:03:55,515036777-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 16.82k objects, 1.0 GiB
    usage:   3.1 GiB used, 397 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:03:55,522156354-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:03:55,528059538-07:00] INFO: > The cluster is idle now.[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:03:55,531561897-07:00] STAGE: Start benchmarking of mode seq ...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:03:55,538176091-07:00] INFO: > Start system activity collection services[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:03:55,543330624-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:341 - rados_bench() > rm --force /mnt/sda8/ceph-research/scripts/output/sys_activity_64KB_seq.dat.rados_bench_host.3
# ./benchmarks/bench-rados:342 - rados_bench() > sar_pid_rados_bench_host=842002
# ./benchmarks/bench-rados:342 - rados_bench() > S_TIME_FORMAT=ISO
# ./benchmarks/bench-rados:342 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/output/sys_activity_64KB_seq.dat.rados_bench_host.3 2

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:03:55,549947664-07:00] INFO: >> for OSD host[0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:03:55,558706318-07:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.5 bash
10.10.2.5: b'294883\n'
[1] 16:03:56 [SUCCESS] ljishen@10.10.2.5
294883

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:03:56,684131306-07:00] INFO: > Run rados bench[0m
# ./benchmarks/bench-rados:379 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/output/rados_bench_64KB_seq.log.3
## ./benchmarks/bench-rados:376 - rados_bench() > count_cpus 32-63,96-127
# ./benchmarks/bench-rados:376 - rados_bench() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 128 --show-time
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:03:56,704310612-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:03:56,707108014-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '4706f4c8-3842-11ec-b51d-53e6e728d2d3', '--', 'rados', 'bench', '99999999', 'seq', '--pool', 'bench_rados', '--concurrent-ios', '128', '--show-time'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 4706f4c8-3842-11ec-b51d-53e6e728d2d3 -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 128 --show-time
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
hints = 1
2021-10-28T23:03:59.780605+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-28T23:03:59.780605+0000     0       0         0         0         0         0           -           0
2021-10-28T23:04:00.780712+0000     1     128      1353      1225   76.5489   76.5625   0.0832762    0.094729
2021-10-28T23:04:01.780796+0000     2     128      2356      2228   69.6159   62.6875    0.106561    0.111188
2021-10-28T23:04:02.780876+0000     3     128      3442      3314   69.0338    67.875   0.0241024    0.112957
2021-10-28T23:04:03.780956+0000     4     128      4492      4364   68.1803    65.625    0.212405    0.111734
2021-10-28T23:04:04.781027+0000     5     128      5603      5475   68.4307   69.4375    0.111318    0.115705
2021-10-28T23:04:05.781101+0000     6     128      6784      6656   69.3268   73.8125   0.0588194    0.114698
2021-10-28T23:04:06.781175+0000     7     128      8036      7908   70.6007     78.25   0.0440345    0.112905
2021-10-28T23:04:07.781255+0000     8     128      9101      8973   70.0952   66.5625    0.224097    0.112045
2021-10-28T23:04:08.781340+0000     9     128     10318     10190   70.7576   76.0625    0.071695    0.112355
2021-10-28T23:04:09.781419+0000    10     128     11127     10999   68.7377   50.5625   0.0553395    0.115585
2021-10-28T23:04:10.781499+0000    11     128     12131     12003   68.1929     62.75   0.0329027    0.116411
2021-10-28T23:04:11.781580+0000    12     128     13120     12992   67.6608   61.8125    0.155397    0.116987
2021-10-28T23:04:12.781655+0000    13     128     14462     14334   68.9075    83.875    0.132362    0.115365
2021-10-28T23:04:13.781731+0000    14     128     15359     15231   67.9897   56.0625    0.152268    0.117227
2021-10-28T23:04:14.781805+0000    15     128     16646     16518   68.8192   80.4375   0.0883523    0.115817
2021-10-28T23:04:15.781914+0000 Total time run:       15.2949
Total reads made:     16823
Read size:            65536
Object size:          65536
Bandwidth (MB/sec):   68.7444
Average IOPS:         1099
Stddev IOPS:          149.611
Max IOPS:             1342
Min IOPS:             809
Average Latency(s):   0.115913
Max latency(s):       0.576508
Min latency(s):       0.00280484

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:04:16,465556736-07:00] INFO: > Stop system activity collection services[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:04:16,471201533-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:391 - rados_bench() > kill -INT 842002

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:04:16,476825450-07:00] INFO: >> for OSD host[0m
# ./benchmarks/bench-rados:396 - rados_bench() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.5 kill -INT 294883
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:04:16,484663240-07:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.5 kill -INT 294883
[1] 16:04:17 [SUCCESS] ljishen@10.10.2.5

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:04:17,584215815-07:00] INFO: >>> Copy system activity file from OSD host[0m
## ./benchmarks/bench-rados:403 - rados_bench() > basename -- /tmp/bench-rados/sys_activity_64KB_seq.dat.osd_host.3
# ./benchmarks/bench-rados:403 - rados_bench() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.5:/tmp/bench-rados/sys_activity_64KB_seq.dat.osd_host.3 /mnt/sda8/ceph-research/scripts/output/sys_activity_64KB_seq.dat.osd_host.3


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:04:18,661527931-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:04:42,645148295-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 16.82k objects, 1.0 GiB
    usage:   3.1 GiB used, 397 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:04:42,652281086-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:04:51,919371097-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 16.82k objects, 1.0 GiB
    usage:   3.1 GiB used, 397 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:04:51,926574380-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:05:00,926654843-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 16.82k objects, 1.0 GiB
    usage:   3.1 GiB used, 397 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:05:00,934325918-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:05:10,009260652-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 16.82k objects, 1.0 GiB
    usage:   3.1 GiB used, 397 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:05:10,016960090-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:05:19,094356548-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 16.82k objects, 1.0 GiB
    usage:   3.1 GiB used, 397 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:05:19,101975034-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:05:19,107567042-07:00] INFO: > The cluster is idle now.[0m


[1;7;39;49m[2021-10-28T16:05:19,110974402-07:00][RUNNING][ROUND 1/4/21] object_size=256KB[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:05:19,114292204-07:00] STAGE: Launch a Ceph cluster (CEPH_HOST: 10.10.2.1, OSD_HOST: 10.10.2.5) ...[0m
# ./benchmarks/bench-rados:183 - launch_ceph_cluster() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.1 bash -euo pipefail
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:05:19,123624750-07:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.1 bash -euo pipefail
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T16:05:19,517276460-07:00] INFO: > Remove existing clusters\x1b[0m\n'
10.10.2.1: b'## bash:17 -  > basename -- /var/lib/ceph/4706f4c8-3842-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'# bash:17 -  > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm --host 10.10.2.1 --host 10.10.2.5 rm-cluster --force --fsid 4706f4c8-3842-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T16:05:19,527892665-07:00] DEBUG: HOST_PARAMS: ['--host', '10.10.2.1', '--host', '10.10.2.5']\x1b[0m\n"
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T16:05:19,531480169-07:00] DEBUG: CEPHADM_PARAMS: ['rm-cluster', '--force', '--fsid', '4706f4c8-3842-11ec-b51d-53e6e728d2d3']\x1b[0m\n"
10.10.2.1: b"# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:63 -  > /tmp/bench-rados/ceph-research/scripts/parallel-ssh --host 10.10.2.1 --host 10.10.2.5 'sudo python3 -u - rm-cluster --force --fsid 4706f4c8-3842-11ec-b51d-53e6e728d2d3'\n"
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T16:05:19,539420554-07:00] DEBUG: command from stdin\x1b[0m\n'
10.10.2.1: b"# /tmp/bench-rados/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host 10.10.2.1 --host 10.10.2.5 'sudo python3 -u - rm-cluster --force --fsid 4706f4c8-3842-11ec-b51d-53e6e728d2d3'\n"
10.10.2.1: b'[1] 16:05:25 [SUCCESS] 10.10.2.1\n[2] 16:05:28 [SUCCESS] 10.10.2.5\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T16:05:28,418932208-07:00] INFO: > Deploy a new cluster\x1b[0m\n'
10.10.2.1: b'# bash:24 -  > /tmp/bench-rados/ceph-research/scripts/ceph-deploy -m 10.10.2.1 -o sm1/10.10.2.5:/dev/nvme0n1\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T16:05:28,430330934-07:00] STAGE: Verify passwordless SSH to hosts, passwordless sudo via ljishen, and the existence of devices on hosts...\x1b[0m\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T16:05:28,434981996-07:00] INFO: > Check the host for the monitor\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:181 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@10.10.2.1 sudo --non-interactive --validate\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T16:05:28,583586802-07:00] INFO: > Check hosts for OSDs\x1b[0m\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T16:05:28,588629781-07:00] INFO: >> Check host 'sm1'\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:200 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 sudo --non-interactive --validate\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T16:05:29,651063783-07:00] INFO: >>> Check device '/dev/nvme0n1'\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:215 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 test -b /dev/nvme0n1\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:226 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 lsblk --nodeps --noheadings --output TYPE /dev/nvme0n1\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:227 - verify_parameters() > grep --word-regexp disk\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T16:05:31,787838066-07:00] STAGE: Reset devices on hosts...\x1b[0m\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T16:05:31,792775276-07:00] INFO: > Resetting device '/dev/nvme0n1' on host 'sm1'\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:288 - reset_host_devices() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 sudo sh\n'
10.10.2.1: b'  Removing ceph--548731b7--78a1--473b--9d1b--e1ab2f9b8f61-osd--block--b9155b29--9564--4a9d--88a5--d67725e6eb6f (252:0)\n'
10.10.2.1: b'  Archiving volume group "ceph-548731b7-78a1-473b-9d1b-e1ab2f9b8f61" metadata (seqno 5).\n'
10.10.2.1: b'  Releasing logical volume "osd-block-b9155b29-9564-4a9d-88a5-d67725e6eb6f"\n'
10.10.2.1: b'  Creating volume group backup "/etc/lvm/backup/ceph-548731b7-78a1-473b-9d1b-e1ab2f9b8f61" (seqno 6).\n'
10.10.2.1: b'  Logical volume "osd-block-b9155b29-9564-4a9d-88a5-d67725e6eb6f" successfully removed\n'
10.10.2.1: b'  Removing physical volume "/dev/nvme0n1" from volume group "ceph-548731b7-78a1-473b-9d1b-e1ab2f9b8f61"\n'
10.10.2.1: b'  Volume group "ceph-548731b7-78a1-473b-9d1b-e1ab2f9b8f61" successfully removed\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T16:05:34,113687055-07:00] STAGE: Bootstrap a new cluster...\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:317 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.1 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T16:05:34,123738428-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T16:05:34,127490961-07:00] DEBUG: CEPHADM_PARAMS: ['bootstrap', '--config', '/tmp/bench-rados/ceph-research/scripts/ceph.conf', '--mon-ip', '10.10.2.1', '--output-dir', '/tmp/bench-rados/deployment_data_root/etc/ceph', '--ssh-user', 'ljishen', '--skip-dashboard', '--no-minimize-config', '--allow-overwrite', '--allow-fqdn-hostname', '--orphan-initial-daemons', '--skip-monitoring-stack', '--single-host-defaults']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.1 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults\n'
10.10.2.1: b'Verifying podman|docker is present...\nVerifying lvm2 is present...\n'
10.10.2.1: b'Verifying time synchronization is in place...\n'
10.10.2.1: b'Unit ntp.service is enabled and running\n'
10.10.2.1: b'Repeating the final host check...\n'
10.10.2.1: b'podman|docker (/usr/bin/docker) is present\n'
10.10.2.1: b'systemctl is present\n'
10.10.2.1: b'lvcreate is present\n'
10.10.2.1: b'Unit ntp.service is enabled and running\n'
10.10.2.1: b'Host looks OK\n'
10.10.2.1: b'Cluster fsid: 8e534f42-3843-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Verifying IP 10.10.2.1 port 3300 ...\n'
10.10.2.1: b'Verifying IP 10.10.2.1 port 6789 ...\n'
10.10.2.1: b'Mon IP `10.10.2.1` is in CIDR network `10.10.2.0/24`\n'
10.10.2.1: b'- internal network (--cluster-network) has not been provided, OSD replication will default to the public_network\n'
10.10.2.1: b'Adjusting default settings to suit single-host cluster...\n'
10.10.2.1: b'Pulling container image quay.io/ceph/ceph:v16...\n'
10.10.2.1: b'Ceph version: ceph version 16.2.6 (ee28fb57e47e9f88813e24bbf4c14496ca299d31) pacific (stable)\nExtracting ceph user uid/gid from container image...\n'
10.10.2.1: b'Creating initial keys...\n'
10.10.2.1: b'Creating initial monmap...\n'
10.10.2.1: b'Creating mon...\n'
10.10.2.1: b'Waiting for mon to start...\nWaiting for mon...\n'
10.10.2.1: b'mon is available\nSetting mon public_network to 10.10.2.0/24\n'
10.10.2.1: b'Wrote config to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf\n'
10.10.2.1: b'Wrote keyring to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring\nCreating mgr...\n'
10.10.2.1: b'Verifying port 9283 ...\n'
10.10.2.1: b'Waiting for mgr to start...\nWaiting for mgr...\n'
10.10.2.1: b'mgr not available, waiting (1/15)...\n'
10.10.2.1: b'mgr not available, waiting (2/15)...\n'
10.10.2.1: b'mgr is available\n'
10.10.2.1: b'Enabling cephadm module...\n'
10.10.2.1: b'Waiting for the mgr to restart...\nWaiting for mgr epoch 5...\n'
10.10.2.1: b'mgr epoch 5 is available\nSetting orchestrator backend to cephadm...\n'
10.10.2.1: b'Generating ssh key...\n'
10.10.2.1: b'Wrote public SSH key to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub\nAdding key to ljishen@localhost authorized_keys...\n'
10.10.2.1: b'Adding host node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us...\n'
10.10.2.1: b'Deploying unmanaged mon service...\n'
10.10.2.1: b'Deploying unmanaged mgr service...\n'
10.10.2.1: b'You can access the Ceph CLI with:\n\n\tsudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --fsid 8e534f42-3843-11ec-b51d-53e6e728d2d3 -c /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf -k /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring\n\n'
10.10.2.1: b'Please consider enabling telemetry to help improve Ceph:\n\n\tceph telemetry on\n\nFor more information see:\n\n\thttps://docs.ceph.com/docs/pacific/mgr/telemetry/\n\n'
10.10.2.1: b'Bootstrap complete.\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T16:06:38,334557678-07:00] INFO: Sleep for 20 seconds\x1b[0m\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T16:06:58,342239246-07:00] INFO: > Set the default number of monitor daemons to 1\x1b[0m\n# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:337 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T16:06:58,352601794-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T16:06:58,356270850-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mon', '1']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1\n'
10.10.2.1: b'Inferring fsid 8e534f42-3843-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/8e534f42-3843-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b'Scheduled mon update...\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T16:07:07,679609537-07:00] INFO: > Set the default number of manager daemons to 1\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:342 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T16:07:07,689772600-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T16:07:07,693638988-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mgr', '1']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1\n'
10.10.2.1: b'Inferring fsid 8e534f42-3843-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/8e534f42-3843-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b'Scheduled mgr update...\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T16:07:17,222741374-07:00] STAGE: Add hosts to the cluster...\x1b[0m\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T16:07:17,229143478-07:00] INFO: > Adding host 'sm1'\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:361 - add_hosts() > ssh-copy-id -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -f -i /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub ljishen@sm1\n'
10.10.2.1: b'/usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: "/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub"\n'
10.10.2.1: b'\nNumber of key(s) added: 1\n\nNow try logging into the machine, with:   "ssh -o \'GlobalKnownHostsFile=/dev/null\' -o \'LogLevel=ERROR\' -o \'PasswordAuthentication=no\' -o \'StrictHostKeyChecking=no\' -o \'UserKnownHostsFile=/dev/null\' \'ljishen@sm1\'"\nand check to make sure that only the key(s) you wanted were added.\n\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:367 - add_hosts() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add sm1 10.10.2.5\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T16:07:18,360834665-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T16:07:18,364893834-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'host', 'add', 'sm1', '10.10.2.5']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add sm1 10.10.2.5\n'
10.10.2.1: b'Inferring fsid 8e534f42-3843-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/8e534f42-3843-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b"Added host 'sm1' with addr '10.10.2.5'\n"
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T16:07:28,874100010-07:00] INFO: Sleep for 20 seconds\x1b[0m\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T16:07:48,879034616-07:00] STAGE: Add OSDs to the cluster...\x1b[0m\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T16:07:48,885585510-07:00] INFO: > Adding devices ['/dev/nvme0n1'] on host 'sm1' as OSDs\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:388 - add_host_devices_as_osds() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd sm1:/dev/nvme0n1\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T16:07:48,895840025-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T16:07:48,899228174-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'daemon', 'add', 'osd', 'sm1:/dev/nvme0n1']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd sm1:/dev/nvme0n1\n'
10.10.2.1: b'Inferring fsid 8e534f42-3843-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/8e534f42-3843-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b"Created osd(s) 0 on host 'sm1'\n"
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T16:08:13,817286450-07:00] INFO: > Sleep for 20 seconds\x1b[0m\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T16:08:33,822679567-07:00] STAGE: Show Cluster Status\x1b[0m\n# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:408 - show_status() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T16:08:33,832928741-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T16:08:33,836468375-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', '--status']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status\n'
10.10.2.1: b'Inferring fsid 8e534f42-3843-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/8e534f42-3843-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b'  cluster:\n    id:     8e534f42-3843-11ec-b51d-53e6e728d2d3\n    health: HEALTH_WARN\n            1 pool(s) have no replicas configured\n \n  services:\n    mon: 1 daemons, quorum node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us (age 2m)\n    mgr: node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us.yqwpfp(active, since 2m)\n    osd: 1 osds: 1 up (since 20s), 1 in (since 40s)\n \n  data:\n    pools:   1 pools, 1 pgs\n    objects: 0 objects, 0 B\n    usage:   4.7 MiB used, 400 GiB / 400 GiB avail\n    pgs:     1 active+clean\n \n'
[1] 16:08:42 [SUCCESS] ljishen@10.10.2.1

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:05:19,517276460-07:00] INFO: > Remove existing clusters[0m
## bash:17 -  > basename -- /var/lib/ceph/4706f4c8-3842-11ec-b51d-53e6e728d2d3
# bash:17 -  > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm --host 10.10.2.1 --host 10.10.2.5 rm-cluster --force --fsid 4706f4c8-3842-11ec-b51d-53e6e728d2d3
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:05:19,527892665-07:00] DEBUG: HOST_PARAMS: ['--host', '10.10.2.1', '--host', '10.10.2.5'][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:05:19,531480169-07:00] DEBUG: CEPHADM_PARAMS: ['rm-cluster', '--force', '--fsid', '4706f4c8-3842-11ec-b51d-53e6e728d2d3'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:63 -  > /tmp/bench-rados/ceph-research/scripts/parallel-ssh --host 10.10.2.1 --host 10.10.2.5 'sudo python3 -u - rm-cluster --force --fsid 4706f4c8-3842-11ec-b51d-53e6e728d2d3'
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:05:19,539420554-07:00] DEBUG: command from stdin[0m
# /tmp/bench-rados/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host 10.10.2.1 --host 10.10.2.5 'sudo python3 -u - rm-cluster --force --fsid 4706f4c8-3842-11ec-b51d-53e6e728d2d3'
[1] 16:05:25 [SUCCESS] 10.10.2.1
[2] 16:05:28 [SUCCESS] 10.10.2.5

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:05:28,418932208-07:00] INFO: > Deploy a new cluster[0m
# bash:24 -  > /tmp/bench-rados/ceph-research/scripts/ceph-deploy -m 10.10.2.1 -o sm1/10.10.2.5:/dev/nvme0n1


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:05:28,430330934-07:00] STAGE: Verify passwordless SSH to hosts, passwordless sudo via ljishen, and the existence of devices on hosts...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:05:28,434981996-07:00] INFO: > Check the host for the monitor[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:181 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@10.10.2.1 sudo --non-interactive --validate

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:05:28,583586802-07:00] INFO: > Check hosts for OSDs[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:05:28,588629781-07:00] INFO: >> Check host 'sm1'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:200 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 sudo --non-interactive --validate

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:05:29,651063783-07:00] INFO: >>> Check device '/dev/nvme0n1'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:215 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 test -b /dev/nvme0n1
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:226 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 lsblk --nodeps --noheadings --output TYPE /dev/nvme0n1
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:227 - verify_parameters() > grep --word-regexp disk


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:05:31,787838066-07:00] STAGE: Reset devices on hosts...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:05:31,792775276-07:00] INFO: > Resetting device '/dev/nvme0n1' on host 'sm1'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:288 - reset_host_devices() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 sudo sh
  Removing ceph--548731b7--78a1--473b--9d1b--e1ab2f9b8f61-osd--block--b9155b29--9564--4a9d--88a5--d67725e6eb6f (252:0)
  Archiving volume group "ceph-548731b7-78a1-473b-9d1b-e1ab2f9b8f61" metadata (seqno 5).
  Releasing logical volume "osd-block-b9155b29-9564-4a9d-88a5-d67725e6eb6f"
  Creating volume group backup "/etc/lvm/backup/ceph-548731b7-78a1-473b-9d1b-e1ab2f9b8f61" (seqno 6).
  Logical volume "osd-block-b9155b29-9564-4a9d-88a5-d67725e6eb6f" successfully removed
  Removing physical volume "/dev/nvme0n1" from volume group "ceph-548731b7-78a1-473b-9d1b-e1ab2f9b8f61"
  Volume group "ceph-548731b7-78a1-473b-9d1b-e1ab2f9b8f61" successfully removed


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:05:34,113687055-07:00] STAGE: Bootstrap a new cluster...[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:317 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.1 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:05:34,123738428-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:05:34,127490961-07:00] DEBUG: CEPHADM_PARAMS: ['bootstrap', '--config', '/tmp/bench-rados/ceph-research/scripts/ceph.conf', '--mon-ip', '10.10.2.1', '--output-dir', '/tmp/bench-rados/deployment_data_root/etc/ceph', '--ssh-user', 'ljishen', '--skip-dashboard', '--no-minimize-config', '--allow-overwrite', '--allow-fqdn-hostname', '--orphan-initial-daemons', '--skip-monitoring-stack', '--single-host-defaults'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.1 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults
Verifying podman|docker is present...
Verifying lvm2 is present...
Verifying time synchronization is in place...
Unit ntp.service is enabled and running
Repeating the final host check...
podman|docker (/usr/bin/docker) is present
systemctl is present
lvcreate is present
Unit ntp.service is enabled and running
Host looks OK
Cluster fsid: 8e534f42-3843-11ec-b51d-53e6e728d2d3
Verifying IP 10.10.2.1 port 3300 ...
Verifying IP 10.10.2.1 port 6789 ...
Mon IP `10.10.2.1` is in CIDR network `10.10.2.0/24`
- internal network (--cluster-network) has not been provided, OSD replication will default to the public_network
Adjusting default settings to suit single-host cluster...
Pulling container image quay.io/ceph/ceph:v16...
Ceph version: ceph version 16.2.6 (ee28fb57e47e9f88813e24bbf4c14496ca299d31) pacific (stable)
Extracting ceph user uid/gid from container image...
Creating initial keys...
Creating initial monmap...
Creating mon...
Waiting for mon to start...
Waiting for mon...
mon is available
Setting mon public_network to 10.10.2.0/24
Wrote config to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf
Wrote keyring to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring
Creating mgr...
Verifying port 9283 ...
Waiting for mgr to start...
Waiting for mgr...
mgr not available, waiting (1/15)...
mgr not available, waiting (2/15)...
mgr is available
Enabling cephadm module...
Waiting for the mgr to restart...
Waiting for mgr epoch 5...
mgr epoch 5 is available
Setting orchestrator backend to cephadm...
Generating ssh key...
Wrote public SSH key to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub
Adding key to ljishen@localhost authorized_keys...
Adding host node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us...
Deploying unmanaged mon service...
Deploying unmanaged mgr service...
You can access the Ceph CLI with:

	sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --fsid 8e534f42-3843-11ec-b51d-53e6e728d2d3 -c /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf -k /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring

Please consider enabling telemetry to help improve Ceph:

	ceph telemetry on

For more information see:

	https://docs.ceph.com/docs/pacific/mgr/telemetry/

Bootstrap complete.

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:06:38,334557678-07:00] INFO: Sleep for 20 seconds[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:06:58,342239246-07:00] INFO: > Set the default number of monitor daemons to 1[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:337 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:06:58,352601794-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:06:58,356270850-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mon', '1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1
Inferring fsid 8e534f42-3843-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/8e534f42-3843-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Scheduled mon update...

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:07:07,679609537-07:00] INFO: > Set the default number of manager daemons to 1[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:342 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:07:07,689772600-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:07:07,693638988-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mgr', '1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1
Inferring fsid 8e534f42-3843-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/8e534f42-3843-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Scheduled mgr update...


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:07:17,222741374-07:00] STAGE: Add hosts to the cluster...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:07:17,229143478-07:00] INFO: > Adding host 'sm1'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:361 - add_hosts() > ssh-copy-id -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -f -i /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub ljishen@sm1
/usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: "/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub"

Number of key(s) added: 1

Now try logging into the machine, with:   "ssh -o 'GlobalKnownHostsFile=/dev/null' -o 'LogLevel=ERROR' -o 'PasswordAuthentication=no' -o 'StrictHostKeyChecking=no' -o 'UserKnownHostsFile=/dev/null' 'ljishen@sm1'"
and check to make sure that only the key(s) you wanted were added.

# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:367 - add_hosts() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add sm1 10.10.2.5
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:07:18,360834665-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:07:18,364893834-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'host', 'add', 'sm1', '10.10.2.5'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add sm1 10.10.2.5
Inferring fsid 8e534f42-3843-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/8e534f42-3843-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Added host 'sm1' with addr '10.10.2.5'

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:07:28,874100010-07:00] INFO: Sleep for 20 seconds[0m


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:07:48,879034616-07:00] STAGE: Add OSDs to the cluster...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:07:48,885585510-07:00] INFO: > Adding devices ['/dev/nvme0n1'] on host 'sm1' as OSDs[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:388 - add_host_devices_as_osds() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd sm1:/dev/nvme0n1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:07:48,895840025-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:07:48,899228174-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'daemon', 'add', 'osd', 'sm1:/dev/nvme0n1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd sm1:/dev/nvme0n1
Inferring fsid 8e534f42-3843-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/8e534f42-3843-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Created osd(s) 0 on host 'sm1'

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:08:13,817286450-07:00] INFO: > Sleep for 20 seconds[0m


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:08:33,822679567-07:00] STAGE: Show Cluster Status[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:408 - show_status() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:08:33,832928741-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:08:33,836468375-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', '--status'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status
Inferring fsid 8e534f42-3843-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/8e534f42-3843-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
  cluster:
    id:     8e534f42-3843-11ec-b51d-53e6e728d2d3
    health: HEALTH_WARN
            1 pool(s) have no replicas configured
 
  services:
    mon: 1 daemons, quorum node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us (age 2m)
    mgr: node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us.yqwpfp(active, since 2m)
    osd: 1 osds: 1 up (since 20s), 1 in (since 40s)
 
  data:
    pools:   1 pools, 1 pgs
    objects: 0 objects, 0 B
    usage:   4.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     1 active+clean
 

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:08:42,596705937-07:00] INFO: > Copy configuration files to local[0m
# ./benchmarks/bench-rados:220 - launch_ceph_cluster() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.1 sudo chown --changes --recursive ljishen /tmp/bench-rados/deployment_data_root/etc/ceph
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:08:42,604622435-07:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.1 sudo chown --changes --recursive ljishen /tmp/bench-rados/deployment_data_root/etc/ceph
[1] 16:08:42 [SUCCESS] ljishen@10.10.2.1
# ./benchmarks/bench-rados:223 - launch_ceph_cluster() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.1:/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf
# ./benchmarks/bench-rados:226 - launch_ceph_cluster() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.1:/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:08:43,081572917-07:00] STAGE: Update local ceph.conf as a client...[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:08:43,085079265-07:00] STAGE: Configure Ceph pool...[0m
# ./benchmarks/bench-rados:257 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_max_backfills 32
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:08:43,106913900-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:08:43,109786203-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '8e534f42-3843-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'config', 'set', 'osd', 'osd_max_backfills', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 8e534f42-3843-11ec-b51d-53e6e728d2d3 -- ceph config set osd osd_max_backfills 32
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:258 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_active 32
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:08:47,053660951-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:08:47,056816287-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '8e534f42-3843-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_active', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 8e534f42-3843-11ec-b51d-53e6e728d2d3 -- ceph config set osd osd_recovery_max_active 32
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:259 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_single_start 8
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:08:51,236841691-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:08:51,239618835-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '8e534f42-3843-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_single_start', '8'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 8e534f42-3843-11ec-b51d-53e6e728d2d3 -- ceph config set osd osd_recovery_max_single_start 8
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:260 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_op_priority 63
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:08:55,378080046-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:08:55,381010638-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '8e534f42-3843-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_op_priority', '63'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 8e534f42-3843-11ec-b51d-53e6e728d2d3 -- ceph config set osd osd_recovery_op_priority 63
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:262 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config show-with-defaults osd.0
# ./benchmarks/bench-rados:263 - configure_ceph_pool() > grep 'osd_max_backfills\|osd_recovery'
# ./benchmarks/bench-rados:263 - configure_ceph_pool() > column -t -s ' '
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
osd_max_backfills                        32         mon
osd_recovery_cost                        20971520   default
osd_recovery_delay_start                 0.000000   default
osd_recovery_max_active                  32         mon
osd_recovery_max_active_hdd              3          default
osd_recovery_max_active_ssd              10         default
osd_recovery_max_chunk                   8388608    default
osd_recovery_max_omap_entries_per_chunk  8096       default
osd_recovery_max_single_start            8          mon
osd_recovery_op_priority                 63         mon
osd_recovery_op_warn_multiple            16         default
osd_recovery_priority                    5          default
osd_recovery_retry_interval              30.000000  default
osd_recovery_sleep                       0.000000   default
osd_recovery_sleep_hdd                   0.100000   default
osd_recovery_sleep_hybrid                0.025000   default
osd_recovery_sleep_ssd                   0.000000   default
# ./benchmarks/bench-rados:265 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool create bench_rados 128 128 replicated --size 1 --pg-num-min 128
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:09:03,278966653-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:09:03,281925890-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '8e534f42-3843-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'create', 'bench_rados', '128', '128', 'replicated', '--size', '1', '--pg-num-min', '128'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 8e534f42-3843-11ec-b51d-53e6e728d2d3 -- ceph osd pool create bench_rados 128 128 replicated --size 1 --pg-num-min 128
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
pool 'bench_rados' created
# ./benchmarks/bench-rados:267 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados min_size 1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:09:08,045468037-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:09:08,048811527-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '8e534f42-3843-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'min_size', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 8e534f42-3843-11ec-b51d-53e6e728d2d3 -- ceph osd pool set bench_rados min_size 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 min_size to 1
# ./benchmarks/bench-rados:268 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados pg_autoscale_mode off
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:09:13,109980599-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:09:13,112999358-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '8e534f42-3843-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'pg_autoscale_mode', 'off'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 8e534f42-3843-11ec-b51d-53e6e728d2d3 -- ceph osd pool set bench_rados pg_autoscale_mode off
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 pg_autoscale_mode to off
# ./benchmarks/bench-rados:269 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool application enable bench_rados benchmark
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:09:17,684806369-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:09:17,687667340-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '8e534f42-3843-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'application', 'enable', 'bench_rados', 'benchmark'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 8e534f42-3843-11ec-b51d-53e6e728d2d3 -- ceph osd pool application enable bench_rados benchmark
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
enabled application 'benchmark' on pool 'bench_rados'
# ./benchmarks/bench-rados:270 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados noscrub 1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:09:22,275445739-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:09:22,278483854-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '8e534f42-3843-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'noscrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 8e534f42-3843-11ec-b51d-53e6e728d2d3 -- ceph osd pool set bench_rados noscrub 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 noscrub to 1
# ./benchmarks/bench-rados:271 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados nodeep-scrub 1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:09:26,508238518-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:09:26,511360210-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '8e534f42-3843-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'nodeep-scrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 8e534f42-3843-11ec-b51d-53e6e728d2d3 -- ceph osd pool set bench_rados nodeep-scrub 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 nodeep-scrub to 1
# ./benchmarks/bench-rados:272 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool ls detail
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:09:31,600091727-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:09:31,602971303-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '8e534f42-3843-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'ls', 'detail'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 8e534f42-3843-11ec-b51d-53e6e728d2d3 -- ceph osd pool ls detail
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
pool 1 'device_health_metrics' replicated size 1 min_size 1 crush_rule 0 object_hash rjenkins pg_num 64 pgp_num 64 autoscale_mode on last_change 18 lfor 0/0/15 flags hashpspool stripe_width 0 pg_num_min 1 application mgr_devicehealth
pool 2 'bench_rados' replicated size 1 min_size 1 crush_rule 0 object_hash rjenkins pg_num 128 pgp_num 128 autoscale_mode off last_change 20 flags hashpspool,noscrub,nodeep-scrub stripe_width 0 pg_num_min 128 application benchmark

# ./benchmarks/bench-rados:273 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd df tree
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:09:35,473853270-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:09:35,476800995-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '8e534f42-3843-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'df', 'tree'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 8e534f42-3843-11ec-b51d-53e6e728d2d3 -- ceph osd df tree
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
ID  CLASS  WEIGHT   REWEIGHT  SIZE     RAW USE  DATA     OMAP  META     AVAIL    %USE  VAR   PGS  STATUS  TYPE NAME    
-1         0.39059         -  400 GiB  5.7 MiB  168 KiB   0 B  5.6 MiB  400 GiB  0.00  1.00    -          root default 
-3         0.39059         -  400 GiB  5.7 MiB  168 KiB   0 B  5.6 MiB  400 GiB  0.00  1.00    -              host sm1 
 0    ssd  0.39059   1.00000  400 GiB  5.7 MiB  168 KiB   0 B  5.6 MiB  400 GiB  0.00  1.00  192      up          osd.0
                       TOTAL  400 GiB  5.7 MiB  168 KiB   0 B  5.6 MiB  400 GiB  0.00                                  
MIN/MAX VAR: 1.00/1.00  STDDEV: 0


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:09:39,401624367-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:10:03,547081097-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:10:12,628242350-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:10:21,627923064-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:10:21,634882067-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:10:30,750861332-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:10:30,757944941-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:10:39,667891136-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:10:39,676319258-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:10:48,773845940-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:10:48,782131934-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:10:57,813289412-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:10:57,820923659-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:10:57,826626425-07:00] INFO: > The cluster is idle now.[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:10:57,830221349-07:00] STAGE: Start benchmarking of mode write ...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:10:57,836748289-07:00] INFO: > Start system activity collection services[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:10:57,842086178-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:341 - rados_bench() > rm --force /mnt/sda8/ceph-research/scripts/output/sys_activity_256KB_write.dat.rados_bench_host.1
# ./benchmarks/bench-rados:342 - rados_bench() > sar_pid_rados_bench_host=854247
# ./benchmarks/bench-rados:342 - rados_bench() > S_TIME_FORMAT=ISO
# ./benchmarks/bench-rados:342 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/output/sys_activity_256KB_write.dat.rados_bench_host.1 2

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:10:57,848639347-07:00] INFO: >> for OSD host[0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:10:57,857537885-07:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.5 bash
10.10.2.5: b'299771\n'
[1] 16:10:58 [SUCCESS] ljishen@10.10.2.5
299771

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:10:58,983962225-07:00] INFO: > Run rados bench[0m
# ./benchmarks/bench-rados:370 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/output/rados_bench_256KB_write.log.1
## ./benchmarks/bench-rados:361 - rados_bench() > count_cpus 32-63,96-127
# ./benchmarks/bench-rados:361 - rados_bench() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 60 write --pool bench_rados -b 262144 -O 262144 --concurrent-ios 128 --show-time --write-object --write-omap --write-xattr --no-cleanup
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:10:59,003700451-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:10:59,006636293-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '8e534f42-3843-11ec-b51d-53e6e728d2d3', '--', 'rados', 'bench', '60', 'write', '--pool', 'bench_rados', '-b', '262144', '-O', '262144', '--concurrent-ios', '128', '--show-time', '--write-object', '--write-omap', '--write-xattr', '--no-cleanup'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 8e534f42-3843-11ec-b51d-53e6e728d2d3 -- rados bench 60 write --pool bench_rados -b 262144 -O 262144 --concurrent-ios 128 --show-time --write-object --write-omap --write-xattr --no-cleanup
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
hints = 1
2021-10-28T23:11:02.084102+0000 Maintaining 128 concurrent writes of 262144 bytes to objects of size 262144 for up to 60 seconds or 0 objects
2021-10-28T23:11:02.084117+0000 Object prefix: benchmark_data_node-2.bluefield2.ucsc-cmps10_7
2021-10-28T23:11:02.093180+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-28T23:11:02.093180+0000     0       0         0         0         0         0           -           0
2021-10-28T23:11:03.093277+0000     1     128       211        83    20.749     20.75    0.792198    0.785618
2021-10-28T23:11:04.093349+0000     2     128       339       211   26.3734        32    0.786473    0.853053
2021-10-28T23:11:05.093420+0000     3     128       467       339   28.2482        32     1.14113    0.944477
2021-10-28T23:11:06.093491+0000     4     128       595       467   29.1856        32    0.864448    0.926657
2021-10-28T23:11:07.093562+0000     5     128       723       595    29.748        32     1.20145     0.99191
2021-10-28T23:11:08.093632+0000     6     128       851       723    30.123        32    0.909498    0.971375
2021-10-28T23:11:09.093698+0000     7     128       979       851   30.3908        32     1.02678    0.973458
2021-10-28T23:11:10.093806+0000     8     128      1107       979   30.5915        32    0.854511      0.9667
2021-10-28T23:11:11.093930+0000     9     128      1235      1107   30.7476        32     1.10917    0.981855
2021-10-28T23:11:12.094055+0000    10     128      1363      1235   30.8724        32     1.03397    0.981614
2021-10-28T23:11:13.094187+0000    11     128      1491      1363   30.9746        32     1.04332    0.993925
2021-10-28T23:11:14.094314+0000    12     128      1619      1491   31.0597        32    0.896277    0.983887
2021-10-28T23:11:15.094466+0000    13     128      1777      1649   31.7085      39.5    0.824867    0.983296
2021-10-28T23:11:16.094583+0000    14     128      1875      1747   31.1934      24.5    0.857244    0.976209
2021-10-28T23:11:17.094700+0000    15     128      2003      1875   31.2469        32      1.0481      0.9811
2021-10-28T23:11:18.094818+0000    16     128      2131      2003   31.2938        32     1.17566    0.989249
2021-10-28T23:11:19.094943+0000    17     128      2177      2049   30.1293      11.5     1.11907     0.99133
2021-10-28T23:11:20.095065+0000    18     128      2177      2049   28.4554         0           -     0.99133
2021-10-28T23:11:21.095191+0000    19     128      2259      2131   28.0366     10.25     2.57445     1.05252
2021-10-28T23:11:22.095295+0000 min lat: 0.47244 max lat: 3.1453 avg lat: 1.09467
2021-10-28T23:11:22.095295+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-28T23:11:22.095295+0000    20     128      2303      2175   27.1847        11     3.14453     1.09467
2021-10-28T23:11:23.095420+0000    21     128      2303      2175   25.8902         0           -     1.09467
2021-10-28T23:11:24.095539+0000    22     128      2387      2259   25.6678      10.5     3.44824     1.18412
2021-10-28T23:11:25.095629+0000    23     128      2387      2259   24.5518         0           -     1.18412
2021-10-28T23:11:26.095738+0000    24     128      2433      2305   24.0079      5.75     3.84112     1.23543
2021-10-28T23:11:27.095860+0000    25     128      2515      2387   23.8675      20.5      2.9295     1.29353
2021-10-28T23:11:28.095976+0000    26     128      2515      2387   22.9495         0           -     1.29353
2021-10-28T23:11:29.096099+0000    27     128      2554      2426   22.4606     4.875     3.20331     1.32407
2021-10-28T23:11:30.096244+0000    28     128      2643      2515   22.4529     22.25     3.06573     1.38819
2021-10-28T23:11:31.096362+0000    29     128      2643      2515   21.6787         0           -     1.38819
2021-10-28T23:11:32.096480+0000    30     128      2689      2561   21.3394      5.75     1.87322     1.41022
2021-10-28T23:11:33.096593+0000    31     128      2771      2643   21.3122      20.5     2.88648     1.45605
2021-10-28T23:11:34.096695+0000    32     128      2817      2689   21.0055      11.5     2.67229     1.47637
2021-10-28T23:11:35.096848+0000    33     128      2817      2689    20.369         0           -     1.47637
2021-10-28T23:11:36.096963+0000    34     128      2899      2771   20.3728     10.25     2.96074     1.52017
2021-10-28T23:11:37.097082+0000    35     128      2945      2817   20.1192      11.5     2.81925     1.54184
2021-10-28T23:11:38.097190+0000    36     128      2945      2817   19.5603         0           -     1.54184
2021-10-28T23:11:39.097307+0000    37     128      3027      2899   19.5857     10.25     2.68483     1.57442
2021-10-28T23:11:40.097387+0000    38     128      3073      2945   19.3729      11.5     2.84625     1.59401
2021-10-28T23:11:41.097476+0000    39     128      3073      2945   18.8761         0           -     1.59401
2021-10-28T23:11:42.097592+0000 min lat: 0.47244 max lat: 5.14053 avg lat: 1.62704
2021-10-28T23:11:42.097592+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-28T23:11:42.097592+0000    40     128      3155      3027   18.9167     10.25     2.81694     1.62704
2021-10-28T23:11:43.097751+0000    41     128      3201      3073   18.7357      11.5     2.79261     1.64435
2021-10-28T23:11:44.097912+0000    42     128      3283      3155   18.7777      20.5     2.84375     1.67399
2021-10-28T23:11:45.098033+0000    43     128      3411      3283   19.0851        32     1.09749      1.6652
2021-10-28T23:11:46.098146+0000    44     128      3539      3411   19.3785        32     1.00305     1.64109
2021-10-28T23:11:47.098264+0000    45     128      3667      3539   19.6589        32    0.810045     1.61332
2021-10-28T23:11:48.098387+0000    46     128      3795      3667   19.9271        32    0.835042     1.58591
2021-10-28T23:11:49.098504+0000    47     128      3967      3839   20.4179        43    0.904709     1.55909
2021-10-28T23:11:50.098613+0000    48     128      4051      3923     20.43        21    0.942821     1.54601
2021-10-28T23:11:51.098739+0000    49     128      4179      4051    20.666        32     1.05278     1.53089
2021-10-28T23:11:52.098873+0000    50     128      4225      4097   20.4827      11.5     1.17814     1.52687
2021-10-28T23:11:53.098990+0000    51     128      4307      4179    20.483      20.5     2.26336     1.54105
2021-10-28T23:11:54.099093+0000    52     128      4307      4179   20.0891         0           -     1.54105
2021-10-28T23:11:55.099208+0000    53     128      4353      4225    19.927      5.75      2.8033     1.55448
2021-10-28T23:11:56.099325+0000    54     128      4353      4225    19.558         0           -     1.55448
2021-10-28T23:11:57.099436+0000    55     128      4435      4307   19.5751     10.25     3.71851      1.5962
2021-10-28T23:11:58.099541+0000    56     128      4435      4307   19.2255         0           -      1.5962
2021-10-28T23:11:59.099656+0000    57     128      4435      4307   18.8882         0           -      1.5962
2021-10-28T23:12:00.099770+0000    58     128      4481      4353   18.7608   3.83333     4.99328     1.63215
2021-10-28T23:12:01.099892+0000    59     128      4481      4353   18.4428         0           -     1.63215
2021-10-28T23:12:02.100026+0000 min lat: 0.47244 max lat: 5.14053 avg lat: 1.63215
2021-10-28T23:12:02.100026+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-28T23:12:02.100026+0000    60     128      4481      4353   18.1354         0           -     1.63215
2021-10-28T23:12:03.100147+0000    61     128      4481      4353   17.8381         0           -     1.63215
2021-10-28T23:12:04.100262+0000    62      46      4481      4435    17.881     5.125      6.5675     1.72344
2021-10-28T23:12:05.100382+0000    63      46      4481      4435   17.5972         0           -     1.72344
2021-10-28T23:12:06.100521+0000 Total time run:         63.8358
Total writes made:      4481
Write size:             262144
Object size:            262144
Bandwidth (MB/sec):     17.5489
Stddev Bandwidth:       13.4835
Max bandwidth (MB/sec): 43
Min bandwidth (MB/sec): 0
Average IOPS:           70
Stddev IOPS:            53.9518
Max IOPS:               172
Min IOPS:               0
Average Latency(s):     1.77515
Stddev Latency(s):      1.28394
Max latency(s):         6.77171
Min latency(s):         0.47244

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:12:07,033643927-07:00] INFO: > Stop system activity collection services[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:12:07,039311979-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:391 - rados_bench() > kill -INT 854247

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:12:07,044809658-07:00] INFO: >> for OSD host[0m
# ./benchmarks/bench-rados:396 - rados_bench() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.5 kill -INT 299771
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:12:07,052496354-07:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.5 kill -INT 299771
[1] 16:12:08 [SUCCESS] ljishen@10.10.2.5

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:12:08,167886114-07:00] INFO: >>> Copy system activity file from OSD host[0m
## ./benchmarks/bench-rados:403 - rados_bench() > basename -- /tmp/bench-rados/sys_activity_256KB_write.dat.osd_host.1
# ./benchmarks/bench-rados:403 - rados_bench() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.5:/tmp/bench-rados/sys_activity_256KB_write.dat.osd_host.1 /mnt/sda8/ceph-research/scripts/output/sys_activity_256KB_write.dat.osd_host.1


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:12:09,335654829-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:12:33,295629080-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 4.48k objects, 1.1 GiB
    usage:   4.6 GiB used, 395 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:12:33,302825290-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:12:42,384793963-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 4.48k objects, 1.1 GiB
    usage:   3.5 GiB used, 396 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:12:42,392135076-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:12:51,376331378-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 4.48k objects, 1.1 GiB
    usage:   3.4 GiB used, 397 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:12:51,383936880-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:13:00,373351180-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 4.48k objects, 1.1 GiB
    usage:   3.3 GiB used, 397 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:13:00,380621390-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:13:09,473265663-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 4.48k objects, 1.1 GiB
    usage:   3.3 GiB used, 397 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:13:09,480917763-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:13:09,486642150-07:00] INFO: > The cluster is idle now.[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:13:09,490147526-07:00] STAGE: Start benchmarking of mode seq ...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:13:09,496569618-07:00] INFO: > Start system activity collection services[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:13:09,502143121-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:341 - rados_bench() > rm --force /mnt/sda8/ceph-research/scripts/output/sys_activity_256KB_seq.dat.rados_bench_host.1
# ./benchmarks/bench-rados:342 - rados_bench() > sar_pid_rados_bench_host=857221
# ./benchmarks/bench-rados:342 - rados_bench() > S_TIME_FORMAT=ISO
# ./benchmarks/bench-rados:342 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/output/sys_activity_256KB_seq.dat.rados_bench_host.1 2

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:13:09,508802951-07:00] INFO: >> for OSD host[0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:13:09,517389481-07:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.5 bash
10.10.2.5: b'300315\n'
[1] 16:13:10 [SUCCESS] ljishen@10.10.2.5
300315

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:13:10,647291944-07:00] INFO: > Run rados bench[0m
# ./benchmarks/bench-rados:379 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/output/rados_bench_256KB_seq.log.1
## ./benchmarks/bench-rados:376 - rados_bench() > count_cpus 32-63,96-127
# ./benchmarks/bench-rados:376 - rados_bench() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 128 --show-time
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:13:10,667745848-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:13:10,670525085-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '8e534f42-3843-11ec-b51d-53e6e728d2d3', '--', 'rados', 'bench', '99999999', 'seq', '--pool', 'bench_rados', '--concurrent-ios', '128', '--show-time'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 8e534f42-3843-11ec-b51d-53e6e728d2d3 -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 128 --show-time
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
hints = 1
2021-10-28T23:13:13.648175+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-28T23:13:13.648175+0000     0       0         0         0         0         0           -           0
2021-10-28T23:13:14.648338+0000     1     128       442       314   78.4806      78.5     0.36219    0.311012
2021-10-28T23:13:15.648465+0000     2     128       918       790   98.7315       119    0.139412    0.299696
2021-10-28T23:13:16.648609+0000     3     128      1234      1106   92.1507        79    0.218622    0.326167
2021-10-28T23:13:17.648724+0000     4     128      1600      1472   91.9854      91.5    0.354139    0.323984
2021-10-28T23:13:18.648866+0000     5     128      2032      1904   95.1852       108     0.47929    0.322932
2021-10-28T23:13:19.649010+0000     6     128      2253      2125   88.5281     55.25    0.671726    0.336051
2021-10-28T23:13:20.649153+0000     7     128      2563      2435   86.9511      77.5    0.246021    0.349365
2021-10-28T23:13:21.649285+0000     8     128      2917      2789   87.1432      88.5    0.562917    0.354949
2021-10-28T23:13:22.649444+0000     9     128      3248      3120   86.6536     82.75    0.098823    0.352859
2021-10-28T23:13:23.649596+0000    10     128      3704      3576   89.3865       114    0.286635     0.35044
2021-10-28T23:13:24.649752+0000    11     128      3967      3839   87.2368     65.75    0.288149    0.355521
2021-10-28T23:13:25.649880+0000    12     128      4374      4246   88.4451    101.75      0.2249    0.356439
2021-10-28T23:13:26.650083+0000 Total time run:       12.9496
Total reads made:     4481
Read size:            262144
Object size:          262144
Bandwidth (MB/sec):   86.5086
Average IOPS:         346
Stddev IOPS:          77.3444
Max IOPS:             476
Min IOPS:             221
Average Latency(s):   0.364301
Max latency(s):       1.26873
Min latency(s):       0.00284409

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:13:27,299519738-07:00] INFO: > Stop system activity collection services[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:13:27,305231562-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:391 - rados_bench() > kill -INT 857221

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:13:27,310845621-07:00] INFO: >> for OSD host[0m
# ./benchmarks/bench-rados:396 - rados_bench() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.5 kill -INT 300315
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:13:27,318729137-07:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.5 kill -INT 300315
[1] 16:13:28 [SUCCESS] ljishen@10.10.2.5

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:13:28,456816384-07:00] INFO: >>> Copy system activity file from OSD host[0m
## ./benchmarks/bench-rados:403 - rados_bench() > basename -- /tmp/bench-rados/sys_activity_256KB_seq.dat.osd_host.1
# ./benchmarks/bench-rados:403 - rados_bench() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.5:/tmp/bench-rados/sys_activity_256KB_seq.dat.osd_host.1 /mnt/sda8/ceph-research/scripts/output/sys_activity_256KB_seq.dat.osd_host.1


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:13:29,544929830-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:13:53,618913181-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 4.48k objects, 1.1 GiB
    usage:   3.3 GiB used, 397 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:13:53,626583725-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:14:02,718381471-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 4.48k objects, 1.1 GiB
    usage:   3.3 GiB used, 397 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:14:02,725709119-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:14:11,759550554-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 4.48k objects, 1.1 GiB
    usage:   3.3 GiB used, 397 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:14:11,767262116-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:14:20,833707896-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 4.48k objects, 1.1 GiB
    usage:   3.3 GiB used, 397 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:14:20,840977135-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:14:29,881423805-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 4.48k objects, 1.1 GiB
    usage:   3.3 GiB used, 397 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:14:29,888858125-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:14:29,895010789-07:00] INFO: > The cluster is idle now.[0m


[1;7;39;49m[2021-10-28T16:14:29,897150702-07:00][RUNNING][ROUND 2/4/21] object_size=256KB[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:14:29,900344140-07:00] STAGE: Launch a Ceph cluster (CEPH_HOST: 10.10.2.1, OSD_HOST: 10.10.2.5) ...[0m
# ./benchmarks/bench-rados:183 - launch_ceph_cluster() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.1 bash -euo pipefail
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:14:29,910038889-07:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.1 bash -euo pipefail
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T16:14:30,382952224-07:00] INFO: > Remove existing clusters\x1b[0m\n'
10.10.2.1: b'## bash:17 -  > basename -- /var/lib/ceph/8e534f42-3843-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'# bash:17 -  > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm --host 10.10.2.1 --host 10.10.2.5 rm-cluster --force --fsid 8e534f42-3843-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T16:14:30,393436020-07:00] DEBUG: HOST_PARAMS: ['--host', '10.10.2.1', '--host', '10.10.2.5']\x1b[0m\n"
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T16:14:30,396910350-07:00] DEBUG: CEPHADM_PARAMS: ['rm-cluster', '--force', '--fsid', '8e534f42-3843-11ec-b51d-53e6e728d2d3']\x1b[0m\n"
10.10.2.1: b"# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:63 -  > /tmp/bench-rados/ceph-research/scripts/parallel-ssh --host 10.10.2.1 --host 10.10.2.5 'sudo python3 -u - rm-cluster --force --fsid 8e534f42-3843-11ec-b51d-53e6e728d2d3'\n"
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T16:14:30,404657673-07:00] DEBUG: command from stdin\x1b[0m\n'
10.10.2.1: b"# /tmp/bench-rados/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host 10.10.2.1 --host 10.10.2.5 'sudo python3 -u - rm-cluster --force --fsid 8e534f42-3843-11ec-b51d-53e6e728d2d3'\n"
10.10.2.1: b'[1] 16:14:36 [SUCCESS] 10.10.2.1\n[2] 16:14:39 [SUCCESS] 10.10.2.5\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T16:14:39,268273979-07:00] INFO: > Deploy a new cluster\x1b[0m\n'
10.10.2.1: b'# bash:24 -  > /tmp/bench-rados/ceph-research/scripts/ceph-deploy -m 10.10.2.1 -o sm1/10.10.2.5:/dev/nvme0n1\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T16:14:39,280123843-07:00] STAGE: Verify passwordless SSH to hosts, passwordless sudo via ljishen, and the existence of devices on hosts...\x1b[0m\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T16:14:39,285360176-07:00] INFO: > Check the host for the monitor\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:181 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@10.10.2.1 sudo --non-interactive --validate\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T16:14:39,435084770-07:00] INFO: > Check hosts for OSDs\x1b[0m\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T16:14:39,439653237-07:00] INFO: >> Check host 'sm1'\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:200 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 sudo --non-interactive --validate\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T16:14:40,531069059-07:00] INFO: >>> Check device '/dev/nvme0n1'\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:215 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 test -b /dev/nvme0n1\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:226 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 lsblk --nodeps --noheadings --output TYPE /dev/nvme0n1\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:227 - verify_parameters() > grep --word-regexp disk\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T16:14:42,648000902-07:00] STAGE: Reset devices on hosts...\x1b[0m\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T16:14:42,652856459-07:00] INFO: > Resetting device '/dev/nvme0n1' on host 'sm1'\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:288 - reset_host_devices() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 sudo sh\n'
10.10.2.1: b'  Removing ceph--1ae856be--517b--42ee--8855--e9e887b88940-osd--block--bbb063c3--5830--49dd--b0bf--d7899d450687 (252:0)\n'
10.10.2.1: b'  Archiving volume group "ceph-1ae856be-517b-42ee-8855-e9e887b88940" metadata (seqno 5).\n'
10.10.2.1: b'  Releasing logical volume "osd-block-bbb063c3-5830-49dd-b0bf-d7899d450687"\n'
10.10.2.1: b'  Creating volume group backup "/etc/lvm/backup/ceph-1ae856be-517b-42ee-8855-e9e887b88940" (seqno 6).\n'
10.10.2.1: b'  Logical volume "osd-block-bbb063c3-5830-49dd-b0bf-d7899d450687" successfully removed\n'
10.10.2.1: b'  Removing physical volume "/dev/nvme0n1" from volume group "ceph-1ae856be-517b-42ee-8855-e9e887b88940"\n'
10.10.2.1: b'  Volume group "ceph-1ae856be-517b-42ee-8855-e9e887b88940" successfully removed\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T16:14:45,018196881-07:00] STAGE: Bootstrap a new cluster...\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:317 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.1 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T16:14:45,028217286-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T16:14:45,031786355-07:00] DEBUG: CEPHADM_PARAMS: ['bootstrap', '--config', '/tmp/bench-rados/ceph-research/scripts/ceph.conf', '--mon-ip', '10.10.2.1', '--output-dir', '/tmp/bench-rados/deployment_data_root/etc/ceph', '--ssh-user', 'ljishen', '--skip-dashboard', '--no-minimize-config', '--allow-overwrite', '--allow-fqdn-hostname', '--orphan-initial-daemons', '--skip-monitoring-stack', '--single-host-defaults']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.1 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults\n'
10.10.2.1: b'Verifying podman|docker is present...\nVerifying lvm2 is present...\n'
10.10.2.1: b'Verifying time synchronization is in place...\n'
10.10.2.1: b'Unit ntp.service is enabled and running\n'
10.10.2.1: b'Repeating the final host check...\npodman|docker (/usr/bin/docker) is present\n'
10.10.2.1: b'systemctl is present\n'
10.10.2.1: b'lvcreate is present\n'
10.10.2.1: b'Unit ntp.service is enabled and running\n'
10.10.2.1: b'Host looks OK\n'
10.10.2.1: b'Cluster fsid: d6b07d4a-3844-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Verifying IP 10.10.2.1 port 3300 ...\n'
10.10.2.1: b'Verifying IP 10.10.2.1 port 6789 ...\n'
10.10.2.1: b'Mon IP `10.10.2.1` is in CIDR network `10.10.2.0/24`\n'
10.10.2.1: b'- internal network (--cluster-network) has not been provided, OSD replication will default to the public_network\n'
10.10.2.1: b'Adjusting default settings to suit single-host cluster...\n'
10.10.2.1: b'Pulling container image quay.io/ceph/ceph:v16...\n'
10.10.2.1: b'Ceph version: ceph version 16.2.6 (ee28fb57e47e9f88813e24bbf4c14496ca299d31) pacific (stable)\n'
10.10.2.1: b'Extracting ceph user uid/gid from container image...\n'
10.10.2.1: b'Creating initial keys...\n'
10.10.2.1: b'Creating initial monmap...\n'
10.10.2.1: b'Creating mon...\n'
10.10.2.1: b'Waiting for mon to start...\n'
10.10.2.1: b'Waiting for mon...\n'
10.10.2.1: b'mon is available\n'
10.10.2.1: b'Setting mon public_network to 10.10.2.0/24\n'
10.10.2.1: b'Wrote config to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf\n'
10.10.2.1: b'Wrote keyring to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring\n'
10.10.2.1: b'Creating mgr...\n'
10.10.2.1: b'Verifying port 9283 ...\n'
10.10.2.1: b'Waiting for mgr to start...\nWaiting for mgr...\n'
10.10.2.1: b'mgr not available, waiting (1/15)...\n'
10.10.2.1: b'mgr not available, waiting (2/15)...\n'
10.10.2.1: b'mgr is available\n'
10.10.2.1: b'Enabling cephadm module...\n'
10.10.2.1: b'Waiting for the mgr to restart...\n'
10.10.2.1: b'Waiting for mgr epoch 5...\n'
10.10.2.1: b'mgr epoch 5 is available\n'
10.10.2.1: b'Setting orchestrator backend to cephadm...\n'
10.10.2.1: b'Generating ssh key...\n'
10.10.2.1: b'Wrote public SSH key to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub\n'
10.10.2.1: b'Adding key to ljishen@localhost authorized_keys...\n'
10.10.2.1: b'Adding host node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us...\n'
10.10.2.1: b'Deploying unmanaged mon service...\n'
10.10.2.1: b'Deploying unmanaged mgr service...\n'
10.10.2.1: b'You can access the Ceph CLI with:\n\n\tsudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --fsid d6b07d4a-3844-11ec-b51d-53e6e728d2d3 -c /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf -k /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring\n\n'
10.10.2.1: b'Please consider enabling telemetry to help improve Ceph:\n\n\tceph telemetry on\n\nFor more information see:\n\n\thttps://docs.ceph.com/docs/pacific/mgr/telemetry/\n\nBootstrap complete.\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T16:15:47,891835155-07:00] INFO: Sleep for 20 seconds\x1b[0m\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T16:16:07,899021822-07:00] INFO: > Set the default number of monitor daemons to 1\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:337 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T16:16:07,909480321-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T16:16:07,913048568-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mon', '1']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1\n'
10.10.2.1: b'Inferring fsid d6b07d4a-3844-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/d6b07d4a-3844-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b'Scheduled mon update...\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T16:16:17,187852709-07:00] INFO: > Set the default number of manager daemons to 1\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:342 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T16:16:17,197500153-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T16:16:17,201272163-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mgr', '1']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1\n'
10.10.2.1: b'Inferring fsid d6b07d4a-3844-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/d6b07d4a-3844-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b'Scheduled mgr update...\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T16:16:26,595766437-07:00] STAGE: Add hosts to the cluster...\x1b[0m\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T16:16:26,602142522-07:00] INFO: > Adding host 'sm1'\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:361 - add_hosts() > ssh-copy-id -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -f -i /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub ljishen@sm1\n'
10.10.2.1: b'/usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: "/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub"\n'
10.10.2.1: b'\nNumber of key(s) added: 1\n\nNow try logging into the machine, with:   "ssh -o \'GlobalKnownHostsFile=/dev/null\' -o \'LogLevel=ERROR\' -o \'PasswordAuthentication=no\' -o \'StrictHostKeyChecking=no\' -o \'UserKnownHostsFile=/dev/null\' \'ljishen@sm1\'"\nand check to make sure that only the key(s) you wanted were added.\n\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:367 - add_hosts() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add sm1 10.10.2.5\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T16:16:27,759863721-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T16:16:27,763402884-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'host', 'add', 'sm1', '10.10.2.5']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add sm1 10.10.2.5\n'
10.10.2.1: b'Inferring fsid d6b07d4a-3844-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/d6b07d4a-3844-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b"Added host 'sm1' with addr '10.10.2.5'\n"
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T16:16:38,767583093-07:00] INFO: Sleep for 20 seconds\x1b[0m\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T16:16:58,772350569-07:00] STAGE: Add OSDs to the cluster...\x1b[0m\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T16:16:58,779298239-07:00] INFO: > Adding devices ['/dev/nvme0n1'] on host 'sm1' as OSDs\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:388 - add_host_devices_as_osds() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd sm1:/dev/nvme0n1\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T16:16:58,789663943-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T16:16:58,793906728-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'daemon', 'add', 'osd', 'sm1:/dev/nvme0n1']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd sm1:/dev/nvme0n1\n'
10.10.2.1: b'Inferring fsid d6b07d4a-3844-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/d6b07d4a-3844-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b"Created osd(s) 0 on host 'sm1'\n"
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T16:17:23,705832103-07:00] INFO: > Sleep for 20 seconds\x1b[0m\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T16:17:43,711288308-07:00] STAGE: Show Cluster Status\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:408 - show_status() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T16:17:43,720709357-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T16:17:43,724836244-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', '--status']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status\n'
10.10.2.1: b'Inferring fsid d6b07d4a-3844-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/d6b07d4a-3844-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b'  cluster:\n    id:     d6b07d4a-3844-11ec-b51d-53e6e728d2d3\n    health: HEALTH_WARN\n            1 pool(s) have no replicas configured\n \n  services:\n    mon: 1 daemons, quorum node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us (age 2m)\n    mgr: node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us.prcjho(active, since 2m)\n    osd: 1 osds: 1 up (since 19s), 1 in (since 40s)\n \n  data:\n    pools:   1 pools, 1 pgs\n    objects: 0 objects, 0 B\n    usage:   4.7 MiB used, 400 GiB / 400 GiB avail\n    pgs:     1 active+clean\n \n'
[1] 16:17:52 [SUCCESS] ljishen@10.10.2.1

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:14:30,382952224-07:00] INFO: > Remove existing clusters[0m
## bash:17 -  > basename -- /var/lib/ceph/8e534f42-3843-11ec-b51d-53e6e728d2d3
# bash:17 -  > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm --host 10.10.2.1 --host 10.10.2.5 rm-cluster --force --fsid 8e534f42-3843-11ec-b51d-53e6e728d2d3
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:14:30,393436020-07:00] DEBUG: HOST_PARAMS: ['--host', '10.10.2.1', '--host', '10.10.2.5'][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:14:30,396910350-07:00] DEBUG: CEPHADM_PARAMS: ['rm-cluster', '--force', '--fsid', '8e534f42-3843-11ec-b51d-53e6e728d2d3'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:63 -  > /tmp/bench-rados/ceph-research/scripts/parallel-ssh --host 10.10.2.1 --host 10.10.2.5 'sudo python3 -u - rm-cluster --force --fsid 8e534f42-3843-11ec-b51d-53e6e728d2d3'
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:14:30,404657673-07:00] DEBUG: command from stdin[0m
# /tmp/bench-rados/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host 10.10.2.1 --host 10.10.2.5 'sudo python3 -u - rm-cluster --force --fsid 8e534f42-3843-11ec-b51d-53e6e728d2d3'
[1] 16:14:36 [SUCCESS] 10.10.2.1
[2] 16:14:39 [SUCCESS] 10.10.2.5

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:14:39,268273979-07:00] INFO: > Deploy a new cluster[0m
# bash:24 -  > /tmp/bench-rados/ceph-research/scripts/ceph-deploy -m 10.10.2.1 -o sm1/10.10.2.5:/dev/nvme0n1


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:14:39,280123843-07:00] STAGE: Verify passwordless SSH to hosts, passwordless sudo via ljishen, and the existence of devices on hosts...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:14:39,285360176-07:00] INFO: > Check the host for the monitor[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:181 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@10.10.2.1 sudo --non-interactive --validate

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:14:39,435084770-07:00] INFO: > Check hosts for OSDs[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:14:39,439653237-07:00] INFO: >> Check host 'sm1'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:200 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 sudo --non-interactive --validate

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:14:40,531069059-07:00] INFO: >>> Check device '/dev/nvme0n1'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:215 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 test -b /dev/nvme0n1
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:226 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 lsblk --nodeps --noheadings --output TYPE /dev/nvme0n1
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:227 - verify_parameters() > grep --word-regexp disk


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:14:42,648000902-07:00] STAGE: Reset devices on hosts...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:14:42,652856459-07:00] INFO: > Resetting device '/dev/nvme0n1' on host 'sm1'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:288 - reset_host_devices() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 sudo sh
  Removing ceph--1ae856be--517b--42ee--8855--e9e887b88940-osd--block--bbb063c3--5830--49dd--b0bf--d7899d450687 (252:0)
  Archiving volume group "ceph-1ae856be-517b-42ee-8855-e9e887b88940" metadata (seqno 5).
  Releasing logical volume "osd-block-bbb063c3-5830-49dd-b0bf-d7899d450687"
  Creating volume group backup "/etc/lvm/backup/ceph-1ae856be-517b-42ee-8855-e9e887b88940" (seqno 6).
  Logical volume "osd-block-bbb063c3-5830-49dd-b0bf-d7899d450687" successfully removed
  Removing physical volume "/dev/nvme0n1" from volume group "ceph-1ae856be-517b-42ee-8855-e9e887b88940"
  Volume group "ceph-1ae856be-517b-42ee-8855-e9e887b88940" successfully removed


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:14:45,018196881-07:00] STAGE: Bootstrap a new cluster...[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:317 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.1 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:14:45,028217286-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:14:45,031786355-07:00] DEBUG: CEPHADM_PARAMS: ['bootstrap', '--config', '/tmp/bench-rados/ceph-research/scripts/ceph.conf', '--mon-ip', '10.10.2.1', '--output-dir', '/tmp/bench-rados/deployment_data_root/etc/ceph', '--ssh-user', 'ljishen', '--skip-dashboard', '--no-minimize-config', '--allow-overwrite', '--allow-fqdn-hostname', '--orphan-initial-daemons', '--skip-monitoring-stack', '--single-host-defaults'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.1 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults
Verifying podman|docker is present...
Verifying lvm2 is present...
Verifying time synchronization is in place...
Unit ntp.service is enabled and running
Repeating the final host check...
podman|docker (/usr/bin/docker) is present
systemctl is present
lvcreate is present
Unit ntp.service is enabled and running
Host looks OK
Cluster fsid: d6b07d4a-3844-11ec-b51d-53e6e728d2d3
Verifying IP 10.10.2.1 port 3300 ...
Verifying IP 10.10.2.1 port 6789 ...
Mon IP `10.10.2.1` is in CIDR network `10.10.2.0/24`
- internal network (--cluster-network) has not been provided, OSD replication will default to the public_network
Adjusting default settings to suit single-host cluster...
Pulling container image quay.io/ceph/ceph:v16...
Ceph version: ceph version 16.2.6 (ee28fb57e47e9f88813e24bbf4c14496ca299d31) pacific (stable)
Extracting ceph user uid/gid from container image...
Creating initial keys...
Creating initial monmap...
Creating mon...
Waiting for mon to start...
Waiting for mon...
mon is available
Setting mon public_network to 10.10.2.0/24
Wrote config to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf
Wrote keyring to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring
Creating mgr...
Verifying port 9283 ...
Waiting for mgr to start...
Waiting for mgr...
mgr not available, waiting (1/15)...
mgr not available, waiting (2/15)...
mgr is available
Enabling cephadm module...
Waiting for the mgr to restart...
Waiting for mgr epoch 5...
mgr epoch 5 is available
Setting orchestrator backend to cephadm...
Generating ssh key...
Wrote public SSH key to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub
Adding key to ljishen@localhost authorized_keys...
Adding host node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us...
Deploying unmanaged mon service...
Deploying unmanaged mgr service...
You can access the Ceph CLI with:

	sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --fsid d6b07d4a-3844-11ec-b51d-53e6e728d2d3 -c /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf -k /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring

Please consider enabling telemetry to help improve Ceph:

	ceph telemetry on

For more information see:

	https://docs.ceph.com/docs/pacific/mgr/telemetry/

Bootstrap complete.

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:15:47,891835155-07:00] INFO: Sleep for 20 seconds[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:16:07,899021822-07:00] INFO: > Set the default number of monitor daemons to 1[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:337 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:16:07,909480321-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:16:07,913048568-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mon', '1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1
Inferring fsid d6b07d4a-3844-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/d6b07d4a-3844-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Scheduled mon update...

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:16:17,187852709-07:00] INFO: > Set the default number of manager daemons to 1[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:342 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:16:17,197500153-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:16:17,201272163-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mgr', '1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1
Inferring fsid d6b07d4a-3844-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/d6b07d4a-3844-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Scheduled mgr update...


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:16:26,595766437-07:00] STAGE: Add hosts to the cluster...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:16:26,602142522-07:00] INFO: > Adding host 'sm1'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:361 - add_hosts() > ssh-copy-id -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -f -i /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub ljishen@sm1
/usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: "/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub"

Number of key(s) added: 1

Now try logging into the machine, with:   "ssh -o 'GlobalKnownHostsFile=/dev/null' -o 'LogLevel=ERROR' -o 'PasswordAuthentication=no' -o 'StrictHostKeyChecking=no' -o 'UserKnownHostsFile=/dev/null' 'ljishen@sm1'"
and check to make sure that only the key(s) you wanted were added.

# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:367 - add_hosts() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add sm1 10.10.2.5
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:16:27,759863721-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:16:27,763402884-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'host', 'add', 'sm1', '10.10.2.5'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add sm1 10.10.2.5
Inferring fsid d6b07d4a-3844-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/d6b07d4a-3844-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Added host 'sm1' with addr '10.10.2.5'

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:16:38,767583093-07:00] INFO: Sleep for 20 seconds[0m


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:16:58,772350569-07:00] STAGE: Add OSDs to the cluster...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:16:58,779298239-07:00] INFO: > Adding devices ['/dev/nvme0n1'] on host 'sm1' as OSDs[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:388 - add_host_devices_as_osds() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd sm1:/dev/nvme0n1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:16:58,789663943-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:16:58,793906728-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'daemon', 'add', 'osd', 'sm1:/dev/nvme0n1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd sm1:/dev/nvme0n1
Inferring fsid d6b07d4a-3844-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/d6b07d4a-3844-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Created osd(s) 0 on host 'sm1'

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:17:23,705832103-07:00] INFO: > Sleep for 20 seconds[0m


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:17:43,711288308-07:00] STAGE: Show Cluster Status[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:408 - show_status() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:17:43,720709357-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:17:43,724836244-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', '--status'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status
Inferring fsid d6b07d4a-3844-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/d6b07d4a-3844-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
  cluster:
    id:     d6b07d4a-3844-11ec-b51d-53e6e728d2d3
    health: HEALTH_WARN
            1 pool(s) have no replicas configured
 
  services:
    mon: 1 daemons, quorum node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us (age 2m)
    mgr: node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us.prcjho(active, since 2m)
    osd: 1 osds: 1 up (since 19s), 1 in (since 40s)
 
  data:
    pools:   1 pools, 1 pgs
    objects: 0 objects, 0 B
    usage:   4.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     1 active+clean
 

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:17:52,567380357-07:00] INFO: > Copy configuration files to local[0m
# ./benchmarks/bench-rados:220 - launch_ceph_cluster() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.1 sudo chown --changes --recursive ljishen /tmp/bench-rados/deployment_data_root/etc/ceph
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:17:52,575098231-07:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.1 sudo chown --changes --recursive ljishen /tmp/bench-rados/deployment_data_root/etc/ceph
[1] 16:17:52 [SUCCESS] ljishen@10.10.2.1
# ./benchmarks/bench-rados:223 - launch_ceph_cluster() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.1:/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf
# ./benchmarks/bench-rados:226 - launch_ceph_cluster() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.1:/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:17:53,053330353-07:00] STAGE: Update local ceph.conf as a client...[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:17:53,057136065-07:00] STAGE: Configure Ceph pool...[0m
# ./benchmarks/bench-rados:257 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_max_backfills 32
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:17:53,079376456-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:17:53,082127189-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'd6b07d4a-3844-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'config', 'set', 'osd', 'osd_max_backfills', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid d6b07d4a-3844-11ec-b51d-53e6e728d2d3 -- ceph config set osd osd_max_backfills 32
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:258 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_active 32
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:17:57,190751259-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:17:57,193885535-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'd6b07d4a-3844-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_active', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid d6b07d4a-3844-11ec-b51d-53e6e728d2d3 -- ceph config set osd osd_recovery_max_active 32
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:259 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_single_start 8
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:18:01,300234970-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:18:01,303120096-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'd6b07d4a-3844-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_single_start', '8'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid d6b07d4a-3844-11ec-b51d-53e6e728d2d3 -- ceph config set osd osd_recovery_max_single_start 8
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:260 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_op_priority 63
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:18:05,323972477-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:18:05,327013297-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'd6b07d4a-3844-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_op_priority', '63'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid d6b07d4a-3844-11ec-b51d-53e6e728d2d3 -- ceph config set osd osd_recovery_op_priority 63
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:262 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config show-with-defaults osd.0
# ./benchmarks/bench-rados:263 - configure_ceph_pool() > grep 'osd_max_backfills\|osd_recovery'
# ./benchmarks/bench-rados:263 - configure_ceph_pool() > column -t -s ' '
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
osd_max_backfills                        32         mon
osd_recovery_cost                        20971520   default
osd_recovery_delay_start                 0.000000   default
osd_recovery_max_active                  32         mon
osd_recovery_max_active_hdd              3          default
osd_recovery_max_active_ssd              10         default
osd_recovery_max_chunk                   8388608    default
osd_recovery_max_omap_entries_per_chunk  8096       default
osd_recovery_max_single_start            8          mon
osd_recovery_op_priority                 63         mon
osd_recovery_op_warn_multiple            16         default
osd_recovery_priority                    5          default
osd_recovery_retry_interval              30.000000  default
osd_recovery_sleep                       0.000000   default
osd_recovery_sleep_hdd                   0.100000   default
osd_recovery_sleep_hybrid                0.025000   default
osd_recovery_sleep_ssd                   0.000000   default
# ./benchmarks/bench-rados:265 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool create bench_rados 128 128 replicated --size 1 --pg-num-min 128
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:18:13,405909436-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:18:13,408829538-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'd6b07d4a-3844-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'create', 'bench_rados', '128', '128', 'replicated', '--size', '1', '--pg-num-min', '128'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid d6b07d4a-3844-11ec-b51d-53e6e728d2d3 -- ceph osd pool create bench_rados 128 128 replicated --size 1 --pg-num-min 128
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
pool 'bench_rados' created
# ./benchmarks/bench-rados:267 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados min_size 1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:18:18,497268551-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:18:18,500265700-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'd6b07d4a-3844-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'min_size', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid d6b07d4a-3844-11ec-b51d-53e6e728d2d3 -- ceph osd pool set bench_rados min_size 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 min_size to 1
# ./benchmarks/bench-rados:268 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados pg_autoscale_mode off
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:18:22,981016478-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:18:22,984215687-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'd6b07d4a-3844-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'pg_autoscale_mode', 'off'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid d6b07d4a-3844-11ec-b51d-53e6e728d2d3 -- ceph osd pool set bench_rados pg_autoscale_mode off
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 pg_autoscale_mode to off
# ./benchmarks/bench-rados:269 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool application enable bench_rados benchmark
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:18:27,488238200-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:18:27,491179814-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'd6b07d4a-3844-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'application', 'enable', 'bench_rados', 'benchmark'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid d6b07d4a-3844-11ec-b51d-53e6e728d2d3 -- ceph osd pool application enable bench_rados benchmark
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
enabled application 'benchmark' on pool 'bench_rados'
# ./benchmarks/bench-rados:270 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados noscrub 1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:18:32,095410295-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:18:32,098345567-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'd6b07d4a-3844-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'noscrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid d6b07d4a-3844-11ec-b51d-53e6e728d2d3 -- ceph osd pool set bench_rados noscrub 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 noscrub to 1
# ./benchmarks/bench-rados:271 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados nodeep-scrub 1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:18:36,880392645-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:18:36,883374314-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'd6b07d4a-3844-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'nodeep-scrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid d6b07d4a-3844-11ec-b51d-53e6e728d2d3 -- ceph osd pool set bench_rados nodeep-scrub 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 nodeep-scrub to 1
# ./benchmarks/bench-rados:272 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool ls detail
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:18:41,552269134-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:18:41,555110619-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'd6b07d4a-3844-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'ls', 'detail'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid d6b07d4a-3844-11ec-b51d-53e6e728d2d3 -- ceph osd pool ls detail
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
pool 1 'device_health_metrics' replicated size 1 min_size 1 crush_rule 0 object_hash rjenkins pg_num 64 pgp_num 64 autoscale_mode on last_change 18 lfor 0/0/15 flags hashpspool stripe_width 0 pg_num_min 1 application mgr_devicehealth
pool 2 'bench_rados' replicated size 1 min_size 1 crush_rule 0 object_hash rjenkins pg_num 128 pgp_num 128 autoscale_mode off last_change 20 flags hashpspool,noscrub,nodeep-scrub stripe_width 0 pg_num_min 128 application benchmark

# ./benchmarks/bench-rados:273 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd df tree
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:18:45,477497965-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:18:45,480683138-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'd6b07d4a-3844-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'df', 'tree'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid d6b07d4a-3844-11ec-b51d-53e6e728d2d3 -- ceph osd df tree
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
ID  CLASS  WEIGHT   REWEIGHT  SIZE     RAW USE  DATA     OMAP  META     AVAIL    %USE  VAR   PGS  STATUS  TYPE NAME    
-1         0.39059         -  400 GiB  5.7 MiB  168 KiB   0 B  5.6 MiB  400 GiB  0.00  1.00    -          root default 
-3         0.39059         -  400 GiB  5.7 MiB  168 KiB   0 B  5.6 MiB  400 GiB  0.00  1.00    -              host sm1 
 0    ssd  0.39059   1.00000  400 GiB  5.7 MiB  168 KiB   0 B  5.6 MiB  400 GiB  0.00  1.00  192      up          osd.0
                       TOTAL  400 GiB  5.7 MiB  168 KiB   0 B  5.6 MiB  400 GiB  0.00                                  
MIN/MAX VAR: 1.00/1.00  STDDEV: 0


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:18:49,486409733-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:19:13,427239073-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:19:22,373262477-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:19:31,341028730-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:19:31,348287058-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:19:40,328926604-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:19:40,336822463-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:19:49,377950731-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:19:49,385652114-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:19:58,424881658-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:19:58,432678260-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:20:07,482380202-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:20:07,490016954-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:20:07,495925157-07:00] INFO: > The cluster is idle now.[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:20:07,499817843-07:00] STAGE: Start benchmarking of mode write ...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:20:07,506353780-07:00] INFO: > Start system activity collection services[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:20:07,511908077-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:341 - rados_bench() > rm --force /mnt/sda8/ceph-research/scripts/output/sys_activity_256KB_write.dat.rados_bench_host.2
# ./benchmarks/bench-rados:342 - rados_bench() > sar_pid_rados_bench_host=865009
# ./benchmarks/bench-rados:342 - rados_bench() > S_TIME_FORMAT=ISO
# ./benchmarks/bench-rados:342 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/output/sys_activity_256KB_write.dat.rados_bench_host.2 2

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:20:07,518735873-07:00] INFO: >> for OSD host[0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:20:07,527786378-07:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.5 bash
10.10.2.5: b'305180\n'
[1] 16:20:08 [SUCCESS] ljishen@10.10.2.5
305180

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:20:08,642258829-07:00] INFO: > Run rados bench[0m
# ./benchmarks/bench-rados:370 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/output/rados_bench_256KB_write.log.2
## ./benchmarks/bench-rados:361 - rados_bench() > count_cpus 32-63,96-127
# ./benchmarks/bench-rados:361 - rados_bench() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 60 write --pool bench_rados -b 262144 -O 262144 --concurrent-ios 128 --show-time --write-object --write-omap --write-xattr --no-cleanup
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:20:08,662515662-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:20:08,665557895-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'd6b07d4a-3844-11ec-b51d-53e6e728d2d3', '--', 'rados', 'bench', '60', 'write', '--pool', 'bench_rados', '-b', '262144', '-O', '262144', '--concurrent-ios', '128', '--show-time', '--write-object', '--write-omap', '--write-xattr', '--no-cleanup'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid d6b07d4a-3844-11ec-b51d-53e6e728d2d3 -- rados bench 60 write --pool bench_rados -b 262144 -O 262144 --concurrent-ios 128 --show-time --write-object --write-omap --write-xattr --no-cleanup
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
hints = 1
2021-10-28T23:20:11.783243+0000 Maintaining 128 concurrent writes of 262144 bytes to objects of size 262144 for up to 60 seconds or 0 objects
2021-10-28T23:20:11.783258+0000 Object prefix: benchmark_data_node-2.bluefield2.ucsc-cmps10_7
2021-10-28T23:20:11.792254+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-28T23:20:11.792254+0000     0       0         0         0         0         0           -           0
2021-10-28T23:20:12.792355+0000     1     128       177        49   12.2493     12.25    0.592674    0.581251
2021-10-28T23:20:13.792429+0000     2     128       305       177   22.1236        32    0.924352    0.928489
2021-10-28T23:20:14.792498+0000     3     128       433       305    25.415        32     1.06857    0.962039
2021-10-28T23:20:15.792578+0000     4     128       561       433   27.0606        32     1.06063    0.993503
2021-10-28T23:20:16.792656+0000     5     128       689       561    28.048        32     1.07166     1.02229
2021-10-28T23:20:17.792735+0000     6     128       817       689   28.7063        32    0.933963     1.00356
2021-10-28T23:20:18.792811+0000     7     128       945       817   29.1764        32    0.974608    0.997091
2021-10-28T23:20:19.792894+0000     8     128      1073       945   29.5291        32     1.05278     1.00822
2021-10-28T23:20:20.792982+0000     9     128      1201      1073   29.8033        32     1.01344    0.997363
2021-10-28T23:20:21.793093+0000    10     128      1329      1201   30.0226        32    0.985741     1.00537
2021-10-28T23:20:22.793165+0000    11     128      1457      1329   30.2022        32    0.927249    0.998925
2021-10-28T23:20:23.793236+0000    12     128      1585      1457   30.3518        32    0.951037    0.994109
2021-10-28T23:20:24.793302+0000    13     128      1713      1585   30.4784        32     1.05328    0.993597
2021-10-28T23:20:25.793371+0000    14     128      1919      1791   31.9797      51.5    0.863506    0.986604
2021-10-28T23:20:26.793443+0000    15     128      2047      1919   31.9809        32    0.958229    0.984845
2021-10-28T23:20:27.793522+0000    16     128      2097      1969   30.7633      12.5     1.12649    0.988372
2021-10-28T23:20:28.793598+0000    17     128      2175      2047   30.1006      19.5     1.20033    0.996434
2021-10-28T23:20:29.793673+0000    18     128      2225      2097   29.1228      12.5     1.98411     1.01867
2021-10-28T23:20:30.793756+0000    19     128      2303      2175   28.6162      19.5     2.83116     1.08357
2021-10-28T23:20:31.793864+0000 min lat: 0.221122 max lat: 2.83149 avg lat: 1.08357
2021-10-28T23:20:31.793864+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-28T23:20:31.793864+0000    20     128      2303      2175   27.1854         0           -     1.08357
2021-10-28T23:20:32.793945+0000    21     128      2353      2225    26.486      6.25     2.92921     1.12497
2021-10-28T23:20:33.794021+0000    22     128      2353      2225   25.2821         0           -     1.12497
2021-10-28T23:20:34.794105+0000    23     128      2431      2303   25.0306      9.75     3.64265      1.2092
2021-10-28T23:20:35.794189+0000    24     128      2481      2353   24.5085      12.5     3.24228     1.25236
2021-10-28T23:20:36.794275+0000    25     128      2481      2353   23.5281         0           -     1.25236
2021-10-28T23:20:37.794356+0000    26     128      2559      2431   23.3732      9.75     3.17482     1.31483
2021-10-28T23:20:38.794436+0000    27     128      2559      2431   22.5075         0           -     1.31483
2021-10-28T23:20:39.794518+0000    28     128      2609      2481     22.15      6.25     3.34604     1.35534
2021-10-28T23:20:40.794605+0000    29     128      2609      2481   21.3862         0           -     1.35534
2021-10-28T23:20:41.794683+0000    30     128      2687      2559   21.3233      9.75     3.43687     1.41852
2021-10-28T23:20:42.794756+0000    31     128      2737      2609   21.0387      12.5      3.2346     1.45315
2021-10-28T23:20:43.794825+0000    32     128      2737      2609   20.3812         0           -     1.45315
2021-10-28T23:20:44.794893+0000    33     128      2815      2687   20.3545      9.75     2.98508     1.49743
2021-10-28T23:20:45.794964+0000    34     128      2865      2737   20.1234      12.5     3.03167     1.52597
2021-10-28T23:20:46.795030+0000    35     128      2943      2815   20.1056      19.5     2.57614     1.55487
2021-10-28T23:20:47.795107+0000    36     128      2984      2856   19.8318     10.25     2.65419     1.57073
2021-10-28T23:20:48.795177+0000    37     128      2993      2865   19.3566      2.25     2.70054     1.57429
2021-10-28T23:20:49.795256+0000    38     128      3071      2943   19.3603      19.5     2.80048     1.60714
2021-10-28T23:20:50.795332+0000    39     128      3121      2993   19.1844      12.5     2.88538     1.62769
2021-10-28T23:20:51.795411+0000 min lat: 0.221122 max lat: 3.64329 avg lat: 1.62769
2021-10-28T23:20:51.795411+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-28T23:20:51.795411+0000    40     128      3121      2993   18.7048         0           -     1.62769
2021-10-28T23:20:52.795494+0000    41     128      3199      3071   18.7242      9.75     2.65011     1.65391
2021-10-28T23:20:53.795564+0000    42     128      3249      3121   18.5759      12.5      2.6801     1.67002
2021-10-28T23:20:54.795633+0000    43     128      3377      3249   18.8881        32     1.29401     1.67437
2021-10-28T23:20:55.795704+0000    44     128      3505      3377    19.186        32      1.0622     1.65204
2021-10-28T23:20:56.795778+0000    45     128      3633      3505   19.4707        32     1.14872     1.63403
2021-10-28T23:20:57.795858+0000    46     128      3761      3633    19.743        32    0.881399     1.60739
2021-10-28T23:20:58.795937+0000    47     128      3889      3761   20.0038        32     1.04503       1.588
2021-10-28T23:20:59.796012+0000    48     128      4017      3889   20.2536        32    0.794846     1.56249
2021-10-28T23:21:00.796085+0000    49     128      4145      4017   20.4933        32     1.14931     1.54816
2021-10-28T23:21:01.796159+0000    50     128      4223      4095   20.4734      19.5     1.10373     1.53968
2021-10-28T23:21:02.796242+0000    51     128      4223      4095    20.072         0           -     1.53968
2021-10-28T23:21:03.796316+0000    52     128      4273      4145   19.9263      6.25     2.29647     1.54858
2021-10-28T23:21:04.796383+0000    53     128      4273      4145   19.5504         0           -     1.54858
2021-10-28T23:21:05.796450+0000    54     128      4351      4223   19.5494      9.75     3.54946     1.58556
2021-10-28T23:21:06.796519+0000    55     128      4401      4273   19.4212      12.5     3.11065     1.60311
2021-10-28T23:21:07.796602+0000    56     128      4401      4273   19.0744         0           -     1.60311
2021-10-28T23:21:08.796681+0000    57     128      4401      4273   18.7398         0           -     1.60311
2021-10-28T23:21:09.796760+0000    58     128      4401      4273   18.4167         0           -     1.60311
2021-10-28T23:21:10.796836+0000    59     128      4479      4351    18.435     4.875     5.23829     1.66826
2021-10-28T23:21:11.796919+0000 min lat: 0.221122 max lat: 5.24267 avg lat: 1.66826
2021-10-28T23:21:11.796919+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-28T23:21:11.796919+0000    60     128      4479      4351   18.1278         0           -     1.66826
2021-10-28T23:21:12.797001+0000    61      78      4479      4401   18.0355      6.25     6.80669     1.72691
2021-10-28T23:21:13.797071+0000    62      78      4479      4401   17.7446         0           -     1.72691
2021-10-28T23:21:14.797136+0000    63      78      4479      4401   17.4629         0           -     1.72691
2021-10-28T23:21:15.797209+0000    64      78      4479      4401   17.1901         0           -     1.72691
2021-10-28T23:21:16.797298+0000 Total time run:         64.9904
Total writes made:      4479
Write size:             262144
Object size:            262144
Bandwidth (MB/sec):     17.2295
Stddev Bandwidth:       13.5709
Max bandwidth (MB/sec): 51.5
Min bandwidth (MB/sec): 0
Average IOPS:           68
Stddev IOPS:            54.2898
Max IOPS:               206
Min IOPS:               0
Average Latency(s):     1.81272
Stddev Latency(s):      1.32257
Max latency(s):         6.84249
Min latency(s):         0.221122

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:21:17,560614311-07:00] INFO: > Stop system activity collection services[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:21:17,566175510-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:391 - rados_bench() > kill -INT 865009

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:21:17,572105134-07:00] INFO: >> for OSD host[0m
# ./benchmarks/bench-rados:396 - rados_bench() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.5 kill -INT 305180
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:21:17,579830262-07:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.5 kill -INT 305180
[1] 16:21:18 [SUCCESS] ljishen@10.10.2.5

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:21:18,723933164-07:00] INFO: >>> Copy system activity file from OSD host[0m
## ./benchmarks/bench-rados:403 - rados_bench() > basename -- /tmp/bench-rados/sys_activity_256KB_write.dat.osd_host.2
# ./benchmarks/bench-rados:403 - rados_bench() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.5:/tmp/bench-rados/sys_activity_256KB_write.dat.osd_host.2 /mnt/sda8/ceph-research/scripts/output/sys_activity_256KB_write.dat.osd_host.2


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:21:19,906015537-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:21:43,880215161-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 4.48k objects, 1.1 GiB
    usage:   4.6 GiB used, 395 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:21:43,887867622-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:21:52,895410065-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 4.48k objects, 1.1 GiB
    usage:   3.6 GiB used, 396 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:21:52,902379418-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:22:01,852453948-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 4.48k objects, 1.1 GiB
    usage:   3.7 GiB used, 396 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:22:01,859656461-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:22:10,785811991-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 4.48k objects, 1.1 GiB
    usage:   3.3 GiB used, 397 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:22:10,793207428-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:22:19,812656931-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 4.48k objects, 1.1 GiB
    usage:   3.3 GiB used, 397 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:22:19,820506373-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:22:19,826225781-07:00] INFO: > The cluster is idle now.[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:22:19,829887170-07:00] STAGE: Start benchmarking of mode seq ...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:22:19,836613155-07:00] INFO: > Start system activity collection services[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:22:19,842030765-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:341 - rados_bench() > rm --force /mnt/sda8/ceph-research/scripts/output/sys_activity_256KB_seq.dat.rados_bench_host.2
# ./benchmarks/bench-rados:342 - rados_bench() > sar_pid_rados_bench_host=866952
# ./benchmarks/bench-rados:342 - rados_bench() > S_TIME_FORMAT=ISO
# ./benchmarks/bench-rados:342 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/output/sys_activity_256KB_seq.dat.rados_bench_host.2 2

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:22:19,849463120-07:00] INFO: >> for OSD host[0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:22:19,858381717-07:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.5 bash
10.10.2.5: b'305736\n'
[1] 16:22:20 [SUCCESS] ljishen@10.10.2.5
305736

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:22:20,993090750-07:00] INFO: > Run rados bench[0m
# ./benchmarks/bench-rados:379 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/output/rados_bench_256KB_seq.log.2
## ./benchmarks/bench-rados:376 - rados_bench() > count_cpus 32-63,96-127
# ./benchmarks/bench-rados:376 - rados_bench() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 128 --show-time
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:22:21,013306676-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:22:21,016368757-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'd6b07d4a-3844-11ec-b51d-53e6e728d2d3', '--', 'rados', 'bench', '99999999', 'seq', '--pool', 'bench_rados', '--concurrent-ios', '128', '--show-time'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid d6b07d4a-3844-11ec-b51d-53e6e728d2d3 -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 128 --show-time
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
hints = 1
2021-10-28T23:22:23.966769+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-28T23:22:23.966769+0000     0       0         0         0         0         0           -           0
2021-10-28T23:22:24.966915+0000     1     128       556       428   106.975       107    0.305187    0.238474
2021-10-28T23:22:25.966991+0000     2     128       994       866   108.233     109.5    0.469989    0.267925
2021-10-28T23:22:26.967113+0000     3     128      1282      1154   96.1527        72    0.137932    0.317488
2021-10-28T23:22:27.967184+0000     4     128      1667      1539   96.1753     96.25    0.488019    0.312043
2021-10-28T23:22:28.967289+0000     5     128      2064      1936   96.7882     99.25    0.367829    0.320079
2021-10-28T23:22:29.967402+0000     6     128      2369      2241   93.3637     76.25    0.366866     0.32731
2021-10-28T23:22:30.967491+0000     7     128      2769      2641   94.3105       100    0.420234    0.326749
2021-10-28T23:22:31.967574+0000     8     128      3209      3081   96.2705       110    0.166907    0.316262
2021-10-28T23:22:32.967689+0000     9     128      3466      3338   92.7118     64.25    0.147019    0.320363
2021-10-28T23:22:33.967809+0000    10     128      3857      3729   93.2145     97.75   0.0317052    0.312706
2021-10-28T23:22:34.967945+0000    11     128      4186      4058   92.2166     82.25   0.0147419    0.319467
2021-10-28T23:22:35.968012+0000    12      94      4479      4385    91.344     81.75     2.38224    0.321489
2021-10-28T23:22:36.968175+0000 Total time run:       12.4614
Total reads made:     4479
Read size:            262144
Object size:          262144
Bandwidth (MB/sec):   89.8574
Average IOPS:         359
Stddev IOPS:          61.9229
Max IOPS:             440
Min IOPS:             257
Average Latency(s):   0.3477
Max latency(s):       2.51226
Min latency(s):       0.00349308

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:22:37,580791343-07:00] INFO: > Stop system activity collection services[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:22:37,586594269-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:391 - rados_bench() > kill -INT 866952

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:22:37,592353742-07:00] INFO: >> for OSD host[0m
# ./benchmarks/bench-rados:396 - rados_bench() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.5 kill -INT 305736
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:22:37,600227891-07:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.5 kill -INT 305736
[1] 16:22:38 [SUCCESS] ljishen@10.10.2.5

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:22:38,724515366-07:00] INFO: >>> Copy system activity file from OSD host[0m
## ./benchmarks/bench-rados:403 - rados_bench() > basename -- /tmp/bench-rados/sys_activity_256KB_seq.dat.osd_host.2
# ./benchmarks/bench-rados:403 - rados_bench() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.5:/tmp/bench-rados/sys_activity_256KB_seq.dat.osd_host.2 /mnt/sda8/ceph-research/scripts/output/sys_activity_256KB_seq.dat.osd_host.2


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:22:39,773099300-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:23:03,743829341-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 4.48k objects, 1.1 GiB
    usage:   3.3 GiB used, 397 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:23:03,751517910-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:23:12,825705227-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 4.48k objects, 1.1 GiB
    usage:   3.3 GiB used, 397 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:23:12,833607188-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:23:21,759953278-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 4.48k objects, 1.1 GiB
    usage:   3.3 GiB used, 397 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:23:21,767725465-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:23:30,782888180-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 4.48k objects, 1.1 GiB
    usage:   3.3 GiB used, 397 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:23:30,790597368-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:23:39,792659700-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 4.48k objects, 1.1 GiB
    usage:   3.3 GiB used, 397 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:23:39,800507809-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:23:39,806760492-07:00] INFO: > The cluster is idle now.[0m


[1;7;39;49m[2021-10-28T16:23:39,809107705-07:00][RUNNING][ROUND 3/4/21] object_size=256KB[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:23:39,812687000-07:00] STAGE: Launch a Ceph cluster (CEPH_HOST: 10.10.2.1, OSD_HOST: 10.10.2.5) ...[0m
# ./benchmarks/bench-rados:183 - launch_ceph_cluster() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.1 bash -euo pipefail
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:23:39,821958723-07:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.1 bash -euo pipefail
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T16:23:40,212204444-07:00] INFO: > Remove existing clusters\x1b[0m\n'
10.10.2.1: b'## bash:17 -  > basename -- /var/lib/ceph/d6b07d4a-3844-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'# bash:17 -  > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm --host 10.10.2.1 --host 10.10.2.5 rm-cluster --force --fsid d6b07d4a-3844-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T16:23:40,224152091-07:00] DEBUG: HOST_PARAMS: ['--host', '10.10.2.1', '--host', '10.10.2.5']\x1b[0m\n"
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T16:23:40,227990206-07:00] DEBUG: CEPHADM_PARAMS: ['rm-cluster', '--force', '--fsid', 'd6b07d4a-3844-11ec-b51d-53e6e728d2d3']\x1b[0m\n"
10.10.2.1: b"# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:63 -  > /tmp/bench-rados/ceph-research/scripts/parallel-ssh --host 10.10.2.1 --host 10.10.2.5 'sudo python3 -u - rm-cluster --force --fsid d6b07d4a-3844-11ec-b51d-53e6e728d2d3'\n"
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T16:23:40,236191151-07:00] DEBUG: command from stdin\x1b[0m\n'
10.10.2.1: b"# /tmp/bench-rados/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host 10.10.2.1 --host 10.10.2.5 'sudo python3 -u - rm-cluster --force --fsid d6b07d4a-3844-11ec-b51d-53e6e728d2d3'\n"
10.10.2.1: b'[1] 16:23:46 [SUCCESS] 10.10.2.1\n[2] 16:23:48 [SUCCESS] 10.10.2.5\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T16:23:49,005376529-07:00] INFO: > Deploy a new cluster\x1b[0m\n'
10.10.2.1: b'# bash:24 -  > /tmp/bench-rados/ceph-research/scripts/ceph-deploy -m 10.10.2.1 -o sm1/10.10.2.5:/dev/nvme0n1\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T16:23:49,019073475-07:00] STAGE: Verify passwordless SSH to hosts, passwordless sudo via ljishen, and the existence of devices on hosts...\x1b[0m\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T16:23:49,023886182-07:00] INFO: > Check the host for the monitor\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:181 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@10.10.2.1 sudo --non-interactive --validate\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T16:23:49,175997836-07:00] INFO: > Check hosts for OSDs\x1b[0m\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T16:23:49,180236493-07:00] INFO: >> Check host 'sm1'\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:200 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 sudo --non-interactive --validate\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T16:23:50,263415534-07:00] INFO: >>> Check device '/dev/nvme0n1'\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:215 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 test -b /dev/nvme0n1\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:226 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 lsblk --nodeps --noheadings --output TYPE /dev/nvme0n1\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:227 - verify_parameters() > grep --word-regexp disk\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T16:23:52,451603378-07:00] STAGE: Reset devices on hosts...\x1b[0m\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T16:23:52,456486708-07:00] INFO: > Resetting device '/dev/nvme0n1' on host 'sm1'\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:288 - reset_host_devices() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 sudo sh\n'
10.10.2.1: b'  Removing ceph--7e546da9--92e0--4fc8--a5a2--c05e590abd40-osd--block--caccf7c9--516e--4aef--a61e--b27a60345ba4 (252:0)\n'
10.10.2.1: b'  Archiving volume group "ceph-7e546da9-92e0-4fc8-a5a2-c05e590abd40" metadata (seqno 5).\n'
10.10.2.1: b'  Releasing logical volume "osd-block-caccf7c9-516e-4aef-a61e-b27a60345ba4"\n'
10.10.2.1: b'  Creating volume group backup "/etc/lvm/backup/ceph-7e546da9-92e0-4fc8-a5a2-c05e590abd40" (seqno 6).\n'
10.10.2.1: b'  Logical volume "osd-block-caccf7c9-516e-4aef-a61e-b27a60345ba4" successfully removed\n'
10.10.2.1: b'  Removing physical volume "/dev/nvme0n1" from volume group "ceph-7e546da9-92e0-4fc8-a5a2-c05e590abd40"\n'
10.10.2.1: b'  Volume group "ceph-7e546da9-92e0-4fc8-a5a2-c05e590abd40" successfully removed\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T16:23:54,749367484-07:00] STAGE: Bootstrap a new cluster...\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:317 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.1 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T16:23:54,759697411-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T16:23:54,763313839-07:00] DEBUG: CEPHADM_PARAMS: ['bootstrap', '--config', '/tmp/bench-rados/ceph-research/scripts/ceph.conf', '--mon-ip', '10.10.2.1', '--output-dir', '/tmp/bench-rados/deployment_data_root/etc/ceph', '--ssh-user', 'ljishen', '--skip-dashboard', '--no-minimize-config', '--allow-overwrite', '--allow-fqdn-hostname', '--orphan-initial-daemons', '--skip-monitoring-stack', '--single-host-defaults']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.1 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults\n'
10.10.2.1: b'Verifying podman|docker is present...\n'
10.10.2.1: b'Verifying lvm2 is present...\n'
10.10.2.1: b'Verifying time synchronization is in place...\n'
10.10.2.1: b'Unit ntp.service is enabled and running\n'
10.10.2.1: b'Repeating the final host check...\n'
10.10.2.1: b'podman|docker (/usr/bin/docker) is present\n'
10.10.2.1: b'systemctl is present\n'
10.10.2.1: b'lvcreate is present\n'
10.10.2.1: b'Unit ntp.service is enabled and running\n'
10.10.2.1: b'Host looks OK\n'
10.10.2.1: b'Cluster fsid: 1e5aa048-3846-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Verifying IP 10.10.2.1 port 3300 ...\n'
10.10.2.1: b'Verifying IP 10.10.2.1 port 6789 ...\n'
10.10.2.1: b'Mon IP `10.10.2.1` is in CIDR network `10.10.2.0/24`\n- internal network (--cluster-network) has not been provided, OSD replication will default to the public_network\n'
10.10.2.1: b'Adjusting default settings to suit single-host cluster...\n'
10.10.2.1: b'Pulling container image quay.io/ceph/ceph:v16...\n'
10.10.2.1: b'Ceph version: ceph version 16.2.6 (ee28fb57e47e9f88813e24bbf4c14496ca299d31) pacific (stable)\nExtracting ceph user uid/gid from container image...\n'
10.10.2.1: b'Creating initial keys...\n'
10.10.2.1: b'Creating initial monmap...\n'
10.10.2.1: b'Creating mon...\n'
10.10.2.1: b'Waiting for mon to start...\n'
10.10.2.1: b'Waiting for mon...\n'
10.10.2.1: b'mon is available\nSetting mon public_network to 10.10.2.0/24\n'
10.10.2.1: b'Wrote config to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf\n'
10.10.2.1: b'Wrote keyring to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring\n'
10.10.2.1: b'Creating mgr...\n'
10.10.2.1: b'Verifying port 9283 ...\n'
10.10.2.1: b'Waiting for mgr to start...\n'
10.10.2.1: b'Waiting for mgr...\n'
10.10.2.1: b'mgr not available, waiting (1/15)...\n'
10.10.2.1: b'mgr not available, waiting (2/15)...\n'
10.10.2.1: b'mgr is available\n'
10.10.2.1: b'Enabling cephadm module...\n'
10.10.2.1: b'Waiting for the mgr to restart...\n'
10.10.2.1: b'Waiting for mgr epoch 5...\n'
10.10.2.1: b'mgr epoch 5 is available\n'
10.10.2.1: b'Setting orchestrator backend to cephadm...\n'
10.10.2.1: b'Generating ssh key...\n'
10.10.2.1: b'Wrote public SSH key to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub\n'
10.10.2.1: b'Adding key to ljishen@localhost authorized_keys...\n'
10.10.2.1: b'Adding host node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us...\n'
10.10.2.1: b'Deploying unmanaged mon service...\n'
10.10.2.1: b'Deploying unmanaged mgr service...\n'
10.10.2.1: b'You can access the Ceph CLI with:\n\n\tsudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --fsid 1e5aa048-3846-11ec-b51d-53e6e728d2d3 -c /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf -k /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring\n\n'
10.10.2.1: b'Please consider enabling telemetry to help improve Ceph:\n\n\tceph telemetry on\n\nFor more information see:\n\n\thttps://docs.ceph.com/docs/pacific/mgr/telemetry/\n\nBootstrap complete.\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T16:24:56,713088636-07:00] INFO: Sleep for 20 seconds\x1b[0m\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T16:25:16,720799321-07:00] INFO: > Set the default number of monitor daemons to 1\x1b[0m\n# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:337 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T16:25:16,731278309-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T16:25:16,735462734-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mon', '1']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1\n'
10.10.2.1: b'Inferring fsid 1e5aa048-3846-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/1e5aa048-3846-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b'Scheduled mon update...\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T16:25:26,656320009-07:00] INFO: > Set the default number of manager daemons to 1\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:342 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T16:25:26,666165827-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T16:25:26,669536743-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mgr', '1']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1\n'
10.10.2.1: b'Inferring fsid 1e5aa048-3846-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/1e5aa048-3846-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b'Scheduled mgr update...\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T16:25:36,096567124-07:00] STAGE: Add hosts to the cluster...\x1b[0m\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T16:25:36,102650179-07:00] INFO: > Adding host 'sm1'\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:361 - add_hosts() > ssh-copy-id -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -f -i /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub ljishen@sm1\n'
10.10.2.1: b'/usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: "/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub"\n'
10.10.2.1: b'\nNumber of key(s) added: 1\n\nNow try logging into the machine, with:   "ssh -o \'GlobalKnownHostsFile=/dev/null\' -o \'LogLevel=ERROR\' -o \'PasswordAuthentication=no\' -o \'StrictHostKeyChecking=no\' -o \'UserKnownHostsFile=/dev/null\' \'ljishen@sm1\'"\nand check to make sure that only the key(s) you wanted were added.\n\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:367 - add_hosts() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add sm1 10.10.2.5\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T16:25:37,260283114-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T16:25:37,264054643-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'host', 'add', 'sm1', '10.10.2.5']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add sm1 10.10.2.5\n'
10.10.2.1: b'Inferring fsid 1e5aa048-3846-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/1e5aa048-3846-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b"Added host 'sm1' with addr '10.10.2.5'\n"
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T16:25:48,118693027-07:00] INFO: Sleep for 20 seconds\x1b[0m\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T16:26:08,123917092-07:00] STAGE: Add OSDs to the cluster...\x1b[0m\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T16:26:08,130244375-07:00] INFO: > Adding devices ['/dev/nvme0n1'] on host 'sm1' as OSDs\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:388 - add_host_devices_as_osds() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd sm1:/dev/nvme0n1\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T16:26:08,140920533-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T16:26:08,145001554-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'daemon', 'add', 'osd', 'sm1:/dev/nvme0n1']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd sm1:/dev/nvme0n1\n'
10.10.2.1: b'Inferring fsid 1e5aa048-3846-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/1e5aa048-3846-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b"Created osd(s) 0 on host 'sm1'\n"
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T16:26:33,420979240-07:00] INFO: > Sleep for 20 seconds\x1b[0m\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T16:26:53,426718101-07:00] STAGE: Show Cluster Status\x1b[0m\n# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:408 - show_status() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T16:26:53,436448852-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T16:26:53,440445986-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', '--status']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status\n'
10.10.2.1: b'Inferring fsid 1e5aa048-3846-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/1e5aa048-3846-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b'  cluster:\n    id:     1e5aa048-3846-11ec-b51d-53e6e728d2d3\n    health: HEALTH_WARN\n            1 pool(s) have no replicas configured\n \n  services:\n    mon: 1 daemons, quorum node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us (age 2m)\n    mgr: node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us.vyydcw(active, since 2m)\n    osd: 1 osds: 1 up (since 20s), 1 in (since 40s)\n \n  data:\n    pools:   1 pools, 1 pgs\n    objects: 0 objects, 0 B\n    usage:   4.7 MiB used, 400 GiB / 400 GiB avail\n    pgs:     1 active+clean\n \n'
[1] 16:27:02 [SUCCESS] ljishen@10.10.2.1

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:23:40,212204444-07:00] INFO: > Remove existing clusters[0m
## bash:17 -  > basename -- /var/lib/ceph/d6b07d4a-3844-11ec-b51d-53e6e728d2d3
# bash:17 -  > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm --host 10.10.2.1 --host 10.10.2.5 rm-cluster --force --fsid d6b07d4a-3844-11ec-b51d-53e6e728d2d3
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:23:40,224152091-07:00] DEBUG: HOST_PARAMS: ['--host', '10.10.2.1', '--host', '10.10.2.5'][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:23:40,227990206-07:00] DEBUG: CEPHADM_PARAMS: ['rm-cluster', '--force', '--fsid', 'd6b07d4a-3844-11ec-b51d-53e6e728d2d3'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:63 -  > /tmp/bench-rados/ceph-research/scripts/parallel-ssh --host 10.10.2.1 --host 10.10.2.5 'sudo python3 -u - rm-cluster --force --fsid d6b07d4a-3844-11ec-b51d-53e6e728d2d3'
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:23:40,236191151-07:00] DEBUG: command from stdin[0m
# /tmp/bench-rados/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host 10.10.2.1 --host 10.10.2.5 'sudo python3 -u - rm-cluster --force --fsid d6b07d4a-3844-11ec-b51d-53e6e728d2d3'
[1] 16:23:46 [SUCCESS] 10.10.2.1
[2] 16:23:48 [SUCCESS] 10.10.2.5

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:23:49,005376529-07:00] INFO: > Deploy a new cluster[0m
# bash:24 -  > /tmp/bench-rados/ceph-research/scripts/ceph-deploy -m 10.10.2.1 -o sm1/10.10.2.5:/dev/nvme0n1


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:23:49,019073475-07:00] STAGE: Verify passwordless SSH to hosts, passwordless sudo via ljishen, and the existence of devices on hosts...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:23:49,023886182-07:00] INFO: > Check the host for the monitor[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:181 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@10.10.2.1 sudo --non-interactive --validate

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:23:49,175997836-07:00] INFO: > Check hosts for OSDs[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:23:49,180236493-07:00] INFO: >> Check host 'sm1'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:200 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 sudo --non-interactive --validate

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:23:50,263415534-07:00] INFO: >>> Check device '/dev/nvme0n1'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:215 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 test -b /dev/nvme0n1
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:226 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 lsblk --nodeps --noheadings --output TYPE /dev/nvme0n1
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:227 - verify_parameters() > grep --word-regexp disk


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:23:52,451603378-07:00] STAGE: Reset devices on hosts...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:23:52,456486708-07:00] INFO: > Resetting device '/dev/nvme0n1' on host 'sm1'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:288 - reset_host_devices() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 sudo sh
  Removing ceph--7e546da9--92e0--4fc8--a5a2--c05e590abd40-osd--block--caccf7c9--516e--4aef--a61e--b27a60345ba4 (252:0)
  Archiving volume group "ceph-7e546da9-92e0-4fc8-a5a2-c05e590abd40" metadata (seqno 5).
  Releasing logical volume "osd-block-caccf7c9-516e-4aef-a61e-b27a60345ba4"
  Creating volume group backup "/etc/lvm/backup/ceph-7e546da9-92e0-4fc8-a5a2-c05e590abd40" (seqno 6).
  Logical volume "osd-block-caccf7c9-516e-4aef-a61e-b27a60345ba4" successfully removed
  Removing physical volume "/dev/nvme0n1" from volume group "ceph-7e546da9-92e0-4fc8-a5a2-c05e590abd40"
  Volume group "ceph-7e546da9-92e0-4fc8-a5a2-c05e590abd40" successfully removed


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:23:54,749367484-07:00] STAGE: Bootstrap a new cluster...[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:317 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.1 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:23:54,759697411-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:23:54,763313839-07:00] DEBUG: CEPHADM_PARAMS: ['bootstrap', '--config', '/tmp/bench-rados/ceph-research/scripts/ceph.conf', '--mon-ip', '10.10.2.1', '--output-dir', '/tmp/bench-rados/deployment_data_root/etc/ceph', '--ssh-user', 'ljishen', '--skip-dashboard', '--no-minimize-config', '--allow-overwrite', '--allow-fqdn-hostname', '--orphan-initial-daemons', '--skip-monitoring-stack', '--single-host-defaults'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.1 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults
Verifying podman|docker is present...
Verifying lvm2 is present...
Verifying time synchronization is in place...
Unit ntp.service is enabled and running
Repeating the final host check...
podman|docker (/usr/bin/docker) is present
systemctl is present
lvcreate is present
Unit ntp.service is enabled and running
Host looks OK
Cluster fsid: 1e5aa048-3846-11ec-b51d-53e6e728d2d3
Verifying IP 10.10.2.1 port 3300 ...
Verifying IP 10.10.2.1 port 6789 ...
Mon IP `10.10.2.1` is in CIDR network `10.10.2.0/24`
- internal network (--cluster-network) has not been provided, OSD replication will default to the public_network
Adjusting default settings to suit single-host cluster...
Pulling container image quay.io/ceph/ceph:v16...
Ceph version: ceph version 16.2.6 (ee28fb57e47e9f88813e24bbf4c14496ca299d31) pacific (stable)
Extracting ceph user uid/gid from container image...
Creating initial keys...
Creating initial monmap...
Creating mon...
Waiting for mon to start...
Waiting for mon...
mon is available
Setting mon public_network to 10.10.2.0/24
Wrote config to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf
Wrote keyring to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring
Creating mgr...
Verifying port 9283 ...
Waiting for mgr to start...
Waiting for mgr...
mgr not available, waiting (1/15)...
mgr not available, waiting (2/15)...
mgr is available
Enabling cephadm module...
Waiting for the mgr to restart...
Waiting for mgr epoch 5...
mgr epoch 5 is available
Setting orchestrator backend to cephadm...
Generating ssh key...
Wrote public SSH key to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub
Adding key to ljishen@localhost authorized_keys...
Adding host node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us...
Deploying unmanaged mon service...
Deploying unmanaged mgr service...
You can access the Ceph CLI with:

	sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --fsid 1e5aa048-3846-11ec-b51d-53e6e728d2d3 -c /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf -k /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring

Please consider enabling telemetry to help improve Ceph:

	ceph telemetry on

For more information see:

	https://docs.ceph.com/docs/pacific/mgr/telemetry/

Bootstrap complete.

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:24:56,713088636-07:00] INFO: Sleep for 20 seconds[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:25:16,720799321-07:00] INFO: > Set the default number of monitor daemons to 1[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:337 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:25:16,731278309-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:25:16,735462734-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mon', '1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1
Inferring fsid 1e5aa048-3846-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/1e5aa048-3846-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Scheduled mon update...

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:25:26,656320009-07:00] INFO: > Set the default number of manager daemons to 1[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:342 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:25:26,666165827-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:25:26,669536743-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mgr', '1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1
Inferring fsid 1e5aa048-3846-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/1e5aa048-3846-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Scheduled mgr update...


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:25:36,096567124-07:00] STAGE: Add hosts to the cluster...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:25:36,102650179-07:00] INFO: > Adding host 'sm1'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:361 - add_hosts() > ssh-copy-id -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -f -i /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub ljishen@sm1
/usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: "/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub"

Number of key(s) added: 1

Now try logging into the machine, with:   "ssh -o 'GlobalKnownHostsFile=/dev/null' -o 'LogLevel=ERROR' -o 'PasswordAuthentication=no' -o 'StrictHostKeyChecking=no' -o 'UserKnownHostsFile=/dev/null' 'ljishen@sm1'"
and check to make sure that only the key(s) you wanted were added.

# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:367 - add_hosts() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add sm1 10.10.2.5
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:25:37,260283114-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:25:37,264054643-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'host', 'add', 'sm1', '10.10.2.5'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add sm1 10.10.2.5
Inferring fsid 1e5aa048-3846-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/1e5aa048-3846-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Added host 'sm1' with addr '10.10.2.5'

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:25:48,118693027-07:00] INFO: Sleep for 20 seconds[0m


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:26:08,123917092-07:00] STAGE: Add OSDs to the cluster...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:26:08,130244375-07:00] INFO: > Adding devices ['/dev/nvme0n1'] on host 'sm1' as OSDs[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:388 - add_host_devices_as_osds() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd sm1:/dev/nvme0n1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:26:08,140920533-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:26:08,145001554-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'daemon', 'add', 'osd', 'sm1:/dev/nvme0n1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd sm1:/dev/nvme0n1
Inferring fsid 1e5aa048-3846-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/1e5aa048-3846-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Created osd(s) 0 on host 'sm1'

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:26:33,420979240-07:00] INFO: > Sleep for 20 seconds[0m


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:26:53,426718101-07:00] STAGE: Show Cluster Status[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:408 - show_status() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:26:53,436448852-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:26:53,440445986-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', '--status'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status
Inferring fsid 1e5aa048-3846-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/1e5aa048-3846-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
  cluster:
    id:     1e5aa048-3846-11ec-b51d-53e6e728d2d3
    health: HEALTH_WARN
            1 pool(s) have no replicas configured
 
  services:
    mon: 1 daemons, quorum node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us (age 2m)
    mgr: node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us.vyydcw(active, since 2m)
    osd: 1 osds: 1 up (since 20s), 1 in (since 40s)
 
  data:
    pools:   1 pools, 1 pgs
    objects: 0 objects, 0 B
    usage:   4.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     1 active+clean
 

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:27:02,037909107-07:00] INFO: > Copy configuration files to local[0m
# ./benchmarks/bench-rados:220 - launch_ceph_cluster() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.1 sudo chown --changes --recursive ljishen /tmp/bench-rados/deployment_data_root/etc/ceph
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:27:02,045905736-07:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.1 sudo chown --changes --recursive ljishen /tmp/bench-rados/deployment_data_root/etc/ceph
[1] 16:27:02 [SUCCESS] ljishen@10.10.2.1
# ./benchmarks/bench-rados:223 - launch_ceph_cluster() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.1:/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf
# ./benchmarks/bench-rados:226 - launch_ceph_cluster() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.1:/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:27:02,524839554-07:00] STAGE: Update local ceph.conf as a client...[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:27:02,528535038-07:00] STAGE: Configure Ceph pool...[0m
# ./benchmarks/bench-rados:257 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_max_backfills 32
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:27:02,550578949-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:27:02,553293875-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '1e5aa048-3846-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'config', 'set', 'osd', 'osd_max_backfills', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 1e5aa048-3846-11ec-b51d-53e6e728d2d3 -- ceph config set osd osd_max_backfills 32
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:258 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_active 32
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:27:06,667353228-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:27:06,670146592-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '1e5aa048-3846-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_active', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 1e5aa048-3846-11ec-b51d-53e6e728d2d3 -- ceph config set osd osd_recovery_max_active 32
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:259 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_single_start 8
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:27:10,794239462-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:27:10,797156038-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '1e5aa048-3846-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_single_start', '8'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 1e5aa048-3846-11ec-b51d-53e6e728d2d3 -- ceph config set osd osd_recovery_max_single_start 8
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:260 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_op_priority 63
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:27:14,960560753-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:27:14,963737639-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '1e5aa048-3846-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_op_priority', '63'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 1e5aa048-3846-11ec-b51d-53e6e728d2d3 -- ceph config set osd osd_recovery_op_priority 63
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:262 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config show-with-defaults osd.0
# ./benchmarks/bench-rados:263 - configure_ceph_pool() > grep 'osd_max_backfills\|osd_recovery'
# ./benchmarks/bench-rados:263 - configure_ceph_pool() > column -t -s ' '
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
osd_max_backfills                        32         mon
osd_recovery_cost                        20971520   default
osd_recovery_delay_start                 0.000000   default
osd_recovery_max_active                  32         mon
osd_recovery_max_active_hdd              3          default
osd_recovery_max_active_ssd              10         default
osd_recovery_max_chunk                   8388608    default
osd_recovery_max_omap_entries_per_chunk  8096       default
osd_recovery_max_single_start            8          mon
osd_recovery_op_priority                 63         mon
osd_recovery_op_warn_multiple            16         default
osd_recovery_priority                    5          default
osd_recovery_retry_interval              30.000000  default
osd_recovery_sleep                       0.000000   default
osd_recovery_sleep_hdd                   0.100000   default
osd_recovery_sleep_hybrid                0.025000   default
osd_recovery_sleep_ssd                   0.000000   default
# ./benchmarks/bench-rados:265 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool create bench_rados 128 128 replicated --size 1 --pg-num-min 128
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:27:22,830549817-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:27:22,833743155-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '1e5aa048-3846-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'create', 'bench_rados', '128', '128', 'replicated', '--size', '1', '--pg-num-min', '128'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 1e5aa048-3846-11ec-b51d-53e6e728d2d3 -- ceph osd pool create bench_rados 128 128 replicated --size 1 --pg-num-min 128
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
pool 'bench_rados' created
# ./benchmarks/bench-rados:267 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados min_size 1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:27:27,212239676-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:27:27,215436761-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '1e5aa048-3846-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'min_size', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 1e5aa048-3846-11ec-b51d-53e6e728d2d3 -- ceph osd pool set bench_rados min_size 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 min_size to 1
# ./benchmarks/bench-rados:268 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados pg_autoscale_mode off
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:27:31,446713360-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:27:31,449999784-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '1e5aa048-3846-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'pg_autoscale_mode', 'off'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 1e5aa048-3846-11ec-b51d-53e6e728d2d3 -- ceph osd pool set bench_rados pg_autoscale_mode off
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 pg_autoscale_mode to off
# ./benchmarks/bench-rados:269 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool application enable bench_rados benchmark
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:27:35,647452163-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:27:35,650370633-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '1e5aa048-3846-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'application', 'enable', 'bench_rados', 'benchmark'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 1e5aa048-3846-11ec-b51d-53e6e728d2d3 -- ceph osd pool application enable bench_rados benchmark
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
enabled application 'benchmark' on pool 'bench_rados'
# ./benchmarks/bench-rados:270 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados noscrub 1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:27:39,790151674-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:27:39,793242318-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '1e5aa048-3846-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'noscrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 1e5aa048-3846-11ec-b51d-53e6e728d2d3 -- ceph osd pool set bench_rados noscrub 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 noscrub to 1
# ./benchmarks/bench-rados:271 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados nodeep-scrub 1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:27:43,897315749-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:27:43,900670601-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '1e5aa048-3846-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'nodeep-scrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 1e5aa048-3846-11ec-b51d-53e6e728d2d3 -- ceph osd pool set bench_rados nodeep-scrub 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 nodeep-scrub to 1
# ./benchmarks/bench-rados:272 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool ls detail
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:27:48,680788842-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:27:48,683875799-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '1e5aa048-3846-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'ls', 'detail'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 1e5aa048-3846-11ec-b51d-53e6e728d2d3 -- ceph osd pool ls detail
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
pool 1 'device_health_metrics' replicated size 1 min_size 1 crush_rule 0 object_hash rjenkins pg_num 64 pgp_num 64 autoscale_mode on last_change 18 lfor 0/0/16 flags hashpspool stripe_width 0 pg_num_min 1 application mgr_devicehealth
pool 2 'bench_rados' replicated size 1 min_size 1 crush_rule 0 object_hash rjenkins pg_num 128 pgp_num 128 autoscale_mode off last_change 19 flags hashpspool,noscrub,nodeep-scrub stripe_width 0 pg_num_min 128 application benchmark

# ./benchmarks/bench-rados:273 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd df tree
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:27:52,740155576-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:27:52,743390532-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '1e5aa048-3846-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'df', 'tree'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 1e5aa048-3846-11ec-b51d-53e6e728d2d3 -- ceph osd df tree
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
ID  CLASS  WEIGHT   REWEIGHT  SIZE     RAW USE  DATA     OMAP  META     AVAIL    %USE  VAR   PGS  STATUS  TYPE NAME    
-1         0.39059         -  400 GiB  5.7 MiB  160 KiB   0 B  5.6 MiB  400 GiB  0.00  1.00    -          root default 
-3         0.39059         -  400 GiB  5.7 MiB  160 KiB   0 B  5.6 MiB  400 GiB  0.00  1.00    -              host sm1 
 0    ssd  0.39059   1.00000  400 GiB  5.7 MiB  160 KiB   0 B  5.6 MiB  400 GiB  0.00  1.00  192      up          osd.0
                       TOTAL  400 GiB  5.7 MiB  160 KiB   0 B  5.6 MiB  400 GiB  0.00                                  
MIN/MAX VAR: 1.00/1.00  STDDEV: 0


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:27:56,902327895-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:28:20,907686096-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:28:29,973366055-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:28:38,923199538-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:28:38,930739948-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:28:47,887997625-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:28:47,895441803-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:28:56,888457448-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:28:56,896005182-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:29:05,878958088-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:29:05,886702322-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:29:14,945965306-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:29:14,953293015-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:29:14,959522334-07:00] INFO: > The cluster is idle now.[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:29:14,963103343-07:00] STAGE: Start benchmarking of mode write ...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:29:14,969828496-07:00] INFO: > Start system activity collection services[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:29:14,975383985-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:341 - rados_bench() > rm --force /mnt/sda8/ceph-research/scripts/output/sys_activity_256KB_write.dat.rados_bench_host.3
# ./benchmarks/bench-rados:342 - rados_bench() > sar_pid_rados_bench_host=876425
# ./benchmarks/bench-rados:342 - rados_bench() > S_TIME_FORMAT=ISO
# ./benchmarks/bench-rados:342 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/output/sys_activity_256KB_write.dat.rados_bench_host.3 2

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:29:14,982352567-07:00] INFO: >> for OSD host[0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:29:14,991397512-07:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.5 bash
10.10.2.5: b'310627\n'
[1] 16:29:16 [SUCCESS] ljishen@10.10.2.5
310627

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:29:16,107606434-07:00] INFO: > Run rados bench[0m
# ./benchmarks/bench-rados:370 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/output/rados_bench_256KB_write.log.3
## ./benchmarks/bench-rados:361 - rados_bench() > count_cpus 32-63,96-127
# ./benchmarks/bench-rados:361 - rados_bench() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 60 write --pool bench_rados -b 262144 -O 262144 --concurrent-ios 128 --show-time --write-object --write-omap --write-xattr --no-cleanup
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:29:16,127640428-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:29:16,130578705-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '1e5aa048-3846-11ec-b51d-53e6e728d2d3', '--', 'rados', 'bench', '60', 'write', '--pool', 'bench_rados', '-b', '262144', '-O', '262144', '--concurrent-ios', '128', '--show-time', '--write-object', '--write-omap', '--write-xattr', '--no-cleanup'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 1e5aa048-3846-11ec-b51d-53e6e728d2d3 -- rados bench 60 write --pool bench_rados -b 262144 -O 262144 --concurrent-ios 128 --show-time --write-object --write-omap --write-xattr --no-cleanup
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
hints = 1
2021-10-28T23:29:19.275719+0000 Maintaining 128 concurrent writes of 262144 bytes to objects of size 262144 for up to 60 seconds or 0 objects
2021-10-28T23:29:19.275734+0000 Object prefix: benchmark_data_node-2.bluefield2.ucsc-cmps10_7
2021-10-28T23:29:19.284764+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-28T23:29:19.284764+0000     0       0         0         0         0         0           -           0
2021-10-28T23:29:20.284884+0000     1     128       187        59   14.7492     14.75    0.612889    0.601612
2021-10-28T23:29:21.284965+0000     2     128       385       257   32.1229      49.5    0.874872    0.856877
2021-10-28T23:29:22.285078+0000     3     128       513       385   32.0807        32    0.915554    0.888428
2021-10-28T23:29:23.285162+0000     4     128       641       513   32.0599        32    0.922419    0.892476
2021-10-28T23:29:24.285255+0000     5     128       769       641   32.0473        32     1.15738    0.937368
2021-10-28T23:29:25.285333+0000     6     128       897       769    32.039        32    0.851547    0.930907
2021-10-28T23:29:26.285412+0000     7     128      1025       897   32.0331        32     1.01538     0.94282
2021-10-28T23:29:27.285494+0000     8     128      1153      1025   32.0286        32    0.838084    0.929208
2021-10-28T23:29:28.285609+0000     9     128      1281      1153    32.025        32    0.966665    0.932902
2021-10-28T23:29:29.285688+0000    10     128      1409      1281   32.0223        32    0.988085    0.938166
2021-10-28T23:29:30.285793+0000    11     128      1537      1409   32.0199        32     1.05808    0.952018
2021-10-28T23:29:31.285878+0000    12     128      1687      1559   32.4763      37.5    0.920187    0.942066
2021-10-28T23:29:32.285998+0000    13     128      1851      1723   33.1316        41    0.856648    0.940335
2021-10-28T23:29:33.286082+0000    14     128      1979      1851   33.0506        32     0.97012    0.937031
2021-10-28T23:29:34.286155+0000    15     128      2062      1934   32.2305     20.75     1.18944    0.939278
2021-10-28T23:29:35.286241+0000    16     128      2177      2049   32.0128     28.75     1.35694    0.959067
2021-10-28T23:29:36.286321+0000    17     128      2177      2049   30.1297         0           -    0.959067
2021-10-28T23:29:37.286401+0000    18     128      2235      2107   29.2613      7.25      2.1114    0.990816
2021-10-28T23:29:38.286510+0000    19     128      2255      2127   27.9844         5     3.34902     1.01307
2021-10-28T23:29:39.286591+0000 min lat: 0.205006 max lat: 3.37449 avg lat: 1.06728
2021-10-28T23:29:39.286591+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-28T23:29:39.286591+0000    20     128      2305      2177   27.2101      12.5     3.37381     1.06728
2021-10-28T23:29:40.286720+0000    21     128      2363      2235   26.6048      14.5     3.82749     1.13891
2021-10-28T23:29:41.286797+0000    22     128      2363      2235   25.3955         0           -     1.13891
2021-10-28T23:29:42.286905+0000    23     128      2433      2305   25.0521      8.75     3.87804     1.22048
2021-10-28T23:29:43.286984+0000    24     128      2491      2363   24.6124      14.5      2.9783     1.26344
2021-10-28T23:29:44.287085+0000    25     128      2491      2363   23.6279         0           -     1.26344
2021-10-28T23:29:45.287166+0000    26     128      2491      2363   22.7191         0           -     1.26344
2021-10-28T23:29:46.287246+0000    27     128      2561      2433   22.5258   5.83333     3.18129      1.3202
2021-10-28T23:29:47.287326+0000    28     128      2619      2491   22.2391      14.5     3.40784     1.36894
2021-10-28T23:29:48.287432+0000    29     128      2689      2561   22.0756      17.5      2.6256     1.40253
2021-10-28T23:29:49.287514+0000    30     128      2747      2619    21.823      14.5      2.3858     1.42436
2021-10-28T23:29:50.287593+0000    31     128      2747      2619   21.1191         0           -     1.42436
2021-10-28T23:29:51.287670+0000    32     128      2817      2689   21.0059      8.75     2.41735     1.44989
2021-10-28T23:29:52.287784+0000    33     128      2875      2747   20.8087      14.5     3.02635     1.48278
2021-10-28T23:29:53.287867+0000    34     128      2945      2817   20.7114      17.5     2.86836     1.51811
2021-10-28T23:29:54.287966+0000    35     128      2945      2817   20.1196         0           -     1.51811
2021-10-28T23:29:55.288043+0000    36     128      3003      2875   19.9635      7.25     2.76815     1.54338
2021-10-28T23:29:56.288121+0000    37     128      3003      2875   19.4239         0           -     1.54338
2021-10-28T23:29:57.288199+0000    38     128      3073      2945   19.3733      8.75     3.46584     1.58867
2021-10-28T23:29:58.288297+0000    39     128      3131      3003   19.2483      14.5     3.41797     1.62434
2021-10-28T23:29:59.288379+0000 min lat: 0.205006 max lat: 3.87867 avg lat: 1.62434
2021-10-28T23:29:59.288379+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-28T23:29:59.288379+0000    40     128      3131      3003   18.7671         0           -     1.62434
2021-10-28T23:30:00.288488+0000    41     128      3201      3073   18.7361      8.75     3.19628     1.66006
2021-10-28T23:30:01.288566+0000    42     128      3201      3073     18.29         0           -     1.66006
2021-10-28T23:30:02.288661+0000    43     128      3329      3201   18.6088        16     2.03055     1.69589
2021-10-28T23:30:03.288741+0000    44     128      3457      3329   18.9131        32     1.16144     1.67689
2021-10-28T23:30:04.288843+0000    45     128      3585      3457   19.2038        32     1.06791     1.65403
2021-10-28T23:30:05.288921+0000    46     128      3713      3585    19.482        32    0.979937     1.62766
2021-10-28T23:30:06.288997+0000    47     128      3841      3713   19.7483        32    0.890168     1.60543
2021-10-28T23:30:07.289076+0000    48     128      3969      3841   20.0034        32    0.886461     1.58039
2021-10-28T23:30:08.289192+0000    49     128      4097      3969   20.2482        32     1.08136      1.5649
2021-10-28T23:30:09.289274+0000    50     128      4180      4052   20.2582     20.75     1.26816     1.55469
2021-10-28T23:30:10.289386+0000    51     128      4225      4097   20.0815     11.25     1.27823     1.55166
2021-10-28T23:30:11.289466+0000    52     128      4283      4155   19.9742      14.5     1.88838     1.55639
2021-10-28T23:30:12.289577+0000    53     128      4310      4182   19.7246      6.75     2.98318      1.5658
2021-10-28T23:30:13.289656+0000    54     128      4353      4225   19.5584     10.75     3.01855     1.58057
2021-10-28T23:30:14.289754+0000    55     128      4411      4283   19.4664      14.5     3.11767     1.60096
2021-10-28T23:30:15.289832+0000    56     128      4411      4283   19.1188         0           -     1.60096
2021-10-28T23:30:16.289906+0000    57     128      4411      4283   18.7834         0           -     1.60096
2021-10-28T23:30:17.289982+0000    58     128      4481      4353   18.7613   5.83333     4.67766     1.65056
2021-10-28T23:30:18.290093+0000    59     128      4481      4353   18.4433         0           -     1.65056
2021-10-28T23:30:19.290177+0000 min lat: 0.205006 max lat: 4.69886 avg lat: 1.65056
2021-10-28T23:30:19.290177+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-28T23:30:19.290177+0000    60     128      4481      4353   18.1359         0           -     1.65056
2021-10-28T23:30:20.290307+0000    61      70      4481      4411   18.0762   4.83333     6.48529     1.71454
2021-10-28T23:30:21.290384+0000    62      70      4481      4411   17.7847         0           -     1.71454
2021-10-28T23:30:22.290492+0000    63      70      4481      4411   17.5024         0           -     1.71454
2021-10-28T23:30:23.290601+0000 Total time run:         63.9
Total writes made:      4481
Write size:             262144
Object size:            262144
Bandwidth (MB/sec):     17.5313
Stddev Bandwidth:       13.5246
Max bandwidth (MB/sec): 49.5
Min bandwidth (MB/sec): 0
Average IOPS:           70
Stddev IOPS:            54.111
Max IOPS:               198
Min IOPS:               0
Average Latency(s):     1.78457
Stddev Latency(s):      1.28845
Max latency(s):         6.53358
Min latency(s):         0.205006

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:30:24,200875345-07:00] INFO: > Stop system activity collection services[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:30:24,206820557-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:391 - rados_bench() > kill -INT 876425

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:30:24,212477416-07:00] INFO: >> for OSD host[0m
# ./benchmarks/bench-rados:396 - rados_bench() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.5 kill -INT 310627
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:30:24,220529928-07:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.5 kill -INT 310627
[1] 16:30:25 [SUCCESS] ljishen@10.10.2.5

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:30:25,352884506-07:00] INFO: >>> Copy system activity file from OSD host[0m
## ./benchmarks/bench-rados:403 - rados_bench() > basename -- /tmp/bench-rados/sys_activity_256KB_write.dat.osd_host.3
# ./benchmarks/bench-rados:403 - rados_bench() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.5:/tmp/bench-rados/sys_activity_256KB_write.dat.osd_host.3 /mnt/sda8/ceph-research/scripts/output/sys_activity_256KB_write.dat.osd_host.3


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:30:26,506148922-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:30:50,404898760-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 4.48k objects, 1.1 GiB
    usage:   4.5 GiB used, 395 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:30:50,412684530-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:30:59,403500258-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 4.48k objects, 1.1 GiB
    usage:   4.8 GiB used, 395 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:30:59,411286228-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:31:08,367349833-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 4.48k objects, 1.1 GiB
    usage:   3.6 GiB used, 396 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:31:08,375396154-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:31:17,507799041-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 4.48k objects, 1.1 GiB
    usage:   3.3 GiB used, 397 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:31:17,515848198-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:31:26,614277789-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 4.48k objects, 1.1 GiB
    usage:   3.3 GiB used, 397 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:31:26,622160541-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:31:26,628300210-07:00] INFO: > The cluster is idle now.[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:31:26,632187123-07:00] STAGE: Start benchmarking of mode seq ...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:31:26,638891025-07:00] INFO: > Start system activity collection services[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:31:26,644589071-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:341 - rados_bench() > rm --force /mnt/sda8/ceph-research/scripts/output/sys_activity_256KB_seq.dat.rados_bench_host.3
# ./benchmarks/bench-rados:342 - rados_bench() > sar_pid_rados_bench_host=878389
# ./benchmarks/bench-rados:342 - rados_bench() > S_TIME_FORMAT=ISO
# ./benchmarks/bench-rados:342 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/output/sys_activity_256KB_seq.dat.rados_bench_host.3 2

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:31:26,651274808-07:00] INFO: >> for OSD host[0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:31:26,660080580-07:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.5 bash
10.10.2.5: b'311174\n'
[1] 16:31:27 [SUCCESS] ljishen@10.10.2.5
311174

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:31:27,791667276-07:00] INFO: > Run rados bench[0m
# ./benchmarks/bench-rados:379 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/output/rados_bench_256KB_seq.log.3
## ./benchmarks/bench-rados:376 - rados_bench() > count_cpus 32-63,96-127
# ./benchmarks/bench-rados:376 - rados_bench() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 128 --show-time
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:31:27,811491439-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:31:27,814224969-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '1e5aa048-3846-11ec-b51d-53e6e728d2d3', '--', 'rados', 'bench', '99999999', 'seq', '--pool', 'bench_rados', '--concurrent-ios', '128', '--show-time'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 1e5aa048-3846-11ec-b51d-53e6e728d2d3 -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 128 --show-time
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
hints = 1
2021-10-28T23:31:30.812829+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-28T23:31:30.812829+0000     0       0         0         0         0         0           -           0
2021-10-28T23:31:31.812928+0000     1     128       530       402   100.483     100.5    0.333663    0.269684
2021-10-28T23:31:32.813022+0000     2     128       943       815   101.862    103.25    0.283371    0.281135
2021-10-28T23:31:33.813100+0000     3     128      1240      1112   92.6561     74.25     0.47108    0.316901
2021-10-28T23:31:34.813197+0000     4     128      1636      1508   94.2397        99    0.393492     0.31838
2021-10-28T23:31:35.813302+0000     5     128      1915      1787   89.3403     69.75    0.162063    0.342774
2021-10-28T23:31:36.813398+0000     6     128      2294      2166   90.2404     94.75    0.112154    0.344516
2021-10-28T23:31:37.813494+0000     7     128      2655      2527   90.2405     90.25    0.378849    0.337925
2021-10-28T23:31:38.813585+0000     8     128      3022      2894   90.4282     91.75    0.100142    0.342719
2021-10-28T23:31:39.813679+0000     9     128      3333      3205   89.0187     77.75    0.448509    0.350696
2021-10-28T23:31:40.813772+0000    10     128      3700      3572    89.291     91.75    0.335061    0.351838
2021-10-28T23:31:41.813877+0000    11     128      4063      3935   89.4227     90.75    0.445762    0.349306
2021-10-28T23:31:42.813971+0000    12     128      4431      4303   89.6368        92     0.51609    0.350232
2021-10-28T23:31:43.814078+0000 Total time run:       12.5453
Total reads made:     4481
Read size:            262144
Object size:          262144
Bandwidth (MB/sec):   89.2964
Average IOPS:         357
Stddev IOPS:          41.897
Max IOPS:             413
Min IOPS:             279
Average Latency(s):   0.350676
Max latency(s):       1.0802
Min latency(s):       0.00473066

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:31:44,433420530-07:00] INFO: > Stop system activity collection services[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:31:44,439553526-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:391 - rados_bench() > kill -INT 878389

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:31:44,445506823-07:00] INFO: >> for OSD host[0m
# ./benchmarks/bench-rados:396 - rados_bench() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.5 kill -INT 311174
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:31:44,453760073-07:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.5 kill -INT 311174
[1] 16:31:45 [SUCCESS] ljishen@10.10.2.5

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:31:45,548857238-07:00] INFO: >>> Copy system activity file from OSD host[0m
## ./benchmarks/bench-rados:403 - rados_bench() > basename -- /tmp/bench-rados/sys_activity_256KB_seq.dat.osd_host.3
# ./benchmarks/bench-rados:403 - rados_bench() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.5:/tmp/bench-rados/sys_activity_256KB_seq.dat.osd_host.3 /mnt/sda8/ceph-research/scripts/output/sys_activity_256KB_seq.dat.osd_host.3


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:31:46,593085936-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:32:10,593091784-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 4.48k objects, 1.1 GiB
    usage:   3.3 GiB used, 397 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:32:10,600524429-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:32:19,651855644-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 4.48k objects, 1.1 GiB
    usage:   3.3 GiB used, 397 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:32:19,659550653-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:32:28,732115202-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 4.48k objects, 1.1 GiB
    usage:   3.3 GiB used, 397 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:32:28,739884220-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:32:37,797649430-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 4.48k objects, 1.1 GiB
    usage:   3.3 GiB used, 397 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:32:37,805099167-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:32:46,864954692-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 4.48k objects, 1.1 GiB
    usage:   3.3 GiB used, 397 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:32:46,872709203-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:32:46,878827421-07:00] INFO: > The cluster is idle now.[0m


[1;7;39;49m[2021-10-28T16:32:46,882606702-07:00][RUNNING][ROUND 1/5/21] object_size=1MB[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:32:46,886219429-07:00] STAGE: Launch a Ceph cluster (CEPH_HOST: 10.10.2.1, OSD_HOST: 10.10.2.5) ...[0m
# ./benchmarks/bench-rados:183 - launch_ceph_cluster() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.1 bash -euo pipefail
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:32:46,895638286-07:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.1 bash -euo pipefail
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T16:32:47,266163739-07:00] INFO: > Remove existing clusters\x1b[0m\n'
10.10.2.1: b'## bash:17 -  > basename -- /var/lib/ceph/1e5aa048-3846-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'# bash:17 -  > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm --host 10.10.2.1 --host 10.10.2.5 rm-cluster --force --fsid 1e5aa048-3846-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T16:32:47,277458159-07:00] DEBUG: HOST_PARAMS: ['--host', '10.10.2.1', '--host', '10.10.2.5']\x1b[0m\n"
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T16:32:47,281089946-07:00] DEBUG: CEPHADM_PARAMS: ['rm-cluster', '--force', '--fsid', '1e5aa048-3846-11ec-b51d-53e6e728d2d3']\x1b[0m\n"
10.10.2.1: b"# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:63 -  > /tmp/bench-rados/ceph-research/scripts/parallel-ssh --host 10.10.2.1 --host 10.10.2.5 'sudo python3 -u - rm-cluster --force --fsid 1e5aa048-3846-11ec-b51d-53e6e728d2d3'\n"
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T16:32:47,289345494-07:00] DEBUG: command from stdin\x1b[0m\n'
10.10.2.1: b"# /tmp/bench-rados/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host 10.10.2.1 --host 10.10.2.5 'sudo python3 -u - rm-cluster --force --fsid 1e5aa048-3846-11ec-b51d-53e6e728d2d3'\n"
10.10.2.1: b'[1] 16:32:52 [SUCCESS] 10.10.2.1\n[2] 16:32:56 [SUCCESS] 10.10.2.5\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T16:32:56,130692264-07:00] INFO: > Deploy a new cluster\x1b[0m\n'
10.10.2.1: b'# bash:24 -  > /tmp/bench-rados/ceph-research/scripts/ceph-deploy -m 10.10.2.1 -o sm1/10.10.2.5:/dev/nvme0n1\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T16:32:56,142748496-07:00] STAGE: Verify passwordless SSH to hosts, passwordless sudo via ljishen, and the existence of devices on hosts...\x1b[0m\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T16:32:56,147593734-07:00] INFO: > Check the host for the monitor\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:181 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@10.10.2.1 sudo --non-interactive --validate\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T16:32:56,299337519-07:00] INFO: > Check hosts for OSDs\x1b[0m\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T16:32:56,304403722-07:00] INFO: >> Check host 'sm1'\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:200 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 sudo --non-interactive --validate\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T16:32:57,378904018-07:00] INFO: >>> Check device '/dev/nvme0n1'\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:215 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 test -b /dev/nvme0n1\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:226 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 lsblk --nodeps --noheadings --output TYPE /dev/nvme0n1\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:227 - verify_parameters() > grep --word-regexp disk\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T16:32:59,523296326-07:00] STAGE: Reset devices on hosts...\x1b[0m\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T16:32:59,528376476-07:00] INFO: > Resetting device '/dev/nvme0n1' on host 'sm1'\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:288 - reset_host_devices() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 sudo sh\n'
10.10.2.1: b'  Removing ceph--82dec906--651e--40b7--88a3--f7ddaaf917ea-osd--block--24169d04--5bc7--46c4--a29c--d566c9a0de6b (252:0)\n'
10.10.2.1: b'  Archiving volume group "ceph-82dec906-651e-40b7-88a3-f7ddaaf917ea" metadata (seqno 5).\n'
10.10.2.1: b'  Releasing logical volume "osd-block-24169d04-5bc7-46c4-a29c-d566c9a0de6b"\n'
10.10.2.1: b'  Creating volume group backup "/etc/lvm/backup/ceph-82dec906-651e-40b7-88a3-f7ddaaf917ea" (seqno 6).\n'
10.10.2.1: b'  Logical volume "osd-block-24169d04-5bc7-46c4-a29c-d566c9a0de6b" successfully removed\n'
10.10.2.1: b'  Removing physical volume "/dev/nvme0n1" from volume group "ceph-82dec906-651e-40b7-88a3-f7ddaaf917ea"\n'
10.10.2.1: b'  Volume group "ceph-82dec906-651e-40b7-88a3-f7ddaaf917ea" successfully removed\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T16:33:01,844484612-07:00] STAGE: Bootstrap a new cluster...\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:317 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.1 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T16:33:01,854316213-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T16:33:01,858012821-07:00] DEBUG: CEPHADM_PARAMS: ['bootstrap', '--config', '/tmp/bench-rados/ceph-research/scripts/ceph.conf', '--mon-ip', '10.10.2.1', '--output-dir', '/tmp/bench-rados/deployment_data_root/etc/ceph', '--ssh-user', 'ljishen', '--skip-dashboard', '--no-minimize-config', '--allow-overwrite', '--allow-fqdn-hostname', '--orphan-initial-daemons', '--skip-monitoring-stack', '--single-host-defaults']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.1 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults\n'
10.10.2.1: b'Verifying podman|docker is present...\n'
10.10.2.1: b'Verifying lvm2 is present...\nVerifying time synchronization is in place...\n'
10.10.2.1: b'Unit ntp.service is enabled and running\n'
10.10.2.1: b'Repeating the final host check...\npodman|docker (/usr/bin/docker) is present\n'
10.10.2.1: b'systemctl is present\n'
10.10.2.1: b'lvcreate is present\n'
10.10.2.1: b'Unit ntp.service is enabled and running\n'
10.10.2.1: b'Host looks OK\n'
10.10.2.1: b'Cluster fsid: 6472f08e-3847-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Verifying IP 10.10.2.1 port 3300 ...\n'
10.10.2.1: b'Verifying IP 10.10.2.1 port 6789 ...\n'
10.10.2.1: b'Mon IP `10.10.2.1` is in CIDR network `10.10.2.0/24`\n'
10.10.2.1: b'- internal network (--cluster-network) has not been provided, OSD replication will default to the public_network\n'
10.10.2.1: b'Adjusting default settings to suit single-host cluster...\n'
10.10.2.1: b'Pulling container image quay.io/ceph/ceph:v16...\n'
10.10.2.1: b'Ceph version: ceph version 16.2.6 (ee28fb57e47e9f88813e24bbf4c14496ca299d31) pacific (stable)\n'
10.10.2.1: b'Extracting ceph user uid/gid from container image...\n'
10.10.2.1: b'Creating initial keys...\n'
10.10.2.1: b'Creating initial monmap...\n'
10.10.2.1: b'Creating mon...\n'
10.10.2.1: b'Waiting for mon to start...\n'
10.10.2.1: b'Waiting for mon...\n'
10.10.2.1: b'mon is available\n'
10.10.2.1: b'Setting mon public_network to 10.10.2.0/24\n'
10.10.2.1: b'Wrote config to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf\n'
10.10.2.1: b'Wrote keyring to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring\n'
10.10.2.1: b'Creating mgr...\n'
10.10.2.1: b'Verifying port 9283 ...\n'
10.10.2.1: b'Waiting for mgr to start...\nWaiting for mgr...\n'
10.10.2.1: b'mgr not available, waiting (1/15)...\n'
10.10.2.1: b'mgr not available, waiting (2/15)...\n'
10.10.2.1: b'mgr is available\n'
10.10.2.1: b'Enabling cephadm module...\n'
10.10.2.1: b'Waiting for the mgr to restart...\n'
10.10.2.1: b'Waiting for mgr epoch 5...\n'
10.10.2.1: b'mgr epoch 5 is available\n'
10.10.2.1: b'Setting orchestrator backend to cephadm...\n'
10.10.2.1: b'Generating ssh key...\n'
10.10.2.1: b'Wrote public SSH key to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub\n'
10.10.2.1: b'Adding key to ljishen@localhost authorized_keys...\n'
10.10.2.1: b'Adding host node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us...\n'
10.10.2.1: b'Deploying unmanaged mon service...\n'
10.10.2.1: b'Deploying unmanaged mgr service...\n'
10.10.2.1: b'You can access the Ceph CLI with:\n\n\tsudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --fsid 6472f08e-3847-11ec-b51d-53e6e728d2d3 -c /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf -k /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring\n\n'
10.10.2.1: b'Please consider enabling telemetry to help improve Ceph:\n\n\tceph telemetry on\n\nFor more information see:\n\n\thttps://docs.ceph.com/docs/pacific/mgr/telemetry/\n\n'
10.10.2.1: b'Bootstrap complete.\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T16:34:03,919119703-07:00] INFO: Sleep for 20 seconds\x1b[0m\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T16:34:23,927073403-07:00] INFO: > Set the default number of monitor daemons to 1\x1b[0m\n# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:337 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T16:34:23,938108715-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T16:34:23,942220874-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mon', '1']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1\n'
10.10.2.1: b'Inferring fsid 6472f08e-3847-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/6472f08e-3847-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b'Scheduled mon update...\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T16:34:33,136553016-07:00] INFO: > Set the default number of manager daemons to 1\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:342 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T16:34:33,146547071-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T16:34:33,150592124-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mgr', '1']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1\n'
10.10.2.1: b'Inferring fsid 6472f08e-3847-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/6472f08e-3847-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b'Scheduled mgr update...\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T16:34:42,587077069-07:00] STAGE: Add hosts to the cluster...\x1b[0m\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T16:34:42,592709285-07:00] INFO: > Adding host 'sm1'\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:361 - add_hosts() > ssh-copy-id -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -f -i /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub ljishen@sm1\n'
10.10.2.1: b'/usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: "/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub"\n'
10.10.2.1: b'\nNumber of key(s) added: 1\n\nNow try logging into the machine, with:   "ssh -o \'GlobalKnownHostsFile=/dev/null\' -o \'LogLevel=ERROR\' -o \'PasswordAuthentication=no\' -o \'StrictHostKeyChecking=no\' -o \'UserKnownHostsFile=/dev/null\' \'ljishen@sm1\'"\nand check to make sure that only the key(s) you wanted were added.\n\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:367 - add_hosts() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add sm1 10.10.2.5\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T16:34:43,736900741-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T16:34:43,740902432-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'host', 'add', 'sm1', '10.10.2.5']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add sm1 10.10.2.5\n'
10.10.2.1: b'Inferring fsid 6472f08e-3847-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/6472f08e-3847-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b"Added host 'sm1' with addr '10.10.2.5'\n"
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T16:34:54,035225038-07:00] INFO: Sleep for 20 seconds\x1b[0m\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T16:35:14,040224113-07:00] STAGE: Add OSDs to the cluster...\x1b[0m\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T16:35:14,046943612-07:00] INFO: > Adding devices ['/dev/nvme0n1'] on host 'sm1' as OSDs\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:388 - add_host_devices_as_osds() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd sm1:/dev/nvme0n1\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T16:35:14,056672869-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T16:35:14,060481899-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'daemon', 'add', 'osd', 'sm1:/dev/nvme0n1']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd sm1:/dev/nvme0n1\n'
10.10.2.1: b'Inferring fsid 6472f08e-3847-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/6472f08e-3847-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b"Created osd(s) 0 on host 'sm1'\n"
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T16:35:38,811833226-07:00] INFO: > Sleep for 20 seconds\x1b[0m\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T16:35:58,817249589-07:00] STAGE: Show Cluster Status\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:408 - show_status() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T16:35:58,827359431-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T16:35:58,830971701-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', '--status']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status\n'
10.10.2.1: b'Inferring fsid 6472f08e-3847-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/6472f08e-3847-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b'  cluster:\n    id:     6472f08e-3847-11ec-b51d-53e6e728d2d3\n    health: HEALTH_WARN\n            1 pool(s) have no replicas configured\n \n  services:\n    mon: 1 daemons, quorum node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us (age 2m)\n    mgr: node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us.ackhmr(active, since 2m)\n    osd: 1 osds: 1 up (since 20s), 1 in (since 40s)\n \n  data:\n    pools:   1 pools, 1 pgs\n    objects: 0 objects, 0 B\n    usage:   4.7 MiB used, 400 GiB / 400 GiB avail\n    pgs:     1 active+clean\n \n'
[1] 16:36:07 [SUCCESS] ljishen@10.10.2.1

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:32:47,266163739-07:00] INFO: > Remove existing clusters[0m
## bash:17 -  > basename -- /var/lib/ceph/1e5aa048-3846-11ec-b51d-53e6e728d2d3
# bash:17 -  > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm --host 10.10.2.1 --host 10.10.2.5 rm-cluster --force --fsid 1e5aa048-3846-11ec-b51d-53e6e728d2d3
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:32:47,277458159-07:00] DEBUG: HOST_PARAMS: ['--host', '10.10.2.1', '--host', '10.10.2.5'][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:32:47,281089946-07:00] DEBUG: CEPHADM_PARAMS: ['rm-cluster', '--force', '--fsid', '1e5aa048-3846-11ec-b51d-53e6e728d2d3'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:63 -  > /tmp/bench-rados/ceph-research/scripts/parallel-ssh --host 10.10.2.1 --host 10.10.2.5 'sudo python3 -u - rm-cluster --force --fsid 1e5aa048-3846-11ec-b51d-53e6e728d2d3'
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:32:47,289345494-07:00] DEBUG: command from stdin[0m
# /tmp/bench-rados/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host 10.10.2.1 --host 10.10.2.5 'sudo python3 -u - rm-cluster --force --fsid 1e5aa048-3846-11ec-b51d-53e6e728d2d3'
[1] 16:32:52 [SUCCESS] 10.10.2.1
[2] 16:32:56 [SUCCESS] 10.10.2.5

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:32:56,130692264-07:00] INFO: > Deploy a new cluster[0m
# bash:24 -  > /tmp/bench-rados/ceph-research/scripts/ceph-deploy -m 10.10.2.1 -o sm1/10.10.2.5:/dev/nvme0n1


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:32:56,142748496-07:00] STAGE: Verify passwordless SSH to hosts, passwordless sudo via ljishen, and the existence of devices on hosts...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:32:56,147593734-07:00] INFO: > Check the host for the monitor[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:181 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@10.10.2.1 sudo --non-interactive --validate

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:32:56,299337519-07:00] INFO: > Check hosts for OSDs[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:32:56,304403722-07:00] INFO: >> Check host 'sm1'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:200 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 sudo --non-interactive --validate

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:32:57,378904018-07:00] INFO: >>> Check device '/dev/nvme0n1'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:215 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 test -b /dev/nvme0n1
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:226 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 lsblk --nodeps --noheadings --output TYPE /dev/nvme0n1
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:227 - verify_parameters() > grep --word-regexp disk


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:32:59,523296326-07:00] STAGE: Reset devices on hosts...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:32:59,528376476-07:00] INFO: > Resetting device '/dev/nvme0n1' on host 'sm1'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:288 - reset_host_devices() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 sudo sh
  Removing ceph--82dec906--651e--40b7--88a3--f7ddaaf917ea-osd--block--24169d04--5bc7--46c4--a29c--d566c9a0de6b (252:0)
  Archiving volume group "ceph-82dec906-651e-40b7-88a3-f7ddaaf917ea" metadata (seqno 5).
  Releasing logical volume "osd-block-24169d04-5bc7-46c4-a29c-d566c9a0de6b"
  Creating volume group backup "/etc/lvm/backup/ceph-82dec906-651e-40b7-88a3-f7ddaaf917ea" (seqno 6).
  Logical volume "osd-block-24169d04-5bc7-46c4-a29c-d566c9a0de6b" successfully removed
  Removing physical volume "/dev/nvme0n1" from volume group "ceph-82dec906-651e-40b7-88a3-f7ddaaf917ea"
  Volume group "ceph-82dec906-651e-40b7-88a3-f7ddaaf917ea" successfully removed


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:33:01,844484612-07:00] STAGE: Bootstrap a new cluster...[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:317 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.1 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:33:01,854316213-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:33:01,858012821-07:00] DEBUG: CEPHADM_PARAMS: ['bootstrap', '--config', '/tmp/bench-rados/ceph-research/scripts/ceph.conf', '--mon-ip', '10.10.2.1', '--output-dir', '/tmp/bench-rados/deployment_data_root/etc/ceph', '--ssh-user', 'ljishen', '--skip-dashboard', '--no-minimize-config', '--allow-overwrite', '--allow-fqdn-hostname', '--orphan-initial-daemons', '--skip-monitoring-stack', '--single-host-defaults'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.1 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults
Verifying podman|docker is present...
Verifying lvm2 is present...
Verifying time synchronization is in place...
Unit ntp.service is enabled and running
Repeating the final host check...
podman|docker (/usr/bin/docker) is present
systemctl is present
lvcreate is present
Unit ntp.service is enabled and running
Host looks OK
Cluster fsid: 6472f08e-3847-11ec-b51d-53e6e728d2d3
Verifying IP 10.10.2.1 port 3300 ...
Verifying IP 10.10.2.1 port 6789 ...
Mon IP `10.10.2.1` is in CIDR network `10.10.2.0/24`
- internal network (--cluster-network) has not been provided, OSD replication will default to the public_network
Adjusting default settings to suit single-host cluster...
Pulling container image quay.io/ceph/ceph:v16...
Ceph version: ceph version 16.2.6 (ee28fb57e47e9f88813e24bbf4c14496ca299d31) pacific (stable)
Extracting ceph user uid/gid from container image...
Creating initial keys...
Creating initial monmap...
Creating mon...
Waiting for mon to start...
Waiting for mon...
mon is available
Setting mon public_network to 10.10.2.0/24
Wrote config to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf
Wrote keyring to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring
Creating mgr...
Verifying port 9283 ...
Waiting for mgr to start...
Waiting for mgr...
mgr not available, waiting (1/15)...
mgr not available, waiting (2/15)...
mgr is available
Enabling cephadm module...
Waiting for the mgr to restart...
Waiting for mgr epoch 5...
mgr epoch 5 is available
Setting orchestrator backend to cephadm...
Generating ssh key...
Wrote public SSH key to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub
Adding key to ljishen@localhost authorized_keys...
Adding host node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us...
Deploying unmanaged mon service...
Deploying unmanaged mgr service...
You can access the Ceph CLI with:

	sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --fsid 6472f08e-3847-11ec-b51d-53e6e728d2d3 -c /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf -k /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring

Please consider enabling telemetry to help improve Ceph:

	ceph telemetry on

For more information see:

	https://docs.ceph.com/docs/pacific/mgr/telemetry/

Bootstrap complete.

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:34:03,919119703-07:00] INFO: Sleep for 20 seconds[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:34:23,927073403-07:00] INFO: > Set the default number of monitor daemons to 1[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:337 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:34:23,938108715-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:34:23,942220874-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mon', '1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1
Inferring fsid 6472f08e-3847-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/6472f08e-3847-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Scheduled mon update...

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:34:33,136553016-07:00] INFO: > Set the default number of manager daemons to 1[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:342 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:34:33,146547071-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:34:33,150592124-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mgr', '1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1
Inferring fsid 6472f08e-3847-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/6472f08e-3847-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Scheduled mgr update...


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:34:42,587077069-07:00] STAGE: Add hosts to the cluster...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:34:42,592709285-07:00] INFO: > Adding host 'sm1'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:361 - add_hosts() > ssh-copy-id -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -f -i /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub ljishen@sm1
/usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: "/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub"

Number of key(s) added: 1

Now try logging into the machine, with:   "ssh -o 'GlobalKnownHostsFile=/dev/null' -o 'LogLevel=ERROR' -o 'PasswordAuthentication=no' -o 'StrictHostKeyChecking=no' -o 'UserKnownHostsFile=/dev/null' 'ljishen@sm1'"
and check to make sure that only the key(s) you wanted were added.

# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:367 - add_hosts() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add sm1 10.10.2.5
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:34:43,736900741-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:34:43,740902432-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'host', 'add', 'sm1', '10.10.2.5'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add sm1 10.10.2.5
Inferring fsid 6472f08e-3847-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/6472f08e-3847-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Added host 'sm1' with addr '10.10.2.5'

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:34:54,035225038-07:00] INFO: Sleep for 20 seconds[0m


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:35:14,040224113-07:00] STAGE: Add OSDs to the cluster...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:35:14,046943612-07:00] INFO: > Adding devices ['/dev/nvme0n1'] on host 'sm1' as OSDs[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:388 - add_host_devices_as_osds() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd sm1:/dev/nvme0n1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:35:14,056672869-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:35:14,060481899-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'daemon', 'add', 'osd', 'sm1:/dev/nvme0n1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd sm1:/dev/nvme0n1
Inferring fsid 6472f08e-3847-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/6472f08e-3847-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Created osd(s) 0 on host 'sm1'

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:35:38,811833226-07:00] INFO: > Sleep for 20 seconds[0m


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:35:58,817249589-07:00] STAGE: Show Cluster Status[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:408 - show_status() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:35:58,827359431-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:35:58,830971701-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', '--status'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status
Inferring fsid 6472f08e-3847-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/6472f08e-3847-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
  cluster:
    id:     6472f08e-3847-11ec-b51d-53e6e728d2d3
    health: HEALTH_WARN
            1 pool(s) have no replicas configured
 
  services:
    mon: 1 daemons, quorum node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us (age 2m)
    mgr: node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us.ackhmr(active, since 2m)
    osd: 1 osds: 1 up (since 20s), 1 in (since 40s)
 
  data:
    pools:   1 pools, 1 pgs
    objects: 0 objects, 0 B
    usage:   4.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     1 active+clean
 

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:36:07,550250501-07:00] INFO: > Copy configuration files to local[0m
# ./benchmarks/bench-rados:220 - launch_ceph_cluster() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.1 sudo chown --changes --recursive ljishen /tmp/bench-rados/deployment_data_root/etc/ceph
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:36:07,558043195-07:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.1 sudo chown --changes --recursive ljishen /tmp/bench-rados/deployment_data_root/etc/ceph
[1] 16:36:07 [SUCCESS] ljishen@10.10.2.1
# ./benchmarks/bench-rados:223 - launch_ceph_cluster() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.1:/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf
# ./benchmarks/bench-rados:226 - launch_ceph_cluster() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.1:/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:36:08,037031289-07:00] STAGE: Update local ceph.conf as a client...[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:36:08,040520213-07:00] STAGE: Configure Ceph pool...[0m
# ./benchmarks/bench-rados:257 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_max_backfills 32
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:36:08,062473338-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:36:08,065394532-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '6472f08e-3847-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'config', 'set', 'osd', 'osd_max_backfills', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 6472f08e-3847-11ec-b51d-53e6e728d2d3 -- ceph config set osd osd_max_backfills 32
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:258 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_active 32
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:36:12,129716416-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:36:12,132674720-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '6472f08e-3847-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_active', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 6472f08e-3847-11ec-b51d-53e6e728d2d3 -- ceph config set osd osd_recovery_max_active 32
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:259 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_single_start 8
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:36:16,155484789-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:36:16,158513706-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '6472f08e-3847-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_single_start', '8'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 6472f08e-3847-11ec-b51d-53e6e728d2d3 -- ceph config set osd osd_recovery_max_single_start 8
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:260 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_op_priority 63
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:36:20,164159685-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:36:20,167251621-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '6472f08e-3847-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_op_priority', '63'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 6472f08e-3847-11ec-b51d-53e6e728d2d3 -- ceph config set osd osd_recovery_op_priority 63
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:262 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config show-with-defaults osd.0
# ./benchmarks/bench-rados:263 - configure_ceph_pool() > grep 'osd_max_backfills\|osd_recovery'
# ./benchmarks/bench-rados:263 - configure_ceph_pool() > column -t -s ' '
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
osd_max_backfills                        32         mon
osd_recovery_cost                        20971520   default
osd_recovery_delay_start                 0.000000   default
osd_recovery_max_active                  32         mon
osd_recovery_max_active_hdd              3          default
osd_recovery_max_active_ssd              10         default
osd_recovery_max_chunk                   8388608    default
osd_recovery_max_omap_entries_per_chunk  8096       default
osd_recovery_max_single_start            8          mon
osd_recovery_op_priority                 3          default
osd_recovery_op_warn_multiple            16         default
osd_recovery_priority                    5          default
osd_recovery_retry_interval              30.000000  default
osd_recovery_sleep                       0.000000   default
osd_recovery_sleep_hdd                   0.100000   default
osd_recovery_sleep_hybrid                0.025000   default
osd_recovery_sleep_ssd                   0.000000   default
# ./benchmarks/bench-rados:265 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool create bench_rados 128 128 replicated --size 1 --pg-num-min 128
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:36:28,263926311-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:36:28,266888012-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '6472f08e-3847-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'create', 'bench_rados', '128', '128', 'replicated', '--size', '1', '--pg-num-min', '128'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 6472f08e-3847-11ec-b51d-53e6e728d2d3 -- ceph osd pool create bench_rados 128 128 replicated --size 1 --pg-num-min 128
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
pool 'bench_rados' created
# ./benchmarks/bench-rados:267 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados min_size 1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:36:32,577327861-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:36:32,580414637-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '6472f08e-3847-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'min_size', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 6472f08e-3847-11ec-b51d-53e6e728d2d3 -- ceph osd pool set bench_rados min_size 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 min_size to 1
# ./benchmarks/bench-rados:268 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados pg_autoscale_mode off
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:36:37,036689091-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:36:37,039880765-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '6472f08e-3847-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'pg_autoscale_mode', 'off'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 6472f08e-3847-11ec-b51d-53e6e728d2d3 -- ceph osd pool set bench_rados pg_autoscale_mode off
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 pg_autoscale_mode to off
# ./benchmarks/bench-rados:269 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool application enable bench_rados benchmark
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:36:42,059860872-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:36:42,062765685-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '6472f08e-3847-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'application', 'enable', 'bench_rados', 'benchmark'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 6472f08e-3847-11ec-b51d-53e6e728d2d3 -- ceph osd pool application enable bench_rados benchmark
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
enabled application 'benchmark' on pool 'bench_rados'
# ./benchmarks/bench-rados:270 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados noscrub 1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:36:46,401555767-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:36:46,404695953-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '6472f08e-3847-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'noscrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 6472f08e-3847-11ec-b51d-53e6e728d2d3 -- ceph osd pool set bench_rados noscrub 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 noscrub to 1
# ./benchmarks/bench-rados:271 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados nodeep-scrub 1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:36:51,056421734-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:36:51,059554777-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '6472f08e-3847-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'nodeep-scrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 6472f08e-3847-11ec-b51d-53e6e728d2d3 -- ceph osd pool set bench_rados nodeep-scrub 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 nodeep-scrub to 1
# ./benchmarks/bench-rados:272 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool ls detail
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:36:55,393444578-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:36:55,396488303-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '6472f08e-3847-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'ls', 'detail'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 6472f08e-3847-11ec-b51d-53e6e728d2d3 -- ceph osd pool ls detail
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
pool 1 'device_health_metrics' replicated size 1 min_size 1 crush_rule 0 object_hash rjenkins pg_num 64 pgp_num 64 autoscale_mode on last_change 18 lfor 0/0/16 flags hashpspool stripe_width 0 pg_num_min 1 application mgr_devicehealth
pool 2 'bench_rados' replicated size 1 min_size 1 crush_rule 0 object_hash rjenkins pg_num 128 pgp_num 128 autoscale_mode off last_change 20 flags hashpspool,noscrub,nodeep-scrub stripe_width 0 pg_num_min 128 application benchmark

# ./benchmarks/bench-rados:273 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd df tree
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:36:59,372280342-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:36:59,375221914-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '6472f08e-3847-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'df', 'tree'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 6472f08e-3847-11ec-b51d-53e6e728d2d3 -- ceph osd df tree
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
ID  CLASS  WEIGHT   REWEIGHT  SIZE     RAW USE  DATA     OMAP  META     AVAIL    %USE  VAR   PGS  STATUS  TYPE NAME    
-1         0.39059         -  400 GiB  5.7 MiB  168 KiB   0 B  5.6 MiB  400 GiB  0.00  1.00    -          root default 
-3         0.39059         -  400 GiB  5.7 MiB  168 KiB   0 B  5.6 MiB  400 GiB  0.00  1.00    -              host sm1 
 0    ssd  0.39059   1.00000  400 GiB  5.7 MiB  168 KiB   0 B  5.6 MiB  400 GiB  0.00  1.00  192      up          osd.0
                       TOTAL  400 GiB  5.7 MiB  168 KiB   0 B  5.6 MiB  400 GiB  0.00                                  
MIN/MAX VAR: 1.00/1.00  STDDEV: 0


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:37:03,316227067-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:37:27,312843381-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:37:36,471066287-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:37:45,393790688-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:37:45,401249081-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:37:54,432888448-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:37:54,440403668-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:38:03,549718238-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:38:03,557206547-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:38:12,548303174-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:38:12,556208338-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:38:21,663627237-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:38:21,671549964-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:38:21,677632726-07:00] INFO: > The cluster is idle now.[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:38:21,681505794-07:00] STAGE: Start benchmarking of mode write ...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:38:21,688541681-07:00] INFO: > Start system activity collection services[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:38:21,694204451-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:341 - rados_bench() > rm --force /mnt/sda8/ceph-research/scripts/output/sys_activity_1MB_write.dat.rados_bench_host.1
# ./benchmarks/bench-rados:342 - rados_bench() > sar_pid_rados_bench_host=885877
# ./benchmarks/bench-rados:342 - rados_bench() > S_TIME_FORMAT=ISO
# ./benchmarks/bench-rados:342 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/output/sys_activity_1MB_write.dat.rados_bench_host.1 2

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:38:21,701529663-07:00] INFO: >> for OSD host[0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:38:21,710248842-07:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.5 bash
10.10.2.5: b'316054\n'
[1] 16:38:22 [SUCCESS] ljishen@10.10.2.5
316054

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:38:22,840629128-07:00] INFO: > Run rados bench[0m
# ./benchmarks/bench-rados:370 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/output/rados_bench_1MB_write.log.1
## ./benchmarks/bench-rados:361 - rados_bench() > count_cpus 32-63,96-127
# ./benchmarks/bench-rados:361 - rados_bench() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 60 write --pool bench_rados -b 1048576 -O 1048576 --concurrent-ios 128 --show-time --write-object --write-omap --write-xattr --no-cleanup
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:38:22,860823239-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:38:22,863694298-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '6472f08e-3847-11ec-b51d-53e6e728d2d3', '--', 'rados', 'bench', '60', 'write', '--pool', 'bench_rados', '-b', '1048576', '-O', '1048576', '--concurrent-ios', '128', '--show-time', '--write-object', '--write-omap', '--write-xattr', '--no-cleanup'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 6472f08e-3847-11ec-b51d-53e6e728d2d3 -- rados bench 60 write --pool bench_rados -b 1048576 -O 1048576 --concurrent-ios 128 --show-time --write-object --write-omap --write-xattr --no-cleanup
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
hints = 1
2021-10-28T23:38:25.878393+0000 Maintaining 128 concurrent writes of 1048576 bytes to objects of size 1048576 for up to 60 seconds or 0 objects
2021-10-28T23:38:25.878405+0000 Object prefix: benchmark_data_node-2.bluefield2.ucsc-cmps10_7
2021-10-28T23:38:25.910709+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-28T23:38:25.910709+0000     0       0         0         0         0         0           -           0
2021-10-28T23:38:26.910832+0000     1      54        54         0         0         0           -           0
2021-10-28T23:38:27.910951+0000     2      87        87         0         0         0           -           0
2021-10-28T23:38:28.911035+0000     3     120       120         0         0         0           -           0
2021-10-28T23:38:29.911107+0000     4     127       158        31   7.74934      7.75     3.95328     3.64668
2021-10-28T23:38:30.911190+0000     5     127       179        52   10.3991        21     3.73309     3.72941
2021-10-28T23:38:31.911277+0000     6     127       212        85   14.1655        33     3.88319     3.81813
2021-10-28T23:38:32.911365+0000     7     127       243       116     16.57        31     3.93468     3.87196
2021-10-28T23:38:33.911472+0000     8     127       276       149   18.6234        33     3.99561     3.90904
2021-10-28T23:38:34.911542+0000     9     127       309       182   20.2205        33     3.39017      3.9117
2021-10-28T23:38:35.911659+0000    10     127       355       228    22.798        46     3.33129     3.88542
2021-10-28T23:38:36.911749+0000    11     127       375       248   22.5434        20     3.53077     3.88307
2021-10-28T23:38:37.911863+0000    12     127       408       281   23.4145        33     3.62895     3.89537
2021-10-28T23:38:38.911958+0000    13     127       439       312   23.9978        31     3.64397     3.91683
2021-10-28T23:38:39.912032+0000    14     127       476       349   24.9263        37     3.93139     3.93712
2021-10-28T23:38:40.912118+0000    15     127       505       378   25.1977        29      3.7731     3.94214
2021-10-28T23:38:41.912228+0000    16     127       529       402   25.1227        24      3.8582     3.94919
2021-10-28T23:38:42.912312+0000    17     127       542       415   24.4095        13     3.89171     3.95461
2021-10-28T23:38:43.912425+0000    18     127       562       435   24.1644        20     4.55695     3.98655
2021-10-28T23:38:44.912493+0000    19     127       575       448   23.5768        13     4.79226     4.02625
2021-10-28T23:38:45.912599+0000 min lat: 3.10598 max lat: 5.60824 avg lat: 4.02625
2021-10-28T23:38:45.912599+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-28T23:38:45.912599+0000    20     127       575       448   22.3979         0           -     4.02625
2021-10-28T23:38:46.912689+0000    21     127       595       468   22.2837        10     6.52644     4.13351
2021-10-28T23:38:47.912802+0000    22     127       608       481   21.8616        13     6.90908     4.22004
2021-10-28T23:38:48.912884+0000    23     127       608       481   20.9111         0           -     4.22004
2021-10-28T23:38:49.912952+0000    24     127       628       501   20.8731        10     8.06707     4.39472
2021-10-28T23:38:50.913038+0000    25     127       641       514   20.5581        13     8.56636     4.50768
2021-10-28T23:38:51.913159+0000    26     127       661       534   20.5366        20     9.69677     4.71577
2021-10-28T23:38:52.913244+0000    27     127       674       547   20.2574        13     9.56531     4.85099
2021-10-28T23:38:53.913356+0000    28     127       674       547   19.5339         0           -     4.85099
2021-10-28T23:38:54.913424+0000    29     127       694       567   19.5499        10     10.1442     5.06742
2021-10-28T23:38:55.913539+0000    30     127       707       580   19.3315        13     9.52187     5.19539
2021-10-28T23:38:56.913621+0000    31     127       707       580    18.708         0           -     5.19539
2021-10-28T23:38:57.913725+0000    32     127       727       600   18.7483        10     9.83227     5.38144
2021-10-28T23:38:58.913808+0000    33     127       740       613    18.574        13     9.41509      5.4922
2021-10-28T23:38:59.913880+0000    34     127       760       633   18.6159        20     9.56113     5.64845
2021-10-28T23:39:00.913964+0000    35     127       793       666   19.0268        33     8.03994     5.82063
2021-10-28T23:39:01.914071+0000    36     127       826       699   19.4149        33     6.29823     5.90062
2021-10-28T23:39:02.914158+0000    37     127       859       732    19.782        33      4.5393     5.89772
2021-10-28T23:39:03.914274+0000    38     127       887       760   19.9982        28     4.14697     5.83898
2021-10-28T23:39:04.914348+0000    39     127       919       792   20.3058        32     3.93778     5.76443
2021-10-28T23:39:05.914455+0000 min lat: 3.10598 max lat: 11.3352 avg lat: 5.68912
2021-10-28T23:39:05.914455+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-28T23:39:05.914455+0000    40     127       952       825   20.6231        33     3.83899     5.68912
2021-10-28T23:39:06.914552+0000    41     127       985       858   20.9249        33     3.86081     5.61841
2021-10-28T23:39:07.914666+0000    42     127      1018       891   21.2123        33      3.6095     5.55713
2021-10-28T23:39:08.914747+0000    43     127      1051       924   21.4864        33     3.71732     5.50212
2021-10-28T23:39:09.914818+0000    44     127      1051       924   20.9981         0           -     5.50212
2021-10-28T23:39:10.914901+0000    45     127      1070       943   20.9536       9.5     4.35985     5.48718
2021-10-28T23:39:11.915022+0000    46     127      1084       957   20.8024        14     4.92005     5.48396
2021-10-28T23:39:12.915107+0000    47     127      1084       957   20.3598         0           -     5.48396
2021-10-28T23:39:13.915212+0000    48     127      1103       976   20.3315       9.5     6.99306     5.52205
2021-10-28T23:39:14.915283+0000    49     127      1103       976   19.9165         0           -     5.52205
2021-10-28T23:39:15.915399+0000    50     127      1117       990   19.7982         7     8.62392     5.57111
2021-10-28T23:39:16.915478+0000    51     127      1117       990     19.41         0           -     5.57111
2021-10-28T23:39:17.915583+0000    52     127      1117       990   19.0367         0           -     5.57111
2021-10-28T23:39:18.915665+0000    53     127      1117       990   18.6775         0           -     5.57111
2021-10-28T23:39:19.915735+0000    54     127      1136      1009   18.6835      4.75     11.6662     5.69356
2021-10-28T23:39:20.915816+0000    55     127      1136      1009   18.3438         0           -     5.69356
2021-10-28T23:39:21.915922+0000    56     127      1136      1009   18.0162         0           -     5.69356
2021-10-28T23:39:22.916002+0000    57     127      1150      1023   17.9457   4.66667     13.8255     5.80958
2021-10-28T23:39:23.916120+0000    58     127      1150      1023   17.6363         0           -     5.80958
2021-10-28T23:39:24.916190+0000    59     127      1169      1042   17.6594       9.5     16.0306     6.00417
2021-10-28T23:39:25.916298+0000 min lat: 3.10598 max lat: 16.6025 avg lat: 6.00417
2021-10-28T23:39:25.916298+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-28T23:39:25.916298+0000    60     127      1169      1042   17.3651         0           -     6.00417
2021-10-28T23:39:26.916393+0000    61     127      1169      1042   17.0804         0           -     6.00417
2021-10-28T23:39:27.916512+0000    62      20      1170      1150   18.5467        36     4.92184     6.60829
2021-10-28T23:39:28.916592+0000    63      20      1170      1150   18.2523         0           -     6.60829
2021-10-28T23:39:29.916661+0000    64      20      1170      1150   17.9671         0           -     6.60829
2021-10-28T23:39:30.916766+0000 Total time run:         64.601
Total writes made:      1170
Write size:             1048576
Object size:            1048576
Bandwidth (MB/sec):     18.1112
Stddev Bandwidth:       13.7326
Max bandwidth (MB/sec): 46
Min bandwidth (MB/sec): 0
Average IOPS:           18
Stddev IOPS:            13.7662
Max IOPS:               46
Min IOPS:               0
Average Latency(s):     6.59065
Stddev Latency(s):      3.69567
Max latency(s):         18.1578
Min latency(s):         3.10598

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:39:31,757974512-07:00] INFO: > Stop system activity collection services[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:39:31,763920597-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:391 - rados_bench() > kill -INT 885877

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:39:31,769711147-07:00] INFO: >> for OSD host[0m
# ./benchmarks/bench-rados:396 - rados_bench() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.5 kill -INT 316054
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:39:31,777636891-07:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.5 kill -INT 316054
[1] 16:39:32 [SUCCESS] ljishen@10.10.2.5

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:39:32,922758018-07:00] INFO: >>> Copy system activity file from OSD host[0m
## ./benchmarks/bench-rados:403 - rados_bench() > basename -- /tmp/bench-rados/sys_activity_1MB_write.dat.osd_host.1
# ./benchmarks/bench-rados:403 - rados_bench() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.5:/tmp/bench-rados/sys_activity_1MB_write.dat.osd_host.1 /mnt/sda8/ceph-research/scripts/output/sys_activity_1MB_write.dat.osd_host.1


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:39:34,110555316-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:39:58,111611750-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 1.17k objects, 1.1 GiB
    usage:   3.7 GiB used, 396 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:39:58,119199467-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:40:07,309205733-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 1.17k objects, 1.1 GiB
    usage:   3.4 GiB used, 397 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:40:07,317147998-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:40:16,341023391-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 1.17k objects, 1.1 GiB
    usage:   3.4 GiB used, 397 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:40:16,348791819-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:40:25,389249342-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 1.17k objects, 1.1 GiB
    usage:   3.4 GiB used, 397 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:40:25,397445875-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:40:34,529262846-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 1.17k objects, 1.1 GiB
    usage:   3.4 GiB used, 397 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:40:34,537263591-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:40:34,543312769-07:00] INFO: > The cluster is idle now.[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:40:34,547119021-07:00] STAGE: Start benchmarking of mode seq ...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:40:34,553884198-07:00] INFO: > Start system activity collection services[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:40:34,559637950-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:341 - rados_bench() > rm --force /mnt/sda8/ceph-research/scripts/output/sys_activity_1MB_seq.dat.rados_bench_host.1
# ./benchmarks/bench-rados:342 - rados_bench() > sar_pid_rados_bench_host=887858
# ./benchmarks/bench-rados:342 - rados_bench() > S_TIME_FORMAT=ISO
# ./benchmarks/bench-rados:342 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/output/sys_activity_1MB_seq.dat.rados_bench_host.1 2

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:40:34,566748959-07:00] INFO: >> for OSD host[0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:40:34,575761941-07:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.5 bash
10.10.2.5: b'316603\n'
[1] 16:40:35 [SUCCESS] ljishen@10.10.2.5
316603

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:40:35,704749573-07:00] INFO: > Run rados bench[0m
# ./benchmarks/bench-rados:379 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/output/rados_bench_1MB_seq.log.1
## ./benchmarks/bench-rados:376 - rados_bench() > count_cpus 32-63,96-127
# ./benchmarks/bench-rados:376 - rados_bench() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 128 --show-time
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:40:35,725076755-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:40:35,727976518-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '6472f08e-3847-11ec-b51d-53e6e728d2d3', '--', 'rados', 'bench', '99999999', 'seq', '--pool', 'bench_rados', '--concurrent-ios', '128', '--show-time'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 6472f08e-3847-11ec-b51d-53e6e728d2d3 -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 128 --show-time
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
hints = 1
2021-10-28T23:40:38.734878+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-28T23:40:38.734878+0000     0       0         0         0         0         0           -           0
2021-10-28T23:40:39.735029+0000     1     127       225        98    97.977        98    0.998919    0.549246
2021-10-28T23:40:40.735147+0000     2     127       353       226    112.98       128    0.461691    0.798595
2021-10-28T23:40:41.735243+0000     3     127       463       336   111.983       110     1.49534    0.856032
2021-10-28T23:40:42.735364+0000     4     127       595       468   116.983       132    0.849123    0.908923
2021-10-28T23:40:43.735435+0000     5     127       722       595   118.985       127     2.15729    0.908825
2021-10-28T23:40:44.735550+0000     6     127       843       716   119.318       121    0.374578    0.919452
2021-10-28T23:40:45.735655+0000     7     127       971       844   120.557       128     0.52589    0.923887
2021-10-28T23:40:46.735772+0000     8     127      1085       958   119.735       114     3.24541    0.952688
2021-10-28T23:40:47.735889+0000     9      89      1170      1081   120.096       123    0.405684    0.965757
2021-10-28T23:40:48.736022+0000 Total time run:       9.77328
Total reads made:     1170
Read size:            1048576
Object size:          1048576
Bandwidth (MB/sec):   119.714
Average IOPS:         119
Stddev IOPS:          10.9023
Max IOPS:             132
Min IOPS:             98
Average Latency(s):   1.00642
Max latency(s):       3.49106
Min latency(s):       0.152273

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:40:49,431639648-07:00] INFO: > Stop system activity collection services[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:40:49,437979634-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:391 - rados_bench() > kill -INT 887858

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:40:49,443855997-07:00] INFO: >> for OSD host[0m
# ./benchmarks/bench-rados:396 - rados_bench() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.5 kill -INT 316603
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:40:49,452011974-07:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.5 kill -INT 316603
[1] 16:40:50 [SUCCESS] ljishen@10.10.2.5

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:40:50,555995965-07:00] INFO: >>> Copy system activity file from OSD host[0m
## ./benchmarks/bench-rados:403 - rados_bench() > basename -- /tmp/bench-rados/sys_activity_1MB_seq.dat.osd_host.1
# ./benchmarks/bench-rados:403 - rados_bench() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.5:/tmp/bench-rados/sys_activity_1MB_seq.dat.osd_host.1 /mnt/sda8/ceph-research/scripts/output/sys_activity_1MB_seq.dat.osd_host.1


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:40:51,673050855-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:41:15,645422853-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 1.17k objects, 1.1 GiB
    usage:   3.4 GiB used, 397 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:41:15,653194826-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:41:24,699468203-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 1.17k objects, 1.1 GiB
    usage:   3.4 GiB used, 397 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:41:24,707091106-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:41:33,814559824-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 1.17k objects, 1.1 GiB
    usage:   3.4 GiB used, 397 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:41:33,822321168-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:41:42,847248284-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 1.17k objects, 1.1 GiB
    usage:   3.4 GiB used, 397 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:41:42,854987727-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:41:52,093361121-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 1.17k objects, 1.1 GiB
    usage:   3.4 GiB used, 397 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:41:52,101116484-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:41:52,107422165-07:00] INFO: > The cluster is idle now.[0m


[1;7;39;49m[2021-10-28T16:41:52,109833919-07:00][RUNNING][ROUND 2/5/21] object_size=1MB[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:41:52,113434333-07:00] STAGE: Launch a Ceph cluster (CEPH_HOST: 10.10.2.1, OSD_HOST: 10.10.2.5) ...[0m
# ./benchmarks/bench-rados:183 - launch_ceph_cluster() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.1 bash -euo pipefail
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:41:52,122810621-07:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.1 bash -euo pipefail
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T16:41:52,514252838-07:00] INFO: > Remove existing clusters\x1b[0m\n'
10.10.2.1: b'## bash:17 -  > basename -- /var/lib/ceph/6472f08e-3847-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'# bash:17 -  > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm --host 10.10.2.1 --host 10.10.2.5 rm-cluster --force --fsid 6472f08e-3847-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T16:41:52,524784894-07:00] DEBUG: HOST_PARAMS: ['--host', '10.10.2.1', '--host', '10.10.2.5']\x1b[0m\n"
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T16:41:52,528474870-07:00] DEBUG: CEPHADM_PARAMS: ['rm-cluster', '--force', '--fsid', '6472f08e-3847-11ec-b51d-53e6e728d2d3']\x1b[0m\n"
10.10.2.1: b"# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:63 -  > /tmp/bench-rados/ceph-research/scripts/parallel-ssh --host 10.10.2.1 --host 10.10.2.5 'sudo python3 -u - rm-cluster --force --fsid 6472f08e-3847-11ec-b51d-53e6e728d2d3'\n"
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T16:41:52,535962584-07:00] DEBUG: command from stdin\x1b[0m\n'
10.10.2.1: b"# /tmp/bench-rados/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host 10.10.2.1 --host 10.10.2.5 'sudo python3 -u - rm-cluster --force --fsid 6472f08e-3847-11ec-b51d-53e6e728d2d3'\n"
10.10.2.1: b'[1] 16:41:58 [SUCCESS] 10.10.2.1\n[2] 16:42:01 [SUCCESS] 10.10.2.5\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T16:42:01,344057230-07:00] INFO: > Deploy a new cluster\x1b[0m\n'
10.10.2.1: b'# bash:24 -  > /tmp/bench-rados/ceph-research/scripts/ceph-deploy -m 10.10.2.1 -o sm1/10.10.2.5:/dev/nvme0n1\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T16:42:01,357740749-07:00] STAGE: Verify passwordless SSH to hosts, passwordless sudo via ljishen, and the existence of devices on hosts...\x1b[0m\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T16:42:01,362567482-07:00] INFO: > Check the host for the monitor\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:181 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@10.10.2.1 sudo --non-interactive --validate\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T16:42:01,511125362-07:00] INFO: > Check hosts for OSDs\x1b[0m\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T16:42:01,515570348-07:00] INFO: >> Check host 'sm1'\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:200 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 sudo --non-interactive --validate\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T16:42:02,607005761-07:00] INFO: >>> Check device '/dev/nvme0n1'\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:215 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 test -b /dev/nvme0n1\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:226 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 lsblk --nodeps --noheadings --output TYPE /dev/nvme0n1\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:227 - verify_parameters() > grep --word-regexp disk\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T16:42:04,751792395-07:00] STAGE: Reset devices on hosts...\x1b[0m\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T16:42:04,756232180-07:00] INFO: > Resetting device '/dev/nvme0n1' on host 'sm1'\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:288 - reset_host_devices() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 sudo sh\n'
10.10.2.1: b'  Removing ceph--d412c6c9--9009--4eed--97a4--9ba0edb19c39-osd--block--930c2cd6--b6db--4237--af9d--02c59d04d8fe (252:0)\n'
10.10.2.1: b'  Archiving volume group "ceph-d412c6c9-9009-4eed-97a4-9ba0edb19c39" metadata (seqno 5).\n'
10.10.2.1: b'  Releasing logical volume "osd-block-930c2cd6-b6db-4237-af9d-02c59d04d8fe"\n'
10.10.2.1: b'  Creating volume group backup "/etc/lvm/backup/ceph-d412c6c9-9009-4eed-97a4-9ba0edb19c39" (seqno 6).\n'
10.10.2.1: b'  Logical volume "osd-block-930c2cd6-b6db-4237-af9d-02c59d04d8fe" successfully removed\n'
10.10.2.1: b'  Removing physical volume "/dev/nvme0n1" from volume group "ceph-d412c6c9-9009-4eed-97a4-9ba0edb19c39"\n'
10.10.2.1: b'  Volume group "ceph-d412c6c9-9009-4eed-97a4-9ba0edb19c39" successfully removed\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T16:42:07,059648085-07:00] STAGE: Bootstrap a new cluster...\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:317 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.1 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T16:42:07,069901387-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T16:42:07,073930340-07:00] DEBUG: CEPHADM_PARAMS: ['bootstrap', '--config', '/tmp/bench-rados/ceph-research/scripts/ceph.conf', '--mon-ip', '10.10.2.1', '--output-dir', '/tmp/bench-rados/deployment_data_root/etc/ceph', '--ssh-user', 'ljishen', '--skip-dashboard', '--no-minimize-config', '--allow-overwrite', '--allow-fqdn-hostname', '--orphan-initial-daemons', '--skip-monitoring-stack', '--single-host-defaults']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.1 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults\n'
10.10.2.1: b'Verifying podman|docker is present...\n'
10.10.2.1: b'Verifying lvm2 is present...\nVerifying time synchronization is in place...\n'
10.10.2.1: b'Unit ntp.service is enabled and running\n'
10.10.2.1: b'Repeating the final host check...\npodman|docker (/usr/bin/docker) is present\n'
10.10.2.1: b'systemctl is present\n'
10.10.2.1: b'lvcreate is present\n'
10.10.2.1: b'Unit ntp.service is enabled and running\n'
10.10.2.1: b'Host looks OK\n'
10.10.2.1: b'Cluster fsid: a96c5bb6-3848-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Verifying IP 10.10.2.1 port 3300 ...\n'
10.10.2.1: b'Verifying IP 10.10.2.1 port 6789 ...\n'
10.10.2.1: b'Mon IP `10.10.2.1` is in CIDR network `10.10.2.0/24`\n'
10.10.2.1: b'- internal network (--cluster-network) has not been provided, OSD replication will default to the public_network\n'
10.10.2.1: b'Adjusting default settings to suit single-host cluster...\n'
10.10.2.1: b'Pulling container image quay.io/ceph/ceph:v16...\n'
10.10.2.1: b'Ceph version: ceph version 16.2.6 (ee28fb57e47e9f88813e24bbf4c14496ca299d31) pacific (stable)\n'
10.10.2.1: b'Extracting ceph user uid/gid from container image...\n'
10.10.2.1: b'Creating initial keys...\n'
10.10.2.1: b'Creating initial monmap...\n'
10.10.2.1: b'Creating mon...\n'
10.10.2.1: b'Waiting for mon to start...\n'
10.10.2.1: b'Waiting for mon...\n'
10.10.2.1: b'mon is available\nSetting mon public_network to 10.10.2.0/24\n'
10.10.2.1: b'Wrote config to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf\n'
10.10.2.1: b'Wrote keyring to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring\n'
10.10.2.1: b'Creating mgr...\nVerifying port 9283 ...\n'
10.10.2.1: b'Waiting for mgr to start...\nWaiting for mgr...\n'
10.10.2.1: b'mgr not available, waiting (1/15)...\n'
10.10.2.1: b'mgr not available, waiting (2/15)...\n'
10.10.2.1: b'mgr is available\n'
10.10.2.1: b'Enabling cephadm module...\n'
10.10.2.1: b'Waiting for the mgr to restart...\n'
10.10.2.1: b'Waiting for mgr epoch 5...\n'
10.10.2.1: b'mgr epoch 5 is available\nSetting orchestrator backend to cephadm...\n'
10.10.2.1: b'Generating ssh key...\n'
10.10.2.1: b'Wrote public SSH key to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub\n'
10.10.2.1: b'Adding key to ljishen@localhost authorized_keys...\n'
10.10.2.1: b'Adding host node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us...\n'
10.10.2.1: b'Deploying unmanaged mon service...\n'
10.10.2.1: b'Deploying unmanaged mgr service...\n'
10.10.2.1: b'You can access the Ceph CLI with:\n\n\tsudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --fsid a96c5bb6-3848-11ec-b51d-53e6e728d2d3 -c /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf -k /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring\n\nPlease consider enabling telemetry to help improve Ceph:\n\n\tceph telemetry on\n\nFor more information see:\n\n\thttps://docs.ceph.com/docs/pacific/mgr/telemetry/\n\n'
10.10.2.1: b'Bootstrap complete.\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T16:43:09,336827240-07:00] INFO: Sleep for 20 seconds\x1b[0m\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T16:43:29,344219829-07:00] INFO: > Set the default number of monitor daemons to 1\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:337 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T16:43:29,354111892-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T16:43:29,357989511-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mon', '1']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1\n'
10.10.2.1: b'Inferring fsid a96c5bb6-3848-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/a96c5bb6-3848-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b'Scheduled mon update...\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T16:43:38,600588226-07:00] INFO: > Set the default number of manager daemons to 1\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:342 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T16:43:38,610451525-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T16:43:38,613961732-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mgr', '1']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1\n'
10.10.2.1: b'Inferring fsid a96c5bb6-3848-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/a96c5bb6-3848-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b'Scheduled mgr update...\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T16:43:47,921925091-07:00] STAGE: Add hosts to the cluster...\x1b[0m\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T16:43:47,927765409-07:00] INFO: > Adding host 'sm1'\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:361 - add_hosts() > ssh-copy-id -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -f -i /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub ljishen@sm1\n'
10.10.2.1: b'/usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: "/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub"\n'
10.10.2.1: b'\nNumber of key(s) added: 1\n\nNow try logging into the machine, with:   "ssh -o \'GlobalKnownHostsFile=/dev/null\' -o \'LogLevel=ERROR\' -o \'PasswordAuthentication=no\' -o \'StrictHostKeyChecking=no\' -o \'UserKnownHostsFile=/dev/null\' \'ljishen@sm1\'"\nand check to make sure that only the key(s) you wanted were added.\n\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:367 - add_hosts() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add sm1 10.10.2.5\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T16:43:49,107374889-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T16:43:49,110632282-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'host', 'add', 'sm1', '10.10.2.5']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add sm1 10.10.2.5\n'
10.10.2.1: b'Inferring fsid a96c5bb6-3848-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/a96c5bb6-3848-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b"Added host 'sm1' with addr '10.10.2.5'\n"
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T16:43:59,981778445-07:00] INFO: Sleep for 20 seconds\x1b[0m\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T16:44:19,986658375-07:00] STAGE: Add OSDs to the cluster...\x1b[0m\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T16:44:19,993595314-07:00] INFO: > Adding devices ['/dev/nvme0n1'] on host 'sm1' as OSDs\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:388 - add_host_devices_as_osds() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd sm1:/dev/nvme0n1\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T16:44:20,004412246-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T16:44:20,008247715-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'daemon', 'add', 'osd', 'sm1:/dev/nvme0n1']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd sm1:/dev/nvme0n1\n'
10.10.2.1: b'Inferring fsid a96c5bb6-3848-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/a96c5bb6-3848-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b"Created osd(s) 0 on host 'sm1'\n"
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T16:44:45,036412455-07:00] INFO: > Sleep for 20 seconds\x1b[0m\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T16:45:05,041935405-07:00] STAGE: Show Cluster Status\x1b[0m\n# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:408 - show_status() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T16:45:05,052240675-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T16:45:05,056118944-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', '--status']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status\n'
10.10.2.1: b'Inferring fsid a96c5bb6-3848-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/a96c5bb6-3848-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b'  cluster:\n    id:     a96c5bb6-3848-11ec-b51d-53e6e728d2d3\n    health: HEALTH_WARN\n            1 pool(s) have no replicas configured\n \n  services:\n    mon: 1 daemons, quorum node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us (age 2m)\n    mgr: node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us.wmsfpg(active, since 2m)\n    osd: 1 osds: 1 up (since 20s), 1 in (since 40s)\n \n  data:\n    pools:   1 pools, 1 pgs\n    objects: 0 objects, 0 B\n    usage:   4.7 MiB used, 400 GiB / 400 GiB avail\n    pgs:     1 active+clean\n \n'
[1] 16:45:13 [SUCCESS] ljishen@10.10.2.1

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:41:52,514252838-07:00] INFO: > Remove existing clusters[0m
## bash:17 -  > basename -- /var/lib/ceph/6472f08e-3847-11ec-b51d-53e6e728d2d3
# bash:17 -  > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm --host 10.10.2.1 --host 10.10.2.5 rm-cluster --force --fsid 6472f08e-3847-11ec-b51d-53e6e728d2d3
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:41:52,524784894-07:00] DEBUG: HOST_PARAMS: ['--host', '10.10.2.1', '--host', '10.10.2.5'][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:41:52,528474870-07:00] DEBUG: CEPHADM_PARAMS: ['rm-cluster', '--force', '--fsid', '6472f08e-3847-11ec-b51d-53e6e728d2d3'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:63 -  > /tmp/bench-rados/ceph-research/scripts/parallel-ssh --host 10.10.2.1 --host 10.10.2.5 'sudo python3 -u - rm-cluster --force --fsid 6472f08e-3847-11ec-b51d-53e6e728d2d3'
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:41:52,535962584-07:00] DEBUG: command from stdin[0m
# /tmp/bench-rados/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host 10.10.2.1 --host 10.10.2.5 'sudo python3 -u - rm-cluster --force --fsid 6472f08e-3847-11ec-b51d-53e6e728d2d3'
[1] 16:41:58 [SUCCESS] 10.10.2.1
[2] 16:42:01 [SUCCESS] 10.10.2.5

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:42:01,344057230-07:00] INFO: > Deploy a new cluster[0m
# bash:24 -  > /tmp/bench-rados/ceph-research/scripts/ceph-deploy -m 10.10.2.1 -o sm1/10.10.2.5:/dev/nvme0n1


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:42:01,357740749-07:00] STAGE: Verify passwordless SSH to hosts, passwordless sudo via ljishen, and the existence of devices on hosts...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:42:01,362567482-07:00] INFO: > Check the host for the monitor[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:181 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@10.10.2.1 sudo --non-interactive --validate

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:42:01,511125362-07:00] INFO: > Check hosts for OSDs[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:42:01,515570348-07:00] INFO: >> Check host 'sm1'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:200 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 sudo --non-interactive --validate

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:42:02,607005761-07:00] INFO: >>> Check device '/dev/nvme0n1'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:215 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 test -b /dev/nvme0n1
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:226 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 lsblk --nodeps --noheadings --output TYPE /dev/nvme0n1
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:227 - verify_parameters() > grep --word-regexp disk


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:42:04,751792395-07:00] STAGE: Reset devices on hosts...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:42:04,756232180-07:00] INFO: > Resetting device '/dev/nvme0n1' on host 'sm1'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:288 - reset_host_devices() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 sudo sh
  Removing ceph--d412c6c9--9009--4eed--97a4--9ba0edb19c39-osd--block--930c2cd6--b6db--4237--af9d--02c59d04d8fe (252:0)
  Archiving volume group "ceph-d412c6c9-9009-4eed-97a4-9ba0edb19c39" metadata (seqno 5).
  Releasing logical volume "osd-block-930c2cd6-b6db-4237-af9d-02c59d04d8fe"
  Creating volume group backup "/etc/lvm/backup/ceph-d412c6c9-9009-4eed-97a4-9ba0edb19c39" (seqno 6).
  Logical volume "osd-block-930c2cd6-b6db-4237-af9d-02c59d04d8fe" successfully removed
  Removing physical volume "/dev/nvme0n1" from volume group "ceph-d412c6c9-9009-4eed-97a4-9ba0edb19c39"
  Volume group "ceph-d412c6c9-9009-4eed-97a4-9ba0edb19c39" successfully removed


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:42:07,059648085-07:00] STAGE: Bootstrap a new cluster...[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:317 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.1 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:42:07,069901387-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:42:07,073930340-07:00] DEBUG: CEPHADM_PARAMS: ['bootstrap', '--config', '/tmp/bench-rados/ceph-research/scripts/ceph.conf', '--mon-ip', '10.10.2.1', '--output-dir', '/tmp/bench-rados/deployment_data_root/etc/ceph', '--ssh-user', 'ljishen', '--skip-dashboard', '--no-minimize-config', '--allow-overwrite', '--allow-fqdn-hostname', '--orphan-initial-daemons', '--skip-monitoring-stack', '--single-host-defaults'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.1 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults
Verifying podman|docker is present...
Verifying lvm2 is present...
Verifying time synchronization is in place...
Unit ntp.service is enabled and running
Repeating the final host check...
podman|docker (/usr/bin/docker) is present
systemctl is present
lvcreate is present
Unit ntp.service is enabled and running
Host looks OK
Cluster fsid: a96c5bb6-3848-11ec-b51d-53e6e728d2d3
Verifying IP 10.10.2.1 port 3300 ...
Verifying IP 10.10.2.1 port 6789 ...
Mon IP `10.10.2.1` is in CIDR network `10.10.2.0/24`
- internal network (--cluster-network) has not been provided, OSD replication will default to the public_network
Adjusting default settings to suit single-host cluster...
Pulling container image quay.io/ceph/ceph:v16...
Ceph version: ceph version 16.2.6 (ee28fb57e47e9f88813e24bbf4c14496ca299d31) pacific (stable)
Extracting ceph user uid/gid from container image...
Creating initial keys...
Creating initial monmap...
Creating mon...
Waiting for mon to start...
Waiting for mon...
mon is available
Setting mon public_network to 10.10.2.0/24
Wrote config to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf
Wrote keyring to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring
Creating mgr...
Verifying port 9283 ...
Waiting for mgr to start...
Waiting for mgr...
mgr not available, waiting (1/15)...
mgr not available, waiting (2/15)...
mgr is available
Enabling cephadm module...
Waiting for the mgr to restart...
Waiting for mgr epoch 5...
mgr epoch 5 is available
Setting orchestrator backend to cephadm...
Generating ssh key...
Wrote public SSH key to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub
Adding key to ljishen@localhost authorized_keys...
Adding host node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us...
Deploying unmanaged mon service...
Deploying unmanaged mgr service...
You can access the Ceph CLI with:

	sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --fsid a96c5bb6-3848-11ec-b51d-53e6e728d2d3 -c /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf -k /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring

Please consider enabling telemetry to help improve Ceph:

	ceph telemetry on

For more information see:

	https://docs.ceph.com/docs/pacific/mgr/telemetry/

Bootstrap complete.

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:43:09,336827240-07:00] INFO: Sleep for 20 seconds[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:43:29,344219829-07:00] INFO: > Set the default number of monitor daemons to 1[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:337 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:43:29,354111892-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:43:29,357989511-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mon', '1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1
Inferring fsid a96c5bb6-3848-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/a96c5bb6-3848-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Scheduled mon update...

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:43:38,600588226-07:00] INFO: > Set the default number of manager daemons to 1[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:342 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:43:38,610451525-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:43:38,613961732-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mgr', '1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1
Inferring fsid a96c5bb6-3848-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/a96c5bb6-3848-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Scheduled mgr update...


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:43:47,921925091-07:00] STAGE: Add hosts to the cluster...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:43:47,927765409-07:00] INFO: > Adding host 'sm1'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:361 - add_hosts() > ssh-copy-id -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -f -i /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub ljishen@sm1
/usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: "/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub"

Number of key(s) added: 1

Now try logging into the machine, with:   "ssh -o 'GlobalKnownHostsFile=/dev/null' -o 'LogLevel=ERROR' -o 'PasswordAuthentication=no' -o 'StrictHostKeyChecking=no' -o 'UserKnownHostsFile=/dev/null' 'ljishen@sm1'"
and check to make sure that only the key(s) you wanted were added.

# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:367 - add_hosts() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add sm1 10.10.2.5
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:43:49,107374889-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:43:49,110632282-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'host', 'add', 'sm1', '10.10.2.5'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add sm1 10.10.2.5
Inferring fsid a96c5bb6-3848-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/a96c5bb6-3848-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Added host 'sm1' with addr '10.10.2.5'

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:43:59,981778445-07:00] INFO: Sleep for 20 seconds[0m


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:44:19,986658375-07:00] STAGE: Add OSDs to the cluster...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:44:19,993595314-07:00] INFO: > Adding devices ['/dev/nvme0n1'] on host 'sm1' as OSDs[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:388 - add_host_devices_as_osds() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd sm1:/dev/nvme0n1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:44:20,004412246-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:44:20,008247715-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'daemon', 'add', 'osd', 'sm1:/dev/nvme0n1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd sm1:/dev/nvme0n1
Inferring fsid a96c5bb6-3848-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/a96c5bb6-3848-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Created osd(s) 0 on host 'sm1'

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:44:45,036412455-07:00] INFO: > Sleep for 20 seconds[0m


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:45:05,041935405-07:00] STAGE: Show Cluster Status[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:408 - show_status() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:45:05,052240675-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:45:05,056118944-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', '--status'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status
Inferring fsid a96c5bb6-3848-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/a96c5bb6-3848-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
  cluster:
    id:     a96c5bb6-3848-11ec-b51d-53e6e728d2d3
    health: HEALTH_WARN
            1 pool(s) have no replicas configured
 
  services:
    mon: 1 daemons, quorum node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us (age 2m)
    mgr: node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us.wmsfpg(active, since 2m)
    osd: 1 osds: 1 up (since 20s), 1 in (since 40s)
 
  data:
    pools:   1 pools, 1 pgs
    objects: 0 objects, 0 B
    usage:   4.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     1 active+clean
 

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:45:13,871244587-07:00] INFO: > Copy configuration files to local[0m
# ./benchmarks/bench-rados:220 - launch_ceph_cluster() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.1 sudo chown --changes --recursive ljishen /tmp/bench-rados/deployment_data_root/etc/ceph
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:45:13,878981826-07:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.1 sudo chown --changes --recursive ljishen /tmp/bench-rados/deployment_data_root/etc/ceph
[1] 16:45:14 [SUCCESS] ljishen@10.10.2.1
# ./benchmarks/bench-rados:223 - launch_ceph_cluster() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.1:/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf
# ./benchmarks/bench-rados:226 - launch_ceph_cluster() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.1:/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:45:14,361037668-07:00] STAGE: Update local ceph.conf as a client...[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:45:14,364870050-07:00] STAGE: Configure Ceph pool...[0m
# ./benchmarks/bench-rados:257 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_max_backfills 32
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:45:14,387189365-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:45:14,390138963-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'a96c5bb6-3848-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'config', 'set', 'osd', 'osd_max_backfills', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid a96c5bb6-3848-11ec-b51d-53e6e728d2d3 -- ceph config set osd osd_max_backfills 32
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:258 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_active 32
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:45:18,593856548-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:45:18,596820633-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'a96c5bb6-3848-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_active', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid a96c5bb6-3848-11ec-b51d-53e6e728d2d3 -- ceph config set osd osd_recovery_max_active 32
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:259 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_single_start 8
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:45:22,810902318-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:45:22,814104642-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'a96c5bb6-3848-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_single_start', '8'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid a96c5bb6-3848-11ec-b51d-53e6e728d2d3 -- ceph config set osd osd_recovery_max_single_start 8
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:260 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_op_priority 63
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:45:26,854511442-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:45:26,857642051-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'a96c5bb6-3848-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_op_priority', '63'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid a96c5bb6-3848-11ec-b51d-53e6e728d2d3 -- ceph config set osd osd_recovery_op_priority 63
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:262 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config show-with-defaults osd.0
# ./benchmarks/bench-rados:263 - configure_ceph_pool() > grep 'osd_max_backfills\|osd_recovery'
# ./benchmarks/bench-rados:263 - configure_ceph_pool() > column -t -s ' '
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
osd_max_backfills                        32         mon
osd_recovery_cost                        20971520   default
osd_recovery_delay_start                 0.000000   default
osd_recovery_max_active                  32         mon
osd_recovery_max_active_hdd              3          default
osd_recovery_max_active_ssd              10         default
osd_recovery_max_chunk                   8388608    default
osd_recovery_max_omap_entries_per_chunk  8096       default
osd_recovery_max_single_start            8          mon
osd_recovery_op_priority                 63         mon
osd_recovery_op_warn_multiple            16         default
osd_recovery_priority                    5          default
osd_recovery_retry_interval              30.000000  default
osd_recovery_sleep                       0.000000   default
osd_recovery_sleep_hdd                   0.100000   default
osd_recovery_sleep_hybrid                0.025000   default
osd_recovery_sleep_ssd                   0.000000   default
# ./benchmarks/bench-rados:265 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool create bench_rados 128 128 replicated --size 1 --pg-num-min 128
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:45:35,100649256-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:45:35,103735282-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'a96c5bb6-3848-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'create', 'bench_rados', '128', '128', 'replicated', '--size', '1', '--pg-num-min', '128'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid a96c5bb6-3848-11ec-b51d-53e6e728d2d3 -- ceph osd pool create bench_rados 128 128 replicated --size 1 --pg-num-min 128
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
pool 'bench_rados' created
# ./benchmarks/bench-rados:267 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados min_size 1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:45:39,401238658-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:45:39,404629928-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'a96c5bb6-3848-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'min_size', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid a96c5bb6-3848-11ec-b51d-53e6e728d2d3 -- ceph osd pool set bench_rados min_size 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 min_size to 1
# ./benchmarks/bench-rados:268 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados pg_autoscale_mode off
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:45:43,817660217-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:45:43,820738477-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'a96c5bb6-3848-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'pg_autoscale_mode', 'off'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid a96c5bb6-3848-11ec-b51d-53e6e728d2d3 -- ceph osd pool set bench_rados pg_autoscale_mode off
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 pg_autoscale_mode to off
# ./benchmarks/bench-rados:269 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool application enable bench_rados benchmark
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:45:48,948330731-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:45:48,951196191-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'a96c5bb6-3848-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'application', 'enable', 'bench_rados', 'benchmark'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid a96c5bb6-3848-11ec-b51d-53e6e728d2d3 -- ceph osd pool application enable bench_rados benchmark
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
enabled application 'benchmark' on pool 'bench_rados'
# ./benchmarks/bench-rados:270 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados noscrub 1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:45:54,014347653-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:45:54,017308893-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'a96c5bb6-3848-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'noscrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid a96c5bb6-3848-11ec-b51d-53e6e728d2d3 -- ceph osd pool set bench_rados noscrub 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 noscrub to 1
# ./benchmarks/bench-rados:271 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados nodeep-scrub 1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:45:58,946900607-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:45:58,949956075-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'a96c5bb6-3848-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'nodeep-scrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid a96c5bb6-3848-11ec-b51d-53e6e728d2d3 -- ceph osd pool set bench_rados nodeep-scrub 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 nodeep-scrub to 1
# ./benchmarks/bench-rados:272 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool ls detail
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:46:03,988237850-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:46:03,991151930-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'a96c5bb6-3848-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'ls', 'detail'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid a96c5bb6-3848-11ec-b51d-53e6e728d2d3 -- ceph osd pool ls detail
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
pool 1 'device_health_metrics' replicated size 1 min_size 1 crush_rule 0 object_hash rjenkins pg_num 64 pgp_num 64 autoscale_mode on last_change 18 lfor 0/0/15 flags hashpspool stripe_width 0 pg_num_min 1 application mgr_devicehealth
pool 2 'bench_rados' replicated size 1 min_size 1 crush_rule 0 object_hash rjenkins pg_num 128 pgp_num 128 autoscale_mode off last_change 20 flags hashpspool,noscrub,nodeep-scrub stripe_width 0 pg_num_min 128 application benchmark

# ./benchmarks/bench-rados:273 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd df tree
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:46:08,119860974-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:46:08,122820931-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'a96c5bb6-3848-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'df', 'tree'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid a96c5bb6-3848-11ec-b51d-53e6e728d2d3 -- ceph osd df tree
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
ID  CLASS  WEIGHT   REWEIGHT  SIZE     RAW USE  DATA     OMAP  META     AVAIL    %USE  VAR   PGS  STATUS  TYPE NAME    
-1         0.39059         -  400 GiB  5.7 MiB  168 KiB   0 B  5.6 MiB  400 GiB  0.00  1.00    -          root default 
-3         0.39059         -  400 GiB  5.7 MiB  168 KiB   0 B  5.6 MiB  400 GiB  0.00  1.00    -              host sm1 
 0    ssd  0.39059   1.00000  400 GiB  5.7 MiB  168 KiB   0 B  5.6 MiB  400 GiB  0.00  1.00  192      up          osd.0
                       TOTAL  400 GiB  5.7 MiB  168 KiB   0 B  5.6 MiB  400 GiB  0.00                                  
MIN/MAX VAR: 1.00/1.00  STDDEV: 0


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:46:12,205083246-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:46:36,345885863-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:46:45,347263305-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:46:54,502562899-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:46:54,510586437-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:47:03,518846347-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:47:03,526516159-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:47:12,668906800-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:47:12,676946989-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:47:21,767847132-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:47:21,775358154-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:47:30,967593537-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:47:30,975097044-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:47:30,980943801-07:00] INFO: > The cluster is idle now.[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:47:30,984775310-07:00] STAGE: Start benchmarking of mode write ...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:47:30,991854398-07:00] INFO: > Start system activity collection services[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:47:30,997507760-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:341 - rados_bench() > rm --force /mnt/sda8/ceph-research/scripts/output/sys_activity_1MB_write.dat.rados_bench_host.2
# ./benchmarks/bench-rados:342 - rados_bench() > sar_pid_rados_bench_host=895953
# ./benchmarks/bench-rados:342 - rados_bench() > S_TIME_FORMAT=ISO
# ./benchmarks/bench-rados:342 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/output/sys_activity_1MB_write.dat.rados_bench_host.2 2

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:47:31,004478224-07:00] INFO: >> for OSD host[0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:47:31,013431304-07:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.5 bash
10.10.2.5: b'321474\n'
[1] 16:47:32 [SUCCESS] ljishen@10.10.2.5
321474

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:47:32,141974012-07:00] INFO: > Run rados bench[0m
# ./benchmarks/bench-rados:370 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/output/rados_bench_1MB_write.log.2
## ./benchmarks/bench-rados:361 - rados_bench() > count_cpus 32-63,96-127
# ./benchmarks/bench-rados:361 - rados_bench() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 60 write --pool bench_rados -b 1048576 -O 1048576 --concurrent-ios 128 --show-time --write-object --write-omap --write-xattr --no-cleanup
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:47:32,162430676-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:47:32,165423546-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'a96c5bb6-3848-11ec-b51d-53e6e728d2d3', '--', 'rados', 'bench', '60', 'write', '--pool', 'bench_rados', '-b', '1048576', '-O', '1048576', '--concurrent-ios', '128', '--show-time', '--write-object', '--write-omap', '--write-xattr', '--no-cleanup'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid a96c5bb6-3848-11ec-b51d-53e6e728d2d3 -- rados bench 60 write --pool bench_rados -b 1048576 -O 1048576 --concurrent-ios 128 --show-time --write-object --write-omap --write-xattr --no-cleanup
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
hints = 1
2021-10-28T23:47:35.227380+0000 Maintaining 128 concurrent writes of 1048576 bytes to objects of size 1048576 for up to 60 seconds or 0 objects
2021-10-28T23:47:35.227393+0000 Object prefix: benchmark_data_node-2.bluefield2.ucsc-cmps10_7
2021-10-28T23:47:35.258925+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-28T23:47:35.258925+0000     0       0         0         0         0         0           -           0
2021-10-28T23:47:36.259048+0000     1      54        54         0         0         0           -           0
2021-10-28T23:47:37.259162+0000     2      87        87         0         0         0           -           0
2021-10-28T23:47:38.259266+0000     3     120       120         0         0         0           -           0
2021-10-28T23:47:39.259369+0000     4     127       149        22   5.49949       5.5     3.84009     3.72647
2021-10-28T23:47:40.259452+0000     5     127       182        55    10.999        33     4.11413     3.99077
2021-10-28T23:47:41.259532+0000     6     127       215        88   14.6654        33      4.2053     4.05889
2021-10-28T23:47:42.259634+0000     7     127       248       121   17.2841        33     4.02217      4.0657
2021-10-28T23:47:43.259729+0000     8     127       281       154   19.2482        33     3.62471     4.04325
2021-10-28T23:47:44.259843+0000     9     127       294       167   18.5538        13     3.62093     4.04939
2021-10-28T23:47:45.259950+0000    10     127       327       200   19.9981        33     3.74163     4.05656
2021-10-28T23:47:46.260089+0000    11     127       354       227   20.6343        27     4.59736     4.07512
2021-10-28T23:47:47.260209+0000    12     127       404       277    23.081        50     3.99038     4.09565
2021-10-28T23:47:48.260322+0000    13     127       437       310   23.8437        33     3.80267     4.08673
2021-10-28T23:47:49.260438+0000    14     127       470       343   24.4975        33     3.72838      4.0642
2021-10-28T23:47:50.260552+0000    15     127       483       356   23.7309        13     3.41768     4.05974
2021-10-28T23:47:51.260666+0000    16     127       516       389     24.31        33     3.56097     4.04401
2021-10-28T23:47:52.260794+0000    17     127       549       422   24.8209        33     3.74961     4.04663
2021-10-28T23:47:53.260908+0000    18     127       549       422    23.442         0           -     4.04663
2021-10-28T23:47:54.261020+0000    19     127       569       442   23.2607        10     5.02812     4.10509
2021-10-28T23:47:55.261140+0000 min lat: 3.41768 max lat: 6.44742 avg lat: 4.16633
2021-10-28T23:47:55.261140+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-28T23:47:55.261140+0000    20     127       582       455   22.7476        13     5.80673     4.16633
2021-10-28T23:47:56.261260+0000    21     127       582       455   21.6643         0           -     4.16633
2021-10-28T23:47:57.261373+0000    22     127       602       475   21.5886        10      6.7352      4.2915
2021-10-28T23:47:58.261484+0000    23     127       615       488   21.2151        13     7.43974     4.38539
2021-10-28T23:47:59.261599+0000    24     127       635       508   21.1644        20     8.45541     4.55784
2021-10-28T23:48:00.261711+0000    25     127       648       521   20.8377        13     8.78394     4.67409
2021-10-28T23:48:01.261827+0000    26     127       648       521   20.0363         0           -     4.67409
2021-10-28T23:48:02.261954+0000    27     127       668       541   20.0348        10     10.0114     4.88943
2021-10-28T23:48:03.262073+0000    28     127       681       554   19.7835        13     9.40785     5.02251
2021-10-28T23:48:04.262185+0000    29     127       681       554   19.1013         0           -     5.02251
2021-10-28T23:48:05.262296+0000    30     127       701       574   19.1312        10     9.69583      5.2249
2021-10-28T23:48:06.262414+0000    31     127       714       587   18.9334        13     9.60504     5.34399
2021-10-28T23:48:07.262528+0000    32     127       714       587   18.3417         0           -     5.34399
2021-10-28T23:48:08.262639+0000    33     127       734       607   18.3919        10     9.76759     5.52259
2021-10-28T23:48:09.262752+0000    34     127       747       620   18.2333        13     10.0576     5.62564
2021-10-28T23:48:10.262868+0000    35     127       780       653   18.6551        33     8.05927     5.83423
2021-10-28T23:48:11.262995+0000    36     127       813       686   19.0534        33     6.35414     5.94446
2021-10-28T23:48:12.263152+0000    37     127       846       719   19.4302        33     4.62598     5.95678
2021-10-28T23:48:13.263274+0000    38     127       879       752   19.7872        33     3.70027      5.8981
2021-10-28T23:48:14.263388+0000    39     127       912       785   20.1259        33     3.30109     5.81671
2021-10-28T23:48:15.263508+0000 min lat: 3.30109 max lat: 11.1158 avg lat: 5.73493
2021-10-28T23:48:15.263508+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-28T23:48:15.263508+0000    40     127       945       818   20.4477        33     3.36056     5.73493
2021-10-28T23:48:16.263636+0000    41     127       972       845   20.6074        27     4.14296     5.68203
2021-10-28T23:48:17.263750+0000    42     127       996       869   20.6881        24     3.70054     5.63641
2021-10-28T23:48:18.263865+0000    43     127      1029       902   20.9744        33     3.79773     5.58063
2021-10-28T23:48:19.263979+0000    44     127      1062       935   21.2476        33     4.09208     5.53524
2021-10-28T23:48:20.264094+0000    45     127      1062       935   20.7754         0           -     5.53524
2021-10-28T23:48:21.264209+0000    46     127      1077       950   20.6498       7.5     4.68468     5.53037
2021-10-28T23:48:22.264328+0000    47     127      1095       968   20.5934        18     5.49394     5.53747
2021-10-28T23:48:23.264443+0000    48     127      1095       968   20.1644         0           -     5.53747
2021-10-28T23:48:24.264558+0000    49     127      1110       983   20.0589       7.5     7.00986     5.56526
2021-10-28T23:48:25.264674+0000    50     127      1110       983   19.6578         0           -     5.56526
2021-10-28T23:48:26.264783+0000    51     127      1110       983   19.2723         0           -     5.56526
2021-10-28T23:48:27.264892+0000    52     127      1128      1001   19.2478         6     9.63517     5.64575
2021-10-28T23:48:28.265005+0000    53     127      1128      1001   18.8847         0           -     5.64575
2021-10-28T23:48:29.265121+0000    54     127      1143      1016   18.8127       7.5     11.2497     5.73366
2021-10-28T23:48:30.265235+0000    55     127      1143      1016   18.4706         0           -     5.73366
2021-10-28T23:48:31.265354+0000    56     127      1143      1016   18.1408         0           -     5.73366
2021-10-28T23:48:32.265484+0000    57     127      1161      1034   18.1383         6     13.4494     5.87482
2021-10-28T23:48:33.265566+0000    58     127      1161      1034   17.8256         0           -     5.87482
2021-10-28T23:48:34.265681+0000    59     127      1161      1034   17.5234         0           -     5.87482
2021-10-28T23:48:35.265792+0000 min lat: 3.30109 max lat: 16.1023 avg lat: 6.01798
2021-10-28T23:48:35.265792+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-28T23:48:35.265792+0000    60     127      1176      1049   17.4814         5      15.326     6.01798
2021-10-28T23:48:36.265909+0000    61     127      1176      1049   17.1948         0           -     6.01798
2021-10-28T23:48:37.266020+0000    62      16      1177      1161   18.7237        56     5.40312     6.62404
2021-10-28T23:48:38.266136+0000    63      16      1177      1161   18.4265         0           -     6.62404
2021-10-28T23:48:39.266247+0000    64       1      1177      1176   18.3729       7.5     4.79636     6.60312
2021-10-28T23:48:40.266386+0000 Total time run:         64.1399
Total writes made:      1177
Write size:             1048576
Object size:            1048576
Bandwidth (MB/sec):     18.3505
Stddev Bandwidth:       14.807
Max bandwidth (MB/sec): 56
Min bandwidth (MB/sec): 0
Average IOPS:           18
Stddev IOPS:            14.83
Max IOPS:               56
Min IOPS:               0
Average Latency(s):     6.60177
Stddev Latency(s):      3.58776
Max latency(s):         18.0561
Min latency(s):         3.30109

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:48:41,572000574-07:00] INFO: > Stop system activity collection services[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:48:41,577911922-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:391 - rados_bench() > kill -INT 895953

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:48:41,584308234-07:00] INFO: >> for OSD host[0m
# ./benchmarks/bench-rados:396 - rados_bench() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.5 kill -INT 321474
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:48:41,592235941-07:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.5 kill -INT 321474
[1] 16:48:42 [SUCCESS] ljishen@10.10.2.5

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:48:42,808349927-07:00] INFO: >>> Copy system activity file from OSD host[0m
## ./benchmarks/bench-rados:403 - rados_bench() > basename -- /tmp/bench-rados/sys_activity_1MB_write.dat.osd_host.2
# ./benchmarks/bench-rados:403 - rados_bench() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.5:/tmp/bench-rados/sys_activity_1MB_write.dat.osd_host.2 /mnt/sda8/ceph-research/scripts/output/sys_activity_1MB_write.dat.osd_host.2


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:48:43,962498578-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:49:07,891969979-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 1.18k objects, 1.1 GiB
    usage:   3.5 GiB used, 397 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:49:07,899786126-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:49:16,976355214-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 1.18k objects, 1.1 GiB
    usage:   3.5 GiB used, 397 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:49:16,984417335-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:49:25,840297049-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 1.18k objects, 1.1 GiB
    usage:   3.5 GiB used, 397 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:49:25,848044547-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:49:34,865180936-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 1.18k objects, 1.1 GiB
    usage:   3.5 GiB used, 397 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:49:34,873459565-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:49:43,998287240-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 1.18k objects, 1.1 GiB
    usage:   3.5 GiB used, 397 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:49:44,006765745-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:49:44,012931914-07:00] INFO: > The cluster is idle now.[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:49:44,016762201-07:00] STAGE: Start benchmarking of mode seq ...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:49:44,023988186-07:00] INFO: > Start system activity collection services[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:49:44,029582757-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:341 - rados_bench() > rm --force /mnt/sda8/ceph-research/scripts/output/sys_activity_1MB_seq.dat.rados_bench_host.2
# ./benchmarks/bench-rados:342 - rados_bench() > sar_pid_rados_bench_host=898091
# ./benchmarks/bench-rados:342 - rados_bench() > S_TIME_FORMAT=ISO
# ./benchmarks/bench-rados:342 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/output/sys_activity_1MB_seq.dat.rados_bench_host.2 2

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:49:44,036660733-07:00] INFO: >> for OSD host[0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:49:44,045649260-07:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.5 bash
10.10.2.5: b'322025\n'
[1] 16:49:45 [SUCCESS] ljishen@10.10.2.5
322025

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:49:45,144501036-07:00] INFO: > Run rados bench[0m
# ./benchmarks/bench-rados:379 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/output/rados_bench_1MB_seq.log.2
## ./benchmarks/bench-rados:376 - rados_bench() > count_cpus 32-63,96-127
# ./benchmarks/bench-rados:376 - rados_bench() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 128 --show-time
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:49:45,164114611-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:49:45,167015788-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'a96c5bb6-3848-11ec-b51d-53e6e728d2d3', '--', 'rados', 'bench', '99999999', 'seq', '--pool', 'bench_rados', '--concurrent-ios', '128', '--show-time'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid a96c5bb6-3848-11ec-b51d-53e6e728d2d3 -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 128 --show-time
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
hints = 1
2021-10-28T23:49:47.915976+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-28T23:49:47.915976+0000     0       0         0         0         0         0           -           0
2021-10-28T23:49:48.916080+0000     1     127       225        98   97.9839        98     0.96577    0.583668
2021-10-28T23:49:49.916169+0000     2     127       368       241   120.485       143     1.57188    0.759513
2021-10-28T23:49:50.916256+0000     3     127       484       357   118.986       116    0.889706    0.849275
2021-10-28T23:49:51.916335+0000     4     127       628       501   125.237       144    0.339701    0.867627
2021-10-28T23:49:52.916416+0000     5     127       738       611   122.188       110     0.50388    0.890841
2021-10-28T23:49:53.916495+0000     6     127       826       699   116.489        88      1.1591    0.936373
2021-10-28T23:49:54.916569+0000     7     127       927       800   114.275       101     2.73416    0.989744
2021-10-28T23:49:55.916660+0000     8     127      1056       929   116.114       129     1.89966     1.00165
2021-10-28T23:49:56.916744+0000     9     100      1177      1077   119.656       148     1.07217     1.01136
2021-10-28T23:49:57.916862+0000 Total time run:       9.87101
Total reads made:     1177
Read size:            1048576
Object size:          1048576
Bandwidth (MB/sec):   119.238
Average IOPS:         119
Stddev IOPS:          22.2317
Max IOPS:             148
Min IOPS:             88
Average Latency(s):   1.01002
Max latency(s):       2.97698
Min latency(s):       0.118281

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:49:58,369420609-07:00] INFO: > Stop system activity collection services[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:49:58,375422468-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:391 - rados_bench() > kill -INT 898091

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:49:58,381348093-07:00] INFO: >> for OSD host[0m
# ./benchmarks/bench-rados:396 - rados_bench() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.5 kill -INT 322025
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:49:58,389485215-07:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.5 kill -INT 322025
[1] 16:49:59 [SUCCESS] ljishen@10.10.2.5

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:49:59,496540008-07:00] INFO: >>> Copy system activity file from OSD host[0m
## ./benchmarks/bench-rados:403 - rados_bench() > basename -- /tmp/bench-rados/sys_activity_1MB_seq.dat.osd_host.2
# ./benchmarks/bench-rados:403 - rados_bench() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.5:/tmp/bench-rados/sys_activity_1MB_seq.dat.osd_host.2 /mnt/sda8/ceph-research/scripts/output/sys_activity_1MB_seq.dat.osd_host.2


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:50:00,569163106-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:50:24,674787470-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 1.18k objects, 1.1 GiB
    usage:   3.5 GiB used, 397 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:50:24,682897350-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:50:33,822295292-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 1.18k objects, 1.1 GiB
    usage:   3.5 GiB used, 397 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:50:33,829986484-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:50:42,997083392-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 1.18k objects, 1.1 GiB
    usage:   3.5 GiB used, 397 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:50:43,004969932-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:50:52,021662436-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 1.18k objects, 1.1 GiB
    usage:   3.5 GiB used, 397 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:50:52,029499863-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:51:00,995489408-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 1.18k objects, 1.1 GiB
    usage:   3.5 GiB used, 397 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:51:01,003577708-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:51:01,009768753-07:00] INFO: > The cluster is idle now.[0m


[1;7;39;49m[2021-10-28T16:51:01,012201697-07:00][RUNNING][ROUND 3/5/21] object_size=1MB[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:51:01,015734283-07:00] STAGE: Launch a Ceph cluster (CEPH_HOST: 10.10.2.1, OSD_HOST: 10.10.2.5) ...[0m
# ./benchmarks/bench-rados:183 - launch_ceph_cluster() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.1 bash -euo pipefail
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:51:01,026261288-07:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.1 bash -euo pipefail
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T16:51:01,529123554-07:00] INFO: > Remove existing clusters\x1b[0m\n'
10.10.2.1: b'## bash:17 -  > basename -- /var/lib/ceph/a96c5bb6-3848-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'# bash:17 -  > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm --host 10.10.2.1 --host 10.10.2.5 rm-cluster --force --fsid a96c5bb6-3848-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T16:51:01,541002102-07:00] DEBUG: HOST_PARAMS: ['--host', '10.10.2.1', '--host', '10.10.2.5']\x1b[0m\n"
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T16:51:01,544916930-07:00] DEBUG: CEPHADM_PARAMS: ['rm-cluster', '--force', '--fsid', 'a96c5bb6-3848-11ec-b51d-53e6e728d2d3']\x1b[0m\n"
10.10.2.1: b"# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:63 -  > /tmp/bench-rados/ceph-research/scripts/parallel-ssh --host 10.10.2.1 --host 10.10.2.5 'sudo python3 -u - rm-cluster --force --fsid a96c5bb6-3848-11ec-b51d-53e6e728d2d3'\n"
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T16:51:01,553090393-07:00] DEBUG: command from stdin\x1b[0m\n'
10.10.2.1: b"# /tmp/bench-rados/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host 10.10.2.1 --host 10.10.2.5 'sudo python3 -u - rm-cluster --force --fsid a96c5bb6-3848-11ec-b51d-53e6e728d2d3'\n"
10.10.2.1: b'[1] 16:51:07 [SUCCESS] 10.10.2.1\n[2] 16:51:10 [SUCCESS] 10.10.2.5\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T16:51:10,470469750-07:00] INFO: > Deploy a new cluster\x1b[0m\n'
10.10.2.1: b'# bash:24 -  > /tmp/bench-rados/ceph-research/scripts/ceph-deploy -m 10.10.2.1 -o sm1/10.10.2.5:/dev/nvme0n1\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T16:51:10,482737128-07:00] STAGE: Verify passwordless SSH to hosts, passwordless sudo via ljishen, and the existence of devices on hosts...\x1b[0m\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T16:51:10,487571966-07:00] INFO: > Check the host for the monitor\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:181 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@10.10.2.1 sudo --non-interactive --validate\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T16:51:10,638903613-07:00] INFO: > Check hosts for OSDs\x1b[0m\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T16:51:10,644097666-07:00] INFO: >> Check host 'sm1'\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:200 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 sudo --non-interactive --validate\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T16:51:11,723313591-07:00] INFO: >>> Check device '/dev/nvme0n1'\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:215 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 test -b /dev/nvme0n1\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:226 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 lsblk --nodeps --noheadings --output TYPE /dev/nvme0n1\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:227 - verify_parameters() > grep --word-regexp disk\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T16:51:13,876281510-07:00] STAGE: Reset devices on hosts...\x1b[0m\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T16:51:13,881499218-07:00] INFO: > Resetting device '/dev/nvme0n1' on host 'sm1'\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:288 - reset_host_devices() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 sudo sh\n'
10.10.2.1: b'  Removing ceph--30de7d37--1220--4e9e--b8c9--d209fc741d95-osd--block--ffeef808--b4cc--486f--9e41--2dffcacb9a77 (252:0)\n'
10.10.2.1: b'  Archiving volume group "ceph-30de7d37-1220-4e9e-b8c9-d209fc741d95" metadata (seqno 5).\n'
10.10.2.1: b'  Releasing logical volume "osd-block-ffeef808-b4cc-486f-9e41-2dffcacb9a77"\n'
10.10.2.1: b'  Creating volume group backup "/etc/lvm/backup/ceph-30de7d37-1220-4e9e-b8c9-d209fc741d95" (seqno 6).\n'
10.10.2.1: b'  Logical volume "osd-block-ffeef808-b4cc-486f-9e41-2dffcacb9a77" successfully removed\n'
10.10.2.1: b'  Removing physical volume "/dev/nvme0n1" from volume group "ceph-30de7d37-1220-4e9e-b8c9-d209fc741d95"\n'
10.10.2.1: b'  Volume group "ceph-30de7d37-1220-4e9e-b8c9-d209fc741d95" successfully removed\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T16:51:16,231582151-07:00] STAGE: Bootstrap a new cluster...\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:317 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.1 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T16:51:16,240977110-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T16:51:16,244539105-07:00] DEBUG: CEPHADM_PARAMS: ['bootstrap', '--config', '/tmp/bench-rados/ceph-research/scripts/ceph.conf', '--mon-ip', '10.10.2.1', '--output-dir', '/tmp/bench-rados/deployment_data_root/etc/ceph', '--ssh-user', 'ljishen', '--skip-dashboard', '--no-minimize-config', '--allow-overwrite', '--allow-fqdn-hostname', '--orphan-initial-daemons', '--skip-monitoring-stack', '--single-host-defaults']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.1 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults\n'
10.10.2.1: b'Verifying podman|docker is present...\nVerifying lvm2 is present...\n'
10.10.2.1: b'Verifying time synchronization is in place...\n'
10.10.2.1: b'Unit ntp.service is enabled and running\n'
10.10.2.1: b'Repeating the final host check...\n'
10.10.2.1: b'podman|docker (/usr/bin/docker) is present\n'
10.10.2.1: b'systemctl is present\n'
10.10.2.1: b'lvcreate is present\n'
10.10.2.1: b'Unit ntp.service is enabled and running\n'
10.10.2.1: b'Host looks OK\n'
10.10.2.1: b'Cluster fsid: f0c13774-3849-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Verifying IP 10.10.2.1 port 3300 ...\n'
10.10.2.1: b'Verifying IP 10.10.2.1 port 6789 ...\n'
10.10.2.1: b'Mon IP `10.10.2.1` is in CIDR network `10.10.2.0/24`\n'
10.10.2.1: b'- internal network (--cluster-network) has not been provided, OSD replication will default to the public_network\n'
10.10.2.1: b'Adjusting default settings to suit single-host cluster...\nPulling container image quay.io/ceph/ceph:v16...\n'
10.10.2.1: b'Ceph version: ceph version 16.2.6 (ee28fb57e47e9f88813e24bbf4c14496ca299d31) pacific (stable)\n'
10.10.2.1: b'Extracting ceph user uid/gid from container image...\n'
10.10.2.1: b'Creating initial keys...\n'
10.10.2.1: b'Creating initial monmap...\n'
10.10.2.1: b'Creating mon...\n'
10.10.2.1: b'Waiting for mon to start...\n'
10.10.2.1: b'Waiting for mon...\n'
10.10.2.1: b'mon is available\nSetting mon public_network to 10.10.2.0/24\n'
10.10.2.1: b'Wrote config to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf\n'
10.10.2.1: b'Wrote keyring to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring\n'
10.10.2.1: b'Creating mgr...\n'
10.10.2.1: b'Verifying port 9283 ...\n'
10.10.2.1: b'Waiting for mgr to start...\nWaiting for mgr...\n'
10.10.2.1: b'mgr not available, waiting (1/15)...\n'
10.10.2.1: b'mgr not available, waiting (2/15)...\n'
10.10.2.1: b'mgr is available\n'
10.10.2.1: b'Enabling cephadm module...\n'
10.10.2.1: b'Waiting for the mgr to restart...\n'
10.10.2.1: b'Waiting for mgr epoch 5...\n'
10.10.2.1: b'mgr epoch 5 is available\n'
10.10.2.1: b'Setting orchestrator backend to cephadm...\n'
10.10.2.1: b'Generating ssh key...\n'
10.10.2.1: b'Wrote public SSH key to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub\n'
10.10.2.1: b'Adding key to ljishen@localhost authorized_keys...\n'
10.10.2.1: b'Adding host node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us...\n'
10.10.2.1: b'Deploying unmanaged mon service...\n'
10.10.2.1: b'Deploying unmanaged mgr service...\n'
10.10.2.1: b'You can access the Ceph CLI with:\n\n\tsudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --fsid f0c13774-3849-11ec-b51d-53e6e728d2d3 -c /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf -k /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring\n\n'
10.10.2.1: b'Please consider enabling telemetry to help improve Ceph:\n\n\tceph telemetry on\n\nFor more information see:\n\n\thttps://docs.ceph.com/docs/pacific/mgr/telemetry/\n\n'
10.10.2.1: b'Bootstrap complete.\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T16:52:17,734526080-07:00] INFO: Sleep for 20 seconds\x1b[0m\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T16:52:37,741820201-07:00] INFO: > Set the default number of monitor daemons to 1\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:337 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T16:52:37,752011627-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T16:52:37,755400998-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mon', '1']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1\n'
10.10.2.1: b'Inferring fsid f0c13774-3849-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/f0c13774-3849-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b'Scheduled mon update...\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T16:52:47,137943455-07:00] INFO: > Set the default number of manager daemons to 1\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:342 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T16:52:47,147778732-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T16:52:47,151662732-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mgr', '1']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1\n'
10.10.2.1: b'Inferring fsid f0c13774-3849-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/f0c13774-3849-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b'Scheduled mgr update...\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T16:52:56,550516171-07:00] STAGE: Add hosts to the cluster...\x1b[0m\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T16:52:56,556950305-07:00] INFO: > Adding host 'sm1'\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:361 - add_hosts() > ssh-copy-id -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -f -i /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub ljishen@sm1\n'
10.10.2.1: b'/usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: "/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub"\n'
10.10.2.1: b'\nNumber of key(s) added: 1\n\nNow try logging into the machine, with:   "ssh -o \'GlobalKnownHostsFile=/dev/null\' -o \'LogLevel=ERROR\' -o \'PasswordAuthentication=no\' -o \'StrictHostKeyChecking=no\' -o \'UserKnownHostsFile=/dev/null\' \'ljishen@sm1\'"\nand check to make sure that only the key(s) you wanted were added.\n\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:367 - add_hosts() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add sm1 10.10.2.5\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T16:52:57,712549165-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T16:52:57,716316045-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'host', 'add', 'sm1', '10.10.2.5']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add sm1 10.10.2.5\n'
10.10.2.1: b'Inferring fsid f0c13774-3849-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/f0c13774-3849-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b"Added host 'sm1' with addr '10.10.2.5'\n"
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T16:53:08,495127069-07:00] INFO: Sleep for 20 seconds\x1b[0m\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T16:53:28,499603234-07:00] STAGE: Add OSDs to the cluster...\x1b[0m\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T16:53:28,506583865-07:00] INFO: > Adding devices ['/dev/nvme0n1'] on host 'sm1' as OSDs\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:388 - add_host_devices_as_osds() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd sm1:/dev/nvme0n1\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T16:53:28,516875339-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T16:53:28,520730025-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'daemon', 'add', 'osd', 'sm1:/dev/nvme0n1']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd sm1:/dev/nvme0n1\n'
10.10.2.1: b'Inferring fsid f0c13774-3849-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/f0c13774-3849-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b"Created osd(s) 0 on host 'sm1'\n"
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T16:53:52,937597607-07:00] INFO: > Sleep for 20 seconds\x1b[0m\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T16:54:12,943049651-07:00] STAGE: Show Cluster Status\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:408 - show_status() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T16:54:12,952893094-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T16:54:12,956493651-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', '--status']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status\n'
10.10.2.1: b'Inferring fsid f0c13774-3849-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/f0c13774-3849-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b'  cluster:\n    id:     f0c13774-3849-11ec-b51d-53e6e728d2d3\n    health: HEALTH_WARN\n            1 pool(s) have no replicas configured\n \n  services:\n    mon: 1 daemons, quorum node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us (age 2m)\n    mgr: node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us.qgxjsw(active, since 2m)\n    osd: 1 osds: 1 up (since 20s), 1 in (since 40s)\n \n  data:\n    pools:   1 pools, 1 pgs\n    objects: 0 objects, 0 B\n    usage:   4.7 MiB used, 400 GiB / 400 GiB avail\n    pgs:     1 active+clean\n \n'
[1] 16:54:21 [SUCCESS] ljishen@10.10.2.1

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:51:01,529123554-07:00] INFO: > Remove existing clusters[0m
## bash:17 -  > basename -- /var/lib/ceph/a96c5bb6-3848-11ec-b51d-53e6e728d2d3
# bash:17 -  > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm --host 10.10.2.1 --host 10.10.2.5 rm-cluster --force --fsid a96c5bb6-3848-11ec-b51d-53e6e728d2d3
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:51:01,541002102-07:00] DEBUG: HOST_PARAMS: ['--host', '10.10.2.1', '--host', '10.10.2.5'][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:51:01,544916930-07:00] DEBUG: CEPHADM_PARAMS: ['rm-cluster', '--force', '--fsid', 'a96c5bb6-3848-11ec-b51d-53e6e728d2d3'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:63 -  > /tmp/bench-rados/ceph-research/scripts/parallel-ssh --host 10.10.2.1 --host 10.10.2.5 'sudo python3 -u - rm-cluster --force --fsid a96c5bb6-3848-11ec-b51d-53e6e728d2d3'
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:51:01,553090393-07:00] DEBUG: command from stdin[0m
# /tmp/bench-rados/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host 10.10.2.1 --host 10.10.2.5 'sudo python3 -u - rm-cluster --force --fsid a96c5bb6-3848-11ec-b51d-53e6e728d2d3'
[1] 16:51:07 [SUCCESS] 10.10.2.1
[2] 16:51:10 [SUCCESS] 10.10.2.5

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:51:10,470469750-07:00] INFO: > Deploy a new cluster[0m
# bash:24 -  > /tmp/bench-rados/ceph-research/scripts/ceph-deploy -m 10.10.2.1 -o sm1/10.10.2.5:/dev/nvme0n1


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:51:10,482737128-07:00] STAGE: Verify passwordless SSH to hosts, passwordless sudo via ljishen, and the existence of devices on hosts...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:51:10,487571966-07:00] INFO: > Check the host for the monitor[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:181 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@10.10.2.1 sudo --non-interactive --validate

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:51:10,638903613-07:00] INFO: > Check hosts for OSDs[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:51:10,644097666-07:00] INFO: >> Check host 'sm1'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:200 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 sudo --non-interactive --validate

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:51:11,723313591-07:00] INFO: >>> Check device '/dev/nvme0n1'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:215 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 test -b /dev/nvme0n1
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:226 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 lsblk --nodeps --noheadings --output TYPE /dev/nvme0n1
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:227 - verify_parameters() > grep --word-regexp disk


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:51:13,876281510-07:00] STAGE: Reset devices on hosts...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:51:13,881499218-07:00] INFO: > Resetting device '/dev/nvme0n1' on host 'sm1'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:288 - reset_host_devices() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 sudo sh
  Removing ceph--30de7d37--1220--4e9e--b8c9--d209fc741d95-osd--block--ffeef808--b4cc--486f--9e41--2dffcacb9a77 (252:0)
  Archiving volume group "ceph-30de7d37-1220-4e9e-b8c9-d209fc741d95" metadata (seqno 5).
  Releasing logical volume "osd-block-ffeef808-b4cc-486f-9e41-2dffcacb9a77"
  Creating volume group backup "/etc/lvm/backup/ceph-30de7d37-1220-4e9e-b8c9-d209fc741d95" (seqno 6).
  Logical volume "osd-block-ffeef808-b4cc-486f-9e41-2dffcacb9a77" successfully removed
  Removing physical volume "/dev/nvme0n1" from volume group "ceph-30de7d37-1220-4e9e-b8c9-d209fc741d95"
  Volume group "ceph-30de7d37-1220-4e9e-b8c9-d209fc741d95" successfully removed


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:51:16,231582151-07:00] STAGE: Bootstrap a new cluster...[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:317 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.1 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:51:16,240977110-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:51:16,244539105-07:00] DEBUG: CEPHADM_PARAMS: ['bootstrap', '--config', '/tmp/bench-rados/ceph-research/scripts/ceph.conf', '--mon-ip', '10.10.2.1', '--output-dir', '/tmp/bench-rados/deployment_data_root/etc/ceph', '--ssh-user', 'ljishen', '--skip-dashboard', '--no-minimize-config', '--allow-overwrite', '--allow-fqdn-hostname', '--orphan-initial-daemons', '--skip-monitoring-stack', '--single-host-defaults'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.1 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults
Verifying podman|docker is present...
Verifying lvm2 is present...
Verifying time synchronization is in place...
Unit ntp.service is enabled and running
Repeating the final host check...
podman|docker (/usr/bin/docker) is present
systemctl is present
lvcreate is present
Unit ntp.service is enabled and running
Host looks OK
Cluster fsid: f0c13774-3849-11ec-b51d-53e6e728d2d3
Verifying IP 10.10.2.1 port 3300 ...
Verifying IP 10.10.2.1 port 6789 ...
Mon IP `10.10.2.1` is in CIDR network `10.10.2.0/24`
- internal network (--cluster-network) has not been provided, OSD replication will default to the public_network
Adjusting default settings to suit single-host cluster...
Pulling container image quay.io/ceph/ceph:v16...
Ceph version: ceph version 16.2.6 (ee28fb57e47e9f88813e24bbf4c14496ca299d31) pacific (stable)
Extracting ceph user uid/gid from container image...
Creating initial keys...
Creating initial monmap...
Creating mon...
Waiting for mon to start...
Waiting for mon...
mon is available
Setting mon public_network to 10.10.2.0/24
Wrote config to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf
Wrote keyring to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring
Creating mgr...
Verifying port 9283 ...
Waiting for mgr to start...
Waiting for mgr...
mgr not available, waiting (1/15)...
mgr not available, waiting (2/15)...
mgr is available
Enabling cephadm module...
Waiting for the mgr to restart...
Waiting for mgr epoch 5...
mgr epoch 5 is available
Setting orchestrator backend to cephadm...
Generating ssh key...
Wrote public SSH key to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub
Adding key to ljishen@localhost authorized_keys...
Adding host node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us...
Deploying unmanaged mon service...
Deploying unmanaged mgr service...
You can access the Ceph CLI with:

	sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --fsid f0c13774-3849-11ec-b51d-53e6e728d2d3 -c /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf -k /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring

Please consider enabling telemetry to help improve Ceph:

	ceph telemetry on

For more information see:

	https://docs.ceph.com/docs/pacific/mgr/telemetry/

Bootstrap complete.

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:52:17,734526080-07:00] INFO: Sleep for 20 seconds[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:52:37,741820201-07:00] INFO: > Set the default number of monitor daemons to 1[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:337 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:52:37,752011627-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:52:37,755400998-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mon', '1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1
Inferring fsid f0c13774-3849-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/f0c13774-3849-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Scheduled mon update...

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:52:47,137943455-07:00] INFO: > Set the default number of manager daemons to 1[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:342 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:52:47,147778732-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:52:47,151662732-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mgr', '1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1
Inferring fsid f0c13774-3849-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/f0c13774-3849-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Scheduled mgr update...


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:52:56,550516171-07:00] STAGE: Add hosts to the cluster...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:52:56,556950305-07:00] INFO: > Adding host 'sm1'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:361 - add_hosts() > ssh-copy-id -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -f -i /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub ljishen@sm1
/usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: "/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub"

Number of key(s) added: 1

Now try logging into the machine, with:   "ssh -o 'GlobalKnownHostsFile=/dev/null' -o 'LogLevel=ERROR' -o 'PasswordAuthentication=no' -o 'StrictHostKeyChecking=no' -o 'UserKnownHostsFile=/dev/null' 'ljishen@sm1'"
and check to make sure that only the key(s) you wanted were added.

# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:367 - add_hosts() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add sm1 10.10.2.5
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:52:57,712549165-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:52:57,716316045-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'host', 'add', 'sm1', '10.10.2.5'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add sm1 10.10.2.5
Inferring fsid f0c13774-3849-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/f0c13774-3849-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Added host 'sm1' with addr '10.10.2.5'

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:53:08,495127069-07:00] INFO: Sleep for 20 seconds[0m


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:53:28,499603234-07:00] STAGE: Add OSDs to the cluster...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:53:28,506583865-07:00] INFO: > Adding devices ['/dev/nvme0n1'] on host 'sm1' as OSDs[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:388 - add_host_devices_as_osds() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd sm1:/dev/nvme0n1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:53:28,516875339-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:53:28,520730025-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'daemon', 'add', 'osd', 'sm1:/dev/nvme0n1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd sm1:/dev/nvme0n1
Inferring fsid f0c13774-3849-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/f0c13774-3849-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Created osd(s) 0 on host 'sm1'

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:53:52,937597607-07:00] INFO: > Sleep for 20 seconds[0m


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:54:12,943049651-07:00] STAGE: Show Cluster Status[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:408 - show_status() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:54:12,952893094-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:54:12,956493651-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', '--status'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status
Inferring fsid f0c13774-3849-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/f0c13774-3849-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
  cluster:
    id:     f0c13774-3849-11ec-b51d-53e6e728d2d3
    health: HEALTH_WARN
            1 pool(s) have no replicas configured
 
  services:
    mon: 1 daemons, quorum node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us (age 2m)
    mgr: node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us.qgxjsw(active, since 2m)
    osd: 1 osds: 1 up (since 20s), 1 in (since 40s)
 
  data:
    pools:   1 pools, 1 pgs
    objects: 0 objects, 0 B
    usage:   4.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     1 active+clean
 

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:54:21,603429513-07:00] INFO: > Copy configuration files to local[0m
# ./benchmarks/bench-rados:220 - launch_ceph_cluster() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.1 sudo chown --changes --recursive ljishen /tmp/bench-rados/deployment_data_root/etc/ceph
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:54:21,611506232-07:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.1 sudo chown --changes --recursive ljishen /tmp/bench-rados/deployment_data_root/etc/ceph
[1] 16:54:21 [SUCCESS] ljishen@10.10.2.1
# ./benchmarks/bench-rados:223 - launch_ceph_cluster() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.1:/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf
# ./benchmarks/bench-rados:226 - launch_ceph_cluster() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.1:/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:54:22,088852793-07:00] STAGE: Update local ceph.conf as a client...[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:54:22,092450502-07:00] STAGE: Configure Ceph pool...[0m
# ./benchmarks/bench-rados:257 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_max_backfills 32
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:54:22,114661853-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:54:22,117797361-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'f0c13774-3849-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'config', 'set', 'osd', 'osd_max_backfills', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid f0c13774-3849-11ec-b51d-53e6e728d2d3 -- ceph config set osd osd_max_backfills 32
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:258 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_active 32
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:54:26,163919669-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:54:26,166890907-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'f0c13774-3849-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_active', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid f0c13774-3849-11ec-b51d-53e6e728d2d3 -- ceph config set osd osd_recovery_max_active 32
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:259 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_single_start 8
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:54:30,326205785-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:54:30,329181402-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'f0c13774-3849-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_single_start', '8'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid f0c13774-3849-11ec-b51d-53e6e728d2d3 -- ceph config set osd osd_recovery_max_single_start 8
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:260 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_op_priority 63
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:54:34,399354730-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:54:34,402107527-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'f0c13774-3849-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_op_priority', '63'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid f0c13774-3849-11ec-b51d-53e6e728d2d3 -- ceph config set osd osd_recovery_op_priority 63
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:262 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config show-with-defaults osd.0
# ./benchmarks/bench-rados:263 - configure_ceph_pool() > grep 'osd_max_backfills\|osd_recovery'
# ./benchmarks/bench-rados:263 - configure_ceph_pool() > column -t -s ' '
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
osd_max_backfills                        32         mon
osd_recovery_cost                        20971520   default
osd_recovery_delay_start                 0.000000   default
osd_recovery_max_active                  32         mon
osd_recovery_max_active_hdd              3          default
osd_recovery_max_active_ssd              10         default
osd_recovery_max_chunk                   8388608    default
osd_recovery_max_omap_entries_per_chunk  8096       default
osd_recovery_max_single_start            8          mon
osd_recovery_op_priority                 63         mon
osd_recovery_op_warn_multiple            16         default
osd_recovery_priority                    5          default
osd_recovery_retry_interval              30.000000  default
osd_recovery_sleep                       0.000000   default
osd_recovery_sleep_hdd                   0.100000   default
osd_recovery_sleep_hybrid                0.025000   default
osd_recovery_sleep_ssd                   0.000000   default
# ./benchmarks/bench-rados:265 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool create bench_rados 128 128 replicated --size 1 --pg-num-min 128
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:54:42,551809333-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:54:42,554799457-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'f0c13774-3849-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'create', 'bench_rados', '128', '128', 'replicated', '--size', '1', '--pg-num-min', '128'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid f0c13774-3849-11ec-b51d-53e6e728d2d3 -- ceph osd pool create bench_rados 128 128 replicated --size 1 --pg-num-min 128
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
pool 'bench_rados' created
# ./benchmarks/bench-rados:267 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados min_size 1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:54:46,959005477-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:54:46,962042910-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'f0c13774-3849-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'min_size', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid f0c13774-3849-11ec-b51d-53e6e728d2d3 -- ceph osd pool set bench_rados min_size 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 min_size to 1
# ./benchmarks/bench-rados:268 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados pg_autoscale_mode off
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:54:51,226613397-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:54:51,229779904-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'f0c13774-3849-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'pg_autoscale_mode', 'off'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid f0c13774-3849-11ec-b51d-53e6e728d2d3 -- ceph osd pool set bench_rados pg_autoscale_mode off
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 pg_autoscale_mode to off
# ./benchmarks/bench-rados:269 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool application enable bench_rados benchmark
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:54:55,608663433-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:54:55,611869905-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'f0c13774-3849-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'application', 'enable', 'bench_rados', 'benchmark'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid f0c13774-3849-11ec-b51d-53e6e728d2d3 -- ceph osd pool application enable bench_rados benchmark
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
enabled application 'benchmark' on pool 'bench_rados'
# ./benchmarks/bench-rados:270 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados noscrub 1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:55:00,151004302-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:55:00,153901471-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'f0c13774-3849-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'noscrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid f0c13774-3849-11ec-b51d-53e6e728d2d3 -- ceph osd pool set bench_rados noscrub 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 noscrub to 1
# ./benchmarks/bench-rados:271 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados nodeep-scrub 1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:55:05,335701279-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:55:05,338450509-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'f0c13774-3849-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'nodeep-scrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid f0c13774-3849-11ec-b51d-53e6e728d2d3 -- ceph osd pool set bench_rados nodeep-scrub 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 nodeep-scrub to 1
# ./benchmarks/bench-rados:272 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool ls detail
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:55:09,734956567-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:55:09,738262617-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'f0c13774-3849-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'ls', 'detail'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid f0c13774-3849-11ec-b51d-53e6e728d2d3 -- ceph osd pool ls detail
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
pool 1 'device_health_metrics' replicated size 1 min_size 1 crush_rule 0 object_hash rjenkins pg_num 64 pgp_num 64 autoscale_mode on last_change 19 lfor 0/0/16 flags hashpspool stripe_width 0 pg_num_min 1 application mgr_devicehealth
pool 2 'bench_rados' replicated size 1 min_size 1 crush_rule 0 object_hash rjenkins pg_num 128 pgp_num 128 autoscale_mode off last_change 21 flags hashpspool,noscrub,nodeep-scrub stripe_width 0 pg_num_min 128 application benchmark

# ./benchmarks/bench-rados:273 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd df tree
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:55:13,703343970-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:55:13,706184302-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'f0c13774-3849-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'df', 'tree'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid f0c13774-3849-11ec-b51d-53e6e728d2d3 -- ceph osd df tree
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
ID  CLASS  WEIGHT   REWEIGHT  SIZE     RAW USE  DATA     OMAP  META     AVAIL    %USE  VAR   PGS  STATUS  TYPE NAME    
-1         0.39059         -  400 GiB  5.7 MiB  176 KiB   0 B  5.6 MiB  400 GiB  0.00  1.00    -          root default 
-3         0.39059         -  400 GiB  5.7 MiB  176 KiB   0 B  5.6 MiB  400 GiB  0.00  1.00    -              host sm1 
 0    ssd  0.39059   1.00000  400 GiB  5.7 MiB  176 KiB   0 B  5.6 MiB  400 GiB  0.00  1.00  192      up          osd.0
                       TOTAL  400 GiB  5.7 MiB  176 KiB   0 B  5.6 MiB  400 GiB  0.00                                  
MIN/MAX VAR: 1.00/1.00  STDDEV: 0


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:55:17,703143991-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:55:41,666951298-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:55:50,723220060-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:55:59,714178148-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:55:59,722514726-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:56:08,872317537-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:56:08,880594964-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:56:17,985691965-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:56:17,993856830-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:56:26,975004599-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:56:26,982768789-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:56:35,939995271-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:56:35,948405047-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:56:35,954768236-07:00] INFO: > The cluster is idle now.[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:56:35,958549651-07:00] STAGE: Start benchmarking of mode write ...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:56:35,965538841-07:00] INFO: > Start system activity collection services[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:56:35,971382872-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:341 - rados_bench() > rm --force /mnt/sda8/ceph-research/scripts/output/sys_activity_1MB_write.dat.rados_bench_host.3
# ./benchmarks/bench-rados:342 - rados_bench() > sar_pid_rados_bench_host=905703
# ./benchmarks/bench-rados:342 - rados_bench() > S_TIME_FORMAT=ISO
# ./benchmarks/bench-rados:342 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/output/sys_activity_1MB_write.dat.rados_bench_host.3 2

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:56:35,978401587-07:00] INFO: >> for OSD host[0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:56:35,987668297-07:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.5 bash
10.10.2.5: b'326898\n'
[1] 16:56:37 [SUCCESS] ljishen@10.10.2.5
326898

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:56:37,108155462-07:00] INFO: > Run rados bench[0m
# ./benchmarks/bench-rados:370 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/output/rados_bench_1MB_write.log.3
## ./benchmarks/bench-rados:361 - rados_bench() > count_cpus 32-63,96-127
# ./benchmarks/bench-rados:361 - rados_bench() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 60 write --pool bench_rados -b 1048576 -O 1048576 --concurrent-ios 128 --show-time --write-object --write-omap --write-xattr --no-cleanup
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:56:37,128707206-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:56:37,131513223-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'f0c13774-3849-11ec-b51d-53e6e728d2d3', '--', 'rados', 'bench', '60', 'write', '--pool', 'bench_rados', '-b', '1048576', '-O', '1048576', '--concurrent-ios', '128', '--show-time', '--write-object', '--write-omap', '--write-xattr', '--no-cleanup'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid f0c13774-3849-11ec-b51d-53e6e728d2d3 -- rados bench 60 write --pool bench_rados -b 1048576 -O 1048576 --concurrent-ios 128 --show-time --write-object --write-omap --write-xattr --no-cleanup
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
hints = 1
2021-10-28T23:56:40.105518+0000 Maintaining 128 concurrent writes of 1048576 bytes to objects of size 1048576 for up to 60 seconds or 0 objects
2021-10-28T23:56:40.105531+0000 Object prefix: benchmark_data_node-2.bluefield2.ucsc-cmps10_7
2021-10-28T23:56:40.137187+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-28T23:56:40.137187+0000     0       0         0         0         0         0           -           0
2021-10-28T23:56:41.137307+0000     1      54        54         0         0         0           -           0
2021-10-28T23:56:42.137432+0000     2      87        87         0         0         0           -           0
2021-10-28T23:56:43.137534+0000     3     120       120         0         0         0           -           0
2021-10-28T23:56:44.137620+0000     4     127       162        35   8.74917      8.75     3.35939     3.59738
2021-10-28T23:56:45.137721+0000     5     127       184        57   11.3989        22     3.33521     3.62309
2021-10-28T23:56:46.137823+0000     6     127       217        90   14.9985        33     3.45102     3.71774
2021-10-28T23:56:47.137916+0000     7     127       256       129   18.4268        39     3.83271     3.76593
2021-10-28T23:56:48.138015+0000     8     127       287       160   19.9981        31     3.87724     3.81041
2021-10-28T23:56:49.138105+0000     9     127       320       193   21.4424        33     3.64951     3.83585
2021-10-28T23:56:50.138206+0000    10     127       349       222   22.1979        29     3.65607     3.83454
2021-10-28T23:56:51.138299+0000    11     127       382       255   23.1796        33     3.77406     3.84462
2021-10-28T23:56:52.138394+0000    12     127       415       288   23.9977        33     3.68462     3.83602
2021-10-28T23:56:53.138494+0000    13     127       447       320    24.613        32     3.79834     3.83109
2021-10-28T23:56:54.138589+0000    14     127       481       354   25.2833        34     3.47982     3.84075
2021-10-28T23:56:55.138699+0000    15     127       514       387   25.7975        33      3.4089     3.83845
2021-10-28T23:56:56.138800+0000    16     127       547       420   26.2474        33     3.49409     3.84281
2021-10-28T23:56:57.138896+0000    17     127       547       420   24.7035         0           -     3.84281
2021-10-28T23:56:58.138983+0000    18     127       567       440   24.4421        10     4.88611     3.90491
2021-10-28T23:56:59.139081+0000    19     127       580       453   23.8398        13     5.37328     3.95885
2021-10-28T23:57:00.139167+0000 min lat: 3.0375 max lat: 6.03035 avg lat: 3.95885
2021-10-28T23:57:00.139167+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-28T23:57:00.139167+0000    20     127       580       453   22.6478         0           -     3.95885
2021-10-28T23:57:01.139251+0000    21     127       600       473   22.5217        10     6.64262     4.08834
2021-10-28T23:57:02.139345+0000    22     127       600       473   21.4979         0           -     4.08834
2021-10-28T23:57:03.139428+0000    23     127       613       486   21.1284       6.5     7.83557     4.19705
2021-10-28T23:57:04.139512+0000    24     127       633       506   21.0813        20     9.04056     4.40016
2021-10-28T23:57:05.139597+0000    25     127       646       519    20.758        13     9.72511     4.54342
2021-10-28T23:57:06.139676+0000    26     127       646       519   19.9597         0           -     4.54342
2021-10-28T23:57:07.139744+0000    27     127       646       519   19.2204         0           -     4.54342
2021-10-28T23:57:08.139816+0000    28     127       666       539   19.2482   6.66667     11.4092     4.81284
2021-10-28T23:57:09.139897+0000    29     127       679       552   19.0327        13     10.7198     4.98134
2021-10-28T23:57:10.139982+0000    30     127       699       572   19.0649        20     11.0671     5.22585
2021-10-28T23:57:11.140076+0000    31     127       712       585   18.8692        13     10.4738     5.36936
2021-10-28T23:57:12.140204+0000    32     127       712       585   18.2796         0           -     5.36936
2021-10-28T23:57:13.140304+0000    33     127       732       605   18.3316        10     10.2876     5.57604
2021-10-28T23:57:14.140419+0000    34     127       745       618   18.1748        13     9.90247     5.69003
2021-10-28T23:57:15.140505+0000    35     127       765       638   18.2269        20     9.71774     5.84785
2021-10-28T23:57:16.140591+0000    36     127       798       671   18.6372        33     7.60147     6.00209
2021-10-28T23:57:17.140687+0000    37     127       831       704   19.0253        33     5.93499       6.055
2021-10-28T23:57:18.140775+0000    38     127       864       737   19.3929        33     4.45675     6.03655
2021-10-28T23:57:19.140882+0000    39     127       877       750    19.229        13     3.84574     6.01144
2021-10-28T23:57:20.140983+0000 min lat: 3.0375 max lat: 12.5257 avg lat: 5.93547
2021-10-28T23:57:20.140983+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-28T23:57:20.140983+0000    40     127       910       783   19.5732        33     3.68801     5.93547
2021-10-28T23:57:21.141072+0000    41     127       963       836   20.3883        53     3.63964      5.8195
2021-10-28T23:57:22.141172+0000    42     127       996       869   20.6885        33     3.53862     5.74772
2021-10-28T23:57:23.141262+0000    43     127      1029       902   20.9748        33     3.52105     5.67651
2021-10-28T23:57:24.141368+0000    44     127      1042       915   20.7935        13     3.33926     5.64896
2021-10-28T23:57:25.141464+0000    45     127      1062       935   20.7758        20     4.09519     5.62231
2021-10-28T23:57:26.141556+0000    46     127      1075       948   20.6068        13     4.85101     5.61623
2021-10-28T23:57:27.141659+0000    47     127      1075       948   20.1683         0           -     5.61623
2021-10-28T23:57:28.141753+0000    48     127      1095       968   20.1648        10     5.61539     5.62496
2021-10-28T23:57:29.141868+0000    49     127      1095       968   19.7532         0           -     5.62496
2021-10-28T23:57:30.141964+0000    50     127      1108       981   19.6181       6.5     7.10632     5.64919
2021-10-28T23:57:31.142071+0000    51     127      1108       981   19.2335         0           -     5.64919
2021-10-28T23:57:32.142167+0000    52     127      1108       981   18.8636         0           -     5.64919
2021-10-28T23:57:33.142243+0000    53     127      1128      1001    18.885   6.66667     10.3793     5.75024
2021-10-28T23:57:34.142348+0000    54     127      1128      1001   18.5353         0           -     5.75024
2021-10-28T23:57:35.142422+0000    55     127      1141      1014   18.4346       6.5     11.8515     5.83365
2021-10-28T23:57:36.142521+0000    56     127      1141      1014   18.1054         0           -     5.83365
2021-10-28T23:57:37.142604+0000    57     127      1141      1014   17.7878         0           -     5.83365
2021-10-28T23:57:38.142688+0000    58     127      1141      1014   17.4811         0           -     5.83365
2021-10-28T23:57:39.142775+0000    59     127      1161      1034   17.5238         5     15.1546     6.02002
2021-10-28T23:57:40.142883+0000 min lat: 3.0375 max lat: 15.5496 avg lat: 6.02002
2021-10-28T23:57:40.142883+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-28T23:57:40.142883+0000    60     127      1161      1034   17.2317         0           -     6.02002
2021-10-28T23:57:41.142973+0000    61      21      1162      1141   18.7032      53.5     6.00408     6.62985
2021-10-28T23:57:42.143058+0000    62      21      1162      1141   18.4015         0           -     6.62985
2021-10-28T23:57:43.143138+0000    63      21      1162      1141   18.1094         0           -     6.62985
2021-10-28T23:57:44.143259+0000 Total time run:         63.7589
Total writes made:      1162
Write size:             1048576
Object size:            1048576
Bandwidth (MB/sec):     18.2249
Stddev Bandwidth:       14.8998
Max bandwidth (MB/sec): 53.5
Min bandwidth (MB/sec): 0
Average IOPS:           18
Stddev IOPS:            14.9122
Max IOPS:               53
Min IOPS:               0
Average Latency(s):     6.60513
Stddev Latency(s):      3.69554
Max latency(s):         17.4481
Min latency(s):         3.0375

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:57:45,076174517-07:00] INFO: > Stop system activity collection services[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:57:45,082382745-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:391 - rados_bench() > kill -INT 905703

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:57:45,088385936-07:00] INFO: >> for OSD host[0m
# ./benchmarks/bench-rados:396 - rados_bench() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.5 kill -INT 326898
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:57:45,096354811-07:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.5 kill -INT 326898
[1] 16:57:46 [SUCCESS] ljishen@10.10.2.5

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:57:46,250227797-07:00] INFO: >>> Copy system activity file from OSD host[0m
## ./benchmarks/bench-rados:403 - rados_bench() > basename -- /tmp/bench-rados/sys_activity_1MB_write.dat.osd_host.3
# ./benchmarks/bench-rados:403 - rados_bench() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.5:/tmp/bench-rados/sys_activity_1MB_write.dat.osd_host.3 /mnt/sda8/ceph-research/scripts/output/sys_activity_1MB_write.dat.osd_host.3


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:57:47,470524460-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:58:11,497741324-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 1.16k objects, 1.1 GiB
    usage:   3.7 GiB used, 396 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:58:11,505680503-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:58:20,578887520-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 1.16k objects, 1.1 GiB
    usage:   3.4 GiB used, 397 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:58:20,586822021-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:58:29,593578445-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 1.16k objects, 1.1 GiB
    usage:   3.4 GiB used, 397 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:58:29,601278935-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:58:38,633952098-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 1.16k objects, 1.1 GiB
    usage:   3.4 GiB used, 397 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:58:38,642141128-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:58:47,775036929-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 1.16k objects, 1.1 GiB
    usage:   3.4 GiB used, 397 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:58:47,783226690-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:58:47,789281368-07:00] INFO: > The cluster is idle now.[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:58:47,792930424-07:00] STAGE: Start benchmarking of mode seq ...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:58:47,800051442-07:00] INFO: > Start system activity collection services[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:58:47,805809260-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:341 - rados_bench() > rm --force /mnt/sda8/ceph-research/scripts/output/sys_activity_1MB_seq.dat.rados_bench_host.3
# ./benchmarks/bench-rados:342 - rados_bench() > sar_pid_rados_bench_host=907659
# ./benchmarks/bench-rados:342 - rados_bench() > S_TIME_FORMAT=ISO
# ./benchmarks/bench-rados:342 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/output/sys_activity_1MB_seq.dat.rados_bench_host.3 2

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:58:47,813151876-07:00] INFO: >> for OSD host[0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:58:47,822168295-07:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.5 bash
10.10.2.5: b'327443\n'
[1] 16:58:48 [SUCCESS] ljishen@10.10.2.5
327443

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:58:48,949156438-07:00] INFO: > Run rados bench[0m
# ./benchmarks/bench-rados:379 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/output/rados_bench_1MB_seq.log.3
## ./benchmarks/bench-rados:376 - rados_bench() > count_cpus 32-63,96-127
# ./benchmarks/bench-rados:376 - rados_bench() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 128 --show-time
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:58:48,969259657-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:58:48,972136307-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'f0c13774-3849-11ec-b51d-53e6e728d2d3', '--', 'rados', 'bench', '99999999', 'seq', '--pool', 'bench_rados', '--concurrent-ios', '128', '--show-time'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid f0c13774-3849-11ec-b51d-53e6e728d2d3 -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 128 --show-time
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
hints = 1
2021-10-28T23:58:51.986302+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-28T23:58:51.986302+0000     0       0         0         0         0         0           -           0
2021-10-28T23:58:52.986420+0000     1     127       232       105   104.981       105    0.459608    0.514867
2021-10-28T23:58:53.986512+0000     2     127       347       220   109.985       115     1.14541    0.783276
2021-10-28T23:58:54.986602+0000     3     127       490       363   120.985       143     1.27005    0.849864
2021-10-28T23:58:55.986689+0000     4     127       598       471   117.737       108     1.06047    0.904884
2021-10-28T23:58:56.986754+0000     5     127       699       572   114.388       101    0.583234    0.940045
2021-10-28T23:58:57.986843+0000     6     127       805       678   112.989       106    0.941593    0.981921
2021-10-28T23:58:58.986936+0000     7     127       925       798   113.989       120    0.544088     1.00964
2021-10-28T23:58:59.987017+0000     8     127      1062       935   116.864       137      2.2049      1.0018
2021-10-28T23:59:00.987098+0000     9      84      1162      1078   119.766       143    0.346144     1.00485
2021-10-28T23:59:01.987188+0000 Total time run:       9.9186
Total reads made:     1162
Read size:            1048576
Object size:          1048576
Bandwidth (MB/sec):   117.154
Average IOPS:         117
Stddev IOPS:          16.9468
Max IOPS:             143
Min IOPS:             101
Average Latency(s):   1.01459
Max latency(s):       2.67118
Min latency(s):       0.143561

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:59:02,677956062-07:00] INFO: > Stop system activity collection services[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:59:02,684268576-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:391 - rados_bench() > kill -INT 907659

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:59:02,690391714-07:00] INFO: >> for OSD host[0m
# ./benchmarks/bench-rados:396 - rados_bench() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.5 kill -INT 327443
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:59:02,698397979-07:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.5 kill -INT 327443
[1] 16:59:03 [SUCCESS] ljishen@10.10.2.5

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:59:03,808393355-07:00] INFO: >>> Copy system activity file from OSD host[0m
## ./benchmarks/bench-rados:403 - rados_bench() > basename -- /tmp/bench-rados/sys_activity_1MB_seq.dat.osd_host.3
# ./benchmarks/bench-rados:403 - rados_bench() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.5:/tmp/bench-rados/sys_activity_1MB_seq.dat.osd_host.3 /mnt/sda8/ceph-research/scripts/output/sys_activity_1MB_seq.dat.osd_host.3


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:59:04,873512074-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:59:28,829071489-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 1.16k objects, 1.1 GiB
    usage:   3.4 GiB used, 397 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:59:28,836564367-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:59:37,911607640-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 1.16k objects, 1.1 GiB
    usage:   3.4 GiB used, 397 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:59:37,919444806-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:59:47,061712851-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 1.16k objects, 1.1 GiB
    usage:   3.4 GiB used, 397 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:59:47,070006348-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:59:56,168759216-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 1.16k objects, 1.1 GiB
    usage:   3.4 GiB used, 397 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T16:59:56,177124378-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:00:05,624135841-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 1.16k objects, 1.1 GiB
    usage:   3.4 GiB used, 397 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:00:05,632077465-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:00:05,638109511-07:00] INFO: > The cluster is idle now.[0m


[1;7;39;49m[2021-10-28T17:00:05,641944186-07:00][RUNNING][ROUND 1/6/21] object_size=4MB[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:00:05,645866416-07:00] STAGE: Launch a Ceph cluster (CEPH_HOST: 10.10.2.1, OSD_HOST: 10.10.2.5) ...[0m
# ./benchmarks/bench-rados:183 - launch_ceph_cluster() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.1 bash -euo pipefail
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:00:05,655262992-07:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.1 bash -euo pipefail
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T17:00:07,198158457-07:00] INFO: > Remove existing clusters\x1b[0m\n'
10.10.2.1: b'## bash:17 -  > basename -- /var/lib/ceph/f0c13774-3849-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'# bash:17 -  > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm --host 10.10.2.1 --host 10.10.2.5 rm-cluster --force --fsid f0c13774-3849-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T17:00:07,209435694-07:00] DEBUG: HOST_PARAMS: ['--host', '10.10.2.1', '--host', '10.10.2.5']\x1b[0m\n"
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T17:00:07,212913070-07:00] DEBUG: CEPHADM_PARAMS: ['rm-cluster', '--force', '--fsid', 'f0c13774-3849-11ec-b51d-53e6e728d2d3']\x1b[0m\n"
10.10.2.1: b"# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:63 -  > /tmp/bench-rados/ceph-research/scripts/parallel-ssh --host 10.10.2.1 --host 10.10.2.5 'sudo python3 -u - rm-cluster --force --fsid f0c13774-3849-11ec-b51d-53e6e728d2d3'\n"
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T17:00:07,221658088-07:00] DEBUG: command from stdin\x1b[0m\n'
10.10.2.1: b"# /tmp/bench-rados/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host 10.10.2.1 --host 10.10.2.5 'sudo python3 -u - rm-cluster --force --fsid f0c13774-3849-11ec-b51d-53e6e728d2d3'\n"
10.10.2.1: b'[1] 17:00:13 [SUCCESS] 10.10.2.1\n[2] 17:00:16 [SUCCESS] 10.10.2.5\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T17:00:16,433598742-07:00] INFO: > Deploy a new cluster\x1b[0m\n'
10.10.2.1: b'# bash:24 -  > /tmp/bench-rados/ceph-research/scripts/ceph-deploy -m 10.10.2.1 -o sm1/10.10.2.5:/dev/nvme0n1\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T17:00:16,445163700-07:00] STAGE: Verify passwordless SSH to hosts, passwordless sudo via ljishen, and the existence of devices on hosts...\x1b[0m\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T17:00:16,450230985-07:00] INFO: > Check the host for the monitor\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:181 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@10.10.2.1 sudo --non-interactive --validate\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T17:00:16,598629641-07:00] INFO: > Check hosts for OSDs\x1b[0m\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T17:00:16,604298717-07:00] INFO: >> Check host 'sm1'\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:200 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 sudo --non-interactive --validate\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T17:00:17,687219512-07:00] INFO: >>> Check device '/dev/nvme0n1'\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:215 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 test -b /dev/nvme0n1\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:226 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 lsblk --nodeps --noheadings --output TYPE /dev/nvme0n1\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:227 - verify_parameters() > grep --word-regexp disk\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T17:00:19,811626331-07:00] STAGE: Reset devices on hosts...\x1b[0m\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T17:00:19,816477409-07:00] INFO: > Resetting device '/dev/nvme0n1' on host 'sm1'\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:288 - reset_host_devices() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 sudo sh\n'
10.10.2.1: b'  Removing ceph--aa4468eb--004f--4fb1--94b6--c87273051237-osd--block--05bbbf86--f73c--421c--98ec--65ab88b9751a (252:0)\n'
10.10.2.1: b'  Archiving volume group "ceph-aa4468eb-004f-4fb1-94b6-c87273051237" metadata (seqno 5).\n'
10.10.2.1: b'  Releasing logical volume "osd-block-05bbbf86-f73c-421c-98ec-65ab88b9751a"\n'
10.10.2.1: b'  Creating volume group backup "/etc/lvm/backup/ceph-aa4468eb-004f-4fb1-94b6-c87273051237" (seqno 6).\n'
10.10.2.1: b'  Logical volume "osd-block-05bbbf86-f73c-421c-98ec-65ab88b9751a" successfully removed\n'
10.10.2.1: b'  Removing physical volume "/dev/nvme0n1" from volume group "ceph-aa4468eb-004f-4fb1-94b6-c87273051237"\n'
10.10.2.1: b'  Volume group "ceph-aa4468eb-004f-4fb1-94b6-c87273051237" successfully removed\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T17:00:22,139570466-07:00] STAGE: Bootstrap a new cluster...\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:317 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.1 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T17:00:22,149755080-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T17:00:22,153398578-07:00] DEBUG: CEPHADM_PARAMS: ['bootstrap', '--config', '/tmp/bench-rados/ceph-research/scripts/ceph.conf', '--mon-ip', '10.10.2.1', '--output-dir', '/tmp/bench-rados/deployment_data_root/etc/ceph', '--ssh-user', 'ljishen', '--skip-dashboard', '--no-minimize-config', '--allow-overwrite', '--allow-fqdn-hostname', '--orphan-initial-daemons', '--skip-monitoring-stack', '--single-host-defaults']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.1 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults\n'
10.10.2.1: b'Verifying podman|docker is present...\nVerifying lvm2 is present...\n'
10.10.2.1: b'Verifying time synchronization is in place...\n'
10.10.2.1: b'Unit ntp.service is enabled and running\n'
10.10.2.1: b'Repeating the final host check...\n'
10.10.2.1: b'podman|docker (/usr/bin/docker) is present\n'
10.10.2.1: b'systemctl is present\n'
10.10.2.1: b'lvcreate is present\n'
10.10.2.1: b'Unit ntp.service is enabled and running\n'
10.10.2.1: b'Host looks OK\nCluster fsid: 36248874-384b-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Verifying IP 10.10.2.1 port 3300 ...\n'
10.10.2.1: b'Verifying IP 10.10.2.1 port 6789 ...\n'
10.10.2.1: b'Mon IP `10.10.2.1` is in CIDR network `10.10.2.0/24`\n- internal network (--cluster-network) has not been provided, OSD replication will default to the public_network\n'
10.10.2.1: b'Adjusting default settings to suit single-host cluster...\n'
10.10.2.1: b'Pulling container image quay.io/ceph/ceph:v16...\n'
10.10.2.1: b'Ceph version: ceph version 16.2.6 (ee28fb57e47e9f88813e24bbf4c14496ca299d31) pacific (stable)\n'
10.10.2.1: b'Extracting ceph user uid/gid from container image...\n'
10.10.2.1: b'Creating initial keys...\n'
10.10.2.1: b'Creating initial monmap...\n'
10.10.2.1: b'Creating mon...\n'
10.10.2.1: b'Waiting for mon to start...\n'
10.10.2.1: b'Waiting for mon...\n'
10.10.2.1: b'mon is available\nSetting mon public_network to 10.10.2.0/24\n'
10.10.2.1: b'Wrote config to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf\n'
10.10.2.1: b'Wrote keyring to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring\n'
10.10.2.1: b'Creating mgr...\n'
10.10.2.1: b'Verifying port 9283 ...\n'
10.10.2.1: b'Waiting for mgr to start...\nWaiting for mgr...\n'
10.10.2.1: b'mgr not available, waiting (1/15)...\n'
10.10.2.1: b'mgr not available, waiting (2/15)...\n'
10.10.2.1: b'mgr is available\n'
10.10.2.1: b'Enabling cephadm module...\n'
10.10.2.1: b'Waiting for the mgr to restart...\n'
10.10.2.1: b'Waiting for mgr epoch 5...\n'
10.10.2.1: b'mgr epoch 5 is available\nSetting orchestrator backend to cephadm...\n'
10.10.2.1: b'Generating ssh key...\n'
10.10.2.1: b'Wrote public SSH key to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub\nAdding key to ljishen@localhost authorized_keys...\n'
10.10.2.1: b'Adding host node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us...\n'
10.10.2.1: b'Deploying unmanaged mon service...\n'
10.10.2.1: b'Deploying unmanaged mgr service...\n'
10.10.2.1: b'You can access the Ceph CLI with:\n\n\tsudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --fsid 36248874-384b-11ec-b51d-53e6e728d2d3 -c /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf -k /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring\n\nPlease consider enabling telemetry to help improve Ceph:\n\n\tceph telemetry on\n\nFor more information see:\n\n\thttps://docs.ceph.com/docs/pacific/mgr/telemetry/\n\n'
10.10.2.1: b'Bootstrap complete.\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T17:01:25,365115823-07:00] INFO: Sleep for 20 seconds\x1b[0m\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T17:01:45,372602760-07:00] INFO: > Set the default number of monitor daemons to 1\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:337 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T17:01:45,383149143-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T17:01:45,386717821-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mon', '1']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1\n'
10.10.2.1: b'Inferring fsid 36248874-384b-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/36248874-384b-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b'Scheduled mon update...\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T17:01:54,765903570-07:00] INFO: > Set the default number of manager daemons to 1\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:342 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T17:01:54,776065882-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T17:01:54,779964440-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mgr', '1']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1\n'
10.10.2.1: b'Inferring fsid 36248874-384b-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/36248874-384b-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b'Scheduled mgr update...\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T17:02:04,318375209-07:00] STAGE: Add hosts to the cluster...\x1b[0m\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T17:02:04,324660253-07:00] INFO: > Adding host 'sm1'\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:361 - add_hosts() > ssh-copy-id -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -f -i /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub ljishen@sm1\n'
10.10.2.1: b'/usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: "/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub"\n'
10.10.2.1: b'\nNumber of key(s) added: 1\n\nNow try logging into the machine, with:   "ssh -o \'GlobalKnownHostsFile=/dev/null\' -o \'LogLevel=ERROR\' -o \'PasswordAuthentication=no\' -o \'StrictHostKeyChecking=no\' -o \'UserKnownHostsFile=/dev/null\' \'ljishen@sm1\'"\nand check to make sure that only the key(s) you wanted were added.\n\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:367 - add_hosts() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add sm1 10.10.2.5\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T17:02:05,476210871-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T17:02:05,479966991-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'host', 'add', 'sm1', '10.10.2.5']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add sm1 10.10.2.5\n'
10.10.2.1: b'Inferring fsid 36248874-384b-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/36248874-384b-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b"Added host 'sm1' with addr '10.10.2.5'\n"
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T17:02:16,156838116-07:00] INFO: Sleep for 20 seconds\x1b[0m\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T17:02:36,161717086-07:00] STAGE: Add OSDs to the cluster...\x1b[0m\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T17:02:36,168494325-07:00] INFO: > Adding devices ['/dev/nvme0n1'] on host 'sm1' as OSDs\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:388 - add_host_devices_as_osds() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd sm1:/dev/nvme0n1\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T17:02:36,178418178-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T17:02:36,182075253-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'daemon', 'add', 'osd', 'sm1:/dev/nvme0n1']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd sm1:/dev/nvme0n1\n'
10.10.2.1: b'Inferring fsid 36248874-384b-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/36248874-384b-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b"Created osd(s) 0 on host 'sm1'\n"
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T17:03:00,700766893-07:00] INFO: > Sleep for 20 seconds\x1b[0m\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T17:03:20,706517214-07:00] STAGE: Show Cluster Status\x1b[0m\n# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:408 - show_status() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T17:03:20,716813367-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T17:03:20,720550843-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', '--status']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status\n'
10.10.2.1: b'Inferring fsid 36248874-384b-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/36248874-384b-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b'  cluster:\n    id:     36248874-384b-11ec-b51d-53e6e728d2d3\n    health: HEALTH_WARN\n            1 pool(s) have no replicas configured\n \n  services:\n    mon: 1 daemons, quorum node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us (age 2m)\n    mgr: node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us.rhwgvl(active, since 2m)\n    osd: 1 osds: 1 up (since 20s), 1 in (since 40s)\n \n  data:\n    pools:   1 pools, 1 pgs\n    objects: 0 objects, 0 B\n    usage:   4.7 MiB used, 400 GiB / 400 GiB avail\n    pgs:     1 active+clean\n \n'
[1] 17:03:29 [SUCCESS] ljishen@10.10.2.1

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:00:07,198158457-07:00] INFO: > Remove existing clusters[0m
## bash:17 -  > basename -- /var/lib/ceph/f0c13774-3849-11ec-b51d-53e6e728d2d3
# bash:17 -  > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm --host 10.10.2.1 --host 10.10.2.5 rm-cluster --force --fsid f0c13774-3849-11ec-b51d-53e6e728d2d3
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:00:07,209435694-07:00] DEBUG: HOST_PARAMS: ['--host', '10.10.2.1', '--host', '10.10.2.5'][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:00:07,212913070-07:00] DEBUG: CEPHADM_PARAMS: ['rm-cluster', '--force', '--fsid', 'f0c13774-3849-11ec-b51d-53e6e728d2d3'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:63 -  > /tmp/bench-rados/ceph-research/scripts/parallel-ssh --host 10.10.2.1 --host 10.10.2.5 'sudo python3 -u - rm-cluster --force --fsid f0c13774-3849-11ec-b51d-53e6e728d2d3'
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:00:07,221658088-07:00] DEBUG: command from stdin[0m
# /tmp/bench-rados/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host 10.10.2.1 --host 10.10.2.5 'sudo python3 -u - rm-cluster --force --fsid f0c13774-3849-11ec-b51d-53e6e728d2d3'
[1] 17:00:13 [SUCCESS] 10.10.2.1
[2] 17:00:16 [SUCCESS] 10.10.2.5

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:00:16,433598742-07:00] INFO: > Deploy a new cluster[0m
# bash:24 -  > /tmp/bench-rados/ceph-research/scripts/ceph-deploy -m 10.10.2.1 -o sm1/10.10.2.5:/dev/nvme0n1


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:00:16,445163700-07:00] STAGE: Verify passwordless SSH to hosts, passwordless sudo via ljishen, and the existence of devices on hosts...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:00:16,450230985-07:00] INFO: > Check the host for the monitor[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:181 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@10.10.2.1 sudo --non-interactive --validate

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:00:16,598629641-07:00] INFO: > Check hosts for OSDs[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:00:16,604298717-07:00] INFO: >> Check host 'sm1'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:200 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 sudo --non-interactive --validate

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:00:17,687219512-07:00] INFO: >>> Check device '/dev/nvme0n1'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:215 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 test -b /dev/nvme0n1
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:226 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 lsblk --nodeps --noheadings --output TYPE /dev/nvme0n1
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:227 - verify_parameters() > grep --word-regexp disk


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:00:19,811626331-07:00] STAGE: Reset devices on hosts...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:00:19,816477409-07:00] INFO: > Resetting device '/dev/nvme0n1' on host 'sm1'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:288 - reset_host_devices() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 sudo sh
  Removing ceph--aa4468eb--004f--4fb1--94b6--c87273051237-osd--block--05bbbf86--f73c--421c--98ec--65ab88b9751a (252:0)
  Archiving volume group "ceph-aa4468eb-004f-4fb1-94b6-c87273051237" metadata (seqno 5).
  Releasing logical volume "osd-block-05bbbf86-f73c-421c-98ec-65ab88b9751a"
  Creating volume group backup "/etc/lvm/backup/ceph-aa4468eb-004f-4fb1-94b6-c87273051237" (seqno 6).
  Logical volume "osd-block-05bbbf86-f73c-421c-98ec-65ab88b9751a" successfully removed
  Removing physical volume "/dev/nvme0n1" from volume group "ceph-aa4468eb-004f-4fb1-94b6-c87273051237"
  Volume group "ceph-aa4468eb-004f-4fb1-94b6-c87273051237" successfully removed


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:00:22,139570466-07:00] STAGE: Bootstrap a new cluster...[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:317 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.1 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:00:22,149755080-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:00:22,153398578-07:00] DEBUG: CEPHADM_PARAMS: ['bootstrap', '--config', '/tmp/bench-rados/ceph-research/scripts/ceph.conf', '--mon-ip', '10.10.2.1', '--output-dir', '/tmp/bench-rados/deployment_data_root/etc/ceph', '--ssh-user', 'ljishen', '--skip-dashboard', '--no-minimize-config', '--allow-overwrite', '--allow-fqdn-hostname', '--orphan-initial-daemons', '--skip-monitoring-stack', '--single-host-defaults'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.1 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults
Verifying podman|docker is present...
Verifying lvm2 is present...
Verifying time synchronization is in place...
Unit ntp.service is enabled and running
Repeating the final host check...
podman|docker (/usr/bin/docker) is present
systemctl is present
lvcreate is present
Unit ntp.service is enabled and running
Host looks OK
Cluster fsid: 36248874-384b-11ec-b51d-53e6e728d2d3
Verifying IP 10.10.2.1 port 3300 ...
Verifying IP 10.10.2.1 port 6789 ...
Mon IP `10.10.2.1` is in CIDR network `10.10.2.0/24`
- internal network (--cluster-network) has not been provided, OSD replication will default to the public_network
Adjusting default settings to suit single-host cluster...
Pulling container image quay.io/ceph/ceph:v16...
Ceph version: ceph version 16.2.6 (ee28fb57e47e9f88813e24bbf4c14496ca299d31) pacific (stable)
Extracting ceph user uid/gid from container image...
Creating initial keys...
Creating initial monmap...
Creating mon...
Waiting for mon to start...
Waiting for mon...
mon is available
Setting mon public_network to 10.10.2.0/24
Wrote config to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf
Wrote keyring to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring
Creating mgr...
Verifying port 9283 ...
Waiting for mgr to start...
Waiting for mgr...
mgr not available, waiting (1/15)...
mgr not available, waiting (2/15)...
mgr is available
Enabling cephadm module...
Waiting for the mgr to restart...
Waiting for mgr epoch 5...
mgr epoch 5 is available
Setting orchestrator backend to cephadm...
Generating ssh key...
Wrote public SSH key to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub
Adding key to ljishen@localhost authorized_keys...
Adding host node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us...
Deploying unmanaged mon service...
Deploying unmanaged mgr service...
You can access the Ceph CLI with:

	sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --fsid 36248874-384b-11ec-b51d-53e6e728d2d3 -c /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf -k /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring

Please consider enabling telemetry to help improve Ceph:

	ceph telemetry on

For more information see:

	https://docs.ceph.com/docs/pacific/mgr/telemetry/

Bootstrap complete.

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:01:25,365115823-07:00] INFO: Sleep for 20 seconds[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:01:45,372602760-07:00] INFO: > Set the default number of monitor daemons to 1[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:337 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:01:45,383149143-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:01:45,386717821-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mon', '1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1
Inferring fsid 36248874-384b-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/36248874-384b-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Scheduled mon update...

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:01:54,765903570-07:00] INFO: > Set the default number of manager daemons to 1[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:342 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:01:54,776065882-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:01:54,779964440-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mgr', '1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1
Inferring fsid 36248874-384b-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/36248874-384b-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Scheduled mgr update...


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:02:04,318375209-07:00] STAGE: Add hosts to the cluster...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:02:04,324660253-07:00] INFO: > Adding host 'sm1'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:361 - add_hosts() > ssh-copy-id -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -f -i /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub ljishen@sm1
/usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: "/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub"

Number of key(s) added: 1

Now try logging into the machine, with:   "ssh -o 'GlobalKnownHostsFile=/dev/null' -o 'LogLevel=ERROR' -o 'PasswordAuthentication=no' -o 'StrictHostKeyChecking=no' -o 'UserKnownHostsFile=/dev/null' 'ljishen@sm1'"
and check to make sure that only the key(s) you wanted were added.

# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:367 - add_hosts() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add sm1 10.10.2.5
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:02:05,476210871-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:02:05,479966991-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'host', 'add', 'sm1', '10.10.2.5'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add sm1 10.10.2.5
Inferring fsid 36248874-384b-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/36248874-384b-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Added host 'sm1' with addr '10.10.2.5'

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:02:16,156838116-07:00] INFO: Sleep for 20 seconds[0m


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:02:36,161717086-07:00] STAGE: Add OSDs to the cluster...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:02:36,168494325-07:00] INFO: > Adding devices ['/dev/nvme0n1'] on host 'sm1' as OSDs[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:388 - add_host_devices_as_osds() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd sm1:/dev/nvme0n1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:02:36,178418178-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:02:36,182075253-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'daemon', 'add', 'osd', 'sm1:/dev/nvme0n1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd sm1:/dev/nvme0n1
Inferring fsid 36248874-384b-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/36248874-384b-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Created osd(s) 0 on host 'sm1'

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:03:00,700766893-07:00] INFO: > Sleep for 20 seconds[0m


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:03:20,706517214-07:00] STAGE: Show Cluster Status[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:408 - show_status() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:03:20,716813367-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:03:20,720550843-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', '--status'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status
Inferring fsid 36248874-384b-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/36248874-384b-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
  cluster:
    id:     36248874-384b-11ec-b51d-53e6e728d2d3
    health: HEALTH_WARN
            1 pool(s) have no replicas configured
 
  services:
    mon: 1 daemons, quorum node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us (age 2m)
    mgr: node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us.rhwgvl(active, since 2m)
    osd: 1 osds: 1 up (since 20s), 1 in (since 40s)
 
  data:
    pools:   1 pools, 1 pgs
    objects: 0 objects, 0 B
    usage:   4.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     1 active+clean
 

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:03:29,709452192-07:00] INFO: > Copy configuration files to local[0m
# ./benchmarks/bench-rados:220 - launch_ceph_cluster() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.1 sudo chown --changes --recursive ljishen /tmp/bench-rados/deployment_data_root/etc/ceph
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:03:29,717653626-07:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.1 sudo chown --changes --recursive ljishen /tmp/bench-rados/deployment_data_root/etc/ceph
[1] 17:03:29 [SUCCESS] ljishen@10.10.2.1
# ./benchmarks/bench-rados:223 - launch_ceph_cluster() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.1:/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf
# ./benchmarks/bench-rados:226 - launch_ceph_cluster() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.1:/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:03:30,193566232-07:00] STAGE: Update local ceph.conf as a client...[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:03:30,197193648-07:00] STAGE: Configure Ceph pool...[0m
# ./benchmarks/bench-rados:257 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_max_backfills 32
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:03:30,219071861-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:03:30,222131857-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '36248874-384b-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'config', 'set', 'osd', 'osd_max_backfills', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 36248874-384b-11ec-b51d-53e6e728d2d3 -- ceph config set osd osd_max_backfills 32
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:258 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_active 32
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:03:34,299628024-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:03:34,302624390-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '36248874-384b-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_active', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 36248874-384b-11ec-b51d-53e6e728d2d3 -- ceph config set osd osd_recovery_max_active 32
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:259 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_single_start 8
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:03:38,473275445-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:03:38,476376207-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '36248874-384b-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_single_start', '8'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 36248874-384b-11ec-b51d-53e6e728d2d3 -- ceph config set osd osd_recovery_max_single_start 8
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:260 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_op_priority 63
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:03:42,556170036-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:03:42,559051766-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '36248874-384b-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_op_priority', '63'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 36248874-384b-11ec-b51d-53e6e728d2d3 -- ceph config set osd osd_recovery_op_priority 63
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:262 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config show-with-defaults osd.0
# ./benchmarks/bench-rados:263 - configure_ceph_pool() > grep 'osd_max_backfills\|osd_recovery'
# ./benchmarks/bench-rados:263 - configure_ceph_pool() > column -t -s ' '
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
osd_max_backfills                        32         mon
osd_recovery_cost                        20971520   default
osd_recovery_delay_start                 0.000000   default
osd_recovery_max_active                  32         mon
osd_recovery_max_active_hdd              3          default
osd_recovery_max_active_ssd              10         default
osd_recovery_max_chunk                   8388608    default
osd_recovery_max_omap_entries_per_chunk  8096       default
osd_recovery_max_single_start            8          mon
osd_recovery_op_priority                 63         mon
osd_recovery_op_warn_multiple            16         default
osd_recovery_priority                    5          default
osd_recovery_retry_interval              30.000000  default
osd_recovery_sleep                       0.000000   default
osd_recovery_sleep_hdd                   0.100000   default
osd_recovery_sleep_hybrid                0.025000   default
osd_recovery_sleep_ssd                   0.000000   default
# ./benchmarks/bench-rados:265 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool create bench_rados 128 128 replicated --size 1 --pg-num-min 128
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:03:50,650217392-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:03:50,653130361-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '36248874-384b-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'create', 'bench_rados', '128', '128', 'replicated', '--size', '1', '--pg-num-min', '128'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 36248874-384b-11ec-b51d-53e6e728d2d3 -- ceph osd pool create bench_rados 128 128 replicated --size 1 --pg-num-min 128
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
pool 'bench_rados' created
# ./benchmarks/bench-rados:267 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados min_size 1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:03:55,489459572-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:03:55,492409992-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '36248874-384b-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'min_size', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 36248874-384b-11ec-b51d-53e6e728d2d3 -- ceph osd pool set bench_rados min_size 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 min_size to 1
# ./benchmarks/bench-rados:268 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados pg_autoscale_mode off
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:03:59,681286213-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:03:59,684424156-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '36248874-384b-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'pg_autoscale_mode', 'off'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 36248874-384b-11ec-b51d-53e6e728d2d3 -- ceph osd pool set bench_rados pg_autoscale_mode off
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 pg_autoscale_mode to off
# ./benchmarks/bench-rados:269 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool application enable bench_rados benchmark
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:04:04,380500719-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:04:04,383620097-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '36248874-384b-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'application', 'enable', 'bench_rados', 'benchmark'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 36248874-384b-11ec-b51d-53e6e728d2d3 -- ceph osd pool application enable bench_rados benchmark
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
enabled application 'benchmark' on pool 'bench_rados'
# ./benchmarks/bench-rados:270 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados noscrub 1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:04:08,936928611-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:04:08,940010949-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '36248874-384b-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'noscrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 36248874-384b-11ec-b51d-53e6e728d2d3 -- ceph osd pool set bench_rados noscrub 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 noscrub to 1
# ./benchmarks/bench-rados:271 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados nodeep-scrub 1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:04:14,137159559-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:04:14,140291471-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '36248874-384b-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'nodeep-scrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 36248874-384b-11ec-b51d-53e6e728d2d3 -- ceph osd pool set bench_rados nodeep-scrub 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 nodeep-scrub to 1
# ./benchmarks/bench-rados:272 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool ls detail
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:04:18,411879295-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:04:18,414845484-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '36248874-384b-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'ls', 'detail'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 36248874-384b-11ec-b51d-53e6e728d2d3 -- ceph osd pool ls detail
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
pool 1 'device_health_metrics' replicated size 1 min_size 1 crush_rule 0 object_hash rjenkins pg_num 64 pgp_num 64 autoscale_mode on last_change 18 lfor 0/0/15 flags hashpspool stripe_width 0 pg_num_min 1 application mgr_devicehealth
pool 2 'bench_rados' replicated size 1 min_size 1 crush_rule 0 object_hash rjenkins pg_num 128 pgp_num 128 autoscale_mode off last_change 20 flags hashpspool,noscrub,nodeep-scrub stripe_width 0 pg_num_min 128 application benchmark

# ./benchmarks/bench-rados:273 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd df tree
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:04:22,526613530-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:04:22,529654230-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '36248874-384b-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'df', 'tree'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 36248874-384b-11ec-b51d-53e6e728d2d3 -- ceph osd df tree
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
ID  CLASS  WEIGHT   REWEIGHT  SIZE     RAW USE  DATA     OMAP  META     AVAIL    %USE  VAR   PGS  STATUS  TYPE NAME    
-1         0.39059         -  400 GiB  5.7 MiB  168 KiB   0 B  5.6 MiB  400 GiB  0.00  1.00    -          root default 
-3         0.39059         -  400 GiB  5.7 MiB  168 KiB   0 B  5.6 MiB  400 GiB  0.00  1.00    -              host sm1 
 0    ssd  0.39059   1.00000  400 GiB  5.7 MiB  168 KiB   0 B  5.6 MiB  400 GiB  0.00  1.00  192      up          osd.0
                       TOTAL  400 GiB  5.7 MiB  168 KiB   0 B  5.6 MiB  400 GiB  0.00                                  
MIN/MAX VAR: 1.00/1.00  STDDEV: 0


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:04:26,636487328-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:04:50,644653251-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:04:59,634790549-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:05:08,907055551-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:05:08,915340612-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:05:17,979630321-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:05:17,988712895-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:05:27,129226875-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:05:27,137313101-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:05:36,105136393-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:05:36,113174258-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:05:45,317452987-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:05:45,325367921-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:05:45,331545932-07:00] INFO: > The cluster is idle now.[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:05:45,335276832-07:00] STAGE: Start benchmarking of mode write ...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:05:45,342571026-07:00] INFO: > Start system activity collection services[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:05:45,348255797-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:341 - rados_bench() > rm --force /mnt/sda8/ceph-research/scripts/output/sys_activity_4MB_write.dat.rados_bench_host.1
# ./benchmarks/bench-rados:342 - rados_bench() > sar_pid_rados_bench_host=915353
# ./benchmarks/bench-rados:342 - rados_bench() > S_TIME_FORMAT=ISO
# ./benchmarks/bench-rados:342 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/output/sys_activity_4MB_write.dat.rados_bench_host.1 2

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:05:45,355679556-07:00] INFO: >> for OSD host[0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:05:45,365106087-07:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.5 bash
10.10.2.5: b'332462\n'
[1] 17:05:46 [SUCCESS] ljishen@10.10.2.5
332462

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:05:46,488255967-07:00] INFO: > Run rados bench[0m
# ./benchmarks/bench-rados:370 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/output/rados_bench_4MB_write.log.1
## ./benchmarks/bench-rados:361 - rados_bench() > count_cpus 32-63,96-127
# ./benchmarks/bench-rados:361 - rados_bench() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 60 write --pool bench_rados -b 4194304 -O 4194304 --concurrent-ios 128 --show-time --write-object --write-omap --write-xattr --no-cleanup
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:05:46,509289249-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:05:46,512196187-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '36248874-384b-11ec-b51d-53e6e728d2d3', '--', 'rados', 'bench', '60', 'write', '--pool', 'bench_rados', '-b', '4194304', '-O', '4194304', '--concurrent-ios', '128', '--show-time', '--write-object', '--write-omap', '--write-xattr', '--no-cleanup'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 36248874-384b-11ec-b51d-53e6e728d2d3 -- rados bench 60 write --pool bench_rados -b 4194304 -O 4194304 --concurrent-ios 128 --show-time --write-object --write-omap --write-xattr --no-cleanup
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
hints = 1
2021-10-29T00:05:49.589487+0000 Maintaining 128 concurrent writes of 4194304 bytes to objects of size 4194304 for up to 60 seconds or 0 objects
2021-10-29T00:05:49.589501+0000 Object prefix: benchmark_data_node-2.bluefield2.ucsc-cmps10_7
2021-10-29T00:05:49.714616+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-29T00:05:49.714616+0000     0       0         0         0         0         0           -           0
2021-10-29T00:05:50.714776+0000     1      14        14         0         0         0           -           0
2021-10-29T00:05:51.714863+0000     2      22        22         0         0         0           -           0
2021-10-29T00:05:52.715016+0000     3      30        30         0         0         0           -           0
2021-10-29T00:05:53.715095+0000     4      38        38         0         0         0           -           0
2021-10-29T00:05:54.715211+0000     5      46        46         0         0         0           -           0
2021-10-29T00:05:55.715311+0000     6      54        54         0         0         0           -           0
2021-10-29T00:05:56.715454+0000     7      65        65         0         0         0           -           0
2021-10-29T00:05:57.715554+0000     8      70        70         0         0         0           -           0
2021-10-29T00:05:58.715677+0000     9      81        81         0         0         0           -           0
2021-10-29T00:05:59.715834+0000    10      89        89         0         0         0           -           0
2021-10-29T00:06:00.715951+0000    11      96        96         0         0         0           -           0
2021-10-29T00:06:01.716077+0000    12     104       104         0         0         0           -           0
2021-10-29T00:06:02.716190+0000    13     112       112         0         0         0           -           0
2021-10-29T00:06:03.716270+0000    14     125       125         0         0         0           -           0
2021-10-29T00:06:04.716389+0000    15     127       133         6   1.59982       1.6     14.9213     14.8518
2021-10-29T00:06:05.716482+0000    16     127       135         8   1.99978         8     15.2053     14.9401
2021-10-29T00:06:06.716591+0000    17     127       140        13   3.05848        20     15.9582     15.4285
2021-10-29T00:06:07.716701+0000    18     127       143        16   3.55516        12     16.6576     15.7245
2021-10-29T00:06:08.716776+0000    19     127       143        16   3.36805         0           -     15.7245
2021-10-29T00:06:09.716869+0000 min lat: 14.5065 max lat: 18.4065 avg lat: 16.3
2021-10-29T00:06:09.716869+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-29T00:06:09.716869+0000    20     127       148        21   4.19954        10     17.9645        16.3
2021-10-29T00:06:10.716947+0000    21     127       151        24   4.57094        12     18.5143     16.6327
2021-10-29T00:06:11.717042+0000    22     127       151        24   4.36317         0           -     16.6327
2021-10-29T00:06:12.717197+0000    23     127       151        24   4.17346         0           -     16.6327
2021-10-29T00:06:13.717275+0000    24     127       156        29   4.83281   6.66667     20.0286     17.2551
2021-10-29T00:06:14.717390+0000    25     127       159        32   5.11945        12     20.8044     17.6085
2021-10-29T00:06:15.717507+0000    26     127       159        32   4.92254         0           -     17.6085
2021-10-29T00:06:16.717617+0000    27     127       164        37   5.48089        10     22.1651     18.2552
2021-10-29T00:06:17.717711+0000    28     127       172        45   6.42788        32     22.9193     19.1195
2021-10-29T00:06:18.717827+0000    29     127       180        53   7.30955        32     23.0387     19.7472
2021-10-29T00:06:19.717892+0000    30     127       188        61   8.13246        32     23.2437      20.225
2021-10-29T00:06:20.718006+0000    31     127       199        72   9.28933        44     22.9372      20.703
2021-10-29T00:06:21.718100+0000    32     127       207        80   9.99893        32     22.9617     20.9461
2021-10-29T00:06:22.718217+0000    33     127       215        88   10.6655        32     22.8367     21.1508
2021-10-29T00:06:23.718331+0000    34     127       223        96   11.2929        32     22.9911     21.3168
2021-10-29T00:06:24.718442+0000    35     127       231       104   11.8844        32       22.82     21.4395
2021-10-29T00:06:25.718535+0000    36     127       239       112   12.4431        32      23.169     21.5632
2021-10-29T00:06:26.718647+0000    37     127       247       120   12.9716        32     23.0302     21.6685
2021-10-29T00:06:27.718752+0000    38     127       260       133   13.9985        52     23.0191     21.8094
2021-10-29T00:06:28.718868+0000    39     127       263       136   13.9472        12     23.0776     21.8395
2021-10-29T00:06:29.718961+0000 min lat: 14.5065 max lat: 23.7423 avg lat: 21.8843
2021-10-29T00:06:29.718961+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-29T00:06:29.718961+0000    40     127       268       141   14.0985        20     23.1024     21.8843
2021-10-29T00:06:30.719087+0000    41     127       268       141   13.7546         0           -     21.8843
2021-10-29T00:06:31.719162+0000    42     127       268       141   13.4271         0           -     21.8843
2021-10-29T00:06:32.719283+0000    43     127       271       144   13.3939         4     24.0788       21.93
2021-10-29T00:06:33.719383+0000    44     127       271       144   13.0895         0           -       21.93
2021-10-29T00:06:34.719497+0000    45     127       271       144   12.7986         0           -       21.93
2021-10-29T00:06:35.719610+0000    46     127       276       149   12.9551   6.66667     25.3123     22.0435
2021-10-29T00:06:36.719724+0000    47     127       276       149   12.6795         0           -     22.0435
2021-10-29T00:06:37.719817+0000    48     127       279       152   12.6653         6     26.4081     22.1296
2021-10-29T00:06:38.719929+0000    49     127       279       152   12.4068         0           -     22.1296
2021-10-29T00:06:39.720041+0000    50     127       279       152   12.1587         0           -     22.1296
2021-10-29T00:06:40.720156+0000    51     127       284       157   12.3124   6.66667     27.4026     22.2976
2021-10-29T00:06:41.720258+0000    52     127       284       157   12.0756         0           -     22.2976
2021-10-29T00:06:42.720386+0000    53     127       287       160   12.0742         6     28.7409     22.4181
2021-10-29T00:06:43.720461+0000    54     127       287       160   11.8506         0           -     22.4181
2021-10-29T00:06:44.720572+0000    55     127       287       160   11.6351         0           -     22.4181
2021-10-29T00:06:45.720666+0000    56     127       292       165   11.7845   6.66667      29.823     22.6429
2021-10-29T00:06:46.720785+0000    57     127       292       165   11.5777         0           -     22.6429
2021-10-29T00:06:47.720900+0000    58     127       295       168    11.585         6     29.9366     22.7732
2021-10-29T00:06:48.721014+0000    59     127       300       173   11.7276        20     30.9989     23.0109
2021-10-29T00:06:49.721106+0000 min lat: 14.5065 max lat: 31.2873 avg lat: 23.152
2021-10-29T00:06:49.721106+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-29T00:06:49.721106+0000    60     127       303       176   11.7321        12     31.2863      23.152
2021-10-29T00:06:50.721233+0000    61       4       304       300     19.67       496     2.00273     22.8725
2021-10-29T00:06:51.721371+0000 Total time run:         61.8216
Total writes made:      304
Write size:             4194304
Object size:            4194304
Bandwidth (MB/sec):     19.6695
Stddev Bandwidth:       63.6789
Max bandwidth (MB/sec): 496
Min bandwidth (MB/sec): 0
Average IOPS:           4
Stddev IOPS:            15.9371
Max IOPS:               124
Min IOPS:               0
Average Latency(s):     22.6018
Stddev Latency(s):      6.52624
Max latency(s):         31.9728
Min latency(s):         2.00273

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:06:52,579387311-07:00] INFO: > Stop system activity collection services[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:06:52,585523684-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:391 - rados_bench() > kill -INT 915353

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:06:52,591237680-07:00] INFO: >> for OSD host[0m
# ./benchmarks/bench-rados:396 - rados_bench() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.5 kill -INT 332462
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:06:52,599217066-07:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.5 kill -INT 332462
[1] 17:06:53 [SUCCESS] ljishen@10.10.2.5

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:06:53,870059250-07:00] INFO: >>> Copy system activity file from OSD host[0m
## ./benchmarks/bench-rados:403 - rados_bench() > basename -- /tmp/bench-rados/sys_activity_4MB_write.dat.osd_host.1
# ./benchmarks/bench-rados:403 - rados_bench() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.5:/tmp/bench-rados/sys_activity_4MB_write.dat.osd_host.1 /mnt/sda8/ceph-research/scripts/output/sys_activity_4MB_write.dat.osd_host.1


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:06:55,149733938-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:07:19,058602366-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 305 objects, 1.2 GiB
    usage:   3.6 GiB used, 396 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:07:19,067121208-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:07:28,081728721-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 305 objects, 1.2 GiB
    usage:   3.6 GiB used, 396 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:07:28,089732202-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:07:37,138409647-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 305 objects, 1.2 GiB
    usage:   3.6 GiB used, 396 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:07:37,147509373-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:07:46,127792759-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 305 objects, 1.2 GiB
    usage:   3.6 GiB used, 396 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:07:46,136316370-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:07:55,258851581-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 305 objects, 1.2 GiB
    usage:   3.6 GiB used, 396 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:07:55,266221007-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:07:55,272286446-07:00] INFO: > The cluster is idle now.[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:07:55,276158842-07:00] STAGE: Start benchmarking of mode seq ...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:07:55,283158301-07:00] INFO: > Start system activity collection services[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:07:55,288978959-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:341 - rados_bench() > rm --force /mnt/sda8/ceph-research/scripts/output/sys_activity_4MB_seq.dat.rados_bench_host.1
# ./benchmarks/bench-rados:342 - rados_bench() > sar_pid_rados_bench_host=919099
# ./benchmarks/bench-rados:342 - rados_bench() > S_TIME_FORMAT=ISO
# ./benchmarks/bench-rados:342 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/output/sys_activity_4MB_seq.dat.rados_bench_host.1 2

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:07:55,296127849-07:00] INFO: >> for OSD host[0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:07:55,305063577-07:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.5 bash
10.10.2.5: b'333009\n'
[1] 17:07:56 [SUCCESS] ljishen@10.10.2.5
333009

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:07:56,431881897-07:00] INFO: > Run rados bench[0m
# ./benchmarks/bench-rados:379 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/output/rados_bench_4MB_seq.log.1
## ./benchmarks/bench-rados:376 - rados_bench() > count_cpus 32-63,96-127
# ./benchmarks/bench-rados:376 - rados_bench() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 128 --show-time
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:07:56,452452387-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:07:56,455340589-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '36248874-384b-11ec-b51d-53e6e728d2d3', '--', 'rados', 'bench', '99999999', 'seq', '--pool', 'bench_rados', '--concurrent-ios', '128', '--show-time'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 36248874-384b-11ec-b51d-53e6e728d2d3 -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 128 --show-time
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
hints = 1
2021-10-29T00:07:59.473246+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-29T00:07:59.473246+0000     0       0         0         0         0         0           -           0
2021-10-29T00:08:00.473362+0000     1      57        57         0         0         0           -           0
2021-10-29T00:08:01.473491+0000     2      97        97         0         0         0           -           0
2021-10-29T00:08:02.473610+0000     3     127       136         9   11.9983        12     2.99697      2.8997
2021-10-29T00:08:03.473769+0000     4     127       172        45   44.9933       144     3.31399     3.21649
2021-10-29T00:08:04.473904+0000     5     127       210        83   66.3903       152     3.36342     3.25886
2021-10-29T00:08:05.474042+0000     6     127       248       121    80.655       152     3.42971     3.29591
2021-10-29T00:08:06.474174+0000     7     127       285       158   90.2728       148     3.45087     3.32284
2021-10-29T00:08:07.474341+0000     8      10       304       294   146.979       544     1.83744     2.72845
2021-10-29T00:08:08.474491+0000 Total time run:       8.42426
Total reads made:     304
Read size:            4194304
Object size:          4194304
Bandwidth (MB/sec):   144.345
Average IOPS:         36
Stddev IOPS:          44.2235
Max IOPS:             136
Min IOPS:             0
Average Latency(s):   2.68417
Max latency(s):       3.47082
Min latency(s):       0.303239

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:08:09,121956273-07:00] INFO: > Stop system activity collection services[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:08:09,128619018-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:391 - rados_bench() > kill -INT 919099

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:08:09,134920952-07:00] INFO: >> for OSD host[0m
# ./benchmarks/bench-rados:396 - rados_bench() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.5 kill -INT 333009
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:08:09,143040011-07:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.5 kill -INT 333009
[1] 17:08:10 [SUCCESS] ljishen@10.10.2.5

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:08:10,228699368-07:00] INFO: >>> Copy system activity file from OSD host[0m
## ./benchmarks/bench-rados:403 - rados_bench() > basename -- /tmp/bench-rados/sys_activity_4MB_seq.dat.osd_host.1
# ./benchmarks/bench-rados:403 - rados_bench() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.5:/tmp/bench-rados/sys_activity_4MB_seq.dat.osd_host.1 /mnt/sda8/ceph-research/scripts/output/sys_activity_4MB_seq.dat.osd_host.1


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:08:11,300927497-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:08:35,233042484-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 305 objects, 1.2 GiB
    usage:   3.6 GiB used, 396 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:08:35,240931229-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:08:44,356033589-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 305 objects, 1.2 GiB
    usage:   3.6 GiB used, 396 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:08:44,364228380-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:08:53,338776274-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 305 objects, 1.2 GiB
    usage:   3.6 GiB used, 396 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:08:53,347098625-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:09:02,404739332-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 305 objects, 1.2 GiB
    usage:   3.6 GiB used, 396 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:09:02,412900931-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:09:11,486200318-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 305 objects, 1.2 GiB
    usage:   3.6 GiB used, 396 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:09:11,494451445-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:09:11,500593187-07:00] INFO: > The cluster is idle now.[0m


[1;7;39;49m[2021-10-28T17:09:11,503191754-07:00][RUNNING][ROUND 2/6/21] object_size=4MB[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:09:11,507035286-07:00] STAGE: Launch a Ceph cluster (CEPH_HOST: 10.10.2.1, OSD_HOST: 10.10.2.5) ...[0m
# ./benchmarks/bench-rados:183 - launch_ceph_cluster() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.1 bash -euo pipefail
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:09:11,516452361-07:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.1 bash -euo pipefail
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T17:09:11,924793520-07:00] INFO: > Remove existing clusters\x1b[0m\n'
10.10.2.1: b'## bash:17 -  > basename -- /var/lib/ceph/36248874-384b-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'# bash:17 -  > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm --host 10.10.2.1 --host 10.10.2.5 rm-cluster --force --fsid 36248874-384b-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T17:09:11,936106484-07:00] DEBUG: HOST_PARAMS: ['--host', '10.10.2.1', '--host', '10.10.2.5']\x1b[0m\n"
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T17:09:11,939894234-07:00] DEBUG: CEPHADM_PARAMS: ['rm-cluster', '--force', '--fsid', '36248874-384b-11ec-b51d-53e6e728d2d3']\x1b[0m\n"
10.10.2.1: b"# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:63 -  > /tmp/bench-rados/ceph-research/scripts/parallel-ssh --host 10.10.2.1 --host 10.10.2.5 'sudo python3 -u - rm-cluster --force --fsid 36248874-384b-11ec-b51d-53e6e728d2d3'\n"
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T17:09:11,948603544-07:00] DEBUG: command from stdin\x1b[0m\n'
10.10.2.1: b"# /tmp/bench-rados/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host 10.10.2.1 --host 10.10.2.5 'sudo python3 -u - rm-cluster --force --fsid 36248874-384b-11ec-b51d-53e6e728d2d3'\n"
10.10.2.1: b'[1] 17:09:17 [SUCCESS] 10.10.2.1\n[2] 17:09:20 [SUCCESS] 10.10.2.5\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T17:09:20,855702479-07:00] INFO: > Deploy a new cluster\x1b[0m\n'
10.10.2.1: b'# bash:24 -  > /tmp/bench-rados/ceph-research/scripts/ceph-deploy -m 10.10.2.1 -o sm1/10.10.2.5:/dev/nvme0n1\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T17:09:20,868969777-07:00] STAGE: Verify passwordless SSH to hosts, passwordless sudo via ljishen, and the existence of devices on hosts...\x1b[0m\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T17:09:20,873521913-07:00] INFO: > Check the host for the monitor\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:181 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@10.10.2.1 sudo --non-interactive --validate\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T17:09:21,022874409-07:00] INFO: > Check hosts for OSDs\x1b[0m\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T17:09:21,027424332-07:00] INFO: >> Check host 'sm1'\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:200 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 sudo --non-interactive --validate\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T17:09:22,102886596-07:00] INFO: >>> Check device '/dev/nvme0n1'\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:215 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 test -b /dev/nvme0n1\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:226 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 lsblk --nodeps --noheadings --output TYPE /dev/nvme0n1\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:227 - verify_parameters() > grep --word-regexp disk\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T17:09:24,247732109-07:00] STAGE: Reset devices on hosts...\x1b[0m\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T17:09:24,252649572-07:00] INFO: > Resetting device '/dev/nvme0n1' on host 'sm1'\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:288 - reset_host_devices() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 sudo sh\n'
10.10.2.1: b'  Removing ceph--6d869a22--9e45--4df6--90e3--65aa63361a72-osd--block--154a0205--4f1b--4e2f--b676--7921d445330e (252:0)\n'
10.10.2.1: b'  Archiving volume group "ceph-6d869a22-9e45-4df6-90e3-65aa63361a72" metadata (seqno 5).\n'
10.10.2.1: b'  Releasing logical volume "osd-block-154a0205-4f1b-4e2f-b676-7921d445330e"\n'
10.10.2.1: b'  Creating volume group backup "/etc/lvm/backup/ceph-6d869a22-9e45-4df6-90e3-65aa63361a72" (seqno 6).\n'
10.10.2.1: b'  Logical volume "osd-block-154a0205-4f1b-4e2f-b676-7921d445330e" successfully removed\n'
10.10.2.1: b'  Removing physical volume "/dev/nvme0n1" from volume group "ceph-6d869a22-9e45-4df6-90e3-65aa63361a72"\n'
10.10.2.1: b'  Volume group "ceph-6d869a22-9e45-4df6-90e3-65aa63361a72" successfully removed\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T17:09:26,587152008-07:00] STAGE: Bootstrap a new cluster...\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:317 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.1 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T17:09:26,597021149-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T17:09:26,600897886-07:00] DEBUG: CEPHADM_PARAMS: ['bootstrap', '--config', '/tmp/bench-rados/ceph-research/scripts/ceph.conf', '--mon-ip', '10.10.2.1', '--output-dir', '/tmp/bench-rados/deployment_data_root/etc/ceph', '--ssh-user', 'ljishen', '--skip-dashboard', '--no-minimize-config', '--allow-overwrite', '--allow-fqdn-hostname', '--orphan-initial-daemons', '--skip-monitoring-stack', '--single-host-defaults']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.1 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults\n'
10.10.2.1: b'Verifying podman|docker is present...\n'
10.10.2.1: b'Verifying lvm2 is present...\n'
10.10.2.1: b'Verifying time synchronization is in place...\n'
10.10.2.1: b'Unit ntp.service is enabled and running\n'
10.10.2.1: b'Repeating the final host check...\n'
10.10.2.1: b'podman|docker (/usr/bin/docker) is present\n'
10.10.2.1: b'systemctl is present\n'
10.10.2.1: b'lvcreate is present\n'
10.10.2.1: b'Unit ntp.service is enabled and running\n'
10.10.2.1: b'Host looks OK\n'
10.10.2.1: b'Cluster fsid: 7aa8adf8-384c-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Verifying IP 10.10.2.1 port 3300 ...\n'
10.10.2.1: b'Verifying IP 10.10.2.1 port 6789 ...\n'
10.10.2.1: b'Mon IP `10.10.2.1` is in CIDR network `10.10.2.0/24`\n'
10.10.2.1: b'- internal network (--cluster-network) has not been provided, OSD replication will default to the public_network\n'
10.10.2.1: b'Adjusting default settings to suit single-host cluster...\n'
10.10.2.1: b'Pulling container image quay.io/ceph/ceph:v16...\n'
10.10.2.1: b'Ceph version: ceph version 16.2.6 (ee28fb57e47e9f88813e24bbf4c14496ca299d31) pacific (stable)\n'
10.10.2.1: b'Extracting ceph user uid/gid from container image...\n'
10.10.2.1: b'Creating initial keys...\n'
10.10.2.1: b'Creating initial monmap...\n'
10.10.2.1: b'Creating mon...\n'
10.10.2.1: b'Waiting for mon to start...\n'
10.10.2.1: b'Waiting for mon...\n'
10.10.2.1: b'mon is available\nSetting mon public_network to 10.10.2.0/24\n'
10.10.2.1: b'Wrote config to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf\n'
10.10.2.1: b'Wrote keyring to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring\n'
10.10.2.1: b'Creating mgr...\n'
10.10.2.1: b'Verifying port 9283 ...\n'
10.10.2.1: b'Waiting for mgr to start...\nWaiting for mgr...\n'
10.10.2.1: b'mgr not available, waiting (1/15)...\n'
10.10.2.1: b'mgr not available, waiting (2/15)...\n'
10.10.2.1: b'mgr is available\n'
10.10.2.1: b'Enabling cephadm module...\n'
10.10.2.1: b'Waiting for the mgr to restart...\nWaiting for mgr epoch 5...\n'
10.10.2.1: b'mgr epoch 5 is available\nSetting orchestrator backend to cephadm...\n'
10.10.2.1: b'Generating ssh key...\n'
10.10.2.1: b'Wrote public SSH key to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub\n'
10.10.2.1: b'Adding key to ljishen@localhost authorized_keys...\n'
10.10.2.1: b'Adding host node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us...\n'
10.10.2.1: b'Deploying unmanaged mon service...\n'
10.10.2.1: b'Deploying unmanaged mgr service...\n'
10.10.2.1: b'You can access the Ceph CLI with:\n\n\tsudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --fsid 7aa8adf8-384c-11ec-b51d-53e6e728d2d3 -c /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf -k /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring\n\nPlease consider enabling telemetry to help improve Ceph:\n\n\tceph telemetry on\n\nFor more information see:\n\n\thttps://docs.ceph.com/docs/pacific/mgr/telemetry/\n\n'
10.10.2.1: b'Bootstrap complete.\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T17:10:29,253735394-07:00] INFO: Sleep for 20 seconds\x1b[0m\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T17:10:49,260758204-07:00] INFO: > Set the default number of monitor daemons to 1\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:337 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T17:10:49,270869660-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T17:10:49,274877423-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mon', '1']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1\n'
10.10.2.1: b'Inferring fsid 7aa8adf8-384c-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/7aa8adf8-384c-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b'Scheduled mon update...\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T17:10:58,392809593-07:00] INFO: > Set the default number of manager daemons to 1\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:342 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T17:10:58,402100577-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T17:10:58,405777628-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mgr', '1']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1\n'
10.10.2.1: b'Inferring fsid 7aa8adf8-384c-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/7aa8adf8-384c-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b'Scheduled mgr update...\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T17:11:08,004768230-07:00] STAGE: Add hosts to the cluster...\x1b[0m\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T17:11:08,011281383-07:00] INFO: > Adding host 'sm1'\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:361 - add_hosts() > ssh-copy-id -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -f -i /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub ljishen@sm1\n'
10.10.2.1: b'/usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: "/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub"\n'
10.10.2.1: b'\nNumber of key(s) added: 1\n\nNow try logging into the machine, with:   "ssh -o \'GlobalKnownHostsFile=/dev/null\' -o \'LogLevel=ERROR\' -o \'PasswordAuthentication=no\' -o \'StrictHostKeyChecking=no\' -o \'UserKnownHostsFile=/dev/null\' \'ljishen@sm1\'"\nand check to make sure that only the key(s) you wanted were added.\n\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:367 - add_hosts() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add sm1 10.10.2.5\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T17:11:09,176188302-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T17:11:09,179898916-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'host', 'add', 'sm1', '10.10.2.5']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add sm1 10.10.2.5\n'
10.10.2.1: b'Inferring fsid 7aa8adf8-384c-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/7aa8adf8-384c-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b"Added host 'sm1' with addr '10.10.2.5'\n"
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T17:11:20,049561058-07:00] INFO: Sleep for 20 seconds\x1b[0m\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T17:11:40,054296008-07:00] STAGE: Add OSDs to the cluster...\x1b[0m\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T17:11:40,061025627-07:00] INFO: > Adding devices ['/dev/nvme0n1'] on host 'sm1' as OSDs\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:388 - add_host_devices_as_osds() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd sm1:/dev/nvme0n1\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T17:11:40,071106586-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T17:11:40,074957534-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'daemon', 'add', 'osd', 'sm1:/dev/nvme0n1']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd sm1:/dev/nvme0n1\n'
10.10.2.1: b'Inferring fsid 7aa8adf8-384c-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/7aa8adf8-384c-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b"Created osd(s) 0 on host 'sm1'\n"
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T17:12:05,373660779-07:00] INFO: > Sleep for 20 seconds\x1b[0m\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T17:12:25,379381099-07:00] STAGE: Show Cluster Status\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:408 - show_status() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T17:12:25,389161533-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T17:12:25,392743395-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', '--status']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status\n'
10.10.2.1: b'Inferring fsid 7aa8adf8-384c-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/7aa8adf8-384c-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b'  cluster:\n    id:     7aa8adf8-384c-11ec-b51d-53e6e728d2d3\n    health: HEALTH_WARN\n            1 pool(s) have no replicas configured\n \n  services:\n    mon: 1 daemons, quorum node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us (age 2m)\n    mgr: node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us.vwsnhw(active, since 2m)\n    osd: 1 osds: 1 up (since 20s), 1 in (since 40s)\n \n  data:\n    pools:   1 pools, 1 pgs\n    objects: 0 objects, 0 B\n    usage:   4.7 MiB used, 400 GiB / 400 GiB avail\n    pgs:     1 active+clean\n \n'
[1] 17:12:33 [SUCCESS] ljishen@10.10.2.1

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:09:11,924793520-07:00] INFO: > Remove existing clusters[0m
## bash:17 -  > basename -- /var/lib/ceph/36248874-384b-11ec-b51d-53e6e728d2d3
# bash:17 -  > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm --host 10.10.2.1 --host 10.10.2.5 rm-cluster --force --fsid 36248874-384b-11ec-b51d-53e6e728d2d3
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:09:11,936106484-07:00] DEBUG: HOST_PARAMS: ['--host', '10.10.2.1', '--host', '10.10.2.5'][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:09:11,939894234-07:00] DEBUG: CEPHADM_PARAMS: ['rm-cluster', '--force', '--fsid', '36248874-384b-11ec-b51d-53e6e728d2d3'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:63 -  > /tmp/bench-rados/ceph-research/scripts/parallel-ssh --host 10.10.2.1 --host 10.10.2.5 'sudo python3 -u - rm-cluster --force --fsid 36248874-384b-11ec-b51d-53e6e728d2d3'
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:09:11,948603544-07:00] DEBUG: command from stdin[0m
# /tmp/bench-rados/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host 10.10.2.1 --host 10.10.2.5 'sudo python3 -u - rm-cluster --force --fsid 36248874-384b-11ec-b51d-53e6e728d2d3'
[1] 17:09:17 [SUCCESS] 10.10.2.1
[2] 17:09:20 [SUCCESS] 10.10.2.5

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:09:20,855702479-07:00] INFO: > Deploy a new cluster[0m
# bash:24 -  > /tmp/bench-rados/ceph-research/scripts/ceph-deploy -m 10.10.2.1 -o sm1/10.10.2.5:/dev/nvme0n1


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:09:20,868969777-07:00] STAGE: Verify passwordless SSH to hosts, passwordless sudo via ljishen, and the existence of devices on hosts...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:09:20,873521913-07:00] INFO: > Check the host for the monitor[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:181 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@10.10.2.1 sudo --non-interactive --validate

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:09:21,022874409-07:00] INFO: > Check hosts for OSDs[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:09:21,027424332-07:00] INFO: >> Check host 'sm1'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:200 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 sudo --non-interactive --validate

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:09:22,102886596-07:00] INFO: >>> Check device '/dev/nvme0n1'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:215 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 test -b /dev/nvme0n1
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:226 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 lsblk --nodeps --noheadings --output TYPE /dev/nvme0n1
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:227 - verify_parameters() > grep --word-regexp disk


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:09:24,247732109-07:00] STAGE: Reset devices on hosts...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:09:24,252649572-07:00] INFO: > Resetting device '/dev/nvme0n1' on host 'sm1'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:288 - reset_host_devices() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 sudo sh
  Removing ceph--6d869a22--9e45--4df6--90e3--65aa63361a72-osd--block--154a0205--4f1b--4e2f--b676--7921d445330e (252:0)
  Archiving volume group "ceph-6d869a22-9e45-4df6-90e3-65aa63361a72" metadata (seqno 5).
  Releasing logical volume "osd-block-154a0205-4f1b-4e2f-b676-7921d445330e"
  Creating volume group backup "/etc/lvm/backup/ceph-6d869a22-9e45-4df6-90e3-65aa63361a72" (seqno 6).
  Logical volume "osd-block-154a0205-4f1b-4e2f-b676-7921d445330e" successfully removed
  Removing physical volume "/dev/nvme0n1" from volume group "ceph-6d869a22-9e45-4df6-90e3-65aa63361a72"
  Volume group "ceph-6d869a22-9e45-4df6-90e3-65aa63361a72" successfully removed


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:09:26,587152008-07:00] STAGE: Bootstrap a new cluster...[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:317 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.1 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:09:26,597021149-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:09:26,600897886-07:00] DEBUG: CEPHADM_PARAMS: ['bootstrap', '--config', '/tmp/bench-rados/ceph-research/scripts/ceph.conf', '--mon-ip', '10.10.2.1', '--output-dir', '/tmp/bench-rados/deployment_data_root/etc/ceph', '--ssh-user', 'ljishen', '--skip-dashboard', '--no-minimize-config', '--allow-overwrite', '--allow-fqdn-hostname', '--orphan-initial-daemons', '--skip-monitoring-stack', '--single-host-defaults'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.1 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults
Verifying podman|docker is present...
Verifying lvm2 is present...
Verifying time synchronization is in place...
Unit ntp.service is enabled and running
Repeating the final host check...
podman|docker (/usr/bin/docker) is present
systemctl is present
lvcreate is present
Unit ntp.service is enabled and running
Host looks OK
Cluster fsid: 7aa8adf8-384c-11ec-b51d-53e6e728d2d3
Verifying IP 10.10.2.1 port 3300 ...
Verifying IP 10.10.2.1 port 6789 ...
Mon IP `10.10.2.1` is in CIDR network `10.10.2.0/24`
- internal network (--cluster-network) has not been provided, OSD replication will default to the public_network
Adjusting default settings to suit single-host cluster...
Pulling container image quay.io/ceph/ceph:v16...
Ceph version: ceph version 16.2.6 (ee28fb57e47e9f88813e24bbf4c14496ca299d31) pacific (stable)
Extracting ceph user uid/gid from container image...
Creating initial keys...
Creating initial monmap...
Creating mon...
Waiting for mon to start...
Waiting for mon...
mon is available
Setting mon public_network to 10.10.2.0/24
Wrote config to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf
Wrote keyring to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring
Creating mgr...
Verifying port 9283 ...
Waiting for mgr to start...
Waiting for mgr...
mgr not available, waiting (1/15)...
mgr not available, waiting (2/15)...
mgr is available
Enabling cephadm module...
Waiting for the mgr to restart...
Waiting for mgr epoch 5...
mgr epoch 5 is available
Setting orchestrator backend to cephadm...
Generating ssh key...
Wrote public SSH key to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub
Adding key to ljishen@localhost authorized_keys...
Adding host node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us...
Deploying unmanaged mon service...
Deploying unmanaged mgr service...
You can access the Ceph CLI with:

	sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --fsid 7aa8adf8-384c-11ec-b51d-53e6e728d2d3 -c /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf -k /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring

Please consider enabling telemetry to help improve Ceph:

	ceph telemetry on

For more information see:

	https://docs.ceph.com/docs/pacific/mgr/telemetry/

Bootstrap complete.

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:10:29,253735394-07:00] INFO: Sleep for 20 seconds[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:10:49,260758204-07:00] INFO: > Set the default number of monitor daemons to 1[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:337 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:10:49,270869660-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:10:49,274877423-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mon', '1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1
Inferring fsid 7aa8adf8-384c-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/7aa8adf8-384c-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Scheduled mon update...

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:10:58,392809593-07:00] INFO: > Set the default number of manager daemons to 1[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:342 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:10:58,402100577-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:10:58,405777628-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mgr', '1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1
Inferring fsid 7aa8adf8-384c-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/7aa8adf8-384c-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Scheduled mgr update...


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:11:08,004768230-07:00] STAGE: Add hosts to the cluster...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:11:08,011281383-07:00] INFO: > Adding host 'sm1'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:361 - add_hosts() > ssh-copy-id -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -f -i /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub ljishen@sm1
/usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: "/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub"

Number of key(s) added: 1

Now try logging into the machine, with:   "ssh -o 'GlobalKnownHostsFile=/dev/null' -o 'LogLevel=ERROR' -o 'PasswordAuthentication=no' -o 'StrictHostKeyChecking=no' -o 'UserKnownHostsFile=/dev/null' 'ljishen@sm1'"
and check to make sure that only the key(s) you wanted were added.

# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:367 - add_hosts() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add sm1 10.10.2.5
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:11:09,176188302-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:11:09,179898916-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'host', 'add', 'sm1', '10.10.2.5'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add sm1 10.10.2.5
Inferring fsid 7aa8adf8-384c-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/7aa8adf8-384c-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Added host 'sm1' with addr '10.10.2.5'

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:11:20,049561058-07:00] INFO: Sleep for 20 seconds[0m


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:11:40,054296008-07:00] STAGE: Add OSDs to the cluster...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:11:40,061025627-07:00] INFO: > Adding devices ['/dev/nvme0n1'] on host 'sm1' as OSDs[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:388 - add_host_devices_as_osds() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd sm1:/dev/nvme0n1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:11:40,071106586-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:11:40,074957534-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'daemon', 'add', 'osd', 'sm1:/dev/nvme0n1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd sm1:/dev/nvme0n1
Inferring fsid 7aa8adf8-384c-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/7aa8adf8-384c-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Created osd(s) 0 on host 'sm1'

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:12:05,373660779-07:00] INFO: > Sleep for 20 seconds[0m


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:12:25,379381099-07:00] STAGE: Show Cluster Status[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:408 - show_status() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:12:25,389161533-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:12:25,392743395-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', '--status'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status
Inferring fsid 7aa8adf8-384c-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/7aa8adf8-384c-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
  cluster:
    id:     7aa8adf8-384c-11ec-b51d-53e6e728d2d3
    health: HEALTH_WARN
            1 pool(s) have no replicas configured
 
  services:
    mon: 1 daemons, quorum node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us (age 2m)
    mgr: node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us.vwsnhw(active, since 2m)
    osd: 1 osds: 1 up (since 20s), 1 in (since 40s)
 
  data:
    pools:   1 pools, 1 pgs
    objects: 0 objects, 0 B
    usage:   4.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     1 active+clean
 

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:12:33,756020435-07:00] INFO: > Copy configuration files to local[0m
# ./benchmarks/bench-rados:220 - launch_ceph_cluster() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.1 sudo chown --changes --recursive ljishen /tmp/bench-rados/deployment_data_root/etc/ceph
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:12:33,764054143-07:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.1 sudo chown --changes --recursive ljishen /tmp/bench-rados/deployment_data_root/etc/ceph
[1] 17:12:33 [SUCCESS] ljishen@10.10.2.1
# ./benchmarks/bench-rados:223 - launch_ceph_cluster() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.1:/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf
# ./benchmarks/bench-rados:226 - launch_ceph_cluster() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.1:/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:12:34,241389285-07:00] STAGE: Update local ceph.conf as a client...[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:12:34,245250089-07:00] STAGE: Configure Ceph pool...[0m
# ./benchmarks/bench-rados:257 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_max_backfills 32
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:12:34,267666799-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:12:34,270477384-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '7aa8adf8-384c-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'config', 'set', 'osd', 'osd_max_backfills', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 7aa8adf8-384c-11ec-b51d-53e6e728d2d3 -- ceph config set osd osd_max_backfills 32
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:258 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_active 32
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:12:38,410868829-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:12:38,413974330-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '7aa8adf8-384c-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_active', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 7aa8adf8-384c-11ec-b51d-53e6e728d2d3 -- ceph config set osd osd_recovery_max_active 32
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:259 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_single_start 8
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:12:42,661450693-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:12:42,664880696-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '7aa8adf8-384c-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_single_start', '8'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 7aa8adf8-384c-11ec-b51d-53e6e728d2d3 -- ceph config set osd osd_recovery_max_single_start 8
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:260 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_op_priority 63
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:12:46,654894640-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:12:46,658103045-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '7aa8adf8-384c-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_op_priority', '63'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 7aa8adf8-384c-11ec-b51d-53e6e728d2d3 -- ceph config set osd osd_recovery_op_priority 63
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:262 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config show-with-defaults osd.0
# ./benchmarks/bench-rados:263 - configure_ceph_pool() > grep 'osd_max_backfills\|osd_recovery'
# ./benchmarks/bench-rados:263 - configure_ceph_pool() > column -t -s ' '
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
osd_max_backfills                        32         mon
osd_recovery_cost                        20971520   default
osd_recovery_delay_start                 0.000000   default
osd_recovery_max_active                  32         mon
osd_recovery_max_active_hdd              3          default
osd_recovery_max_active_ssd              10         default
osd_recovery_max_chunk                   8388608    default
osd_recovery_max_omap_entries_per_chunk  8096       default
osd_recovery_max_single_start            8          mon
osd_recovery_op_priority                 63         mon
osd_recovery_op_warn_multiple            16         default
osd_recovery_priority                    5          default
osd_recovery_retry_interval              30.000000  default
osd_recovery_sleep                       0.000000   default
osd_recovery_sleep_hdd                   0.100000   default
osd_recovery_sleep_hybrid                0.025000   default
osd_recovery_sleep_ssd                   0.000000   default
# ./benchmarks/bench-rados:265 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool create bench_rados 128 128 replicated --size 1 --pg-num-min 128
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:12:54,906429123-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:12:54,909368071-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '7aa8adf8-384c-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'create', 'bench_rados', '128', '128', 'replicated', '--size', '1', '--pg-num-min', '128'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 7aa8adf8-384c-11ec-b51d-53e6e728d2d3 -- ceph osd pool create bench_rados 128 128 replicated --size 1 --pg-num-min 128
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
pool 'bench_rados' created
# ./benchmarks/bench-rados:267 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados min_size 1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:12:59,795742178-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:12:59,798940956-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '7aa8adf8-384c-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'min_size', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 7aa8adf8-384c-11ec-b51d-53e6e728d2d3 -- ceph osd pool set bench_rados min_size 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 min_size to 1
# ./benchmarks/bench-rados:268 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados pg_autoscale_mode off
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:13:04,152528727-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:13:04,155465841-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '7aa8adf8-384c-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'pg_autoscale_mode', 'off'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 7aa8adf8-384c-11ec-b51d-53e6e728d2d3 -- ceph osd pool set bench_rados pg_autoscale_mode off
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 pg_autoscale_mode to off
# ./benchmarks/bench-rados:269 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool application enable bench_rados benchmark
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:13:08,776765648-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:13:08,779719654-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '7aa8adf8-384c-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'application', 'enable', 'bench_rados', 'benchmark'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 7aa8adf8-384c-11ec-b51d-53e6e728d2d3 -- ceph osd pool application enable bench_rados benchmark
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
enabled application 'benchmark' on pool 'bench_rados'
# ./benchmarks/bench-rados:270 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados noscrub 1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:13:13,433061267-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:13:13,436084804-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '7aa8adf8-384c-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'noscrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 7aa8adf8-384c-11ec-b51d-53e6e728d2d3 -- ceph osd pool set bench_rados noscrub 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 noscrub to 1
# ./benchmarks/bench-rados:271 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados nodeep-scrub 1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:13:18,498142413-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:13:18,501269145-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '7aa8adf8-384c-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'nodeep-scrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 7aa8adf8-384c-11ec-b51d-53e6e728d2d3 -- ceph osd pool set bench_rados nodeep-scrub 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 nodeep-scrub to 1
# ./benchmarks/bench-rados:272 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool ls detail
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:13:23,122640399-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:13:23,125604524-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '7aa8adf8-384c-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'ls', 'detail'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 7aa8adf8-384c-11ec-b51d-53e6e728d2d3 -- ceph osd pool ls detail
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
pool 1 'device_health_metrics' replicated size 1 min_size 1 crush_rule 0 object_hash rjenkins pg_num 64 pgp_num 64 autoscale_mode on last_change 18 lfor 0/0/15 flags hashpspool stripe_width 0 pg_num_min 1 application mgr_devicehealth
pool 2 'bench_rados' replicated size 1 min_size 1 crush_rule 0 object_hash rjenkins pg_num 128 pgp_num 128 autoscale_mode off last_change 20 flags hashpspool,noscrub,nodeep-scrub stripe_width 0 pg_num_min 128 application benchmark

# ./benchmarks/bench-rados:273 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd df tree
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:13:27,179404057-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:13:27,182400533-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '7aa8adf8-384c-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'df', 'tree'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 7aa8adf8-384c-11ec-b51d-53e6e728d2d3 -- ceph osd df tree
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
ID  CLASS  WEIGHT   REWEIGHT  SIZE     RAW USE  DATA     OMAP  META     AVAIL    %USE  VAR   PGS  STATUS  TYPE NAME    
-1         0.39059         -  400 GiB  5.7 MiB  168 KiB   0 B  5.6 MiB  400 GiB  0.00  1.00    -          root default 
-3         0.39059         -  400 GiB  5.7 MiB  168 KiB   0 B  5.6 MiB  400 GiB  0.00  1.00    -              host sm1 
 0    ssd  0.39059   1.00000  400 GiB  5.7 MiB  168 KiB   0 B  5.6 MiB  400 GiB  0.00  1.00  192      up          osd.0
                       TOTAL  400 GiB  5.7 MiB  168 KiB   0 B  5.6 MiB  400 GiB  0.00                                  
MIN/MAX VAR: 1.00/1.00  STDDEV: 0


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:13:31,057305598-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:13:55,037993120-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:14:04,136417512-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:14:13,160377786-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:14:13,168822298-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:14:22,166587708-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:14:22,174422220-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:14:31,249425993-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:14:31,257654217-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:14:40,300715115-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:14:40,308774291-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:14:49,412997931-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:14:49,421432404-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:14:49,427705414-07:00] INFO: > The cluster is idle now.[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:14:49,431634908-07:00] STAGE: Start benchmarking of mode write ...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:14:49,439135962-07:00] INFO: > Start system activity collection services[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:14:49,445320346-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:341 - rados_bench() > rm --force /mnt/sda8/ceph-research/scripts/output/sys_activity_4MB_write.dat.rados_bench_host.2
# ./benchmarks/bench-rados:342 - rados_bench() > sar_pid_rados_bench_host=926745
# ./benchmarks/bench-rados:342 - rados_bench() > S_TIME_FORMAT=ISO
# ./benchmarks/bench-rados:342 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/output/sys_activity_4MB_write.dat.rados_bench_host.2 2

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:14:49,452367665-07:00] INFO: >> for OSD host[0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:14:49,461501095-07:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.5 bash
10.10.2.5: b'337896\n'
[1] 17:14:50 [SUCCESS] ljishen@10.10.2.5
337896

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:14:50,588002914-07:00] INFO: > Run rados bench[0m
# ./benchmarks/bench-rados:370 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/output/rados_bench_4MB_write.log.2
## ./benchmarks/bench-rados:361 - rados_bench() > count_cpus 32-63,96-127
# ./benchmarks/bench-rados:361 - rados_bench() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 60 write --pool bench_rados -b 4194304 -O 4194304 --concurrent-ios 128 --show-time --write-object --write-omap --write-xattr --no-cleanup
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:14:50,608568646-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:14:50,611405030-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '7aa8adf8-384c-11ec-b51d-53e6e728d2d3', '--', 'rados', 'bench', '60', 'write', '--pool', 'bench_rados', '-b', '4194304', '-O', '4194304', '--concurrent-ios', '128', '--show-time', '--write-object', '--write-omap', '--write-xattr', '--no-cleanup'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 7aa8adf8-384c-11ec-b51d-53e6e728d2d3 -- rados bench 60 write --pool bench_rados -b 4194304 -O 4194304 --concurrent-ios 128 --show-time --write-object --write-omap --write-xattr --no-cleanup
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
hints = 1
2021-10-29T00:14:53.744271+0000 Maintaining 128 concurrent writes of 4194304 bytes to objects of size 4194304 for up to 60 seconds or 0 objects
2021-10-29T00:14:53.744286+0000 Object prefix: benchmark_data_node-2.bluefield2.ucsc-cmps10_7
2021-10-29T00:14:53.870655+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-29T00:14:53.870655+0000     0       0         0         0         0         0           -           0
2021-10-29T00:14:54.870781+0000     1      14        14         0         0         0           -           0
2021-10-29T00:14:55.870893+0000     2      22        22         0         0         0           -           0
2021-10-29T00:14:56.871000+0000     3      33        33         0         0         0           -           0
2021-10-29T00:14:57.871100+0000     4      41        41         0         0         0           -           0
2021-10-29T00:14:58.871208+0000     5      49        49         0         0         0           -           0
2021-10-29T00:14:59.871313+0000     6      57        57         0         0         0           -           0
2021-10-29T00:15:00.871415+0000     7      70        70         0         0         0           -           0
2021-10-29T00:15:01.871527+0000     8      78        78         0         0         0           -           0
2021-10-29T00:15:02.871643+0000     9      86        86         0         0         0           -           0
2021-10-29T00:15:03.871753+0000    10      94        94         0         0         0           -           0
2021-10-29T00:15:04.871857+0000    11     102       102         0         0         0           -           0
2021-10-29T00:15:05.871959+0000    12     110       110         0         0         0           -           0
2021-10-29T00:15:06.872047+0000    13     121       121         0         0         0           -           0
2021-10-29T00:15:07.872139+0000    14     127       129         2  0.571371  0.571429     13.9292     13.9293
2021-10-29T00:15:08.872256+0000    15     127       134         7   1.86648        20     14.4542     14.2948
2021-10-29T00:15:09.872382+0000    16     127       137        10   2.49974        12     14.7852     14.5315
2021-10-29T00:15:10.872511+0000    17     127       137        10   2.35269         0           -     14.5315
2021-10-29T00:15:11.872612+0000    18     127       142        15   3.33298        10     16.1936     15.0856
2021-10-29T00:15:12.872714+0000    19     127       145        18   3.78908        12     17.1979     15.4376
2021-10-29T00:15:13.872831+0000 min lat: 13.9292 max lat: 17.198 avg lat: 15.4376
2021-10-29T00:15:13.872831+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-29T00:15:13.872831+0000    20     127       145        18   3.59962         0           -     15.4376
2021-10-29T00:15:14.872924+0000    21     127       150        23   4.38049        10     18.6122     16.1278
2021-10-29T00:15:15.873017+0000    22     127       153        26   4.72678        12     19.5966      16.528
2021-10-29T00:15:16.873139+0000    23     127       153        26   4.52127         0           -      16.528
2021-10-29T00:15:17.873221+0000    24     127       158        31   5.16613        10     21.1221      17.269
2021-10-29T00:15:18.873310+0000    25     127       161        34   5.43944        12     22.0837     17.6939
2021-10-29T00:15:19.873432+0000    26     127       161        34   5.23023         0           -     17.6939
2021-10-29T00:15:20.873509+0000    27     127       166        39   5.77718        10     23.3026      18.412
2021-10-29T00:15:21.873603+0000    28     127       174        47    6.7136        32     23.5021     19.2775
2021-10-29T00:15:22.873708+0000    29     127       182        55   7.58543        32     23.7715     19.9239
2021-10-29T00:15:23.873806+0000    30     127       190        63   8.39914        32     23.7376     20.4052
2021-10-29T00:15:24.873917+0000    31     127       198        71   9.16035        32     23.7943     20.7869
2021-10-29T00:15:25.874032+0000    32     127       206        79   9.87398        32     23.8541     21.0984
2021-10-29T00:15:26.874139+0000    33     127       217        90    10.908        44     23.8955     21.4456
2021-10-29T00:15:27.874247+0000    34     127       225        98   11.5282        32     23.7995     21.6394
2021-10-29T00:15:28.874357+0000    35     127       233       106    12.113        32     23.7773     21.8009
2021-10-29T00:15:29.874454+0000    36     127       241       114   12.6654        32     23.7103     21.9338
2021-10-29T00:15:30.874547+0000    37     127       249       122   13.1878        32     23.8273     22.0591
2021-10-29T00:15:31.874645+0000    38     127       257       130   13.6828        32     23.7605     22.1641
2021-10-29T00:15:32.874750+0000    39     127       265       138   14.1524        32     23.5811     22.2574
2021-10-29T00:15:33.874879+0000 min lat: 13.9292 max lat: 23.9697 avg lat: 22.2574
2021-10-29T00:15:33.874879+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-29T00:15:33.874879+0000    40     127       265       138   13.7986         0           -     22.2574
2021-10-29T00:15:34.874967+0000    41     127       270       143   13.9498        10     23.6211     22.3051
2021-10-29T00:15:35.875103+0000    42     127       270       143   13.6176         0           -     22.3051
2021-10-29T00:15:36.875217+0000    43     127       270       143   13.3009         0           -     22.3051
2021-10-29T00:15:37.875320+0000    44     127       273       146   13.2713         4     24.5784     22.3518
2021-10-29T00:15:38.875437+0000    45     127       273       146   12.9764         0           -     22.3518
2021-10-29T00:15:39.875547+0000    46     127       273       146   12.6943         0           -     22.3518
2021-10-29T00:15:40.875640+0000    47     127       278       151   12.8497   6.66667     25.9073     22.4696
2021-10-29T00:15:41.875741+0000    48     127       278       151    12.582         0           -     22.4696
2021-10-29T00:15:42.875838+0000    49     127       281       154   12.5701         6     26.7219     22.5524
2021-10-29T00:15:43.875961+0000    50     127       281       154   12.3187         0           -     22.5524
2021-10-29T00:15:44.876076+0000    51     127       281       154   12.0772         0           -     22.5524
2021-10-29T00:15:45.876184+0000    52     127       281       154   11.8449         0           -     22.5524
2021-10-29T00:15:46.876289+0000    53     127       286       159   11.9987         5     28.3867     22.7359
2021-10-29T00:15:47.876421+0000    54     127       286       159   11.7765         0           -     22.7359
2021-10-29T00:15:48.876515+0000    55     127       289       162   11.7806         6     29.2865     22.8572
2021-10-29T00:15:49.876624+0000    56     127       289       162   11.5702         0           -     22.8572
2021-10-29T00:15:50.876704+0000    57     127       289       162   11.3672         0           -     22.8572
2021-10-29T00:15:51.876792+0000    58     127       294       167    11.516   6.66667     30.5933      23.089
2021-10-29T00:15:52.876930+0000    59     127       297       170   11.5242        12     31.0024     23.2287
2021-10-29T00:15:53.877045+0000 min lat: 13.9292 max lat: 31.7371 avg lat: 23.4717
2021-10-29T00:15:53.877045+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-29T00:15:53.877045+0000    60     127       302       175   11.6654        20     31.7196     23.4717
2021-10-29T00:15:54.877174+0000    61       6       303       297   19.4734       488     1.98904     23.1755
2021-10-29T00:15:55.877331+0000 Total time run:         61.8857
Total writes made:      303
Write size:             4194304
Object size:            4194304
Bandwidth (MB/sec):     19.5845
Stddev Bandwidth:       62.5614
Max bandwidth (MB/sec): 488
Min bandwidth (MB/sec): 0
Average IOPS:           4
Stddev IOPS:            15.655
Max IOPS:               122
Min IOPS:               0
Average Latency(s):     22.7616
Stddev Latency(s):      6.55568
Max latency(s):         32.0009
Min latency(s):         1.98904

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:15:56,699985639-07:00] INFO: > Stop system activity collection services[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:15:56,706553976-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:391 - rados_bench() > kill -INT 926745

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:15:56,712976368-07:00] INFO: >> for OSD host[0m
# ./benchmarks/bench-rados:396 - rados_bench() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.5 kill -INT 337896
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:15:56,720826279-07:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.5 kill -INT 337896
[1] 17:15:57 [SUCCESS] ljishen@10.10.2.5

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:15:58,009867263-07:00] INFO: >>> Copy system activity file from OSD host[0m
## ./benchmarks/bench-rados:403 - rados_bench() > basename -- /tmp/bench-rados/sys_activity_4MB_write.dat.osd_host.2
# ./benchmarks/bench-rados:403 - rados_bench() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.5:/tmp/bench-rados/sys_activity_4MB_write.dat.osd_host.2 /mnt/sda8/ceph-research/scripts/output/sys_activity_4MB_write.dat.osd_host.2


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:15:59,235259414-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:16:23,286906956-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 304 objects, 1.2 GiB
    usage:   3.6 GiB used, 396 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:16:23,295433492-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:16:32,360329515-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 304 objects, 1.2 GiB
    usage:   3.6 GiB used, 396 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:16:32,368559302-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:16:41,449508588-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 304 objects, 1.2 GiB
    usage:   3.6 GiB used, 396 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:16:41,458023262-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:16:50,473120268-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 304 objects, 1.2 GiB
    usage:   3.6 GiB used, 396 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:16:50,480977653-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:16:59,446800415-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 304 objects, 1.2 GiB
    usage:   3.6 GiB used, 396 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:16:59,455234688-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:16:59,461516785-07:00] INFO: > The cluster is idle now.[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:16:59,465517995-07:00] STAGE: Start benchmarking of mode seq ...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:16:59,473005303-07:00] INFO: > Start system activity collection services[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:16:59,478649217-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:341 - rados_bench() > rm --force /mnt/sda8/ceph-research/scripts/output/sys_activity_4MB_seq.dat.rados_bench_host.2
# ./benchmarks/bench-rados:342 - rados_bench() > sar_pid_rados_bench_host=928691
# ./benchmarks/bench-rados:342 - rados_bench() > S_TIME_FORMAT=ISO
# ./benchmarks/bench-rados:342 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/output/sys_activity_4MB_seq.dat.rados_bench_host.2 2

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:16:59,485942080-07:00] INFO: >> for OSD host[0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:16:59,495323697-07:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.5 bash
10.10.2.5: b'338446\n'
[1] 17:17:00 [SUCCESS] ljishen@10.10.2.5
338446

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:17:00,624730568-07:00] INFO: > Run rados bench[0m
# ./benchmarks/bench-rados:379 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/output/rados_bench_4MB_seq.log.2
## ./benchmarks/bench-rados:376 - rados_bench() > count_cpus 32-63,96-127
# ./benchmarks/bench-rados:376 - rados_bench() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 128 --show-time
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:17:00,644981046-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:17:00,647978473-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '7aa8adf8-384c-11ec-b51d-53e6e728d2d3', '--', 'rados', 'bench', '99999999', 'seq', '--pool', 'bench_rados', '--concurrent-ios', '128', '--show-time'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 7aa8adf8-384c-11ec-b51d-53e6e728d2d3 -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 128 --show-time
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
hints = 1
2021-10-29T00:17:03.693446+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-29T00:17:03.693446+0000     0       0         0         0         0         0           -           0
2021-10-29T00:17:04.693545+0000     1      64        64         0         0         0           -           0
2021-10-29T00:17:05.693619+0000     2     101       101         0         0         0           -           0
2021-10-29T00:17:06.693688+0000     3     127       143        16   21.3311   21.3333      2.9923     2.82311
2021-10-29T00:17:07.693760+0000     4     127       179        52    51.995       144     3.30278     3.09018
2021-10-29T00:17:08.693830+0000     5     127       219        92   73.5933       160     3.28571     3.17822
2021-10-29T00:17:09.693908+0000     6     127       258       131   87.3256       156     3.28785     3.20144
2021-10-29T00:17:10.693993+0000     7     127       294       167   95.4202       144     3.36813     3.23103
2021-10-29T00:17:11.694067+0000     8       1       303       302   150.987       540     1.30205     2.63756
2021-10-29T00:17:12.694179+0000 Total time run:       8.00625
Total reads made:     303
Read size:            4194304
Object size:          4194304
Bandwidth (MB/sec):   151.382
Average IOPS:         37
Stddev IOPS:          43.7164
Max IOPS:             135
Min IOPS:             0
Average Latency(s):   2.63153
Max latency(s):       3.43939
Min latency(s):       0.263278

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:17:13,349206801-07:00] INFO: > Stop system activity collection services[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:17:13,355582455-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:391 - rados_bench() > kill -INT 928691

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:17:13,361881334-07:00] INFO: >> for OSD host[0m
# ./benchmarks/bench-rados:396 - rados_bench() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.5 kill -INT 338446
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:17:13,369749059-07:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.5 kill -INT 338446
[1] 17:17:14 [SUCCESS] ljishen@10.10.2.5

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:17:14,472587948-07:00] INFO: >>> Copy system activity file from OSD host[0m
## ./benchmarks/bench-rados:403 - rados_bench() > basename -- /tmp/bench-rados/sys_activity_4MB_seq.dat.osd_host.2
# ./benchmarks/bench-rados:403 - rados_bench() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.5:/tmp/bench-rados/sys_activity_4MB_seq.dat.osd_host.2 /mnt/sda8/ceph-research/scripts/output/sys_activity_4MB_seq.dat.osd_host.2


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:17:15,545203248-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:17:39,543917499-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 304 objects, 1.2 GiB
    usage:   3.6 GiB used, 396 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:17:39,552036948-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:17:48,660177861-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 304 objects, 1.2 GiB
    usage:   3.6 GiB used, 396 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:17:48,668216829-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:17:57,748934572-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 304 objects, 1.2 GiB
    usage:   3.6 GiB used, 396 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:17:57,757076574-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:18:06,833247895-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 304 objects, 1.2 GiB
    usage:   3.6 GiB used, 396 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:18:06,841549548-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:18:15,947105893-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 304 objects, 1.2 GiB
    usage:   3.6 GiB used, 396 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:18:15,955241323-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:18:15,961554689-07:00] INFO: > The cluster is idle now.[0m


[1;7;39;49m[2021-10-28T17:18:15,964096599-07:00][RUNNING][ROUND 3/6/21] object_size=4MB[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:18:15,968010875-07:00] STAGE: Launch a Ceph cluster (CEPH_HOST: 10.10.2.1, OSD_HOST: 10.10.2.5) ...[0m
# ./benchmarks/bench-rados:183 - launch_ceph_cluster() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.1 bash -euo pipefail
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:18:15,977497139-07:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.1 bash -euo pipefail
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T17:18:16,353709418-07:00] INFO: > Remove existing clusters\x1b[0m\n'
10.10.2.1: b'## bash:17 -  > basename -- /var/lib/ceph/7aa8adf8-384c-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'# bash:17 -  > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm --host 10.10.2.1 --host 10.10.2.5 rm-cluster --force --fsid 7aa8adf8-384c-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T17:18:16,363805254-07:00] DEBUG: HOST_PARAMS: ['--host', '10.10.2.1', '--host', '10.10.2.5']\x1b[0m\n"
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T17:18:16,366656223-07:00] DEBUG: CEPHADM_PARAMS: ['rm-cluster', '--force', '--fsid', '7aa8adf8-384c-11ec-b51d-53e6e728d2d3']\x1b[0m\n"
10.10.2.1: b"# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:63 -  > /tmp/bench-rados/ceph-research/scripts/parallel-ssh --host 10.10.2.1 --host 10.10.2.5 'sudo python3 -u - rm-cluster --force --fsid 7aa8adf8-384c-11ec-b51d-53e6e728d2d3'\n"
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T17:18:16,374712246-07:00] DEBUG: command from stdin\x1b[0m\n'
10.10.2.1: b"# /tmp/bench-rados/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host 10.10.2.1 --host 10.10.2.5 'sudo python3 -u - rm-cluster --force --fsid 7aa8adf8-384c-11ec-b51d-53e6e728d2d3'\n"
10.10.2.1: b'[1] 17:18:22 [SUCCESS] 10.10.2.1\n[2] 17:18:25 [SUCCESS] 10.10.2.5\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T17:18:25,360116108-07:00] INFO: > Deploy a new cluster\x1b[0m\n'
10.10.2.1: b'# bash:24 -  > /tmp/bench-rados/ceph-research/scripts/ceph-deploy -m 10.10.2.1 -o sm1/10.10.2.5:/dev/nvme0n1\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T17:18:25,373019371-07:00] STAGE: Verify passwordless SSH to hosts, passwordless sudo via ljishen, and the existence of devices on hosts...\x1b[0m\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T17:18:25,377304195-07:00] INFO: > Check the host for the monitor\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:181 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@10.10.2.1 sudo --non-interactive --validate\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T17:18:25,527828316-07:00] INFO: > Check hosts for OSDs\x1b[0m\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T17:18:25,532576350-07:00] INFO: >> Check host 'sm1'\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:200 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 sudo --non-interactive --validate\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T17:18:26,598678913-07:00] INFO: >>> Check device '/dev/nvme0n1'\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:215 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 test -b /dev/nvme0n1\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:226 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 lsblk --nodeps --noheadings --output TYPE /dev/nvme0n1\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:227 - verify_parameters() > grep --word-regexp disk\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T17:18:28,715511124-07:00] STAGE: Reset devices on hosts...\x1b[0m\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T17:18:28,720275089-07:00] INFO: > Resetting device '/dev/nvme0n1' on host 'sm1'\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:288 - reset_host_devices() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 sudo sh\n'
10.10.2.1: b'  Removing ceph--edb33f44--3723--4020--8c4b--3e50929bf398-osd--block--7248cd6a--f6ee--4f23--89a2--3ed52b5e8dd7 (252:0)\n'
10.10.2.1: b'  Archiving volume group "ceph-edb33f44-3723-4020-8c4b-3e50929bf398" metadata (seqno 5).\n'
10.10.2.1: b'  Releasing logical volume "osd-block-7248cd6a-f6ee-4f23-89a2-3ed52b5e8dd7"\n'
10.10.2.1: b'  Creating volume group backup "/etc/lvm/backup/ceph-edb33f44-3723-4020-8c4b-3e50929bf398" (seqno 6).\n'
10.10.2.1: b'  Logical volume "osd-block-7248cd6a-f6ee-4f23-89a2-3ed52b5e8dd7" successfully removed\n'
10.10.2.1: b'  Removing physical volume "/dev/nvme0n1" from volume group "ceph-edb33f44-3723-4020-8c4b-3e50929bf398"\n'
10.10.2.1: b'  Volume group "ceph-edb33f44-3723-4020-8c4b-3e50929bf398" successfully removed\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T17:18:31,030704431-07:00] STAGE: Bootstrap a new cluster...\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:317 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.1 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T17:18:31,040731498-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T17:18:31,044874886-07:00] DEBUG: CEPHADM_PARAMS: ['bootstrap', '--config', '/tmp/bench-rados/ceph-research/scripts/ceph.conf', '--mon-ip', '10.10.2.1', '--output-dir', '/tmp/bench-rados/deployment_data_root/etc/ceph', '--ssh-user', 'ljishen', '--skip-dashboard', '--no-minimize-config', '--allow-overwrite', '--allow-fqdn-hostname', '--orphan-initial-daemons', '--skip-monitoring-stack', '--single-host-defaults']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.1 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults\n'
10.10.2.1: b'Verifying podman|docker is present...\n'
10.10.2.1: b'Verifying lvm2 is present...\n'
10.10.2.1: b'Verifying time synchronization is in place...\n'
10.10.2.1: b'Unit ntp.service is enabled and running\n'
10.10.2.1: b'Repeating the final host check...\n'
10.10.2.1: b'podman|docker (/usr/bin/docker) is present\n'
10.10.2.1: b'systemctl is present\n'
10.10.2.1: b'lvcreate is present\n'
10.10.2.1: b'Unit ntp.service is enabled and running\n'
10.10.2.1: b'Host looks OK\n'
10.10.2.1: b'Cluster fsid: bf2c5226-384d-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Verifying IP 10.10.2.1 port 3300 ...\n'
10.10.2.1: b'Verifying IP 10.10.2.1 port 6789 ...\n'
10.10.2.1: b'Mon IP `10.10.2.1` is in CIDR network `10.10.2.0/24`\n'
10.10.2.1: b'- internal network (--cluster-network) has not been provided, OSD replication will default to the public_network\n'
10.10.2.1: b'Adjusting default settings to suit single-host cluster...\n'
10.10.2.1: b'Pulling container image quay.io/ceph/ceph:v16...\n'
10.10.2.1: b'Ceph version: ceph version 16.2.6 (ee28fb57e47e9f88813e24bbf4c14496ca299d31) pacific (stable)\n'
10.10.2.1: b'Extracting ceph user uid/gid from container image...\n'
10.10.2.1: b'Creating initial keys...\n'
10.10.2.1: b'Creating initial monmap...\n'
10.10.2.1: b'Creating mon...\n'
10.10.2.1: b'Waiting for mon to start...\n'
10.10.2.1: b'Waiting for mon...\n'
10.10.2.1: b'mon is available\n'
10.10.2.1: b'Setting mon public_network to 10.10.2.0/24\n'
10.10.2.1: b'Wrote config to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf\n'
10.10.2.1: b'Wrote keyring to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring\n'
10.10.2.1: b'Creating mgr...\n'
10.10.2.1: b'Verifying port 9283 ...\n'
10.10.2.1: b'Waiting for mgr to start...\n'
10.10.2.1: b'Waiting for mgr...\n'
10.10.2.1: b'mgr not available, waiting (1/15)...\n'
10.10.2.1: b'mgr not available, waiting (2/15)...\n'
10.10.2.1: b'mgr is available\n'
10.10.2.1: b'Enabling cephadm module...\n'
10.10.2.1: b'Waiting for the mgr to restart...\n'
10.10.2.1: b'Waiting for mgr epoch 5...\n'
10.10.2.1: b'mgr epoch 5 is available\n'
10.10.2.1: b'Setting orchestrator backend to cephadm...\n'
10.10.2.1: b'Generating ssh key...\n'
10.10.2.1: b'Wrote public SSH key to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub\n'
10.10.2.1: b'Adding key to ljishen@localhost authorized_keys...\n'
10.10.2.1: b'Adding host node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us...\n'
10.10.2.1: b'Deploying unmanaged mon service...\n'
10.10.2.1: b'Deploying unmanaged mgr service...\n'
10.10.2.1: b'You can access the Ceph CLI with:\n\n\tsudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --fsid bf2c5226-384d-11ec-b51d-53e6e728d2d3 -c /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf -k /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring\n\n'
10.10.2.1: b'Please consider enabling telemetry to help improve Ceph:\n\n\tceph telemetry on\n\nFor more information see:\n\n\thttps://docs.ceph.com/docs/pacific/mgr/telemetry/\n\nBootstrap complete.\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T17:19:32,527606110-07:00] INFO: Sleep for 20 seconds\x1b[0m\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T17:19:52,534692283-07:00] INFO: > Set the default number of monitor daemons to 1\x1b[0m\n# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:337 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T17:19:52,544352581-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T17:19:52,548375182-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mon', '1']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1\n'
10.10.2.1: b'Inferring fsid bf2c5226-384d-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/bf2c5226-384d-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b'Scheduled mon update...\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T17:20:01,545282463-07:00] INFO: > Set the default number of manager daemons to 1\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:342 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T17:20:01,554314169-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T17:20:01,557673223-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mgr', '1']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1\n'
10.10.2.1: b'Inferring fsid bf2c5226-384d-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/bf2c5226-384d-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b'Scheduled mgr update...\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T17:20:11,178789400-07:00] STAGE: Add hosts to the cluster...\x1b[0m\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T17:20:11,184579674-07:00] INFO: > Adding host 'sm1'\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:361 - add_hosts() > ssh-copy-id -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -f -i /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub ljishen@sm1\n'
10.10.2.1: b'/usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: "/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub"\n'
10.10.2.1: b'\nNumber of key(s) added: 1\n\nNow try logging into the machine, with:   "ssh -o \'GlobalKnownHostsFile=/dev/null\' -o \'LogLevel=ERROR\' -o \'PasswordAuthentication=no\' -o \'StrictHostKeyChecking=no\' -o \'UserKnownHostsFile=/dev/null\' \'ljishen@sm1\'"\nand check to make sure that only the key(s) you wanted were added.\n\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:367 - add_hosts() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add sm1 10.10.2.5\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T17:20:12,339322681-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T17:20:12,342493581-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'host', 'add', 'sm1', '10.10.2.5']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add sm1 10.10.2.5\n'
10.10.2.1: b'Inferring fsid bf2c5226-384d-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/bf2c5226-384d-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b"Added host 'sm1' with addr '10.10.2.5'\n"
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T17:20:23,450238842-07:00] INFO: Sleep for 20 seconds\x1b[0m\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T17:20:43,455053034-07:00] STAGE: Add OSDs to the cluster...\x1b[0m\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T17:20:43,461240344-07:00] INFO: > Adding devices ['/dev/nvme0n1'] on host 'sm1' as OSDs\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:388 - add_host_devices_as_osds() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd sm1:/dev/nvme0n1\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T17:20:43,470356409-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T17:20:43,473433663-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'daemon', 'add', 'osd', 'sm1:/dev/nvme0n1']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd sm1:/dev/nvme0n1\n'
10.10.2.1: b'Inferring fsid bf2c5226-384d-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/bf2c5226-384d-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b"Created osd(s) 0 on host 'sm1'\n"
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T17:21:07,875608133-07:00] INFO: > Sleep for 20 seconds\x1b[0m\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T17:21:27,880510835-07:00] STAGE: Show Cluster Status\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:408 - show_status() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T17:21:27,890410993-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T17:21:27,893406103-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', '--status']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status\n'
10.10.2.1: b'Inferring fsid bf2c5226-384d-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/bf2c5226-384d-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b'  cluster:\n    id:     bf2c5226-384d-11ec-b51d-53e6e728d2d3\n    health: HEALTH_WARN\n            1 pool(s) have no replicas configured\n \n  services:\n    mon: 1 daemons, quorum node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us (age 2m)\n    mgr: node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us.lchfnj(active, since 2m)\n    osd: 1 osds: 1 up (since 20s), 1 in (since 41s)\n \n  data:\n    pools:   1 pools, 1 pgs\n    objects: 0 objects, 0 B\n    usage:   4.7 MiB used, 400 GiB / 400 GiB avail\n    pgs:     1 active+clean\n \n'
[1] 17:21:36 [SUCCESS] ljishen@10.10.2.1

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:18:16,353709418-07:00] INFO: > Remove existing clusters[0m
## bash:17 -  > basename -- /var/lib/ceph/7aa8adf8-384c-11ec-b51d-53e6e728d2d3
# bash:17 -  > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm --host 10.10.2.1 --host 10.10.2.5 rm-cluster --force --fsid 7aa8adf8-384c-11ec-b51d-53e6e728d2d3
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:18:16,363805254-07:00] DEBUG: HOST_PARAMS: ['--host', '10.10.2.1', '--host', '10.10.2.5'][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:18:16,366656223-07:00] DEBUG: CEPHADM_PARAMS: ['rm-cluster', '--force', '--fsid', '7aa8adf8-384c-11ec-b51d-53e6e728d2d3'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:63 -  > /tmp/bench-rados/ceph-research/scripts/parallel-ssh --host 10.10.2.1 --host 10.10.2.5 'sudo python3 -u - rm-cluster --force --fsid 7aa8adf8-384c-11ec-b51d-53e6e728d2d3'
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:18:16,374712246-07:00] DEBUG: command from stdin[0m
# /tmp/bench-rados/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host 10.10.2.1 --host 10.10.2.5 'sudo python3 -u - rm-cluster --force --fsid 7aa8adf8-384c-11ec-b51d-53e6e728d2d3'
[1] 17:18:22 [SUCCESS] 10.10.2.1
[2] 17:18:25 [SUCCESS] 10.10.2.5

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:18:25,360116108-07:00] INFO: > Deploy a new cluster[0m
# bash:24 -  > /tmp/bench-rados/ceph-research/scripts/ceph-deploy -m 10.10.2.1 -o sm1/10.10.2.5:/dev/nvme0n1


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:18:25,373019371-07:00] STAGE: Verify passwordless SSH to hosts, passwordless sudo via ljishen, and the existence of devices on hosts...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:18:25,377304195-07:00] INFO: > Check the host for the monitor[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:181 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@10.10.2.1 sudo --non-interactive --validate

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:18:25,527828316-07:00] INFO: > Check hosts for OSDs[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:18:25,532576350-07:00] INFO: >> Check host 'sm1'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:200 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 sudo --non-interactive --validate

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:18:26,598678913-07:00] INFO: >>> Check device '/dev/nvme0n1'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:215 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 test -b /dev/nvme0n1
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:226 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 lsblk --nodeps --noheadings --output TYPE /dev/nvme0n1
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:227 - verify_parameters() > grep --word-regexp disk


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:18:28,715511124-07:00] STAGE: Reset devices on hosts...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:18:28,720275089-07:00] INFO: > Resetting device '/dev/nvme0n1' on host 'sm1'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:288 - reset_host_devices() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 sudo sh
  Removing ceph--edb33f44--3723--4020--8c4b--3e50929bf398-osd--block--7248cd6a--f6ee--4f23--89a2--3ed52b5e8dd7 (252:0)
  Archiving volume group "ceph-edb33f44-3723-4020-8c4b-3e50929bf398" metadata (seqno 5).
  Releasing logical volume "osd-block-7248cd6a-f6ee-4f23-89a2-3ed52b5e8dd7"
  Creating volume group backup "/etc/lvm/backup/ceph-edb33f44-3723-4020-8c4b-3e50929bf398" (seqno 6).
  Logical volume "osd-block-7248cd6a-f6ee-4f23-89a2-3ed52b5e8dd7" successfully removed
  Removing physical volume "/dev/nvme0n1" from volume group "ceph-edb33f44-3723-4020-8c4b-3e50929bf398"
  Volume group "ceph-edb33f44-3723-4020-8c4b-3e50929bf398" successfully removed


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:18:31,030704431-07:00] STAGE: Bootstrap a new cluster...[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:317 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.1 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:18:31,040731498-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:18:31,044874886-07:00] DEBUG: CEPHADM_PARAMS: ['bootstrap', '--config', '/tmp/bench-rados/ceph-research/scripts/ceph.conf', '--mon-ip', '10.10.2.1', '--output-dir', '/tmp/bench-rados/deployment_data_root/etc/ceph', '--ssh-user', 'ljishen', '--skip-dashboard', '--no-minimize-config', '--allow-overwrite', '--allow-fqdn-hostname', '--orphan-initial-daemons', '--skip-monitoring-stack', '--single-host-defaults'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.1 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults
Verifying podman|docker is present...
Verifying lvm2 is present...
Verifying time synchronization is in place...
Unit ntp.service is enabled and running
Repeating the final host check...
podman|docker (/usr/bin/docker) is present
systemctl is present
lvcreate is present
Unit ntp.service is enabled and running
Host looks OK
Cluster fsid: bf2c5226-384d-11ec-b51d-53e6e728d2d3
Verifying IP 10.10.2.1 port 3300 ...
Verifying IP 10.10.2.1 port 6789 ...
Mon IP `10.10.2.1` is in CIDR network `10.10.2.0/24`
- internal network (--cluster-network) has not been provided, OSD replication will default to the public_network
Adjusting default settings to suit single-host cluster...
Pulling container image quay.io/ceph/ceph:v16...
Ceph version: ceph version 16.2.6 (ee28fb57e47e9f88813e24bbf4c14496ca299d31) pacific (stable)
Extracting ceph user uid/gid from container image...
Creating initial keys...
Creating initial monmap...
Creating mon...
Waiting for mon to start...
Waiting for mon...
mon is available
Setting mon public_network to 10.10.2.0/24
Wrote config to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf
Wrote keyring to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring
Creating mgr...
Verifying port 9283 ...
Waiting for mgr to start...
Waiting for mgr...
mgr not available, waiting (1/15)...
mgr not available, waiting (2/15)...
mgr is available
Enabling cephadm module...
Waiting for the mgr to restart...
Waiting for mgr epoch 5...
mgr epoch 5 is available
Setting orchestrator backend to cephadm...
Generating ssh key...
Wrote public SSH key to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub
Adding key to ljishen@localhost authorized_keys...
Adding host node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us...
Deploying unmanaged mon service...
Deploying unmanaged mgr service...
You can access the Ceph CLI with:

	sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --fsid bf2c5226-384d-11ec-b51d-53e6e728d2d3 -c /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf -k /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring

Please consider enabling telemetry to help improve Ceph:

	ceph telemetry on

For more information see:

	https://docs.ceph.com/docs/pacific/mgr/telemetry/

Bootstrap complete.

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:19:32,527606110-07:00] INFO: Sleep for 20 seconds[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:19:52,534692283-07:00] INFO: > Set the default number of monitor daemons to 1[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:337 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:19:52,544352581-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:19:52,548375182-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mon', '1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1
Inferring fsid bf2c5226-384d-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/bf2c5226-384d-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Scheduled mon update...

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:20:01,545282463-07:00] INFO: > Set the default number of manager daemons to 1[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:342 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:20:01,554314169-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:20:01,557673223-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mgr', '1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1
Inferring fsid bf2c5226-384d-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/bf2c5226-384d-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Scheduled mgr update...


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:20:11,178789400-07:00] STAGE: Add hosts to the cluster...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:20:11,184579674-07:00] INFO: > Adding host 'sm1'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:361 - add_hosts() > ssh-copy-id -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -f -i /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub ljishen@sm1
/usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: "/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub"

Number of key(s) added: 1

Now try logging into the machine, with:   "ssh -o 'GlobalKnownHostsFile=/dev/null' -o 'LogLevel=ERROR' -o 'PasswordAuthentication=no' -o 'StrictHostKeyChecking=no' -o 'UserKnownHostsFile=/dev/null' 'ljishen@sm1'"
and check to make sure that only the key(s) you wanted were added.

# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:367 - add_hosts() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add sm1 10.10.2.5
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:20:12,339322681-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:20:12,342493581-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'host', 'add', 'sm1', '10.10.2.5'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add sm1 10.10.2.5
Inferring fsid bf2c5226-384d-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/bf2c5226-384d-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Added host 'sm1' with addr '10.10.2.5'

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:20:23,450238842-07:00] INFO: Sleep for 20 seconds[0m


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:20:43,455053034-07:00] STAGE: Add OSDs to the cluster...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:20:43,461240344-07:00] INFO: > Adding devices ['/dev/nvme0n1'] on host 'sm1' as OSDs[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:388 - add_host_devices_as_osds() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd sm1:/dev/nvme0n1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:20:43,470356409-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:20:43,473433663-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'daemon', 'add', 'osd', 'sm1:/dev/nvme0n1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd sm1:/dev/nvme0n1
Inferring fsid bf2c5226-384d-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/bf2c5226-384d-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Created osd(s) 0 on host 'sm1'

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:21:07,875608133-07:00] INFO: > Sleep for 20 seconds[0m


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:21:27,880510835-07:00] STAGE: Show Cluster Status[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:408 - show_status() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:21:27,890410993-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:21:27,893406103-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', '--status'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status
Inferring fsid bf2c5226-384d-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/bf2c5226-384d-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
  cluster:
    id:     bf2c5226-384d-11ec-b51d-53e6e728d2d3
    health: HEALTH_WARN
            1 pool(s) have no replicas configured
 
  services:
    mon: 1 daemons, quorum node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us (age 2m)
    mgr: node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us.lchfnj(active, since 2m)
    osd: 1 osds: 1 up (since 20s), 1 in (since 41s)
 
  data:
    pools:   1 pools, 1 pgs
    objects: 0 objects, 0 B
    usage:   4.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     1 active+clean
 

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:21:36,980784631-07:00] INFO: > Copy configuration files to local[0m
# ./benchmarks/bench-rados:220 - launch_ceph_cluster() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.1 sudo chown --changes --recursive ljishen /tmp/bench-rados/deployment_data_root/etc/ceph
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:21:36,988823659-07:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.1 sudo chown --changes --recursive ljishen /tmp/bench-rados/deployment_data_root/etc/ceph
[1] 17:21:37 [SUCCESS] ljishen@10.10.2.1
# ./benchmarks/bench-rados:223 - launch_ceph_cluster() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.1:/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf
# ./benchmarks/bench-rados:226 - launch_ceph_cluster() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.1:/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:21:37,464890764-07:00] STAGE: Update local ceph.conf as a client...[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:21:37,468913023-07:00] STAGE: Configure Ceph pool...[0m
# ./benchmarks/bench-rados:257 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_max_backfills 32
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:21:37,491558103-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:21:37,494335356-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'bf2c5226-384d-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'config', 'set', 'osd', 'osd_max_backfills', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid bf2c5226-384d-11ec-b51d-53e6e728d2d3 -- ceph config set osd osd_max_backfills 32
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:258 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_active 32
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:21:41,696996667-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:21:41,700096799-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'bf2c5226-384d-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_active', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid bf2c5226-384d-11ec-b51d-53e6e728d2d3 -- ceph config set osd osd_recovery_max_active 32
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:259 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_single_start 8
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:21:45,930665076-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:21:45,933673404-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'bf2c5226-384d-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_single_start', '8'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid bf2c5226-384d-11ec-b51d-53e6e728d2d3 -- ceph config set osd osd_recovery_max_single_start 8
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:260 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_op_priority 63
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:21:49,955782291-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:21:49,958505512-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'bf2c5226-384d-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_op_priority', '63'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid bf2c5226-384d-11ec-b51d-53e6e728d2d3 -- ceph config set osd osd_recovery_op_priority 63
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:262 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config show-with-defaults osd.0
# ./benchmarks/bench-rados:263 - configure_ceph_pool() > grep 'osd_max_backfills\|osd_recovery'
# ./benchmarks/bench-rados:263 - configure_ceph_pool() > column -t -s ' '
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
osd_max_backfills                        32         mon
osd_recovery_cost                        20971520   default
osd_recovery_delay_start                 0.000000   default
osd_recovery_max_active                  32         mon
osd_recovery_max_active_hdd              3          default
osd_recovery_max_active_ssd              10         default
osd_recovery_max_chunk                   8388608    default
osd_recovery_max_omap_entries_per_chunk  8096       default
osd_recovery_max_single_start            8          mon
osd_recovery_op_priority                 63         mon
osd_recovery_op_warn_multiple            16         default
osd_recovery_priority                    5          default
osd_recovery_retry_interval              30.000000  default
osd_recovery_sleep                       0.000000   default
osd_recovery_sleep_hdd                   0.100000   default
osd_recovery_sleep_hybrid                0.025000   default
osd_recovery_sleep_ssd                   0.000000   default
# ./benchmarks/bench-rados:265 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool create bench_rados 128 128 replicated --size 1 --pg-num-min 128
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:21:57,888781747-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:21:57,891943786-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'bf2c5226-384d-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'create', 'bench_rados', '128', '128', 'replicated', '--size', '1', '--pg-num-min', '128'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid bf2c5226-384d-11ec-b51d-53e6e728d2d3 -- ceph osd pool create bench_rados 128 128 replicated --size 1 --pg-num-min 128
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
pool 'bench_rados' created
# ./benchmarks/bench-rados:267 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados min_size 1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:22:02,687685514-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:22:02,690456556-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'bf2c5226-384d-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'min_size', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid bf2c5226-384d-11ec-b51d-53e6e728d2d3 -- ceph osd pool set bench_rados min_size 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 min_size to 1
# ./benchmarks/bench-rados:268 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados pg_autoscale_mode off
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:22:07,003240338-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:22:07,006358342-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'bf2c5226-384d-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'pg_autoscale_mode', 'off'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid bf2c5226-384d-11ec-b51d-53e6e728d2d3 -- ceph osd pool set bench_rados pg_autoscale_mode off
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 pg_autoscale_mode to off
# ./benchmarks/bench-rados:269 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool application enable bench_rados benchmark
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:22:11,810626533-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:22:11,813460703-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'bf2c5226-384d-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'application', 'enable', 'bench_rados', 'benchmark'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid bf2c5226-384d-11ec-b51d-53e6e728d2d3 -- ceph osd pool application enable bench_rados benchmark
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
enabled application 'benchmark' on pool 'bench_rados'
# ./benchmarks/bench-rados:270 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados noscrub 1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:22:16,203790889-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:22:16,207126434-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'bf2c5226-384d-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'noscrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid bf2c5226-384d-11ec-b51d-53e6e728d2d3 -- ceph osd pool set bench_rados noscrub 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 noscrub to 1
# ./benchmarks/bench-rados:271 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados nodeep-scrub 1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:22:21,201971327-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:22:21,204844841-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'bf2c5226-384d-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'nodeep-scrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid bf2c5226-384d-11ec-b51d-53e6e728d2d3 -- ceph osd pool set bench_rados nodeep-scrub 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 nodeep-scrub to 1
# ./benchmarks/bench-rados:272 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool ls detail
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:22:25,835003132-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:22:25,838000308-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'bf2c5226-384d-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'ls', 'detail'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid bf2c5226-384d-11ec-b51d-53e6e728d2d3 -- ceph osd pool ls detail
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
pool 1 'device_health_metrics' replicated size 1 min_size 1 crush_rule 0 object_hash rjenkins pg_num 64 pgp_num 64 autoscale_mode on last_change 19 lfor 0/0/16 flags hashpspool stripe_width 0 pg_num_min 1 application mgr_devicehealth
pool 2 'bench_rados' replicated size 1 min_size 1 crush_rule 0 object_hash rjenkins pg_num 128 pgp_num 128 autoscale_mode off last_change 21 flags hashpspool,noscrub,nodeep-scrub stripe_width 0 pg_num_min 128 application benchmark

# ./benchmarks/bench-rados:273 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd df tree
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:22:29,885759302-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:22:29,888610935-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'bf2c5226-384d-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'df', 'tree'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid bf2c5226-384d-11ec-b51d-53e6e728d2d3 -- ceph osd df tree
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
ID  CLASS  WEIGHT   REWEIGHT  SIZE     RAW USE  DATA     OMAP  META     AVAIL    %USE  VAR   PGS  STATUS  TYPE NAME    
-1         0.39059         -  400 GiB  5.7 MiB  176 KiB   0 B  5.6 MiB  400 GiB  0.00  1.00    -          root default 
-3         0.39059         -  400 GiB  5.7 MiB  176 KiB   0 B  5.6 MiB  400 GiB  0.00  1.00    -              host sm1 
 0    ssd  0.39059   1.00000  400 GiB  5.7 MiB  176 KiB   0 B  5.6 MiB  400 GiB  0.00  1.00  192      up          osd.0
                       TOTAL  400 GiB  5.7 MiB  176 KiB   0 B  5.6 MiB  400 GiB  0.00                                  
MIN/MAX VAR: 1.00/1.00  STDDEV: 0


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:22:34,086386130-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:22:58,042076003-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:23:07,080668567-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:23:16,231172279-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:23:16,239371617-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:23:25,403626048-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:23:25,411717293-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:23:34,392974689-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:23:34,401117331-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:23:43,481881311-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:23:43,490293600-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:23:52,513454246-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:23:52,521484135-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:23:52,527620517-07:00] INFO: > The cluster is idle now.[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:23:52,531301181-07:00] STAGE: Start benchmarking of mode write ...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:23:52,538236548-07:00] INFO: > Start system activity collection services[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:23:52,543945895-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:341 - rados_bench() > rm --force /mnt/sda8/ceph-research/scripts/output/sys_activity_4MB_write.dat.rados_bench_host.3
# ./benchmarks/bench-rados:342 - rados_bench() > sar_pid_rados_bench_host=936206
# ./benchmarks/bench-rados:342 - rados_bench() > S_TIME_FORMAT=ISO
# ./benchmarks/bench-rados:342 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/output/sys_activity_4MB_write.dat.rados_bench_host.3 2

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:23:52,551210231-07:00] INFO: >> for OSD host[0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:23:52,560376231-07:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.5 bash
10.10.2.5: b'343305\n'
[1] 17:23:53 [SUCCESS] ljishen@10.10.2.5
343305

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:23:53,695903115-07:00] INFO: > Run rados bench[0m
# ./benchmarks/bench-rados:370 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/output/rados_bench_4MB_write.log.3
## ./benchmarks/bench-rados:361 - rados_bench() > count_cpus 32-63,96-127
# ./benchmarks/bench-rados:361 - rados_bench() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 60 write --pool bench_rados -b 4194304 -O 4194304 --concurrent-ios 128 --show-time --write-object --write-omap --write-xattr --no-cleanup
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:23:53,716668999-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:23:53,719607255-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'bf2c5226-384d-11ec-b51d-53e6e728d2d3', '--', 'rados', 'bench', '60', 'write', '--pool', 'bench_rados', '-b', '4194304', '-O', '4194304', '--concurrent-ios', '128', '--show-time', '--write-object', '--write-omap', '--write-xattr', '--no-cleanup'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid bf2c5226-384d-11ec-b51d-53e6e728d2d3 -- rados bench 60 write --pool bench_rados -b 4194304 -O 4194304 --concurrent-ios 128 --show-time --write-object --write-omap --write-xattr --no-cleanup
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
hints = 1
2021-10-29T00:23:56.732264+0000 Maintaining 128 concurrent writes of 4194304 bytes to objects of size 4194304 for up to 60 seconds or 0 objects
2021-10-29T00:23:56.732278+0000 Object prefix: benchmark_data_node-2.bluefield2.ucsc-cmps10_7
2021-10-29T00:23:56.857321+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-29T00:23:56.857321+0000     0       0         0         0         0         0           -           0
2021-10-29T00:23:57.857452+0000     1      14        14         0         0         0           -           0
2021-10-29T00:23:58.857569+0000     2      22        22         0         0         0           -           0
2021-10-29T00:23:59.857679+0000     3      30        30         0         0         0           -           0
2021-10-29T00:24:00.857792+0000     4      38        38         0         0         0           -           0
2021-10-29T00:24:01.857875+0000     5      46        46         0         0         0           -           0
2021-10-29T00:24:02.858000+0000     6      57        57         0         0         0           -           0
2021-10-29T00:24:03.858118+0000     7      65        65         0         0         0           -           0
2021-10-29T00:24:04.858228+0000     8      73        73         0         0         0           -           0
2021-10-29T00:24:05.858331+0000     9      78        78         0         0         0           -           0
2021-10-29T00:24:06.858445+0000    10      89        89         0         0         0           -           0
2021-10-29T00:24:07.858554+0000    11      96        96         0         0         0           -           0
2021-10-29T00:24:08.858666+0000    12     104       104         0         0         0           -           0
2021-10-29T00:24:09.858769+0000    13     112       112         0         0         0           -           0
2021-10-29T00:24:10.858882+0000    14     120       120         0         0         0           -           0
2021-10-29T00:24:11.858991+0000    15     127       133         6   1.59983       1.6     14.9277     14.8542
2021-10-29T00:24:12.859100+0000    16     127       136         9   2.24976        12     15.3795     15.0291
2021-10-29T00:24:13.859209+0000    17     127       136         9   2.11742         0           -     15.0291
2021-10-29T00:24:14.859325+0000    18     127       141        14   3.11077        10     16.2394     15.4935
2021-10-29T00:24:15.859443+0000    19     127       144        17   3.57855        12      17.087     15.7969
2021-10-29T00:24:16.859566+0000 min lat: 14.4881 max lat: 17.4635 avg lat: 15.7969
2021-10-29T00:24:16.859566+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-29T00:24:16.859566+0000    20     127       144        17   3.39962         0           -     15.7969
2021-10-29T00:24:17.859680+0000    21     127       149        22   4.19001        10      18.348        16.4
2021-10-29T00:24:18.859806+0000    22     127       152        25   4.54495        12      19.152     16.7466
2021-10-29T00:24:19.859926+0000    23     127       152        25   4.34734         0           -     16.7466
2021-10-29T00:24:20.860044+0000    24     127       157        30   4.99944        10      20.392     17.3836
2021-10-29T00:24:21.860116+0000    25     127       160        33   5.27942        12     21.1374      17.736
2021-10-29T00:24:22.860244+0000    26     127       160        33   5.07636         0           -      17.736
2021-10-29T00:24:23.860337+0000    27     127       168        41    6.0734        16     22.3812     18.6469
2021-10-29T00:24:24.860441+0000    28     127       176        49   6.99923        32     22.4368     19.2687
2021-10-29T00:24:25.860541+0000    29     127       184        57   7.86121        32     22.6196     19.7548
2021-10-29T00:24:26.860653+0000    30     127       192        65   8.66572        32     22.5565     20.1116
2021-10-29T00:24:27.860766+0000    31     127       200        73   9.41832        32     22.5943      20.399
2021-10-29T00:24:28.860881+0000    32     127       213        86   10.7488        52     22.3194     20.7173
2021-10-29T00:24:29.860982+0000    33     127       221        94   11.3927        32     22.3402     20.8641
2021-10-29T00:24:30.861103+0000    34     127       229       102   11.9987        32     22.5724     20.9995
2021-10-29T00:24:31.861216+0000    35     127       237       110     12.57        32     22.6187     21.1145
2021-10-29T00:24:32.861294+0000    36     127       248       121    13.443        44     22.4776     21.2419
2021-10-29T00:24:33.861400+0000    37     127       256       129   13.9444        32     22.3612      21.315
2021-10-29T00:24:34.861518+0000    38     127       264       137   14.4195        32     22.3944     21.3808
2021-10-29T00:24:35.861633+0000    39     127       264       137   14.0497         0           -     21.3808
2021-10-29T00:24:36.861747+0000 min lat: 14.4881 max lat: 23.1561 avg lat: 21.4312
2021-10-29T00:24:36.861747+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-29T00:24:36.861747+0000    40     127       269       142   14.1984        10     22.8135     21.4312
2021-10-29T00:24:37.861862+0000    41     127       269       142   13.8521         0           -     21.4312
2021-10-29T00:24:38.861975+0000    42     127       269       142   13.5223         0           -     21.4312
2021-10-29T00:24:39.862087+0000    43     127       272       145   13.4869         4     23.9259     21.4828
2021-10-29T00:24:40.862200+0000    44     127       272       145   13.1804         0           -     21.4828
2021-10-29T00:24:41.862298+0000    45     127       272       145   12.8875         0           -     21.4828
2021-10-29T00:24:42.862409+0000    46     127       277       150    13.042   6.66667     25.6861      21.623
2021-10-29T00:24:43.862522+0000    47     127       277       150   12.7646         0           -      21.623
2021-10-29T00:24:44.862632+0000    48     127       280       153   12.7486         6     26.4443     21.7175
2021-10-29T00:24:45.862733+0000    49     127       280       153   12.4884         0           -     21.7175
2021-10-29T00:24:46.862850+0000    50     127       280       153   12.2387         0           -     21.7175
2021-10-29T00:24:47.862963+0000    51     127       280       153   11.9987         0           -     21.7175
2021-10-29T00:24:48.863075+0000    52     127       285       158   12.1525         5     27.6881      21.906
2021-10-29T00:24:49.863176+0000    53     127       285       158   11.9232         0           -      21.906
2021-10-29T00:24:50.863291+0000    54     127       288       161   11.9246         6     28.7615     22.0338
2021-10-29T00:24:51.863406+0000    55     127       288       161   11.7078         0           -     22.0338
2021-10-29T00:24:52.863521+0000    56     127       288       161   11.4987         0           -     22.0338
2021-10-29T00:24:53.863656+0000    57     127       293       166   11.6478   6.66667     30.3186     22.2833
2021-10-29T00:24:54.863770+0000    58     127       296       169   11.6539        12     30.5945     22.4308
2021-10-29T00:24:55.863887+0000    59     127       301       174   11.7953        20     31.3987     22.6885
2021-10-29T00:24:56.864004+0000 min lat: 14.4881 max lat: 31.7427 avg lat: 22.842
2021-10-29T00:24:56.864004+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-29T00:24:56.864004+0000    60     127       304       177   11.7987        12     31.7427      22.842
2021-10-29T00:24:57.864111+0000    61       4       305       301   19.7355       496     1.96343     22.7141
2021-10-29T00:24:58.864258+0000 Total time run:         61.5112
Total writes made:      305
Write size:             4194304
Object size:            4194304
Bandwidth (MB/sec):     19.8338
Stddev Bandwidth:       63.6972
Max bandwidth (MB/sec): 496
Min bandwidth (MB/sec): 0
Average IOPS:           4
Stddev IOPS:            15.9386
Max IOPS:               124
Min IOPS:               0
Average Latency(s):     22.4438
Stddev Latency(s):      6.7194
Max latency(s):         32.195
Min latency(s):         1.80009

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:24:59,651443454-07:00] INFO: > Stop system activity collection services[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:24:59,657533629-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:391 - rados_bench() > kill -INT 936206

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:24:59,663799985-07:00] INFO: >> for OSD host[0m
# ./benchmarks/bench-rados:396 - rados_bench() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.5 kill -INT 343305
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:24:59,671737710-07:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.5 kill -INT 343305
[1] 17:25:01 [SUCCESS] ljishen@10.10.2.5

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:25:01,021810901-07:00] INFO: >>> Copy system activity file from OSD host[0m
## ./benchmarks/bench-rados:403 - rados_bench() > basename -- /tmp/bench-rados/sys_activity_4MB_write.dat.osd_host.3
# ./benchmarks/bench-rados:403 - rados_bench() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.5:/tmp/bench-rados/sys_activity_4MB_write.dat.osd_host.3 /mnt/sda8/ceph-research/scripts/output/sys_activity_4MB_write.dat.osd_host.3


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:25:02,288818810-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:25:26,481590882-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 306 objects, 1.2 GiB
    usage:   3.6 GiB used, 396 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:25:26,488977640-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:25:35,512464111-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 306 objects, 1.2 GiB
    usage:   3.6 GiB used, 396 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:25:35,520849080-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:25:44,593847040-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 306 objects, 1.2 GiB
    usage:   3.6 GiB used, 396 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:25:44,602191131-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:25:53,842413412-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 306 objects, 1.2 GiB
    usage:   3.6 GiB used, 396 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:25:53,850724601-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:26:02,966497153-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 306 objects, 1.2 GiB
    usage:   3.6 GiB used, 396 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:26:02,974459224-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:26:02,980904137-07:00] INFO: > The cluster is idle now.[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:26:02,985220389-07:00] STAGE: Start benchmarking of mode seq ...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:26:02,993041535-07:00] INFO: > Start system activity collection services[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:26:02,999135366-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:341 - rados_bench() > rm --force /mnt/sda8/ceph-research/scripts/output/sys_activity_4MB_seq.dat.rados_bench_host.3
# ./benchmarks/bench-rados:342 - rados_bench() > sar_pid_rados_bench_host=938238
# ./benchmarks/bench-rados:342 - rados_bench() > S_TIME_FORMAT=ISO
# ./benchmarks/bench-rados:342 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/output/sys_activity_4MB_seq.dat.rados_bench_host.3 2

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:26:03,006629756-07:00] INFO: >> for OSD host[0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:26:03,015798641-07:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.5 bash
10.10.2.5: b'343854\n'
[1] 17:26:04 [SUCCESS] ljishen@10.10.2.5
343854

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:26:04,144747351-07:00] INFO: > Run rados bench[0m
# ./benchmarks/bench-rados:379 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/output/rados_bench_4MB_seq.log.3
## ./benchmarks/bench-rados:376 - rados_bench() > count_cpus 32-63,96-127
# ./benchmarks/bench-rados:376 - rados_bench() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 128 --show-time
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:26:04,165334168-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:26:04,168172957-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'bf2c5226-384d-11ec-b51d-53e6e728d2d3', '--', 'rados', 'bench', '99999999', 'seq', '--pool', 'bench_rados', '--concurrent-ios', '128', '--show-time'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid bf2c5226-384d-11ec-b51d-53e6e728d2d3 -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 128 --show-time
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
hints = 1
2021-10-29T00:26:07.222248+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-29T00:26:07.222248+0000     0       0         0         0         0         0           -           0
2021-10-29T00:26:08.222395+0000     1      64        64         0         0         0           -           0
2021-10-29T00:26:09.222475+0000     2      99        99         0         0         0           -           0
2021-10-29T00:26:10.222549+0000     3     127       140        13   17.3311   17.3333     2.98149     2.84807
2021-10-29T00:26:11.222621+0000     4     127       170        43   42.9952       120      3.5574     3.20958
2021-10-29T00:26:12.222695+0000     5     127       214        87   69.5927       176     3.24436     3.33148
2021-10-29T00:26:13.222777+0000     6     127       254       127   84.6581       160     3.32289     3.34114
2021-10-29T00:26:14.222858+0000     7     127       292       165   94.2765       152     3.21292     3.33054
2021-10-29T00:26:15.222927+0000     8       4       305       301   150.486       544    0.747868     2.68257
2021-10-29T00:26:16.223031+0000 Total time run:       8.18764
Total reads made:     305
Read size:            4194304
Object size:          4194304
Bandwidth (MB/sec):   149.005
Average IOPS:         37
Stddev IOPS:          44.2945
Max IOPS:             136
Min IOPS:             0
Average Latency(s):   2.66241
Max latency(s):       3.57688
Min latency(s):       0.332995

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:26:16,938169469-07:00] INFO: > Stop system activity collection services[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:26:16,944652043-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:391 - rados_bench() > kill -INT 938238

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:26:16,950967982-07:00] INFO: >> for OSD host[0m
# ./benchmarks/bench-rados:396 - rados_bench() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.5 kill -INT 343854
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:26:16,959006678-07:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.5 kill -INT 343854
[1] 17:26:18 [SUCCESS] ljishen@10.10.2.5

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:26:18,040853996-07:00] INFO: >>> Copy system activity file from OSD host[0m
## ./benchmarks/bench-rados:403 - rados_bench() > basename -- /tmp/bench-rados/sys_activity_4MB_seq.dat.osd_host.3
# ./benchmarks/bench-rados:403 - rados_bench() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.5:/tmp/bench-rados/sys_activity_4MB_seq.dat.osd_host.3 /mnt/sda8/ceph-research/scripts/output/sys_activity_4MB_seq.dat.osd_host.3


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:26:19,112650698-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:26:43,082499686-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 306 objects, 1.2 GiB
    usage:   3.6 GiB used, 396 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:26:43,090527551-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:26:52,131434309-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 306 objects, 1.2 GiB
    usage:   3.6 GiB used, 396 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:26:52,139329083-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:27:01,048346837-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 306 objects, 1.2 GiB
    usage:   3.6 GiB used, 396 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:27:01,056750120-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:27:10,155717148-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 306 objects, 1.2 GiB
    usage:   3.6 GiB used, 396 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:27:10,164142482-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:27:19,073344433-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 306 objects, 1.2 GiB
    usage:   3.6 GiB used, 396 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:27:19,082453445-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:27:19,089408640-07:00] INFO: > The cluster is idle now.[0m


[1;7;39;49m[2021-10-28T17:27:19,093446297-07:00][RUNNING][ROUND 1/7/21] object_size=16MB[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:27:19,097113456-07:00] STAGE: Launch a Ceph cluster (CEPH_HOST: 10.10.2.1, OSD_HOST: 10.10.2.5) ...[0m
# ./benchmarks/bench-rados:183 - launch_ceph_cluster() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.1 bash -euo pipefail
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:27:19,106780129-07:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.1 bash -euo pipefail
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T17:27:19,531690609-07:00] INFO: > Remove existing clusters\x1b[0m\n'
10.10.2.1: b'## bash:17 -  > basename -- /var/lib/ceph/bf2c5226-384d-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'# bash:17 -  > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm --host 10.10.2.1 --host 10.10.2.5 rm-cluster --force --fsid bf2c5226-384d-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T17:27:19,542446897-07:00] DEBUG: HOST_PARAMS: ['--host', '10.10.2.1', '--host', '10.10.2.5']\x1b[0m\n"
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T17:27:19,546285011-07:00] DEBUG: CEPHADM_PARAMS: ['rm-cluster', '--force', '--fsid', 'bf2c5226-384d-11ec-b51d-53e6e728d2d3']\x1b[0m\n"
10.10.2.1: b"# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:63 -  > /tmp/bench-rados/ceph-research/scripts/parallel-ssh --host 10.10.2.1 --host 10.10.2.5 'sudo python3 -u - rm-cluster --force --fsid bf2c5226-384d-11ec-b51d-53e6e728d2d3'\n"
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T17:27:19,555137672-07:00] DEBUG: command from stdin\x1b[0m\n'
10.10.2.1: b"# /tmp/bench-rados/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host 10.10.2.1 --host 10.10.2.5 'sudo python3 -u - rm-cluster --force --fsid bf2c5226-384d-11ec-b51d-53e6e728d2d3'\n"
10.10.2.1: b'[1] 17:27:25 [SUCCESS] 10.10.2.1\n[2] 17:27:28 [SUCCESS] 10.10.2.5\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T17:27:28,510926301-07:00] INFO: > Deploy a new cluster\x1b[0m\n'
10.10.2.1: b'# bash:24 -  > /tmp/bench-rados/ceph-research/scripts/ceph-deploy -m 10.10.2.1 -o sm1/10.10.2.5:/dev/nvme0n1\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T17:27:28,522645579-07:00] STAGE: Verify passwordless SSH to hosts, passwordless sudo via ljishen, and the existence of devices on hosts...\x1b[0m\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T17:27:28,527424753-07:00] INFO: > Check the host for the monitor\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:181 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@10.10.2.1 sudo --non-interactive --validate\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T17:27:28,679070483-07:00] INFO: > Check hosts for OSDs\x1b[0m\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T17:27:28,683228619-07:00] INFO: >> Check host 'sm1'\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:200 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 sudo --non-interactive --validate\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T17:27:29,771237582-07:00] INFO: >>> Check device '/dev/nvme0n1'\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:215 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 test -b /dev/nvme0n1\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:226 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 lsblk --nodeps --noheadings --output TYPE /dev/nvme0n1\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:227 - verify_parameters() > grep --word-regexp disk\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T17:27:31,900283076-07:00] STAGE: Reset devices on hosts...\x1b[0m\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T17:27:31,905199627-07:00] INFO: > Resetting device '/dev/nvme0n1' on host 'sm1'\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:288 - reset_host_devices() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 sudo sh\n'
10.10.2.1: b'  Removing ceph--370516e1--57bc--4c81--947d--712246660dd1-osd--block--84ae0339--87af--4fc2--bb0a--984d39e8d262 (252:0)\n'
10.10.2.1: b'  Archiving volume group "ceph-370516e1-57bc-4c81-947d-712246660dd1" metadata (seqno 5).\n'
10.10.2.1: b'  Releasing logical volume "osd-block-84ae0339-87af-4fc2-bb0a-984d39e8d262"\n'
10.10.2.1: b'  Creating volume group backup "/etc/lvm/backup/ceph-370516e1-57bc-4c81-947d-712246660dd1" (seqno 6).\n'
10.10.2.1: b'  Logical volume "osd-block-84ae0339-87af-4fc2-bb0a-984d39e8d262" successfully removed\n'
10.10.2.1: b'  Removing physical volume "/dev/nvme0n1" from volume group "ceph-370516e1-57bc-4c81-947d-712246660dd1"\n'
10.10.2.1: b'  Volume group "ceph-370516e1-57bc-4c81-947d-712246660dd1" successfully removed\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T17:27:34,226824543-07:00] STAGE: Bootstrap a new cluster...\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:317 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.1 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T17:27:34,236971966-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T17:27:34,240666080-07:00] DEBUG: CEPHADM_PARAMS: ['bootstrap', '--config', '/tmp/bench-rados/ceph-research/scripts/ceph.conf', '--mon-ip', '10.10.2.1', '--output-dir', '/tmp/bench-rados/deployment_data_root/etc/ceph', '--ssh-user', 'ljishen', '--skip-dashboard', '--no-minimize-config', '--allow-overwrite', '--allow-fqdn-hostname', '--orphan-initial-daemons', '--skip-monitoring-stack', '--single-host-defaults']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.1 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults\n'
10.10.2.1: b'Verifying podman|docker is present...\nVerifying lvm2 is present...\n'
10.10.2.1: b'Verifying time synchronization is in place...\n'
10.10.2.1: b'Unit ntp.service is enabled and running\n'
10.10.2.1: b'Repeating the final host check...\n'
10.10.2.1: b'podman|docker (/usr/bin/docker) is present\n'
10.10.2.1: b'systemctl is present\n'
10.10.2.1: b'lvcreate is present\n'
10.10.2.1: b'Unit ntp.service is enabled and running\n'
10.10.2.1: b'Host looks OK\n'
10.10.2.1: b'Cluster fsid: 02f106fe-384f-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Verifying IP 10.10.2.1 port 3300 ...\n'
10.10.2.1: b'Verifying IP 10.10.2.1 port 6789 ...\n'
10.10.2.1: b'Mon IP `10.10.2.1` is in CIDR network `10.10.2.0/24`\n'
10.10.2.1: b'- internal network (--cluster-network) has not been provided, OSD replication will default to the public_network\n'
10.10.2.1: b'Adjusting default settings to suit single-host cluster...\n'
10.10.2.1: b'Pulling container image quay.io/ceph/ceph:v16...\n'
10.10.2.1: b'Ceph version: ceph version 16.2.6 (ee28fb57e47e9f88813e24bbf4c14496ca299d31) pacific (stable)\nExtracting ceph user uid/gid from container image...\n'
10.10.2.1: b'Creating initial keys...\n'
10.10.2.1: b'Creating initial monmap...\n'
10.10.2.1: b'Creating mon...\n'
10.10.2.1: b'Waiting for mon to start...\n'
10.10.2.1: b'Waiting for mon...\n'
10.10.2.1: b'mon is available\n'
10.10.2.1: b'Setting mon public_network to 10.10.2.0/24\n'
10.10.2.1: b'Wrote config to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf\n'
10.10.2.1: b'Wrote keyring to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring\n'
10.10.2.1: b'Creating mgr...\n'
10.10.2.1: b'Verifying port 9283 ...\n'
10.10.2.1: b'Waiting for mgr to start...\n'
10.10.2.1: b'Waiting for mgr...\n'
10.10.2.1: b'mgr not available, waiting (1/15)...\n'
10.10.2.1: b'mgr not available, waiting (2/15)...\n'
10.10.2.1: b'mgr is available\n'
10.10.2.1: b'Enabling cephadm module...\n'
10.10.2.1: b'Waiting for the mgr to restart...\n'
10.10.2.1: b'Waiting for mgr epoch 5...\n'
10.10.2.1: b'mgr epoch 5 is available\nSetting orchestrator backend to cephadm...\n'
10.10.2.1: b'Generating ssh key...\n'
10.10.2.1: b'Wrote public SSH key to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub\nAdding key to ljishen@localhost authorized_keys...\n'
10.10.2.1: b'Adding host node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us...\n'
10.10.2.1: b'Deploying unmanaged mon service...\n'
10.10.2.1: b'Deploying unmanaged mgr service...\n'
10.10.2.1: b'You can access the Ceph CLI with:\n\n\tsudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --fsid 02f106fe-384f-11ec-b51d-53e6e728d2d3 -c /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf -k /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring\n\n'
10.10.2.1: b'Please consider enabling telemetry to help improve Ceph:\n\n\tceph telemetry on\n\nFor more information see:\n\n\thttps://docs.ceph.com/docs/pacific/mgr/telemetry/\n\n'
10.10.2.1: b'Bootstrap complete.\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T17:28:36,940785799-07:00] INFO: Sleep for 20 seconds\x1b[0m\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T17:28:56,948514387-07:00] INFO: > Set the default number of monitor daemons to 1\x1b[0m\n# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:337 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T17:28:56,957981552-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T17:28:56,961776465-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mon', '1']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1\n'
10.10.2.1: b'Inferring fsid 02f106fe-384f-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/02f106fe-384f-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b'Scheduled mon update...\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T17:29:06,730560133-07:00] INFO: > Set the default number of manager daemons to 1\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:342 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T17:29:06,740976843-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T17:29:06,744985548-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mgr', '1']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1\n'
10.10.2.1: b'Inferring fsid 02f106fe-384f-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/02f106fe-384f-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b'Scheduled mgr update...\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T17:29:16,647296861-07:00] STAGE: Add hosts to the cluster...\x1b[0m\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T17:29:16,653170642-07:00] INFO: > Adding host 'sm1'\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:361 - add_hosts() > ssh-copy-id -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -f -i /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub ljishen@sm1\n'
10.10.2.1: b'/usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: "/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub"\n'
10.10.2.1: b'\nNumber of key(s) added: 1\n\nNow try logging into the machine, with:   "ssh -o \'GlobalKnownHostsFile=/dev/null\' -o \'LogLevel=ERROR\' -o \'PasswordAuthentication=no\' -o \'StrictHostKeyChecking=no\' -o \'UserKnownHostsFile=/dev/null\' \'ljishen@sm1\'"\nand check to make sure that only the key(s) you wanted were added.\n\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:367 - add_hosts() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add sm1 10.10.2.5\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T17:29:17,808608509-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T17:29:17,812513259-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'host', 'add', 'sm1', '10.10.2.5']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add sm1 10.10.2.5\n'
10.10.2.1: b'Inferring fsid 02f106fe-384f-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/02f106fe-384f-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b"Added host 'sm1' with addr '10.10.2.5'\n"
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T17:29:28,742043255-07:00] INFO: Sleep for 20 seconds\x1b[0m\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T17:29:48,747044281-07:00] STAGE: Add OSDs to the cluster...\x1b[0m\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T17:29:48,753983204-07:00] INFO: > Adding devices ['/dev/nvme0n1'] on host 'sm1' as OSDs\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:388 - add_host_devices_as_osds() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd sm1:/dev/nvme0n1\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T17:29:48,764555927-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T17:29:48,768169490-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'daemon', 'add', 'osd', 'sm1:/dev/nvme0n1']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd sm1:/dev/nvme0n1\n'
10.10.2.1: b'Inferring fsid 02f106fe-384f-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/02f106fe-384f-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b"Created osd(s) 0 on host 'sm1'\n"
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T17:30:13,600164698-07:00] INFO: > Sleep for 20 seconds\x1b[0m\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T17:30:33,604966664-07:00] STAGE: Show Cluster Status\x1b[0m\n# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:408 - show_status() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T17:30:33,615032975-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T17:30:33,619061738-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', '--status']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status\n'
10.10.2.1: b'Inferring fsid 02f106fe-384f-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/02f106fe-384f-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b'  cluster:\n    id:     02f106fe-384f-11ec-b51d-53e6e728d2d3\n    health: HEALTH_WARN\n            1 pool(s) have no replicas configured\n \n  services:\n    mon: 1 daemons, quorum node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us (age 2m)\n    mgr: node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us.ffebky(active, since 2m)\n    osd: 1 osds: 1 up (since 20s), 1 in (since 40s)\n \n  data:\n    pools:   1 pools, 1 pgs\n    objects: 0 objects, 0 B\n    usage:   4.7 MiB used, 400 GiB / 400 GiB avail\n    pgs:     1 active+clean\n \n'
[1] 17:30:42 [SUCCESS] ljishen@10.10.2.1

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:27:19,531690609-07:00] INFO: > Remove existing clusters[0m
## bash:17 -  > basename -- /var/lib/ceph/bf2c5226-384d-11ec-b51d-53e6e728d2d3
# bash:17 -  > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm --host 10.10.2.1 --host 10.10.2.5 rm-cluster --force --fsid bf2c5226-384d-11ec-b51d-53e6e728d2d3
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:27:19,542446897-07:00] DEBUG: HOST_PARAMS: ['--host', '10.10.2.1', '--host', '10.10.2.5'][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:27:19,546285011-07:00] DEBUG: CEPHADM_PARAMS: ['rm-cluster', '--force', '--fsid', 'bf2c5226-384d-11ec-b51d-53e6e728d2d3'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:63 -  > /tmp/bench-rados/ceph-research/scripts/parallel-ssh --host 10.10.2.1 --host 10.10.2.5 'sudo python3 -u - rm-cluster --force --fsid bf2c5226-384d-11ec-b51d-53e6e728d2d3'
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:27:19,555137672-07:00] DEBUG: command from stdin[0m
# /tmp/bench-rados/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host 10.10.2.1 --host 10.10.2.5 'sudo python3 -u - rm-cluster --force --fsid bf2c5226-384d-11ec-b51d-53e6e728d2d3'
[1] 17:27:25 [SUCCESS] 10.10.2.1
[2] 17:27:28 [SUCCESS] 10.10.2.5

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:27:28,510926301-07:00] INFO: > Deploy a new cluster[0m
# bash:24 -  > /tmp/bench-rados/ceph-research/scripts/ceph-deploy -m 10.10.2.1 -o sm1/10.10.2.5:/dev/nvme0n1


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:27:28,522645579-07:00] STAGE: Verify passwordless SSH to hosts, passwordless sudo via ljishen, and the existence of devices on hosts...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:27:28,527424753-07:00] INFO: > Check the host for the monitor[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:181 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@10.10.2.1 sudo --non-interactive --validate

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:27:28,679070483-07:00] INFO: > Check hosts for OSDs[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:27:28,683228619-07:00] INFO: >> Check host 'sm1'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:200 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 sudo --non-interactive --validate

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:27:29,771237582-07:00] INFO: >>> Check device '/dev/nvme0n1'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:215 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 test -b /dev/nvme0n1
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:226 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 lsblk --nodeps --noheadings --output TYPE /dev/nvme0n1
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:227 - verify_parameters() > grep --word-regexp disk


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:27:31,900283076-07:00] STAGE: Reset devices on hosts...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:27:31,905199627-07:00] INFO: > Resetting device '/dev/nvme0n1' on host 'sm1'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:288 - reset_host_devices() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 sudo sh
  Removing ceph--370516e1--57bc--4c81--947d--712246660dd1-osd--block--84ae0339--87af--4fc2--bb0a--984d39e8d262 (252:0)
  Archiving volume group "ceph-370516e1-57bc-4c81-947d-712246660dd1" metadata (seqno 5).
  Releasing logical volume "osd-block-84ae0339-87af-4fc2-bb0a-984d39e8d262"
  Creating volume group backup "/etc/lvm/backup/ceph-370516e1-57bc-4c81-947d-712246660dd1" (seqno 6).
  Logical volume "osd-block-84ae0339-87af-4fc2-bb0a-984d39e8d262" successfully removed
  Removing physical volume "/dev/nvme0n1" from volume group "ceph-370516e1-57bc-4c81-947d-712246660dd1"
  Volume group "ceph-370516e1-57bc-4c81-947d-712246660dd1" successfully removed


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:27:34,226824543-07:00] STAGE: Bootstrap a new cluster...[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:317 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.1 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:27:34,236971966-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:27:34,240666080-07:00] DEBUG: CEPHADM_PARAMS: ['bootstrap', '--config', '/tmp/bench-rados/ceph-research/scripts/ceph.conf', '--mon-ip', '10.10.2.1', '--output-dir', '/tmp/bench-rados/deployment_data_root/etc/ceph', '--ssh-user', 'ljishen', '--skip-dashboard', '--no-minimize-config', '--allow-overwrite', '--allow-fqdn-hostname', '--orphan-initial-daemons', '--skip-monitoring-stack', '--single-host-defaults'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.1 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults
Verifying podman|docker is present...
Verifying lvm2 is present...
Verifying time synchronization is in place...
Unit ntp.service is enabled and running
Repeating the final host check...
podman|docker (/usr/bin/docker) is present
systemctl is present
lvcreate is present
Unit ntp.service is enabled and running
Host looks OK
Cluster fsid: 02f106fe-384f-11ec-b51d-53e6e728d2d3
Verifying IP 10.10.2.1 port 3300 ...
Verifying IP 10.10.2.1 port 6789 ...
Mon IP `10.10.2.1` is in CIDR network `10.10.2.0/24`
- internal network (--cluster-network) has not been provided, OSD replication will default to the public_network
Adjusting default settings to suit single-host cluster...
Pulling container image quay.io/ceph/ceph:v16...
Ceph version: ceph version 16.2.6 (ee28fb57e47e9f88813e24bbf4c14496ca299d31) pacific (stable)
Extracting ceph user uid/gid from container image...
Creating initial keys...
Creating initial monmap...
Creating mon...
Waiting for mon to start...
Waiting for mon...
mon is available
Setting mon public_network to 10.10.2.0/24
Wrote config to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf
Wrote keyring to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring
Creating mgr...
Verifying port 9283 ...
Waiting for mgr to start...
Waiting for mgr...
mgr not available, waiting (1/15)...
mgr not available, waiting (2/15)...
mgr is available
Enabling cephadm module...
Waiting for the mgr to restart...
Waiting for mgr epoch 5...
mgr epoch 5 is available
Setting orchestrator backend to cephadm...
Generating ssh key...
Wrote public SSH key to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub
Adding key to ljishen@localhost authorized_keys...
Adding host node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us...
Deploying unmanaged mon service...
Deploying unmanaged mgr service...
You can access the Ceph CLI with:

	sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --fsid 02f106fe-384f-11ec-b51d-53e6e728d2d3 -c /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf -k /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring

Please consider enabling telemetry to help improve Ceph:

	ceph telemetry on

For more information see:

	https://docs.ceph.com/docs/pacific/mgr/telemetry/

Bootstrap complete.

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:28:36,940785799-07:00] INFO: Sleep for 20 seconds[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:28:56,948514387-07:00] INFO: > Set the default number of monitor daemons to 1[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:337 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:28:56,957981552-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:28:56,961776465-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mon', '1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1
Inferring fsid 02f106fe-384f-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/02f106fe-384f-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Scheduled mon update...

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:29:06,730560133-07:00] INFO: > Set the default number of manager daemons to 1[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:342 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:29:06,740976843-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:29:06,744985548-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mgr', '1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1
Inferring fsid 02f106fe-384f-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/02f106fe-384f-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Scheduled mgr update...


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:29:16,647296861-07:00] STAGE: Add hosts to the cluster...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:29:16,653170642-07:00] INFO: > Adding host 'sm1'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:361 - add_hosts() > ssh-copy-id -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -f -i /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub ljishen@sm1
/usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: "/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub"

Number of key(s) added: 1

Now try logging into the machine, with:   "ssh -o 'GlobalKnownHostsFile=/dev/null' -o 'LogLevel=ERROR' -o 'PasswordAuthentication=no' -o 'StrictHostKeyChecking=no' -o 'UserKnownHostsFile=/dev/null' 'ljishen@sm1'"
and check to make sure that only the key(s) you wanted were added.

# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:367 - add_hosts() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add sm1 10.10.2.5
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:29:17,808608509-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:29:17,812513259-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'host', 'add', 'sm1', '10.10.2.5'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add sm1 10.10.2.5
Inferring fsid 02f106fe-384f-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/02f106fe-384f-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Added host 'sm1' with addr '10.10.2.5'

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:29:28,742043255-07:00] INFO: Sleep for 20 seconds[0m


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:29:48,747044281-07:00] STAGE: Add OSDs to the cluster...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:29:48,753983204-07:00] INFO: > Adding devices ['/dev/nvme0n1'] on host 'sm1' as OSDs[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:388 - add_host_devices_as_osds() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd sm1:/dev/nvme0n1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:29:48,764555927-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:29:48,768169490-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'daemon', 'add', 'osd', 'sm1:/dev/nvme0n1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd sm1:/dev/nvme0n1
Inferring fsid 02f106fe-384f-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/02f106fe-384f-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Created osd(s) 0 on host 'sm1'

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:30:13,600164698-07:00] INFO: > Sleep for 20 seconds[0m


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:30:33,604966664-07:00] STAGE: Show Cluster Status[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:408 - show_status() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:30:33,615032975-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:30:33,619061738-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', '--status'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status
Inferring fsid 02f106fe-384f-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/02f106fe-384f-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
  cluster:
    id:     02f106fe-384f-11ec-b51d-53e6e728d2d3
    health: HEALTH_WARN
            1 pool(s) have no replicas configured
 
  services:
    mon: 1 daemons, quorum node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us (age 2m)
    mgr: node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us.ffebky(active, since 2m)
    osd: 1 osds: 1 up (since 20s), 1 in (since 40s)
 
  data:
    pools:   1 pools, 1 pgs
    objects: 0 objects, 0 B
    usage:   4.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     1 active+clean
 

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:30:42,286162994-07:00] INFO: > Copy configuration files to local[0m
# ./benchmarks/bench-rados:220 - launch_ceph_cluster() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.1 sudo chown --changes --recursive ljishen /tmp/bench-rados/deployment_data_root/etc/ceph
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:30:42,293991834-07:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.1 sudo chown --changes --recursive ljishen /tmp/bench-rados/deployment_data_root/etc/ceph
[1] 17:30:42 [SUCCESS] ljishen@10.10.2.1
# ./benchmarks/bench-rados:223 - launch_ceph_cluster() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.1:/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf
# ./benchmarks/bench-rados:226 - launch_ceph_cluster() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.1:/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:30:42,768932228-07:00] STAGE: Update local ceph.conf as a client...[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:30:42,772932866-07:00] STAGE: Configure Ceph pool...[0m
# ./benchmarks/bench-rados:257 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_max_backfills 32
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:30:42,795098217-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:30:42,797888515-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '02f106fe-384f-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'config', 'set', 'osd', 'osd_max_backfills', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 02f106fe-384f-11ec-b51d-53e6e728d2d3 -- ceph config set osd osd_max_backfills 32
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:258 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_active 32
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:30:46,913449396-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:30:46,916608458-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '02f106fe-384f-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_active', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 02f106fe-384f-11ec-b51d-53e6e728d2d3 -- ceph config set osd osd_recovery_max_active 32
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:259 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_single_start 8
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:30:51,055197803-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:30:51,058405957-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '02f106fe-384f-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_single_start', '8'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 02f106fe-384f-11ec-b51d-53e6e728d2d3 -- ceph config set osd osd_recovery_max_single_start 8
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:260 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_op_priority 63
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:30:55,122342874-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:30:55,125403640-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '02f106fe-384f-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_op_priority', '63'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 02f106fe-384f-11ec-b51d-53e6e728d2d3 -- ceph config set osd osd_recovery_op_priority 63
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:262 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config show-with-defaults osd.0
# ./benchmarks/bench-rados:263 - configure_ceph_pool() > grep 'osd_max_backfills\|osd_recovery'
# ./benchmarks/bench-rados:263 - configure_ceph_pool() > column -t -s ' '
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
osd_max_backfills                        32         mon
osd_recovery_cost                        20971520   default
osd_recovery_delay_start                 0.000000   default
osd_recovery_max_active                  32         mon
osd_recovery_max_active_hdd              3          default
osd_recovery_max_active_ssd              10         default
osd_recovery_max_chunk                   8388608    default
osd_recovery_max_omap_entries_per_chunk  8096       default
osd_recovery_max_single_start            8          mon
osd_recovery_op_priority                 63         mon
osd_recovery_op_warn_multiple            16         default
osd_recovery_priority                    5          default
osd_recovery_retry_interval              30.000000  default
osd_recovery_sleep                       0.000000   default
osd_recovery_sleep_hdd                   0.100000   default
osd_recovery_sleep_hybrid                0.025000   default
osd_recovery_sleep_ssd                   0.000000   default
# ./benchmarks/bench-rados:265 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool create bench_rados 128 128 replicated --size 1 --pg-num-min 128
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:31:03,175074173-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:31:03,178031896-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '02f106fe-384f-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'create', 'bench_rados', '128', '128', 'replicated', '--size', '1', '--pg-num-min', '128'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 02f106fe-384f-11ec-b51d-53e6e728d2d3 -- ceph osd pool create bench_rados 128 128 replicated --size 1 --pg-num-min 128
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
pool 'bench_rados' created
# ./benchmarks/bench-rados:267 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados min_size 1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:31:08,058790759-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:31:08,062002360-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '02f106fe-384f-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'min_size', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 02f106fe-384f-11ec-b51d-53e6e728d2d3 -- ceph osd pool set bench_rados min_size 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 min_size to 1
# ./benchmarks/bench-rados:268 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados pg_autoscale_mode off
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:31:12,484427137-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:31:12,487281816-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '02f106fe-384f-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'pg_autoscale_mode', 'off'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 02f106fe-384f-11ec-b51d-53e6e728d2d3 -- ceph osd pool set bench_rados pg_autoscale_mode off
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 pg_autoscale_mode to off
# ./benchmarks/bench-rados:269 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool application enable bench_rados benchmark
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:31:17,309534009-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:31:17,312534172-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '02f106fe-384f-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'application', 'enable', 'bench_rados', 'benchmark'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 02f106fe-384f-11ec-b51d-53e6e728d2d3 -- ceph osd pool application enable bench_rados benchmark
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
enabled application 'benchmark' on pool 'bench_rados'
# ./benchmarks/bench-rados:270 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados noscrub 1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:31:22,084817350-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:31:22,087847399-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '02f106fe-384f-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'noscrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 02f106fe-384f-11ec-b51d-53e6e728d2d3 -- ceph osd pool set bench_rados noscrub 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 noscrub to 1
# ./benchmarks/bench-rados:271 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados nodeep-scrub 1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:31:26,593292249-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:31:26,596391327-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '02f106fe-384f-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'nodeep-scrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 02f106fe-384f-11ec-b51d-53e6e728d2d3 -- ceph osd pool set bench_rados nodeep-scrub 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 nodeep-scrub to 1
# ./benchmarks/bench-rados:272 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool ls detail
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:31:31,492437009-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:31:31,495442642-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '02f106fe-384f-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'ls', 'detail'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 02f106fe-384f-11ec-b51d-53e6e728d2d3 -- ceph osd pool ls detail
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
pool 1 'device_health_metrics' replicated size 1 min_size 1 crush_rule 0 object_hash rjenkins pg_num 64 pgp_num 64 autoscale_mode on last_change 18 lfor 0/0/15 flags hashpspool stripe_width 0 pg_num_min 1 application mgr_devicehealth
pool 2 'bench_rados' replicated size 1 min_size 1 crush_rule 0 object_hash rjenkins pg_num 128 pgp_num 128 autoscale_mode off last_change 20 flags hashpspool,noscrub,nodeep-scrub stripe_width 0 pg_num_min 128 application benchmark

# ./benchmarks/bench-rados:273 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd df tree
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:31:35,344150746-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:31:35,347248913-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '02f106fe-384f-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'df', 'tree'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 02f106fe-384f-11ec-b51d-53e6e728d2d3 -- ceph osd df tree
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
ID  CLASS  WEIGHT   REWEIGHT  SIZE     RAW USE  DATA     OMAP  META     AVAIL    %USE  VAR   PGS  STATUS  TYPE NAME    
-1         0.39059         -  400 GiB  5.7 MiB  168 KiB   0 B  5.6 MiB  400 GiB  0.00  1.00    -          root default 
-3         0.39059         -  400 GiB  5.7 MiB  168 KiB   0 B  5.6 MiB  400 GiB  0.00  1.00    -              host sm1 
 0    ssd  0.39059   1.00000  400 GiB  5.7 MiB  168 KiB   0 B  5.6 MiB  400 GiB  0.00  1.00  192      up          osd.0
                       TOTAL  400 GiB  5.7 MiB  168 KiB   0 B  5.6 MiB  400 GiB  0.00                                  
MIN/MAX VAR: 1.00/1.00  STDDEV: 0


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:31:39,511930393-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:32:03,485160098-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:32:12,492733102-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:32:21,434811832-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:32:21,442989680-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:32:30,566404060-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:32:30,574495845-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:32:39,607501471-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:32:39,616534641-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:32:48,747595186-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:32:48,755941121-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:32:57,745887699-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:32:57,754413152-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:32:57,760691651-07:00] INFO: > The cluster is idle now.[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:32:57,764684735-07:00] STAGE: Start benchmarking of mode write ...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:32:57,771880352-07:00] INFO: > Start system activity collection services[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:32:57,777987548-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:341 - rados_bench() > rm --force /mnt/sda8/ceph-research/scripts/output/sys_activity_16MB_write.dat.rados_bench_host.1
# ./benchmarks/bench-rados:342 - rados_bench() > sar_pid_rados_bench_host=946448
# ./benchmarks/bench-rados:342 - rados_bench() > S_TIME_FORMAT=ISO
# ./benchmarks/bench-rados:342 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/output/sys_activity_16MB_write.dat.rados_bench_host.1 2

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:32:57,785149612-07:00] INFO: >> for OSD host[0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:32:57,794225593-07:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.5 bash
10.10.2.5: b'348711\n'
[1] 17:32:58 [SUCCESS] ljishen@10.10.2.5
348711

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:32:58,908931416-07:00] INFO: > Run rados bench[0m
# ./benchmarks/bench-rados:370 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/output/rados_bench_16MB_write.log.1
## ./benchmarks/bench-rados:361 - rados_bench() > count_cpus 32-63,96-127
# ./benchmarks/bench-rados:361 - rados_bench() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 60 write --pool bench_rados -b 16777216 -O 16777216 --concurrent-ios 128 --show-time --write-object --write-omap --write-xattr --no-cleanup
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:32:58,929283992-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:32:58,932118964-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '02f106fe-384f-11ec-b51d-53e6e728d2d3', '--', 'rados', 'bench', '60', 'write', '--pool', 'bench_rados', '-b', '16777216', '-O', '16777216', '--concurrent-ios', '128', '--show-time', '--write-object', '--write-omap', '--write-xattr', '--no-cleanup'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 02f106fe-384f-11ec-b51d-53e6e728d2d3 -- rados bench 60 write --pool bench_rados -b 16777216 -O 16777216 --concurrent-ios 128 --show-time --write-object --write-omap --write-xattr --no-cleanup
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
hints = 1
2021-10-29T00:33:01.928063+0000 Maintaining 128 concurrent writes of 16777216 bytes to objects of size 16777216 for up to 60 seconds or 0 objects
2021-10-29T00:33:01.928098+0000 Object prefix: benchmark_data_node-2.bluefield2.ucsc-cmps10_7
2021-10-29T00:33:02.600350+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-29T00:33:02.600350+0000     0       0         0         0         0         0           -           0
2021-10-29T00:33:03.600523+0000     1       3         3         0         0         0           -           0
2021-10-29T00:33:04.600641+0000     2       5         5         0         0         0           -           0
2021-10-29T00:33:05.600723+0000     3       7         7         0         0         0           -           0
2021-10-29T00:33:06.600836+0000     4       9         9         0         0         0           -           0
2021-10-29T00:33:07.600946+0000     5      12        12         0         0         0           -           0
2021-10-29T00:33:08.601056+0000     6      14        14         0         0         0           -           0
2021-10-29T00:33:09.601145+0000     7      16        16         0         0         0           -           0
2021-10-29T00:33:10.601259+0000     8      18        18         0         0         0           -           0
2021-10-29T00:33:11.601368+0000     9      21        21         0         0         0           -           0
2021-10-29T00:33:12.601459+0000    10      23        23         0         0         0           -           0
2021-10-29T00:33:13.601571+0000    11      25        25         0         0         0           -           0
2021-10-29T00:33:14.601682+0000    12      28        28         0         0         0           -           0
2021-10-29T00:33:15.601790+0000    13      29        29         0         0         0           -           0
2021-10-29T00:33:16.601900+0000    14      32        32         0         0         0           -           0
2021-10-29T00:33:17.601982+0000    15      34        34         0         0         0           -           0
2021-10-29T00:33:18.602098+0000    16      34        34         0         0         0           -           0
2021-10-29T00:33:19.602216+0000    17      34        34         0         0         0           -           0
2021-10-29T00:33:20.602325+0000    18      35        35         0         0         0           -           0
2021-10-29T00:33:21.602407+0000    19      35        35         0         0         0           -           0
2021-10-29T00:33:22.602501+0000 min lat: 9999 max lat: 0 avg lat: 0
2021-10-29T00:33:22.602501+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-29T00:33:22.602501+0000    20      35        35         0         0         0           -           0
2021-10-29T00:33:23.602657+0000    21      35        35         0         0         0           -           0
2021-10-29T00:33:24.602765+0000    22      36        36         0         0         0           -           0
2021-10-29T00:33:25.602846+0000    23      36        36         0         0         0           -           0
2021-10-29T00:33:26.602963+0000    24      36        36         0         0         0           -           0
2021-10-29T00:33:27.603080+0000    25      36        36         0         0         0           -           0
2021-10-29T00:33:28.603192+0000    26      37        37         0         0         0           -           0
2021-10-29T00:33:29.603270+0000    27      39        39         0         0         0           -           0
2021-10-29T00:33:30.603381+0000    28      42        42         0         0         0           -           0
2021-10-29T00:33:31.603492+0000    29      44        44         0         0         0           -           0
2021-10-29T00:33:32.603603+0000    30      46        46         0         0         0           -           0
2021-10-29T00:33:33.603689+0000    31      48        48         0         0         0           -           0
2021-10-29T00:33:34.603800+0000    32      50        50         0         0         0           -           0
2021-10-29T00:33:35.603914+0000    33      53        53         0         0         0           -           0
2021-10-29T00:33:36.604033+0000    34      55        55         0         0         0           -           0
2021-10-29T00:33:37.604116+0000    35      57        57         0         0         0           -           0
2021-10-29T00:33:38.604227+0000    36      60        60         0         0         0           -           0
2021-10-29T00:33:39.604341+0000    37      62        62         0         0         0           -           0
2021-10-29T00:33:40.604454+0000    38      64        64         0         0         0           -           0
2021-10-29T00:33:41.604536+0000    39      66        66         0         0         0           -           0
2021-10-29T00:33:42.604637+0000 min lat: 9999 max lat: 0 avg lat: 0
2021-10-29T00:33:42.604637+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-29T00:33:42.604637+0000    40      66        66         0         0         0           -           0
2021-10-29T00:33:43.604803+0000    41      66        66         0         0         0           -           0
2021-10-29T00:33:44.604918+0000    42      66        66         0         0         0           -           0
2021-10-29T00:33:45.605000+0000    43      67        67         0         0         0           -           0
2021-10-29T00:33:46.605114+0000    44      67        67         0         0         0           -           0
2021-10-29T00:33:47.605228+0000    45      67        67         0         0         0           -           0
2021-10-29T00:33:48.605340+0000    46      67        67         0         0         0           -           0
2021-10-29T00:33:49.605421+0000    47      67        67         0         0         0           -           0
2021-10-29T00:33:50.605532+0000    48      67        67         0         0         0           -           0
2021-10-29T00:33:51.605645+0000    49      67        67         0         0         0           -           0
2021-10-29T00:33:52.605734+0000    50      67        67         0         0         0           -           0
2021-10-29T00:33:53.605822+0000    51      68        68         0         0         0           -           0
2021-10-29T00:33:54.605933+0000    52      68        68         0         0         0           -           0
2021-10-29T00:33:55.606043+0000    53      68        68         0         0         0           -           0
2021-10-29T00:33:56.606155+0000    54      69        69         0         0         0           -           0
2021-10-29T00:33:57.606238+0000    55      70        70         0         0         0           -           0
2021-10-29T00:33:58.606347+0000    56      71        71         0         0         0           -           0
2021-10-29T00:33:59.606458+0000    57      71        71         0         0         0           -           0
2021-10-29T00:34:00.606572+0000    58      72        72         0         0         0           -           0
2021-10-29T00:34:01.606656+0000    59      72        72         0         0         0           -           0
2021-10-29T00:34:02.606768+0000 min lat: 9999 max lat: 0 avg lat: 0
2021-10-29T00:34:02.606768+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-29T00:34:02.606768+0000    60      73        73         0         0         0           -           0
2021-10-29T00:34:03.606901+0000    61      74        74         0         0         0           -           0
2021-10-29T00:34:04.607012+0000    62      75        75         0         0         0           -           0
2021-10-29T00:34:05.607097+0000    63      76        76         0         0         0           -           0
2021-10-29T00:34:06.607165+0000    64      77        77         0         0         0           -           0
2021-10-29T00:34:07.607245+0000    65      78        78         0         0         0           -           0
2021-10-29T00:34:08.607331+0000    66      79        79         0         0         0           -           0
2021-10-29T00:34:09.607429+0000    67      80        80         0         0         0           -           0
2021-10-29T00:34:10.607560+0000    68      80        80         0         0         0           -           0
2021-10-29T00:34:11.607658+0000    69      81        81         0         0         0           -           0
2021-10-29T00:34:12.607773+0000    70      82        82         0         0         0           -           0
2021-10-29T00:34:13.607891+0000    71      83        83         0         0         0           -           0
2021-10-29T00:34:14.608033+0000    72      83        83         0         0         0           -           0
2021-10-29T00:34:15.608100+0000    73      86        86         0         0         0           -           0
2021-10-29T00:34:16.608208+0000    74      88        88         0         0         0           -           0
2021-10-29T00:34:17.608321+0000    75      90        90         0         0         0           -           0
2021-10-29T00:34:18.608437+0000    76      92        92         0         0         0           -           0
2021-10-29T00:34:19.608561+0000    77      94        94         0         0         0           -           0
2021-10-29T00:34:20.608674+0000    78      97        97         0         0         0           -           0
2021-10-29T00:34:21.608791+0000    79      98        98         0         0         0           -           0
2021-10-29T00:34:22.608930+0000 min lat: 9999 max lat: 0 avg lat: 0
2021-10-29T00:34:22.608930+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-29T00:34:22.608930+0000    80      98        98         0         0         0           -           0
2021-10-29T00:34:23.609053+0000    81      98        98         0         0         0           -           0
2021-10-29T00:34:24.609127+0000    82      99        99         0         0         0           -           0
2021-10-29T00:34:25.609247+0000    83      99        99         0         0         0           -           0
2021-10-29T00:34:26.609332+0000    84      99        99         0         0         0           -           0
2021-10-29T00:34:27.609438+0000    85      99        99         0         0         0           -           0
2021-10-29T00:34:28.609550+0000    86      99        99         0         0         0           -           0
2021-10-29T00:34:29.609634+0000    87      99        99         0         0         0           -           0
2021-10-29T00:34:30.609713+0000    88     100       100         0         0         0           -           0
2021-10-29T00:34:31.609849+0000    89     100       100         0         0         0           -           0
2021-10-29T00:34:32.609939+0000    90     100       100         0         0         0           -           0
2021-10-29T00:34:33.610026+0000    91     100       100         0         0         0           -           0
2021-10-29T00:34:34.610140+0000    92     100       100         0         0         0           -           0
2021-10-29T00:34:35.610210+0000    93     102       102         0         0         0           -           0
2021-10-29T00:34:36.610317+0000    94     104       104         0         0         0           -           0
2021-10-29T00:34:37.610433+0000    95     106       106         0         0         0           -           0
2021-10-29T00:34:38.610543+0000    96     109       109         0         0         0           -           0
2021-10-29T00:34:39.610662+0000    97     111       111         0         0         0           -           0
2021-10-29T00:34:40.610786+0000    98     113       113         0         0         0           -           0
2021-10-29T00:34:41.610896+0000    99     116       116         0         0         0           -           0
2021-10-29T00:34:42.611040+0000 min lat: 9999 max lat: 0 avg lat: 0
2021-10-29T00:34:42.611040+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-29T00:34:42.611040+0000   100     118       118         0         0         0           -           0
2021-10-29T00:34:43.611215+0000   101     121       121         0         0         0           -           0
2021-10-29T00:34:44.611326+0000   102     123       123         0         0         0           -           0
2021-10-29T00:34:45.611407+0000   103     125       125         0         0         0           -           0
2021-10-29T00:34:46.611521+0000   104     126       126         0         0         0           -           0
2021-10-29T00:34:47.611630+0000   105     126       126         0         0         0           -           0
2021-10-29T00:34:48.611741+0000   106     126       126         0         0         0           -           0
2021-10-29T00:34:49.611819+0000   107     127       127         0         0         0           -           0
2021-10-29T00:34:50.611929+0000   108     127       127         0         0         0           -           0
2021-10-29T00:34:51.612036+0000   109       2       128       126   18.4934   18.4954     5.93216     60.7403
2021-10-29T00:34:52.612126+0000   110       2       128       126   18.3253         0           -     60.7403
2021-10-29T00:34:53.612217+0000   111       1       128       127   18.3044         8     7.04907     60.3176
2021-10-29T00:34:54.612368+0000 Total time run:         111.239
Total writes made:      128
Write size:             16777216
Object size:            16777216
Bandwidth (MB/sec):     18.4108
Stddev Bandwidth:       1.90634
Max bandwidth (MB/sec): 18.4954
Min bandwidth (MB/sec): 0
Average IOPS:           1
Stddev IOPS:            0.0949158
Max IOPS:               1
Min IOPS:               0
Average Latency(s):     59.8852
Stddev Latency(s):      34.788
Max latency(s):         108.457
Min latency(s):         4.97366

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:34:55,553535184-07:00] INFO: > Stop system activity collection services[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:34:55,559487588-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:391 - rados_bench() > kill -INT 946448

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:34:55,565940226-07:00] INFO: >> for OSD host[0m
# ./benchmarks/bench-rados:396 - rados_bench() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.5 kill -INT 348711
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:34:55,573989452-07:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.5 kill -INT 348711
[1] 17:34:56 [SUCCESS] ljishen@10.10.2.5

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:34:56,881332373-07:00] INFO: >>> Copy system activity file from OSD host[0m
## ./benchmarks/bench-rados:403 - rados_bench() > basename -- /tmp/bench-rados/sys_activity_16MB_write.dat.osd_host.1
# ./benchmarks/bench-rados:403 - rados_bench() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.5:/tmp/bench-rados/sys_activity_16MB_write.dat.osd_host.1 /mnt/sda8/ceph-research/scripts/output/sys_activity_16MB_write.dat.osd_host.1


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:34:58,155145791-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:35:22,174477735-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 129 objects, 2.0 GiB
    usage:   6.6 GiB used, 393 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:35:22,182561466-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:35:31,224283143-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 129 objects, 2.0 GiB
    usage:   6.5 GiB used, 394 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:35:31,232970531-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:35:40,162308258-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 129 objects, 2.0 GiB
    usage:   6.1 GiB used, 394 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:35:40,170908362-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:35:49,303853193-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 129 objects, 2.0 GiB
    usage:   6.1 GiB used, 394 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:35:49,312179180-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:35:58,193923275-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 129 objects, 2.0 GiB
    usage:   6.1 GiB used, 394 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:35:58,202682589-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:35:58,209303022-07:00] INFO: > The cluster is idle now.[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:35:58,213367159-07:00] STAGE: Start benchmarking of mode seq ...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:35:58,220781509-07:00] INFO: > Start system activity collection services[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:35:58,226964899-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:341 - rados_bench() > rm --force /mnt/sda8/ceph-research/scripts/output/sys_activity_16MB_seq.dat.rados_bench_host.1
# ./benchmarks/bench-rados:342 - rados_bench() > sar_pid_rados_bench_host=948755
# ./benchmarks/bench-rados:342 - rados_bench() > S_TIME_FORMAT=ISO
# ./benchmarks/bench-rados:342 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/output/sys_activity_16MB_seq.dat.rados_bench_host.1 2

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:35:58,234268220-07:00] INFO: >> for OSD host[0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:35:58,244351808-07:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.5 bash
10.10.2.5: b'349411\n'
[1] 17:35:59 [SUCCESS] ljishen@10.10.2.5
349411

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:35:59,582327931-07:00] INFO: > Run rados bench[0m
# ./benchmarks/bench-rados:379 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/output/rados_bench_16MB_seq.log.1
## ./benchmarks/bench-rados:376 - rados_bench() > count_cpus 32-63,96-127
# ./benchmarks/bench-rados:376 - rados_bench() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 128 --show-time
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:35:59,602866257-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:35:59,605812428-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '02f106fe-384f-11ec-b51d-53e6e728d2d3', '--', 'rados', 'bench', '99999999', 'seq', '--pool', 'bench_rados', '--concurrent-ios', '128', '--show-time'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 02f106fe-384f-11ec-b51d-53e6e728d2d3 -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 128 --show-time
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
hints = 1
2021-10-29T00:36:02.672461+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-29T00:36:02.672461+0000     0       0         0         0         0         0           -           0
2021-10-29T00:36:03.672570+0000     1      10        10         0         0         0           -           0
2021-10-29T00:36:04.672648+0000     2      22        22         0         0         0           -           0
2021-10-29T00:36:05.672724+0000     3      32        32         0         0         0           -           0
2021-10-29T00:36:06.672803+0000     4      42        42         0         0         0           -           0
2021-10-29T00:36:07.672882+0000     5      53        53         0         0         0           -           0
2021-10-29T00:36:08.672961+0000     6      62        62         0         0         0           -           0
2021-10-29T00:36:09.673037+0000     7      73        73         0         0         0           -           0
2021-10-29T00:36:10.673119+0000     8      85        85         0         0         0           -           0
2021-10-29T00:36:11.673217+0000     9      94        94         0         0         0           -           0
2021-10-29T00:36:12.673293+0000    10     103       103         0         0         0           -           0
2021-10-29T00:36:13.673370+0000    11     113       113         0         0         0           -           0
2021-10-29T00:36:14.673446+0000    12     123       123         0         0         0           -           0
2021-10-29T00:36:15.673522+0000    13       5       128       123   151.371   151.385    0.792764     6.73889
2021-10-29T00:36:16.673642+0000 Total time run:       13.4521
Total reads made:     128
Read size:            16777216
Object size:          16777216
Bandwidth (MB/sec):   152.243
Average IOPS:         9
Stddev IOPS:          2.49615
Max IOPS:             9
Min IOPS:             0
Average Latency(s):   6.52492
Max latency(s):       12.418
Min latency(s):       0.792764

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:36:17,394843958-07:00] INFO: > Stop system activity collection services[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:36:17,401536318-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:391 - rados_bench() > kill -INT 948755

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:36:17,407835256-07:00] INFO: >> for OSD host[0m
# ./benchmarks/bench-rados:396 - rados_bench() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.5 kill -INT 349411
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:36:17,415848464-07:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.5 kill -INT 349411
[1] 17:36:18 [SUCCESS] ljishen@10.10.2.5

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:36:18,557054509-07:00] INFO: >>> Copy system activity file from OSD host[0m
## ./benchmarks/bench-rados:403 - rados_bench() > basename -- /tmp/bench-rados/sys_activity_16MB_seq.dat.osd_host.1
# ./benchmarks/bench-rados:403 - rados_bench() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.5:/tmp/bench-rados/sys_activity_16MB_seq.dat.osd_host.1 /mnt/sda8/ceph-research/scripts/output/sys_activity_16MB_seq.dat.osd_host.1


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:36:19,637453438-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:36:43,697092606-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 129 objects, 2.0 GiB
    usage:   6.1 GiB used, 394 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:36:43,705195422-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:36:52,695401720-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 129 objects, 2.0 GiB
    usage:   6.1 GiB used, 394 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:36:52,703923365-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:37:01,753153626-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 129 objects, 2.0 GiB
    usage:   6.1 GiB used, 394 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:37:01,761732790-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:37:10,851002263-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 129 objects, 2.0 GiB
    usage:   6.1 GiB used, 394 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:37:10,859769752-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:37:19,999016333-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 129 objects, 2.0 GiB
    usage:   6.1 GiB used, 394 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:37:20,007833215-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:37:20,014756369-07:00] INFO: > The cluster is idle now.[0m


[1;7;39;49m[2021-10-28T17:37:20,017240579-07:00][RUNNING][ROUND 2/7/21] object_size=16MB[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:37:20,021061419-07:00] STAGE: Launch a Ceph cluster (CEPH_HOST: 10.10.2.1, OSD_HOST: 10.10.2.5) ...[0m
# ./benchmarks/bench-rados:183 - launch_ceph_cluster() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.1 bash -euo pipefail
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:37:20,030598458-07:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.1 bash -euo pipefail
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T17:37:20,475719748-07:00] INFO: > Remove existing clusters\x1b[0m\n'
10.10.2.1: b'## bash:17 -  > basename -- /var/lib/ceph/02f106fe-384f-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'# bash:17 -  > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm --host 10.10.2.1 --host 10.10.2.5 rm-cluster --force --fsid 02f106fe-384f-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T17:37:20,486183797-07:00] DEBUG: HOST_PARAMS: ['--host', '10.10.2.1', '--host', '10.10.2.5']\x1b[0m\n"
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T17:37:20,489579350-07:00] DEBUG: CEPHADM_PARAMS: ['rm-cluster', '--force', '--fsid', '02f106fe-384f-11ec-b51d-53e6e728d2d3']\x1b[0m\n"
10.10.2.1: b"# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:63 -  > /tmp/bench-rados/ceph-research/scripts/parallel-ssh --host 10.10.2.1 --host 10.10.2.5 'sudo python3 -u - rm-cluster --force --fsid 02f106fe-384f-11ec-b51d-53e6e728d2d3'\n"
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T17:37:20,497327223-07:00] DEBUG: command from stdin\x1b[0m\n'
10.10.2.1: b"# /tmp/bench-rados/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host 10.10.2.1 --host 10.10.2.5 'sudo python3 -u - rm-cluster --force --fsid 02f106fe-384f-11ec-b51d-53e6e728d2d3'\n"
10.10.2.1: b'[1] 17:37:26 [SUCCESS] 10.10.2.1\n[2] 17:37:32 [SUCCESS] 10.10.2.5\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T17:37:32,597063765-07:00] INFO: > Deploy a new cluster\x1b[0m\n'
10.10.2.1: b'# bash:24 -  > /tmp/bench-rados/ceph-research/scripts/ceph-deploy -m 10.10.2.1 -o sm1/10.10.2.5:/dev/nvme0n1\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T17:37:32,608035348-07:00] STAGE: Verify passwordless SSH to hosts, passwordless sudo via ljishen, and the existence of devices on hosts...\x1b[0m\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T17:37:32,612070602-07:00] INFO: > Check the host for the monitor\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:181 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@10.10.2.1 sudo --non-interactive --validate\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T17:37:32,763608622-07:00] INFO: > Check hosts for OSDs\x1b[0m\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T17:37:32,768197868-07:00] INFO: >> Check host 'sm1'\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:200 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 sudo --non-interactive --validate\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T17:37:33,858680850-07:00] INFO: >>> Check device '/dev/nvme0n1'\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:215 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 test -b /dev/nvme0n1\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:226 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 lsblk --nodeps --noheadings --output TYPE /dev/nvme0n1\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:227 - verify_parameters() > grep --word-regexp disk\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T17:37:36,019397592-07:00] STAGE: Reset devices on hosts...\x1b[0m\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T17:37:36,024213404-07:00] INFO: > Resetting device '/dev/nvme0n1' on host 'sm1'\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:288 - reset_host_devices() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 sudo sh\n'
10.10.2.1: b'  Removing ceph--505c3b9c--256e--4ece--8c84--1e822dc5aae7-osd--block--f4b6498a--cf6f--4a97--aed6--26798920e817 (252:0)\n'
10.10.2.1: b'  Archiving volume group "ceph-505c3b9c-256e-4ece-8c84-1e822dc5aae7" metadata (seqno 5).\n'
10.10.2.1: b'  Releasing logical volume "osd-block-f4b6498a-cf6f-4a97-aed6-26798920e817"\n'
10.10.2.1: b'  Creating volume group backup "/etc/lvm/backup/ceph-505c3b9c-256e-4ece-8c84-1e822dc5aae7" (seqno 6).\n'
10.10.2.1: b'  Logical volume "osd-block-f4b6498a-cf6f-4a97-aed6-26798920e817" successfully removed\n'
10.10.2.1: b'  Removing physical volume "/dev/nvme0n1" from volume group "ceph-505c3b9c-256e-4ece-8c84-1e822dc5aae7"\n'
10.10.2.1: b'  Volume group "ceph-505c3b9c-256e-4ece-8c84-1e822dc5aae7" successfully removed\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T17:37:38,383670250-07:00] STAGE: Bootstrap a new cluster...\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:317 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.1 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T17:37:38,393159607-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T17:37:38,396796823-07:00] DEBUG: CEPHADM_PARAMS: ['bootstrap', '--config', '/tmp/bench-rados/ceph-research/scripts/ceph.conf', '--mon-ip', '10.10.2.1', '--output-dir', '/tmp/bench-rados/deployment_data_root/etc/ceph', '--ssh-user', 'ljishen', '--skip-dashboard', '--no-minimize-config', '--allow-overwrite', '--allow-fqdn-hostname', '--orphan-initial-daemons', '--skip-monitoring-stack', '--single-host-defaults']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.1 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults\n'
10.10.2.1: b'Verifying podman|docker is present...\n'
10.10.2.1: b'Verifying lvm2 is present...\n'
10.10.2.1: b'Verifying time synchronization is in place...\n'
10.10.2.1: b'Unit ntp.service is enabled and running\n'
10.10.2.1: b'Repeating the final host check...\n'
10.10.2.1: b'podman|docker (/usr/bin/docker) is present\n'
10.10.2.1: b'systemctl is present\n'
10.10.2.1: b'lvcreate is present\n'
10.10.2.1: b'Unit ntp.service is enabled and running\nHost looks OK\n'
10.10.2.1: b'Cluster fsid: 6b0c4acc-3850-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Verifying IP 10.10.2.1 port 3300 ...\n'
10.10.2.1: b'Verifying IP 10.10.2.1 port 6789 ...\n'
10.10.2.1: b'Mon IP `10.10.2.1` is in CIDR network `10.10.2.0/24`\n'
10.10.2.1: b'- internal network (--cluster-network) has not been provided, OSD replication will default to the public_network\n'
10.10.2.1: b'Adjusting default settings to suit single-host cluster...\n'
10.10.2.1: b'Pulling container image quay.io/ceph/ceph:v16...\n'
10.10.2.1: b'Ceph version: ceph version 16.2.6 (ee28fb57e47e9f88813e24bbf4c14496ca299d31) pacific (stable)\n'
10.10.2.1: b'Extracting ceph user uid/gid from container image...\n'
10.10.2.1: b'Creating initial keys...\n'
10.10.2.1: b'Creating initial monmap...\n'
10.10.2.1: b'Creating mon...\n'
10.10.2.1: b'Waiting for mon to start...\n'
10.10.2.1: b'Waiting for mon...\n'
10.10.2.1: b'mon is available\n'
10.10.2.1: b'Setting mon public_network to 10.10.2.0/24\n'
10.10.2.1: b'Wrote config to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf\n'
10.10.2.1: b'Wrote keyring to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring\n'
10.10.2.1: b'Creating mgr...\n'
10.10.2.1: b'Verifying port 9283 ...\n'
10.10.2.1: b'Waiting for mgr to start...\n'
10.10.2.1: b'Waiting for mgr...\n'
10.10.2.1: b'mgr not available, waiting (1/15)...\n'
10.10.2.1: b'mgr not available, waiting (2/15)...\n'
10.10.2.1: b'mgr is available\n'
10.10.2.1: b'Enabling cephadm module...\n'
10.10.2.1: b'Waiting for the mgr to restart...\n'
10.10.2.1: b'Waiting for mgr epoch 5...\n'
10.10.2.1: b'mgr epoch 5 is available\nSetting orchestrator backend to cephadm...\n'
10.10.2.1: b'Generating ssh key...\n'
10.10.2.1: b'Wrote public SSH key to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub\n'
10.10.2.1: b'Adding key to ljishen@localhost authorized_keys...\n'
10.10.2.1: b'Adding host node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us...\n'
10.10.2.1: b'Deploying unmanaged mon service...\n'
10.10.2.1: b'Deploying unmanaged mgr service...\n'
10.10.2.1: b'You can access the Ceph CLI with:\n\n\tsudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --fsid 6b0c4acc-3850-11ec-b51d-53e6e728d2d3 -c /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf -k /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring\n\n'
10.10.2.1: b'Please consider enabling telemetry to help improve Ceph:\n\n\tceph telemetry on\n\nFor more information see:\n\n\thttps://docs.ceph.com/docs/pacific/mgr/telemetry/\n\n'
10.10.2.1: b'Bootstrap complete.\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T17:38:41,044746566-07:00] INFO: Sleep for 20 seconds\x1b[0m\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T17:39:01,051991036-07:00] INFO: > Set the default number of monitor daemons to 1\x1b[0m\n# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:337 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T17:39:01,061626878-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T17:39:01,064959873-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mon', '1']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1\n'
10.10.2.1: b'Inferring fsid 6b0c4acc-3850-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/6b0c4acc-3850-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b'Scheduled mon update...\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T17:39:10,754927391-07:00] INFO: > Set the default number of manager daemons to 1\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:342 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T17:39:10,764368418-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T17:39:10,767496537-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mgr', '1']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1\n'
10.10.2.1: b'Inferring fsid 6b0c4acc-3850-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/6b0c4acc-3850-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b'Scheduled mgr update...\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T17:39:20,466874365-07:00] STAGE: Add hosts to the cluster...\x1b[0m\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T17:39:20,472463681-07:00] INFO: > Adding host 'sm1'\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:361 - add_hosts() > ssh-copy-id -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -f -i /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub ljishen@sm1\n'
10.10.2.1: b'/usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: "/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub"\n'
10.10.2.1: b'\nNumber of key(s) added: 1\n\nNow try logging into the machine, with:   "ssh -o \'GlobalKnownHostsFile=/dev/null\' -o \'LogLevel=ERROR\' -o \'PasswordAuthentication=no\' -o \'StrictHostKeyChecking=no\' -o \'UserKnownHostsFile=/dev/null\' \'ljishen@sm1\'"\nand check to make sure that only the key(s) you wanted were added.\n\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:367 - add_hosts() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add sm1 10.10.2.5\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T17:39:21,629024799-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T17:39:21,632172055-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'host', 'add', 'sm1', '10.10.2.5']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add sm1 10.10.2.5\n'
10.10.2.1: b'Inferring fsid 6b0c4acc-3850-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/6b0c4acc-3850-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b"Added host 'sm1' with addr '10.10.2.5'\n"
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T17:39:32,431782895-07:00] INFO: Sleep for 20 seconds\x1b[0m\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T17:39:52,436564970-07:00] STAGE: Add OSDs to the cluster...\x1b[0m\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T17:39:52,443166709-07:00] INFO: > Adding devices ['/dev/nvme0n1'] on host 'sm1' as OSDs\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:388 - add_host_devices_as_osds() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd sm1:/dev/nvme0n1\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T17:39:52,453086746-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T17:39:52,456539035-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'daemon', 'add', 'osd', 'sm1:/dev/nvme0n1']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd sm1:/dev/nvme0n1\n'
10.10.2.1: b'Inferring fsid 6b0c4acc-3850-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/6b0c4acc-3850-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b"Created osd(s) 0 on host 'sm1'\n"
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T17:40:17,332406735-07:00] INFO: > Sleep for 20 seconds\x1b[0m\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T17:40:37,336834153-07:00] STAGE: Show Cluster Status\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:408 - show_status() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T17:40:37,346295137-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T17:40:37,349431252-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', '--status']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status\n'
10.10.2.1: b'Inferring fsid 6b0c4acc-3850-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/6b0c4acc-3850-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b'  cluster:\n    id:     6b0c4acc-3850-11ec-b51d-53e6e728d2d3\n    health: HEALTH_WARN\n            1 pool(s) have no replicas configured\n \n  services:\n    mon: 1 daemons, quorum node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us (age 2m)\n    mgr: node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us.lhiqnz(active, since 2m)\n    osd: 1 osds: 1 up (since 20s), 1 in (since 41s)\n \n  data:\n    pools:   1 pools, 1 pgs\n    objects: 0 objects, 0 B\n    usage:   4.7 MiB used, 400 GiB / 400 GiB avail\n    pgs:     1 active+clean\n \n'
[1] 17:40:46 [SUCCESS] ljishen@10.10.2.1

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:37:20,475719748-07:00] INFO: > Remove existing clusters[0m
## bash:17 -  > basename -- /var/lib/ceph/02f106fe-384f-11ec-b51d-53e6e728d2d3
# bash:17 -  > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm --host 10.10.2.1 --host 10.10.2.5 rm-cluster --force --fsid 02f106fe-384f-11ec-b51d-53e6e728d2d3
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:37:20,486183797-07:00] DEBUG: HOST_PARAMS: ['--host', '10.10.2.1', '--host', '10.10.2.5'][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:37:20,489579350-07:00] DEBUG: CEPHADM_PARAMS: ['rm-cluster', '--force', '--fsid', '02f106fe-384f-11ec-b51d-53e6e728d2d3'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:63 -  > /tmp/bench-rados/ceph-research/scripts/parallel-ssh --host 10.10.2.1 --host 10.10.2.5 'sudo python3 -u - rm-cluster --force --fsid 02f106fe-384f-11ec-b51d-53e6e728d2d3'
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:37:20,497327223-07:00] DEBUG: command from stdin[0m
# /tmp/bench-rados/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host 10.10.2.1 --host 10.10.2.5 'sudo python3 -u - rm-cluster --force --fsid 02f106fe-384f-11ec-b51d-53e6e728d2d3'
[1] 17:37:26 [SUCCESS] 10.10.2.1
[2] 17:37:32 [SUCCESS] 10.10.2.5

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:37:32,597063765-07:00] INFO: > Deploy a new cluster[0m
# bash:24 -  > /tmp/bench-rados/ceph-research/scripts/ceph-deploy -m 10.10.2.1 -o sm1/10.10.2.5:/dev/nvme0n1


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:37:32,608035348-07:00] STAGE: Verify passwordless SSH to hosts, passwordless sudo via ljishen, and the existence of devices on hosts...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:37:32,612070602-07:00] INFO: > Check the host for the monitor[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:181 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@10.10.2.1 sudo --non-interactive --validate

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:37:32,763608622-07:00] INFO: > Check hosts for OSDs[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:37:32,768197868-07:00] INFO: >> Check host 'sm1'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:200 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 sudo --non-interactive --validate

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:37:33,858680850-07:00] INFO: >>> Check device '/dev/nvme0n1'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:215 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 test -b /dev/nvme0n1
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:226 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 lsblk --nodeps --noheadings --output TYPE /dev/nvme0n1
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:227 - verify_parameters() > grep --word-regexp disk


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:37:36,019397592-07:00] STAGE: Reset devices on hosts...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:37:36,024213404-07:00] INFO: > Resetting device '/dev/nvme0n1' on host 'sm1'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:288 - reset_host_devices() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 sudo sh
  Removing ceph--505c3b9c--256e--4ece--8c84--1e822dc5aae7-osd--block--f4b6498a--cf6f--4a97--aed6--26798920e817 (252:0)
  Archiving volume group "ceph-505c3b9c-256e-4ece-8c84-1e822dc5aae7" metadata (seqno 5).
  Releasing logical volume "osd-block-f4b6498a-cf6f-4a97-aed6-26798920e817"
  Creating volume group backup "/etc/lvm/backup/ceph-505c3b9c-256e-4ece-8c84-1e822dc5aae7" (seqno 6).
  Logical volume "osd-block-f4b6498a-cf6f-4a97-aed6-26798920e817" successfully removed
  Removing physical volume "/dev/nvme0n1" from volume group "ceph-505c3b9c-256e-4ece-8c84-1e822dc5aae7"
  Volume group "ceph-505c3b9c-256e-4ece-8c84-1e822dc5aae7" successfully removed


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:37:38,383670250-07:00] STAGE: Bootstrap a new cluster...[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:317 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.1 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:37:38,393159607-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:37:38,396796823-07:00] DEBUG: CEPHADM_PARAMS: ['bootstrap', '--config', '/tmp/bench-rados/ceph-research/scripts/ceph.conf', '--mon-ip', '10.10.2.1', '--output-dir', '/tmp/bench-rados/deployment_data_root/etc/ceph', '--ssh-user', 'ljishen', '--skip-dashboard', '--no-minimize-config', '--allow-overwrite', '--allow-fqdn-hostname', '--orphan-initial-daemons', '--skip-monitoring-stack', '--single-host-defaults'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.1 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults
Verifying podman|docker is present...
Verifying lvm2 is present...
Verifying time synchronization is in place...
Unit ntp.service is enabled and running
Repeating the final host check...
podman|docker (/usr/bin/docker) is present
systemctl is present
lvcreate is present
Unit ntp.service is enabled and running
Host looks OK
Cluster fsid: 6b0c4acc-3850-11ec-b51d-53e6e728d2d3
Verifying IP 10.10.2.1 port 3300 ...
Verifying IP 10.10.2.1 port 6789 ...
Mon IP `10.10.2.1` is in CIDR network `10.10.2.0/24`
- internal network (--cluster-network) has not been provided, OSD replication will default to the public_network
Adjusting default settings to suit single-host cluster...
Pulling container image quay.io/ceph/ceph:v16...
Ceph version: ceph version 16.2.6 (ee28fb57e47e9f88813e24bbf4c14496ca299d31) pacific (stable)
Extracting ceph user uid/gid from container image...
Creating initial keys...
Creating initial monmap...
Creating mon...
Waiting for mon to start...
Waiting for mon...
mon is available
Setting mon public_network to 10.10.2.0/24
Wrote config to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf
Wrote keyring to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring
Creating mgr...
Verifying port 9283 ...
Waiting for mgr to start...
Waiting for mgr...
mgr not available, waiting (1/15)...
mgr not available, waiting (2/15)...
mgr is available
Enabling cephadm module...
Waiting for the mgr to restart...
Waiting for mgr epoch 5...
mgr epoch 5 is available
Setting orchestrator backend to cephadm...
Generating ssh key...
Wrote public SSH key to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub
Adding key to ljishen@localhost authorized_keys...
Adding host node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us...
Deploying unmanaged mon service...
Deploying unmanaged mgr service...
You can access the Ceph CLI with:

	sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --fsid 6b0c4acc-3850-11ec-b51d-53e6e728d2d3 -c /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf -k /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring

Please consider enabling telemetry to help improve Ceph:

	ceph telemetry on

For more information see:

	https://docs.ceph.com/docs/pacific/mgr/telemetry/

Bootstrap complete.

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:38:41,044746566-07:00] INFO: Sleep for 20 seconds[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:39:01,051991036-07:00] INFO: > Set the default number of monitor daemons to 1[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:337 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:39:01,061626878-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:39:01,064959873-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mon', '1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1
Inferring fsid 6b0c4acc-3850-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/6b0c4acc-3850-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Scheduled mon update...

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:39:10,754927391-07:00] INFO: > Set the default number of manager daemons to 1[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:342 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:39:10,764368418-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:39:10,767496537-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mgr', '1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1
Inferring fsid 6b0c4acc-3850-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/6b0c4acc-3850-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Scheduled mgr update...


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:39:20,466874365-07:00] STAGE: Add hosts to the cluster...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:39:20,472463681-07:00] INFO: > Adding host 'sm1'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:361 - add_hosts() > ssh-copy-id -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -f -i /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub ljishen@sm1
/usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: "/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub"

Number of key(s) added: 1

Now try logging into the machine, with:   "ssh -o 'GlobalKnownHostsFile=/dev/null' -o 'LogLevel=ERROR' -o 'PasswordAuthentication=no' -o 'StrictHostKeyChecking=no' -o 'UserKnownHostsFile=/dev/null' 'ljishen@sm1'"
and check to make sure that only the key(s) you wanted were added.

# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:367 - add_hosts() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add sm1 10.10.2.5
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:39:21,629024799-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:39:21,632172055-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'host', 'add', 'sm1', '10.10.2.5'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add sm1 10.10.2.5
Inferring fsid 6b0c4acc-3850-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/6b0c4acc-3850-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Added host 'sm1' with addr '10.10.2.5'

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:39:32,431782895-07:00] INFO: Sleep for 20 seconds[0m


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:39:52,436564970-07:00] STAGE: Add OSDs to the cluster...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:39:52,443166709-07:00] INFO: > Adding devices ['/dev/nvme0n1'] on host 'sm1' as OSDs[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:388 - add_host_devices_as_osds() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd sm1:/dev/nvme0n1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:39:52,453086746-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:39:52,456539035-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'daemon', 'add', 'osd', 'sm1:/dev/nvme0n1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd sm1:/dev/nvme0n1
Inferring fsid 6b0c4acc-3850-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/6b0c4acc-3850-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Created osd(s) 0 on host 'sm1'

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:40:17,332406735-07:00] INFO: > Sleep for 20 seconds[0m


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:40:37,336834153-07:00] STAGE: Show Cluster Status[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:408 - show_status() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:40:37,346295137-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:40:37,349431252-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', '--status'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status
Inferring fsid 6b0c4acc-3850-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/6b0c4acc-3850-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
  cluster:
    id:     6b0c4acc-3850-11ec-b51d-53e6e728d2d3
    health: HEALTH_WARN
            1 pool(s) have no replicas configured
 
  services:
    mon: 1 daemons, quorum node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us (age 2m)
    mgr: node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us.lhiqnz(active, since 2m)
    osd: 1 osds: 1 up (since 20s), 1 in (since 41s)
 
  data:
    pools:   1 pools, 1 pgs
    objects: 0 objects, 0 B
    usage:   4.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     1 active+clean
 

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:40:46,278437472-07:00] INFO: > Copy configuration files to local[0m
# ./benchmarks/bench-rados:220 - launch_ceph_cluster() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.1 sudo chown --changes --recursive ljishen /tmp/bench-rados/deployment_data_root/etc/ceph
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:40:46,286524048-07:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.1 sudo chown --changes --recursive ljishen /tmp/bench-rados/deployment_data_root/etc/ceph
[1] 17:40:46 [SUCCESS] ljishen@10.10.2.1
# ./benchmarks/bench-rados:223 - launch_ceph_cluster() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.1:/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf
# ./benchmarks/bench-rados:226 - launch_ceph_cluster() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.1:/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:40:46,760871160-07:00] STAGE: Update local ceph.conf as a client...[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:40:46,764857952-07:00] STAGE: Configure Ceph pool...[0m
# ./benchmarks/bench-rados:257 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_max_backfills 32
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:40:46,786839057-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:40:46,789737618-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '6b0c4acc-3850-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'config', 'set', 'osd', 'osd_max_backfills', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 6b0c4acc-3850-11ec-b51d-53e6e728d2d3 -- ceph config set osd osd_max_backfills 32
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:258 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_active 32
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:40:50,873248592-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:40:50,876535044-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '6b0c4acc-3850-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_active', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 6b0c4acc-3850-11ec-b51d-53e6e728d2d3 -- ceph config set osd osd_recovery_max_active 32
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:259 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_single_start 8
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:40:54,914288435-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:40:54,917233173-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '6b0c4acc-3850-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_single_start', '8'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 6b0c4acc-3850-11ec-b51d-53e6e728d2d3 -- ceph config set osd osd_recovery_max_single_start 8
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:260 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_op_priority 63
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:40:58,954178471-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:40:58,957224621-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '6b0c4acc-3850-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_op_priority', '63'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 6b0c4acc-3850-11ec-b51d-53e6e728d2d3 -- ceph config set osd osd_recovery_op_priority 63
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:262 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config show-with-defaults osd.0
# ./benchmarks/bench-rados:263 - configure_ceph_pool() > grep 'osd_max_backfills\|osd_recovery'
# ./benchmarks/bench-rados:263 - configure_ceph_pool() > column -t -s ' '
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
osd_max_backfills                        32         mon
osd_recovery_cost                        20971520   default
osd_recovery_delay_start                 0.000000   default
osd_recovery_max_active                  32         mon
osd_recovery_max_active_hdd              3          default
osd_recovery_max_active_ssd              10         default
osd_recovery_max_chunk                   8388608    default
osd_recovery_max_omap_entries_per_chunk  8096       default
osd_recovery_max_single_start            8          mon
osd_recovery_op_priority                 3          default
osd_recovery_op_warn_multiple            16         default
osd_recovery_priority                    5          default
osd_recovery_retry_interval              30.000000  default
osd_recovery_sleep                       0.000000   default
osd_recovery_sleep_hdd                   0.100000   default
osd_recovery_sleep_hybrid                0.025000   default
osd_recovery_sleep_ssd                   0.000000   default
# ./benchmarks/bench-rados:265 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool create bench_rados 128 128 replicated --size 1 --pg-num-min 128
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:41:07,030114245-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:41:07,033133484-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '6b0c4acc-3850-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'create', 'bench_rados', '128', '128', 'replicated', '--size', '1', '--pg-num-min', '128'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 6b0c4acc-3850-11ec-b51d-53e6e728d2d3 -- ceph osd pool create bench_rados 128 128 replicated --size 1 --pg-num-min 128
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
pool 'bench_rados' created
# ./benchmarks/bench-rados:267 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados min_size 1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:41:12,028195324-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:41:12,031240382-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '6b0c4acc-3850-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'min_size', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 6b0c4acc-3850-11ec-b51d-53e6e728d2d3 -- ceph osd pool set bench_rados min_size 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 min_size to 1
# ./benchmarks/bench-rados:268 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados pg_autoscale_mode off
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:41:16,311094587-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:41:16,314203174-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '6b0c4acc-3850-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'pg_autoscale_mode', 'off'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 6b0c4acc-3850-11ec-b51d-53e6e728d2d3 -- ceph osd pool set bench_rados pg_autoscale_mode off
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 pg_autoscale_mode to off
# ./benchmarks/bench-rados:269 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool application enable bench_rados benchmark
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:41:21,250168151-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:41:21,253107479-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '6b0c4acc-3850-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'application', 'enable', 'bench_rados', 'benchmark'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 6b0c4acc-3850-11ec-b51d-53e6e728d2d3 -- ceph osd pool application enable bench_rados benchmark
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
enabled application 'benchmark' on pool 'bench_rados'
# ./benchmarks/bench-rados:270 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados noscrub 1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:41:26,051971259-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:41:26,055094514-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '6b0c4acc-3850-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'noscrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 6b0c4acc-3850-11ec-b51d-53e6e728d2d3 -- ceph osd pool set bench_rados noscrub 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 noscrub to 1
# ./benchmarks/bench-rados:271 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados nodeep-scrub 1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:41:30,316905657-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:41:30,319903025-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '6b0c4acc-3850-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'nodeep-scrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 6b0c4acc-3850-11ec-b51d-53e6e728d2d3 -- ceph osd pool set bench_rados nodeep-scrub 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 nodeep-scrub to 1
# ./benchmarks/bench-rados:272 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool ls detail
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:41:35,340381659-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:41:35,343273839-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '6b0c4acc-3850-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'ls', 'detail'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 6b0c4acc-3850-11ec-b51d-53e6e728d2d3 -- ceph osd pool ls detail
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
pool 1 'device_health_metrics' replicated size 1 min_size 1 crush_rule 0 object_hash rjenkins pg_num 64 pgp_num 64 autoscale_mode on last_change 18 lfor 0/0/15 flags hashpspool stripe_width 0 pg_num_min 1 application mgr_devicehealth
pool 2 'bench_rados' replicated size 1 min_size 1 crush_rule 0 object_hash rjenkins pg_num 128 pgp_num 128 autoscale_mode off last_change 20 flags hashpspool,noscrub,nodeep-scrub stripe_width 0 pg_num_min 128 application benchmark

# ./benchmarks/bench-rados:273 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd df tree
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:41:39,475183292-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:41:39,478057587-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '6b0c4acc-3850-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'df', 'tree'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 6b0c4acc-3850-11ec-b51d-53e6e728d2d3 -- ceph osd df tree
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
ID  CLASS  WEIGHT   REWEIGHT  SIZE     RAW USE  DATA     OMAP  META     AVAIL    %USE  VAR   PGS  STATUS  TYPE NAME    
-1         0.39059         -  400 GiB  5.7 MiB  168 KiB   0 B  5.6 MiB  400 GiB  0.00  1.00    -          root default 
-3         0.39059         -  400 GiB  5.7 MiB  168 KiB   0 B  5.6 MiB  400 GiB  0.00  1.00    -              host sm1 
 0    ssd  0.39059   1.00000  400 GiB  5.7 MiB  168 KiB   0 B  5.6 MiB  400 GiB  0.00  1.00  192      up          osd.0
                       TOTAL  400 GiB  5.7 MiB  168 KiB   0 B  5.6 MiB  400 GiB  0.00                                  
MIN/MAX VAR: 1.00/1.00  STDDEV: 0


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:41:43,517240992-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:42:07,568047676-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:42:16,746308881-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:42:25,818910632-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:42:25,827684413-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:42:35,033909183-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:42:35,042831604-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:42:44,050634274-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:42:44,058692126-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:42:53,206645312-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:42:53,214889075-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:43:02,197203499-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:43:02,205716649-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:43:02,212093163-07:00] INFO: > The cluster is idle now.[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:43:02,216133706-07:00] STAGE: Start benchmarking of mode write ...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:43:02,224021398-07:00] INFO: > Start system activity collection services[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:43:02,229806438-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:341 - rados_bench() > rm --force /mnt/sda8/ceph-research/scripts/output/sys_activity_16MB_write.dat.rados_bench_host.2
# ./benchmarks/bench-rados:342 - rados_bench() > sar_pid_rados_bench_host=956271
# ./benchmarks/bench-rados:342 - rados_bench() > S_TIME_FORMAT=ISO
# ./benchmarks/bench-rados:342 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/output/sys_activity_16MB_write.dat.rados_bench_host.2 2

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:43:02,237123494-07:00] INFO: >> for OSD host[0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:43:02,246462129-07:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.5 bash
10.10.2.5: b'354268\n'
[1] 17:43:03 [SUCCESS] ljishen@10.10.2.5
354268

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:43:03,368476420-07:00] INFO: > Run rados bench[0m
# ./benchmarks/bench-rados:370 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/output/rados_bench_16MB_write.log.2
## ./benchmarks/bench-rados:361 - rados_bench() > count_cpus 32-63,96-127
# ./benchmarks/bench-rados:361 - rados_bench() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 60 write --pool bench_rados -b 16777216 -O 16777216 --concurrent-ios 128 --show-time --write-object --write-omap --write-xattr --no-cleanup
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:43:03,389276911-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:43:03,392325946-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '6b0c4acc-3850-11ec-b51d-53e6e728d2d3', '--', 'rados', 'bench', '60', 'write', '--pool', 'bench_rados', '-b', '16777216', '-O', '16777216', '--concurrent-ios', '128', '--show-time', '--write-object', '--write-omap', '--write-xattr', '--no-cleanup'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 6b0c4acc-3850-11ec-b51d-53e6e728d2d3 -- rados bench 60 write --pool bench_rados -b 16777216 -O 16777216 --concurrent-ios 128 --show-time --write-object --write-omap --write-xattr --no-cleanup
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
hints = 1
2021-10-29T00:43:06.339140+0000 Maintaining 128 concurrent writes of 16777216 bytes to objects of size 16777216 for up to 60 seconds or 0 objects
2021-10-29T00:43:06.339174+0000 Object prefix: benchmark_data_node-2.bluefield2.ucsc-cmps10_7
2021-10-29T00:43:07.014494+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-29T00:43:07.014494+0000     0       0         0         0         0         0           -           0
2021-10-29T00:43:08.014656+0000     1       3         3         0         0         0           -           0
2021-10-29T00:43:09.014731+0000     2       6         6         0         0         0           -           0
2021-10-29T00:43:10.014823+0000     3       8         8         0         0         0           -           0
2021-10-29T00:43:11.014918+0000     4      10        10         0         0         0           -           0
2021-10-29T00:43:12.015008+0000     5      13        13         0         0         0           -           0
2021-10-29T00:43:13.015100+0000     6      15        15         0         0         0           -           0
2021-10-29T00:43:14.015206+0000     7      17        17         0         0         0           -           0
2021-10-29T00:43:15.015282+0000     8      20        20         0         0         0           -           0
2021-10-29T00:43:16.015369+0000     9      22        22         0         0         0           -           0
2021-10-29T00:43:17.015460+0000    10      24        24         0         0         0           -           0
2021-10-29T00:43:18.015546+0000    11      26        26         0         0         0           -           0
2021-10-29T00:43:19.015636+0000    12      29        29         0         0         0           -           0
2021-10-29T00:43:20.015738+0000    13      31        31         0         0         0           -           0
2021-10-29T00:43:21.015812+0000    14      34        34         0         0         0           -           0
2021-10-29T00:43:22.015909+0000    15      34        34         0         0         0           -           0
2021-10-29T00:43:23.016025+0000    16      34        34         0         0         0           -           0
2021-10-29T00:43:24.016145+0000    17      34        34         0         0         0           -           0
2021-10-29T00:43:25.016255+0000    18      35        35         0         0         0           -           0
2021-10-29T00:43:26.016352+0000    19      35        35         0         0         0           -           0
2021-10-29T00:43:27.016463+0000 min lat: 9999 max lat: 0 avg lat: 0
2021-10-29T00:43:27.016463+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-29T00:43:27.016463+0000    20      35        35         0         0         0           -           0
2021-10-29T00:43:28.016581+0000    21      35        35         0         0         0           -           0
2021-10-29T00:43:29.016653+0000    22      36        36         0         0         0           -           0
2021-10-29T00:43:30.016748+0000    23      36        36         0         0         0           -           0
2021-10-29T00:43:31.016863+0000    24      36        36         0         0         0           -           0
2021-10-29T00:43:32.016982+0000    25      36        36         0         0         0           -           0
2021-10-29T00:43:33.017070+0000    26      38        38         0         0         0           -           0
2021-10-29T00:43:34.017161+0000    27      41        41         0         0         0           -           0
2021-10-29T00:43:35.017250+0000    28      43        43         0         0         0           -           0
2021-10-29T00:43:36.017337+0000    29      45        45         0         0         0           -           0
2021-10-29T00:43:37.017434+0000    30      46        46         0         0         0           -           0
2021-10-29T00:43:38.017536+0000    31      49        49         0         0         0           -           0
2021-10-29T00:43:39.017644+0000    32      51        51         0         0         0           -           0
2021-10-29T00:43:40.017719+0000    33      54        54         0         0         0           -           0
2021-10-29T00:43:41.017791+0000    34      56        56         0         0         0           -           0
2021-10-29T00:43:42.017876+0000    35      58        58         0         0         0           -           0
2021-10-29T00:43:43.017974+0000    36      61        61         0         0         0           -           0
2021-10-29T00:43:44.018081+0000    37      63        63         0         0         0           -           0
2021-10-29T00:43:45.018150+0000    38      66        66         0         0         0           -           0
2021-10-29T00:43:46.018252+0000    39      66        66         0         0         0           -           0
2021-10-29T00:43:47.018366+0000 min lat: 9999 max lat: 0 avg lat: 0
2021-10-29T00:43:47.018366+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-29T00:43:47.018366+0000    40      66        66         0         0         0           -           0
2021-10-29T00:43:48.018484+0000    41      66        66         0         0         0           -           0
2021-10-29T00:43:49.018597+0000    42      67        67         0         0         0           -           0
2021-10-29T00:43:50.018701+0000    43      67        67         0         0         0           -           0
2021-10-29T00:43:51.018817+0000    44      67        67         0         0         0           -           0
2021-10-29T00:43:52.018935+0000    45      67        67         0         0         0           -           0
2021-10-29T00:43:53.019054+0000    46      67        67         0         0         0           -           0
2021-10-29T00:43:54.019164+0000    47      67        67         0         0         0           -           0
2021-10-29T00:43:55.019274+0000    48      67        67         0         0         0           -           0
2021-10-29T00:43:56.019376+0000    49      68        68         0         0         0           -           0
2021-10-29T00:43:57.019488+0000    50      68        68         0         0         0           -           0
2021-10-29T00:43:58.019589+0000    51      68        68         0         0         0           -           0
2021-10-29T00:43:59.019704+0000    52      68        68         0         0         0           -           0
2021-10-29T00:44:00.019820+0000    53      68        68         0         0         0           -           0
2021-10-29T00:44:01.019934+0000    54      69        69         0         0         0           -           0
2021-10-29T00:44:02.020034+0000    55      70        70         0         0         0           -           0
2021-10-29T00:44:03.020150+0000    56      71        71         0         0         0           -           0
2021-10-29T00:44:04.020260+0000    57      72        72         0         0         0           -           0
2021-10-29T00:44:05.020373+0000    58      73        73         0         0         0           -           0
2021-10-29T00:44:06.020447+0000    59      74        74         0         0         0           -           0
2021-10-29T00:44:07.020544+0000 min lat: 9999 max lat: 0 avg lat: 0
2021-10-29T00:44:07.020544+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-29T00:44:07.020544+0000    60      74        74         0         0         0           -           0
2021-10-29T00:44:08.020662+0000    61      75        75         0         0         0           -           0
2021-10-29T00:44:09.020747+0000    62      76        76         0         0         0           -           0
2021-10-29T00:44:10.020842+0000    63      77        77         0         0         0           -           0
2021-10-29T00:44:11.020955+0000    64      77        77         0         0         0           -           0
2021-10-29T00:44:12.021069+0000    65      77        77         0         0         0           -           0
2021-10-29T00:44:13.021183+0000    66      78        78         0         0         0           -           0
2021-10-29T00:44:14.021292+0000    67      79        79         0         0         0           -           0
2021-10-29T00:44:15.021387+0000    68      80        80         0         0         0           -           0
2021-10-29T00:44:16.021470+0000    69      80        80         0         0         0           -           0
2021-10-29T00:44:17.021575+0000    70      81        81         0         0         0           -           0
2021-10-29T00:44:18.021676+0000    71      83        83         0         0         0           -           0
2021-10-29T00:44:19.021779+0000    72      85        85         0         0         0           -           0
2021-10-29T00:44:20.021874+0000    73      87        87         0         0         0           -           0
2021-10-29T00:44:21.021976+0000    74      89        89         0         0         0           -           0
2021-10-29T00:44:22.022048+0000    75      92        92         0         0         0           -           0
2021-10-29T00:44:23.022127+0000    76      94        94         0         0         0           -           0
2021-10-29T00:44:24.022246+0000    77      96        96         0         0         0           -           0
2021-10-29T00:44:25.022328+0000    78      98        98         0         0         0           -           0
2021-10-29T00:44:26.022429+0000    79      98        98         0         0         0           -           0
2021-10-29T00:44:27.022538+0000 min lat: 9999 max lat: 0 avg lat: 0
2021-10-29T00:44:27.022538+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-29T00:44:27.022538+0000    80      98        98         0         0         0           -           0
2021-10-29T00:44:28.022657+0000    81      98        98         0         0         0           -           0
2021-10-29T00:44:29.022766+0000    82      99        99         0         0         0           -           0
2021-10-29T00:44:30.022865+0000    83      99        99         0         0         0           -           0
2021-10-29T00:44:31.022979+0000    84      99        99         0         0         0           -           0
2021-10-29T00:44:32.023089+0000    85      99        99         0         0         0           -           0
2021-10-29T00:44:33.023204+0000    86      99        99         0         0         0           -           0
2021-10-29T00:44:34.023311+0000    87      99        99         0         0         0           -           0
2021-10-29T00:44:35.023422+0000    88      99        99         0         0         0           -           0
2021-10-29T00:44:36.023534+0000    89      99        99         0         0         0           -           0
2021-10-29T00:44:37.023627+0000    90     100       100         0         0         0           -           0
2021-10-29T00:44:38.023724+0000    91     100       100         0         0         0           -           0
2021-10-29T00:44:39.023835+0000    92     101       101         0         0         0           -           0
2021-10-29T00:44:40.023948+0000    93     103       103         0         0         0           -           0
2021-10-29T00:44:41.024058+0000    94     105       105         0         0         0           -           0
2021-10-29T00:44:42.024153+0000    95     107       107         0         0         0           -           0
2021-10-29T00:44:43.024233+0000    96     110       110         0         0         0           -           0
2021-10-29T00:44:44.024320+0000    97     112       112         0         0         0           -           0
2021-10-29T00:44:45.024395+0000    98     114       114         0         0         0           -           0
2021-10-29T00:44:46.024481+0000    99     117       117         0         0         0           -           0
2021-10-29T00:44:47.024576+0000 min lat: 9999 max lat: 0 avg lat: 0
2021-10-29T00:44:47.024576+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-29T00:44:47.024576+0000   100     119       119         0         0         0           -           0
2021-10-29T00:44:48.024674+0000   101     121       121         0         0         0           -           0
2021-10-29T00:44:49.024770+0000   102     123       123         0         0         0           -           0
2021-10-29T00:44:50.024875+0000   103     125       125         0         0         0           -           0
2021-10-29T00:44:51.024985+0000   104     126       126         0         0         0           -           0
2021-10-29T00:44:52.025096+0000   105     126       126         0         0         0           -           0
2021-10-29T00:44:53.025164+0000   106     126       126         0         0         0           -           0
2021-10-29T00:44:54.025271+0000   107     127       127         0         0         0           -           0
2021-10-29T00:44:55.025383+0000   108     127       127         0         0         0           -           0
2021-10-29T00:44:56.025471+0000   109       2       128       126   18.4936   18.4954     5.71876     61.1101
2021-10-29T00:44:57.025584+0000   110       1       128       127   18.4709        16     6.55111     60.6805
2021-10-29T00:44:58.025702+0000 Total time run:         110.594
Total writes made:      128
Write size:             16777216
Object size:            16777216
Bandwidth (MB/sec):     18.5182
Stddev Bandwidth:       2.32115
Max bandwidth (MB/sec): 18.4954
Min bandwidth (MB/sec): 0
Average IOPS:           1
Stddev IOPS:            0.13422
Max IOPS:               1
Min IOPS:               0
Average Latency(s):     60.2358
Stddev Latency(s):      34.8656
Max latency(s):         108.448
Min latency(s):         3.749

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:44:59,810599411-07:00] INFO: > Stop system activity collection services[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:44:59,817096632-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:391 - rados_bench() > kill -INT 956271

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:44:59,823538029-07:00] INFO: >> for OSD host[0m
# ./benchmarks/bench-rados:396 - rados_bench() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.5 kill -INT 354268
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:44:59,831827968-07:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.5 kill -INT 354268
[1] 17:45:01 [SUCCESS] ljishen@10.10.2.5

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:45:01,161074471-07:00] INFO: >>> Copy system activity file from OSD host[0m
## ./benchmarks/bench-rados:403 - rados_bench() > basename -- /tmp/bench-rados/sys_activity_16MB_write.dat.osd_host.2
# ./benchmarks/bench-rados:403 - rados_bench() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.5:/tmp/bench-rados/sys_activity_16MB_write.dat.osd_host.2 /mnt/sda8/ceph-research/scripts/output/sys_activity_16MB_write.dat.osd_host.2


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:45:02,479765897-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:45:26,655991409-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 129 objects, 2.0 GiB
    usage:   6.5 GiB used, 394 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:45:26,664595160-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:45:35,720620165-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 129 objects, 2.0 GiB
    usage:   6.1 GiB used, 394 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:45:35,728855050-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:45:44,909792552-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 129 objects, 2.0 GiB
    usage:   6.1 GiB used, 394 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:45:44,917929043-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:45:53,891591694-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 129 objects, 2.0 GiB
    usage:   6.1 GiB used, 394 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:45:53,899675966-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:46:03,019634082-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 129 objects, 2.0 GiB
    usage:   6.1 GiB used, 394 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:46:03,027747318-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:46:03,034295486-07:00] INFO: > The cluster is idle now.[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:46:03,038290233-07:00] STAGE: Start benchmarking of mode seq ...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:46:03,045718870-07:00] INFO: > Start system activity collection services[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:46:03,051759501-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:341 - rados_bench() > rm --force /mnt/sda8/ceph-research/scripts/output/sys_activity_16MB_seq.dat.rados_bench_host.2
# ./benchmarks/bench-rados:342 - rados_bench() > sar_pid_rados_bench_host=958503
# ./benchmarks/bench-rados:342 - rados_bench() > S_TIME_FORMAT=ISO
# ./benchmarks/bench-rados:342 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/output/sys_activity_16MB_seq.dat.rados_bench_host.2 2

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:46:03,059154855-07:00] INFO: >> for OSD host[0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:46:03,068395616-07:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.5 bash
10.10.2.5: b'354996\n'
[1] 17:46:04 [SUCCESS] ljishen@10.10.2.5
354996

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:46:04,381316543-07:00] INFO: > Run rados bench[0m
# ./benchmarks/bench-rados:379 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/output/rados_bench_16MB_seq.log.2
## ./benchmarks/bench-rados:376 - rados_bench() > count_cpus 32-63,96-127
# ./benchmarks/bench-rados:376 - rados_bench() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 128 --show-time
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:46:04,402217744-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:46:04,404980720-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '6b0c4acc-3850-11ec-b51d-53e6e728d2d3', '--', 'rados', 'bench', '99999999', 'seq', '--pool', 'bench_rados', '--concurrent-ios', '128', '--show-time'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 6b0c4acc-3850-11ec-b51d-53e6e728d2d3 -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 128 --show-time
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
hints = 1
2021-10-29T00:46:07.399826+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-29T00:46:07.399826+0000     0       0         0         0         0         0           -           0
2021-10-29T00:46:08.399925+0000     1      12        12         0         0         0           -           0
2021-10-29T00:46:09.400003+0000     2      24        24         0         0         0           -           0
2021-10-29T00:46:10.400073+0000     3      33        33         0         0         0           -           0
2021-10-29T00:46:11.400146+0000     4      45        45         0         0         0           -           0
2021-10-29T00:46:12.400169+0000     5      55        55         0         0         0           -           0
2021-10-29T00:46:13.400244+0000     6      65        65         0         0         0           -           0
2021-10-29T00:46:14.400316+0000     7      75        75         0         0         0           -           0
2021-10-29T00:46:15.400385+0000     8      84        84         0         0         0           -           0
2021-10-29T00:46:16.400455+0000     9      96        96         0         0         0           -           0
2021-10-29T00:46:17.400524+0000    10     106       106         0         0         0           -           0
2021-10-29T00:46:18.400591+0000    11     116       116         0         0         0           -           0
2021-10-29T00:46:19.400659+0000    12     126       126         0         0         0           -           0
2021-10-29T00:46:20.400726+0000    13       1       128       127   156.296   156.308    0.966866     6.52584
2021-10-29T00:46:21.400816+0000 Total time run:       13.0923
Total reads made:     128
Read size:            16777216
Object size:          16777216
Bandwidth (MB/sec):   156.428
Average IOPS:         9
Stddev IOPS:          2.49615
Max IOPS:             9
Min IOPS:             0
Average Latency(s):   6.48324
Max latency(s):       12.1881
Min latency(s):       0.672602

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:46:22,134741526-07:00] INFO: > Stop system activity collection services[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:46:22,141428877-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:391 - rados_bench() > kill -INT 958503

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:46:22,147730930-07:00] INFO: >> for OSD host[0m
# ./benchmarks/bench-rados:396 - rados_bench() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.5 kill -INT 354996
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:46:22,155703883-07:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.5 kill -INT 354996
[1] 17:46:23 [SUCCESS] ljishen@10.10.2.5

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:46:23,285757582-07:00] INFO: >>> Copy system activity file from OSD host[0m
## ./benchmarks/bench-rados:403 - rados_bench() > basename -- /tmp/bench-rados/sys_activity_16MB_seq.dat.osd_host.2
# ./benchmarks/bench-rados:403 - rados_bench() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.5:/tmp/bench-rados/sys_activity_16MB_seq.dat.osd_host.2 /mnt/sda8/ceph-research/scripts/output/sys_activity_16MB_seq.dat.osd_host.2


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:46:24,369495014-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:46:48,389347904-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 129 objects, 2.0 GiB
    usage:   6.1 GiB used, 394 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:46:48,397612586-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:46:57,468415950-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 129 objects, 2.0 GiB
    usage:   6.1 GiB used, 394 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:46:57,476652830-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:47:06,691081070-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 129 objects, 2.0 GiB
    usage:   6.1 GiB used, 394 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:47:06,699389384-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:47:15,756329847-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 129 objects, 2.0 GiB
    usage:   6.1 GiB used, 394 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:47:15,764706560-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:47:24,880168401-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 129 objects, 2.0 GiB
    usage:   6.1 GiB used, 394 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:47:24,888515347-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:47:24,895280174-07:00] INFO: > The cluster is idle now.[0m


[1;7;39;49m[2021-10-28T17:47:24,897957899-07:00][RUNNING][ROUND 3/7/21] object_size=16MB[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:47:24,901800428-07:00] STAGE: Launch a Ceph cluster (CEPH_HOST: 10.10.2.1, OSD_HOST: 10.10.2.5) ...[0m
# ./benchmarks/bench-rados:183 - launch_ceph_cluster() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.1 bash -euo pipefail
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:47:24,911691515-07:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.1 bash -euo pipefail
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T17:47:25,341865824-07:00] INFO: > Remove existing clusters\x1b[0m\n'
10.10.2.1: b'## bash:17 -  > basename -- /var/lib/ceph/6b0c4acc-3850-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'# bash:17 -  > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm --host 10.10.2.1 --host 10.10.2.5 rm-cluster --force --fsid 6b0c4acc-3850-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T17:47:25,352450820-07:00] DEBUG: HOST_PARAMS: ['--host', '10.10.2.1', '--host', '10.10.2.5']\x1b[0m\n"
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T17:47:25,356269869-07:00] DEBUG: CEPHADM_PARAMS: ['rm-cluster', '--force', '--fsid', '6b0c4acc-3850-11ec-b51d-53e6e728d2d3']\x1b[0m\n"
10.10.2.1: b"# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:63 -  > /tmp/bench-rados/ceph-research/scripts/parallel-ssh --host 10.10.2.1 --host 10.10.2.5 'sudo python3 -u - rm-cluster --force --fsid 6b0c4acc-3850-11ec-b51d-53e6e728d2d3'\n"
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T17:47:25,364399019-07:00] DEBUG: command from stdin\x1b[0m\n'
10.10.2.1: b"# /tmp/bench-rados/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host 10.10.2.1 --host 10.10.2.5 'sudo python3 -u - rm-cluster --force --fsid 6b0c4acc-3850-11ec-b51d-53e6e728d2d3'\n"
10.10.2.1: b'[1] 17:47:30 [SUCCESS] 10.10.2.1\n[2] 17:47:36 [SUCCESS] 10.10.2.5\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T17:47:36,992867752-07:00] INFO: > Deploy a new cluster\x1b[0m\n'
10.10.2.1: b'# bash:24 -  > /tmp/bench-rados/ceph-research/scripts/ceph-deploy -m 10.10.2.1 -o sm1/10.10.2.5:/dev/nvme0n1\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T17:47:37,004876314-07:00] STAGE: Verify passwordless SSH to hosts, passwordless sudo via ljishen, and the existence of devices on hosts...\x1b[0m\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T17:47:37,009801282-07:00] INFO: > Check the host for the monitor\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:181 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@10.10.2.1 sudo --non-interactive --validate\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T17:47:37,162974017-07:00] INFO: > Check hosts for OSDs\x1b[0m\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T17:47:37,167982792-07:00] INFO: >> Check host 'sm1'\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:200 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 sudo --non-interactive --validate\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T17:47:38,255281886-07:00] INFO: >>> Check device '/dev/nvme0n1'\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:215 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 test -b /dev/nvme0n1\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:226 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 lsblk --nodeps --noheadings --output TYPE /dev/nvme0n1\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:227 - verify_parameters() > grep --word-regexp disk\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T17:47:40,383699198-07:00] STAGE: Reset devices on hosts...\x1b[0m\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T17:47:40,388396337-07:00] INFO: > Resetting device '/dev/nvme0n1' on host 'sm1'\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:288 - reset_host_devices() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 sudo sh\n'
10.10.2.1: b'  Removing ceph--253a98ee--cdf9--4b3b--bff0--006f9f8d895e-osd--block--f5133000--d8c2--4f25--bf4c--b8c400e38d2b (252:0)\n'
10.10.2.1: b'  Archiving volume group "ceph-253a98ee-cdf9-4b3b-bff0-006f9f8d895e" metadata (seqno 5).\n'
10.10.2.1: b'  Releasing logical volume "osd-block-f5133000-d8c2-4f25-bf4c-b8c400e38d2b"\n'
10.10.2.1: b'  Creating volume group backup "/etc/lvm/backup/ceph-253a98ee-cdf9-4b3b-bff0-006f9f8d895e" (seqno 6).\n'
10.10.2.1: b'  Logical volume "osd-block-f5133000-d8c2-4f25-bf4c-b8c400e38d2b" successfully removed\n'
10.10.2.1: b'  Removing physical volume "/dev/nvme0n1" from volume group "ceph-253a98ee-cdf9-4b3b-bff0-006f9f8d895e"\n'
10.10.2.1: b'  Volume group "ceph-253a98ee-cdf9-4b3b-bff0-006f9f8d895e" successfully removed\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T17:47:42,740429225-07:00] STAGE: Bootstrap a new cluster...\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:317 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.1 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T17:47:42,750988854-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T17:47:42,754606333-07:00] DEBUG: CEPHADM_PARAMS: ['bootstrap', '--config', '/tmp/bench-rados/ceph-research/scripts/ceph.conf', '--mon-ip', '10.10.2.1', '--output-dir', '/tmp/bench-rados/deployment_data_root/etc/ceph', '--ssh-user', 'ljishen', '--skip-dashboard', '--no-minimize-config', '--allow-overwrite', '--allow-fqdn-hostname', '--orphan-initial-daemons', '--skip-monitoring-stack', '--single-host-defaults']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.1 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults\n'
10.10.2.1: b'Verifying podman|docker is present...\n'
10.10.2.1: b'Verifying lvm2 is present...\nVerifying time synchronization is in place...\n'
10.10.2.1: b'Unit ntp.service is enabled and running\n'
10.10.2.1: b'Repeating the final host check...\n'
10.10.2.1: b'podman|docker (/usr/bin/docker) is present\n'
10.10.2.1: b'systemctl is present\n'
10.10.2.1: b'lvcreate is present\n'
10.10.2.1: b'Unit ntp.service is enabled and running\n'
10.10.2.1: b'Host looks OK\n'
10.10.2.1: b'Cluster fsid: d345935e-3851-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Verifying IP 10.10.2.1 port 3300 ...\n'
10.10.2.1: b'Verifying IP 10.10.2.1 port 6789 ...\n'
10.10.2.1: b'Mon IP `10.10.2.1` is in CIDR network `10.10.2.0/24`\n- internal network (--cluster-network) has not been provided, OSD replication will default to the public_network\n'
10.10.2.1: b'Adjusting default settings to suit single-host cluster...\n'
10.10.2.1: b'Pulling container image quay.io/ceph/ceph:v16...\n'
10.10.2.1: b'Ceph version: ceph version 16.2.6 (ee28fb57e47e9f88813e24bbf4c14496ca299d31) pacific (stable)\n'
10.10.2.1: b'Extracting ceph user uid/gid from container image...\n'
10.10.2.1: b'Creating initial keys...\n'
10.10.2.1: b'Creating initial monmap...\n'
10.10.2.1: b'Creating mon...\n'
10.10.2.1: b'Waiting for mon to start...\n'
10.10.2.1: b'Waiting for mon...\n'
10.10.2.1: b'mon is available\n'
10.10.2.1: b'Setting mon public_network to 10.10.2.0/24\n'
10.10.2.1: b'Wrote config to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf\n'
10.10.2.1: b'Wrote keyring to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring\n'
10.10.2.1: b'Creating mgr...\n'
10.10.2.1: b'Verifying port 9283 ...\n'
10.10.2.1: b'Waiting for mgr to start...\nWaiting for mgr...\n'
10.10.2.1: b'mgr not available, waiting (1/15)...\n'
10.10.2.1: b'mgr not available, waiting (2/15)...\n'
10.10.2.1: b'mgr is available\n'
10.10.2.1: b'Enabling cephadm module...\n'
10.10.2.1: b'Waiting for the mgr to restart...\n'
10.10.2.1: b'Waiting for mgr epoch 5...\n'
10.10.2.1: b'mgr epoch 5 is available\nSetting orchestrator backend to cephadm...\n'
10.10.2.1: b'Generating ssh key...\n'
10.10.2.1: b'Wrote public SSH key to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub\n'
10.10.2.1: b'Adding key to ljishen@localhost authorized_keys...\n'
10.10.2.1: b'Adding host node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us...\n'
10.10.2.1: b'Deploying unmanaged mon service...\n'
10.10.2.1: b'Deploying unmanaged mgr service...\n'
10.10.2.1: b'You can access the Ceph CLI with:\n\n\tsudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --fsid d345935e-3851-11ec-b51d-53e6e728d2d3 -c /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf -k /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring\n\n'
10.10.2.1: b'Please consider enabling telemetry to help improve Ceph:\n\n\tceph telemetry on\n\nFor more information see:\n\n\thttps://docs.ceph.com/docs/pacific/mgr/telemetry/\n\n'
10.10.2.1: b'Bootstrap complete.\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T17:48:48,298033149-07:00] INFO: Sleep for 20 seconds\x1b[0m\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T17:49:08,305062310-07:00] INFO: > Set the default number of monitor daemons to 1\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:337 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T17:49:08,315661193-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T17:49:08,319393919-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mon', '1']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1\n'
10.10.2.1: b'Inferring fsid d345935e-3851-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/d345935e-3851-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b'Scheduled mon update...\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T17:49:17,444941709-07:00] INFO: > Set the default number of manager daemons to 1\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:342 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T17:49:17,455559026-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T17:49:17,459246227-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mgr', '1']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1\n'
10.10.2.1: b'Inferring fsid d345935e-3851-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/d345935e-3851-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b'Scheduled mgr update...\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T17:49:26,800344443-07:00] STAGE: Add hosts to the cluster...\x1b[0m\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T17:49:26,806373215-07:00] INFO: > Adding host 'sm1'\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:361 - add_hosts() > ssh-copy-id -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -f -i /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub ljishen@sm1\n'
10.10.2.1: b'/usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: "/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub"\n'
10.10.2.1: b'\nNumber of key(s) added: 1\n\nNow try logging into the machine, with:   "ssh -o \'GlobalKnownHostsFile=/dev/null\' -o \'LogLevel=ERROR\' -o \'PasswordAuthentication=no\' -o \'StrictHostKeyChecking=no\' -o \'UserKnownHostsFile=/dev/null\' \'ljishen@sm1\'"\nand check to make sure that only the key(s) you wanted were added.\n\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:367 - add_hosts() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add sm1 10.10.2.5\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T17:49:27,969419269-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T17:49:27,973020959-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'host', 'add', 'sm1', '10.10.2.5']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add sm1 10.10.2.5\n'
10.10.2.1: b'Inferring fsid d345935e-3851-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/d345935e-3851-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b"Added host 'sm1' with addr '10.10.2.5'\n"
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T17:49:38,985245559-07:00] INFO: Sleep for 20 seconds\x1b[0m\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T17:49:58,989986687-07:00] STAGE: Add OSDs to the cluster...\x1b[0m\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T17:49:58,996344477-07:00] INFO: > Adding devices ['/dev/nvme0n1'] on host 'sm1' as OSDs\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:388 - add_host_devices_as_osds() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd sm1:/dev/nvme0n1\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T17:49:59,005828504-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T17:49:59,009302113-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'daemon', 'add', 'osd', 'sm1:/dev/nvme0n1']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd sm1:/dev/nvme0n1\n'
10.10.2.1: b'Inferring fsid d345935e-3851-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/d345935e-3851-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b"Created osd(s) 0 on host 'sm1'\n"
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T17:50:24,477868208-07:00] INFO: > Sleep for 20 seconds\x1b[0m\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T17:50:44,483008580-07:00] STAGE: Show Cluster Status\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:408 - show_status() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T17:50:44,493115679-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T17:50:44,496690559-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', '--status']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status\n'
10.10.2.1: b'Inferring fsid d345935e-3851-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/d345935e-3851-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b'  cluster:\n    id:     d345935e-3851-11ec-b51d-53e6e728d2d3\n    health: HEALTH_WARN\n            1 pool(s) have no replicas configured\n \n  services:\n    mon: 1 daemons, quorum node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us (age 2m)\n    mgr: node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us.yncdgp(active, since 2m)\n    osd: 1 osds: 1 up (since 20s), 1 in (since 41s)\n \n  data:\n    pools:   1 pools, 1 pgs\n    objects: 0 objects, 0 B\n    usage:   4.7 MiB used, 400 GiB / 400 GiB avail\n    pgs:     1 active+clean\n \n'
[1] 17:50:53 [SUCCESS] ljishen@10.10.2.1

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:47:25,341865824-07:00] INFO: > Remove existing clusters[0m
## bash:17 -  > basename -- /var/lib/ceph/6b0c4acc-3850-11ec-b51d-53e6e728d2d3
# bash:17 -  > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm --host 10.10.2.1 --host 10.10.2.5 rm-cluster --force --fsid 6b0c4acc-3850-11ec-b51d-53e6e728d2d3
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:47:25,352450820-07:00] DEBUG: HOST_PARAMS: ['--host', '10.10.2.1', '--host', '10.10.2.5'][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:47:25,356269869-07:00] DEBUG: CEPHADM_PARAMS: ['rm-cluster', '--force', '--fsid', '6b0c4acc-3850-11ec-b51d-53e6e728d2d3'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:63 -  > /tmp/bench-rados/ceph-research/scripts/parallel-ssh --host 10.10.2.1 --host 10.10.2.5 'sudo python3 -u - rm-cluster --force --fsid 6b0c4acc-3850-11ec-b51d-53e6e728d2d3'
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:47:25,364399019-07:00] DEBUG: command from stdin[0m
# /tmp/bench-rados/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host 10.10.2.1 --host 10.10.2.5 'sudo python3 -u - rm-cluster --force --fsid 6b0c4acc-3850-11ec-b51d-53e6e728d2d3'
[1] 17:47:30 [SUCCESS] 10.10.2.1
[2] 17:47:36 [SUCCESS] 10.10.2.5

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:47:36,992867752-07:00] INFO: > Deploy a new cluster[0m
# bash:24 -  > /tmp/bench-rados/ceph-research/scripts/ceph-deploy -m 10.10.2.1 -o sm1/10.10.2.5:/dev/nvme0n1


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:47:37,004876314-07:00] STAGE: Verify passwordless SSH to hosts, passwordless sudo via ljishen, and the existence of devices on hosts...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:47:37,009801282-07:00] INFO: > Check the host for the monitor[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:181 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@10.10.2.1 sudo --non-interactive --validate

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:47:37,162974017-07:00] INFO: > Check hosts for OSDs[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:47:37,167982792-07:00] INFO: >> Check host 'sm1'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:200 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 sudo --non-interactive --validate

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:47:38,255281886-07:00] INFO: >>> Check device '/dev/nvme0n1'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:215 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 test -b /dev/nvme0n1
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:226 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 lsblk --nodeps --noheadings --output TYPE /dev/nvme0n1
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:227 - verify_parameters() > grep --word-regexp disk


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:47:40,383699198-07:00] STAGE: Reset devices on hosts...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:47:40,388396337-07:00] INFO: > Resetting device '/dev/nvme0n1' on host 'sm1'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:288 - reset_host_devices() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 sudo sh
  Removing ceph--253a98ee--cdf9--4b3b--bff0--006f9f8d895e-osd--block--f5133000--d8c2--4f25--bf4c--b8c400e38d2b (252:0)
  Archiving volume group "ceph-253a98ee-cdf9-4b3b-bff0-006f9f8d895e" metadata (seqno 5).
  Releasing logical volume "osd-block-f5133000-d8c2-4f25-bf4c-b8c400e38d2b"
  Creating volume group backup "/etc/lvm/backup/ceph-253a98ee-cdf9-4b3b-bff0-006f9f8d895e" (seqno 6).
  Logical volume "osd-block-f5133000-d8c2-4f25-bf4c-b8c400e38d2b" successfully removed
  Removing physical volume "/dev/nvme0n1" from volume group "ceph-253a98ee-cdf9-4b3b-bff0-006f9f8d895e"
  Volume group "ceph-253a98ee-cdf9-4b3b-bff0-006f9f8d895e" successfully removed


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:47:42,740429225-07:00] STAGE: Bootstrap a new cluster...[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:317 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.1 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:47:42,750988854-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:47:42,754606333-07:00] DEBUG: CEPHADM_PARAMS: ['bootstrap', '--config', '/tmp/bench-rados/ceph-research/scripts/ceph.conf', '--mon-ip', '10.10.2.1', '--output-dir', '/tmp/bench-rados/deployment_data_root/etc/ceph', '--ssh-user', 'ljishen', '--skip-dashboard', '--no-minimize-config', '--allow-overwrite', '--allow-fqdn-hostname', '--orphan-initial-daemons', '--skip-monitoring-stack', '--single-host-defaults'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.1 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults
Verifying podman|docker is present...
Verifying lvm2 is present...
Verifying time synchronization is in place...
Unit ntp.service is enabled and running
Repeating the final host check...
podman|docker (/usr/bin/docker) is present
systemctl is present
lvcreate is present
Unit ntp.service is enabled and running
Host looks OK
Cluster fsid: d345935e-3851-11ec-b51d-53e6e728d2d3
Verifying IP 10.10.2.1 port 3300 ...
Verifying IP 10.10.2.1 port 6789 ...
Mon IP `10.10.2.1` is in CIDR network `10.10.2.0/24`
- internal network (--cluster-network) has not been provided, OSD replication will default to the public_network
Adjusting default settings to suit single-host cluster...
Pulling container image quay.io/ceph/ceph:v16...
Ceph version: ceph version 16.2.6 (ee28fb57e47e9f88813e24bbf4c14496ca299d31) pacific (stable)
Extracting ceph user uid/gid from container image...
Creating initial keys...
Creating initial monmap...
Creating mon...
Waiting for mon to start...
Waiting for mon...
mon is available
Setting mon public_network to 10.10.2.0/24
Wrote config to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf
Wrote keyring to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring
Creating mgr...
Verifying port 9283 ...
Waiting for mgr to start...
Waiting for mgr...
mgr not available, waiting (1/15)...
mgr not available, waiting (2/15)...
mgr is available
Enabling cephadm module...
Waiting for the mgr to restart...
Waiting for mgr epoch 5...
mgr epoch 5 is available
Setting orchestrator backend to cephadm...
Generating ssh key...
Wrote public SSH key to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub
Adding key to ljishen@localhost authorized_keys...
Adding host node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us...
Deploying unmanaged mon service...
Deploying unmanaged mgr service...
You can access the Ceph CLI with:

	sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --fsid d345935e-3851-11ec-b51d-53e6e728d2d3 -c /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf -k /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring

Please consider enabling telemetry to help improve Ceph:

	ceph telemetry on

For more information see:

	https://docs.ceph.com/docs/pacific/mgr/telemetry/

Bootstrap complete.

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:48:48,298033149-07:00] INFO: Sleep for 20 seconds[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:49:08,305062310-07:00] INFO: > Set the default number of monitor daemons to 1[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:337 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:49:08,315661193-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:49:08,319393919-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mon', '1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1
Inferring fsid d345935e-3851-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/d345935e-3851-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Scheduled mon update...

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:49:17,444941709-07:00] INFO: > Set the default number of manager daemons to 1[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:342 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:49:17,455559026-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:49:17,459246227-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mgr', '1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1
Inferring fsid d345935e-3851-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/d345935e-3851-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Scheduled mgr update...


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:49:26,800344443-07:00] STAGE: Add hosts to the cluster...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:49:26,806373215-07:00] INFO: > Adding host 'sm1'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:361 - add_hosts() > ssh-copy-id -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -f -i /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub ljishen@sm1
/usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: "/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub"

Number of key(s) added: 1

Now try logging into the machine, with:   "ssh -o 'GlobalKnownHostsFile=/dev/null' -o 'LogLevel=ERROR' -o 'PasswordAuthentication=no' -o 'StrictHostKeyChecking=no' -o 'UserKnownHostsFile=/dev/null' 'ljishen@sm1'"
and check to make sure that only the key(s) you wanted were added.

# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:367 - add_hosts() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add sm1 10.10.2.5
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:49:27,969419269-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:49:27,973020959-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'host', 'add', 'sm1', '10.10.2.5'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add sm1 10.10.2.5
Inferring fsid d345935e-3851-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/d345935e-3851-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Added host 'sm1' with addr '10.10.2.5'

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:49:38,985245559-07:00] INFO: Sleep for 20 seconds[0m


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:49:58,989986687-07:00] STAGE: Add OSDs to the cluster...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:49:58,996344477-07:00] INFO: > Adding devices ['/dev/nvme0n1'] on host 'sm1' as OSDs[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:388 - add_host_devices_as_osds() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd sm1:/dev/nvme0n1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:49:59,005828504-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:49:59,009302113-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'daemon', 'add', 'osd', 'sm1:/dev/nvme0n1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd sm1:/dev/nvme0n1
Inferring fsid d345935e-3851-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/d345935e-3851-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Created osd(s) 0 on host 'sm1'

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:50:24,477868208-07:00] INFO: > Sleep for 20 seconds[0m


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:50:44,483008580-07:00] STAGE: Show Cluster Status[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:408 - show_status() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:50:44,493115679-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:50:44,496690559-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', '--status'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status
Inferring fsid d345935e-3851-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/d345935e-3851-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
  cluster:
    id:     d345935e-3851-11ec-b51d-53e6e728d2d3
    health: HEALTH_WARN
            1 pool(s) have no replicas configured
 
  services:
    mon: 1 daemons, quorum node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us (age 2m)
    mgr: node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us.yncdgp(active, since 2m)
    osd: 1 osds: 1 up (since 20s), 1 in (since 41s)
 
  data:
    pools:   1 pools, 1 pgs
    objects: 0 objects, 0 B
    usage:   4.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     1 active+clean
 

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:50:53,592267401-07:00] INFO: > Copy configuration files to local[0m
# ./benchmarks/bench-rados:220 - launch_ceph_cluster() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.1 sudo chown --changes --recursive ljishen /tmp/bench-rados/deployment_data_root/etc/ceph
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:50:53,600297831-07:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.1 sudo chown --changes --recursive ljishen /tmp/bench-rados/deployment_data_root/etc/ceph
[1] 17:50:53 [SUCCESS] ljishen@10.10.2.1
# ./benchmarks/bench-rados:223 - launch_ceph_cluster() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.1:/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf
# ./benchmarks/bench-rados:226 - launch_ceph_cluster() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.1:/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:50:54,077798489-07:00] STAGE: Update local ceph.conf as a client...[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:50:54,081641060-07:00] STAGE: Configure Ceph pool...[0m
# ./benchmarks/bench-rados:257 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_max_backfills 32
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:50:54,103945074-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:50:54,106713280-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'd345935e-3851-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'config', 'set', 'osd', 'osd_max_backfills', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid d345935e-3851-11ec-b51d-53e6e728d2d3 -- ceph config set osd osd_max_backfills 32
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:258 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_active 32
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:50:58,295912778-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:50:58,299097117-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'd345935e-3851-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_active', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid d345935e-3851-11ec-b51d-53e6e728d2d3 -- ceph config set osd osd_recovery_max_active 32
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:259 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_single_start 8
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:51:02,436576698-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:51:02,439672000-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'd345935e-3851-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_single_start', '8'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid d345935e-3851-11ec-b51d-53e6e728d2d3 -- ceph config set osd osd_recovery_max_single_start 8
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:260 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_op_priority 63
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:51:06,726694203-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:51:06,729565794-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'd345935e-3851-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_op_priority', '63'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid d345935e-3851-11ec-b51d-53e6e728d2d3 -- ceph config set osd osd_recovery_op_priority 63
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:262 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config show-with-defaults osd.0
# ./benchmarks/bench-rados:263 - configure_ceph_pool() > grep 'osd_max_backfills\|osd_recovery'
# ./benchmarks/bench-rados:263 - configure_ceph_pool() > column -t -s ' '
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
osd_max_backfills                        32         mon
osd_recovery_cost                        20971520   default
osd_recovery_delay_start                 0.000000   default
osd_recovery_max_active                  32         mon
osd_recovery_max_active_hdd              3          default
osd_recovery_max_active_ssd              10         default
osd_recovery_max_chunk                   8388608    default
osd_recovery_max_omap_entries_per_chunk  8096       default
osd_recovery_max_single_start            8          mon
osd_recovery_op_priority                 63         mon
osd_recovery_op_warn_multiple            16         default
osd_recovery_priority                    5          default
osd_recovery_retry_interval              30.000000  default
osd_recovery_sleep                       0.000000   default
osd_recovery_sleep_hdd                   0.100000   default
osd_recovery_sleep_hybrid                0.025000   default
osd_recovery_sleep_ssd                   0.000000   default
# ./benchmarks/bench-rados:265 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool create bench_rados 128 128 replicated --size 1 --pg-num-min 128
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:51:14,803403843-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:51:14,806475611-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'd345935e-3851-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'create', 'bench_rados', '128', '128', 'replicated', '--size', '1', '--pg-num-min', '128'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid d345935e-3851-11ec-b51d-53e6e728d2d3 -- ceph osd pool create bench_rados 128 128 replicated --size 1 --pg-num-min 128
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
pool 'bench_rados' created
# ./benchmarks/bench-rados:267 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados min_size 1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:51:19,206582166-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:51:19,209647582-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'd345935e-3851-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'min_size', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid d345935e-3851-11ec-b51d-53e6e728d2d3 -- ceph osd pool set bench_rados min_size 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 min_size to 1
# ./benchmarks/bench-rados:268 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados pg_autoscale_mode off
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:51:23,873676440-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:51:23,876804865-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'd345935e-3851-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'pg_autoscale_mode', 'off'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid d345935e-3851-11ec-b51d-53e6e728d2d3 -- ceph osd pool set bench_rados pg_autoscale_mode off
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 pg_autoscale_mode to off
# ./benchmarks/bench-rados:269 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool application enable bench_rados benchmark
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:51:28,299543434-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:51:28,302567893-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'd345935e-3851-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'application', 'enable', 'bench_rados', 'benchmark'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid d345935e-3851-11ec-b51d-53e6e728d2d3 -- ceph osd pool application enable bench_rados benchmark
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
enabled application 'benchmark' on pool 'bench_rados'
# ./benchmarks/bench-rados:270 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados noscrub 1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:51:32,680696330-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:51:32,683670975-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'd345935e-3851-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'noscrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid d345935e-3851-11ec-b51d-53e6e728d2d3 -- ceph osd pool set bench_rados noscrub 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 noscrub to 1
# ./benchmarks/bench-rados:271 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados nodeep-scrub 1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:51:37,612994675-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:51:37,616172242-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'd345935e-3851-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'nodeep-scrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid d345935e-3851-11ec-b51d-53e6e728d2d3 -- ceph osd pool set bench_rados nodeep-scrub 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 nodeep-scrub to 1
# ./benchmarks/bench-rados:272 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool ls detail
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:51:42,696244649-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:51:42,699097294-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'd345935e-3851-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'ls', 'detail'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid d345935e-3851-11ec-b51d-53e6e728d2d3 -- ceph osd pool ls detail
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
pool 1 'device_health_metrics' replicated size 1 min_size 1 crush_rule 0 object_hash rjenkins pg_num 64 pgp_num 64 autoscale_mode on last_change 18 lfor 0/0/15 flags hashpspool stripe_width 0 pg_num_min 1 application mgr_devicehealth
pool 2 'bench_rados' replicated size 1 min_size 1 crush_rule 0 object_hash rjenkins pg_num 128 pgp_num 128 autoscale_mode off last_change 20 flags hashpspool,noscrub,nodeep-scrub stripe_width 0 pg_num_min 128 application benchmark

# ./benchmarks/bench-rados:273 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd df tree
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:51:46,862505546-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:51:46,865657594-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'd345935e-3851-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'df', 'tree'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid d345935e-3851-11ec-b51d-53e6e728d2d3 -- ceph osd df tree
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
ID  CLASS  WEIGHT   REWEIGHT  SIZE     RAW USE  DATA     OMAP  META     AVAIL    %USE  VAR   PGS  STATUS  TYPE NAME    
-1         0.39059         -  400 GiB  5.7 MiB  168 KiB   0 B  5.6 MiB  400 GiB  0.00  1.00    -          root default 
-3         0.39059         -  400 GiB  5.7 MiB  168 KiB   0 B  5.6 MiB  400 GiB  0.00  1.00    -              host sm1 
 0    ssd  0.39059   1.00000  400 GiB  5.7 MiB  168 KiB   0 B  5.6 MiB  400 GiB  0.00  1.00  192      up          osd.0
                       TOTAL  400 GiB  5.7 MiB  168 KiB   0 B  5.6 MiB  400 GiB  0.00                                  
MIN/MAX VAR: 1.00/1.00  STDDEV: 0


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:51:50,997143511-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:52:15,011815411-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:52:24,096953211-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:52:33,017622779-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:52:33,025410373-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:52:42,118395686-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:52:42,126635481-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:52:51,191914955-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:52:51,200020588-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:53:00,227181624-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:53:00,235735552-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:53:09,381027350-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:53:09,389230927-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:53:09,395965947-07:00] INFO: > The cluster is idle now.[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:53:09,400023582-07:00] STAGE: Start benchmarking of mode write ...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:53:09,408000992-07:00] INFO: > Start system activity collection services[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:53:09,414284361-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:341 - rados_bench() > rm --force /mnt/sda8/ceph-research/scripts/output/sys_activity_16MB_write.dat.rados_bench_host.3
# ./benchmarks/bench-rados:342 - rados_bench() > sar_pid_rados_bench_host=965976
# ./benchmarks/bench-rados:342 - rados_bench() > S_TIME_FORMAT=ISO
# ./benchmarks/bench-rados:342 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/output/sys_activity_16MB_write.dat.rados_bench_host.3 2

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:53:09,421712277-07:00] INFO: >> for OSD host[0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:53:09,431066362-07:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.5 bash
10.10.2.5: b'359871\n'
[1] 17:53:10 [SUCCESS] ljishen@10.10.2.5
359871

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:53:10,564713202-07:00] INFO: > Run rados bench[0m
# ./benchmarks/bench-rados:370 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/output/rados_bench_16MB_write.log.3
## ./benchmarks/bench-rados:361 - rados_bench() > count_cpus 32-63,96-127
# ./benchmarks/bench-rados:361 - rados_bench() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 60 write --pool bench_rados -b 16777216 -O 16777216 --concurrent-ios 128 --show-time --write-object --write-omap --write-xattr --no-cleanup
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:53:10,585495559-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:53:10,588328918-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'd345935e-3851-11ec-b51d-53e6e728d2d3', '--', 'rados', 'bench', '60', 'write', '--pool', 'bench_rados', '-b', '16777216', '-O', '16777216', '--concurrent-ios', '128', '--show-time', '--write-object', '--write-omap', '--write-xattr', '--no-cleanup'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid d345935e-3851-11ec-b51d-53e6e728d2d3 -- rados bench 60 write --pool bench_rados -b 16777216 -O 16777216 --concurrent-ios 128 --show-time --write-object --write-omap --write-xattr --no-cleanup
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
hints = 1
2021-10-29T00:53:13.639374+0000 Maintaining 128 concurrent writes of 16777216 bytes to objects of size 16777216 for up to 60 seconds or 0 objects
2021-10-29T00:53:13.639408+0000 Object prefix: benchmark_data_node-2.bluefield2.ucsc-cmps10_7
2021-10-29T00:53:14.318346+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-29T00:53:14.318346+0000     0       0         0         0         0         0           -           0
2021-10-29T00:53:15.318467+0000     1       3         3         0         0         0           -           0
2021-10-29T00:53:16.318587+0000     2       5         5         0         0         0           -           0
2021-10-29T00:53:17.318701+0000     3       7         7         0         0         0           -           0
2021-10-29T00:53:18.318827+0000     4       9         9         0         0         0           -           0
2021-10-29T00:53:19.318937+0000     5      12        12         0         0         0           -           0
2021-10-29T00:53:20.319052+0000     6      14        14         0         0         0           -           0
2021-10-29T00:53:21.319155+0000     7      17        17         0         0         0           -           0
2021-10-29T00:53:22.319280+0000     8      18        18         0         0         0           -           0
2021-10-29T00:53:23.319378+0000     9      21        21         0         0         0           -           0
2021-10-29T00:53:24.319492+0000    10      23        23         0         0         0           -           0
2021-10-29T00:53:25.319607+0000    11      25        25         0         0         0           -           0
2021-10-29T00:53:26.319707+0000    12      28        28         0         0         0           -           0
2021-10-29T00:53:27.319808+0000    13      30        30         0         0         0           -           0
2021-10-29T00:53:28.319930+0000    14      31        31         0         0         0           -           0
2021-10-29T00:53:29.320040+0000    15      34        34         0         0         0           -           0
2021-10-29T00:53:30.320180+0000    16      34        34         0         0         0           -           0
2021-10-29T00:53:31.320278+0000    17      34        34         0         0         0           -           0
2021-10-29T00:53:32.320377+0000    18      34        34         0         0         0           -           0
2021-10-29T00:53:33.320446+0000    19      35        35         0         0         0           -           0
2021-10-29T00:53:34.320573+0000 min lat: 9999 max lat: 0 avg lat: 0
2021-10-29T00:53:34.320573+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-29T00:53:34.320573+0000    20      35        35         0         0         0           -           0
2021-10-29T00:53:35.320676+0000    21      35        35         0         0         0           -           0
2021-10-29T00:53:36.320770+0000    22      35        35         0         0         0           -           0
2021-10-29T00:53:37.320863+0000    23      35        35         0         0         0           -           0
2021-10-29T00:53:38.320956+0000    24      36        36         0         0         0           -           0
2021-10-29T00:53:39.321075+0000    25      36        36         0         0         0           -           0
2021-10-29T00:53:40.321150+0000    26      37        37         0         0         0           -           0
2021-10-29T00:53:41.321248+0000    27      38        38         0         0         0           -           0
2021-10-29T00:53:42.321319+0000    28      41        41         0         0         0           -           0
2021-10-29T00:53:43.321438+0000    29      43        43         0         0         0           -           0
2021-10-29T00:53:44.321553+0000    30      45        45         0         0         0           -           0
2021-10-29T00:53:45.321628+0000    31      48        48         0         0         0           -           0
2021-10-29T00:53:46.321739+0000    32      50        50         0         0         0           -           0
2021-10-29T00:53:47.321853+0000    33      53        53         0         0         0           -           0
2021-10-29T00:53:48.321963+0000    34      55        55         0         0         0           -           0
2021-10-29T00:53:49.322036+0000    35      57        57         0         0         0           -           0
2021-10-29T00:53:50.322149+0000    36      59        59         0         0         0           -           0
2021-10-29T00:53:51.322259+0000    37      61        61         0         0         0           -           0
2021-10-29T00:53:52.322368+0000    38      64        64         0         0         0           -           0
2021-10-29T00:53:53.322451+0000    39      66        66         0         0         0           -           0
2021-10-29T00:53:54.322597+0000 min lat: 9999 max lat: 0 avg lat: 0
2021-10-29T00:53:54.322597+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-29T00:53:54.322597+0000    40      66        66         0         0         0           -           0
2021-10-29T00:53:55.322716+0000    41      66        66         0         0         0           -           0
2021-10-29T00:53:56.322830+0000    42      67        67         0         0         0           -           0
2021-10-29T00:53:57.322903+0000    43      67        67         0         0         0           -           0
2021-10-29T00:53:58.323015+0000    44      67        67         0         0         0           -           0
2021-10-29T00:53:59.323129+0000    45      67        67         0         0         0           -           0
2021-10-29T00:54:00.323238+0000    46      67        67         0         0         0           -           0
2021-10-29T00:54:01.323307+0000    47      68        68         0         0         0           -           0
2021-10-29T00:54:02.323417+0000    48      68        68         0         0         0           -           0
2021-10-29T00:54:03.323516+0000    49      68        68         0         0         0           -           0
2021-10-29T00:54:04.323627+0000    50      68        68         0         0         0           -           0
2021-10-29T00:54:05.323700+0000    51      68        68         0         0         0           -           0
2021-10-29T00:54:06.323797+0000    52      68        68         0         0         0           -           0
2021-10-29T00:54:07.323892+0000    53      69        69         0         0         0           -           0
2021-10-29T00:54:08.323963+0000    54      70        70         0         0         0           -           0
2021-10-29T00:54:09.324045+0000    55      71        71         0         0         0           -           0
2021-10-29T00:54:10.324145+0000    56      72        72         0         0         0           -           0
2021-10-29T00:54:11.324247+0000    57      72        72         0         0         0           -           0
2021-10-29T00:54:12.324347+0000    58      73        73         0         0         0           -           0
2021-10-29T00:54:13.324435+0000    59      74        74         0         0         0           -           0
2021-10-29T00:54:14.324519+0000 min lat: 9999 max lat: 0 avg lat: 0
2021-10-29T00:54:14.324519+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-29T00:54:14.324519+0000    60      75        75         0         0         0           -           0
2021-10-29T00:54:15.324638+0000    61      75        75         0         0         0           -           0
2021-10-29T00:54:16.324735+0000    62      76        76         0         0         0           -           0
2021-10-29T00:54:17.324809+0000    63      77        77         0         0         0           -           0
2021-10-29T00:54:18.324907+0000    64      78        78         0         0         0           -           0
2021-10-29T00:54:19.325007+0000    65      79        79         0         0         0           -           0
2021-10-29T00:54:20.325107+0000    66      80        80         0         0         0           -           0
2021-10-29T00:54:21.325177+0000    67      81        81         0         0         0           -           0
2021-10-29T00:54:22.325278+0000    68      81        81         0         0         0           -           0
2021-10-29T00:54:23.325375+0000    69      81        81         0         0         0           -           0
2021-10-29T00:54:24.325480+0000    70      82        82         0         0         0           -           0
2021-10-29T00:54:25.325557+0000    71      83        83         0         0         0           -           0
2021-10-29T00:54:26.325656+0000    72      85        85         0         0         0           -           0
2021-10-29T00:54:27.325748+0000    73      87        87         0         0         0           -           0
2021-10-29T00:54:28.325840+0000    74      89        89         0         0         0           -           0
2021-10-29T00:54:29.325959+0000    75      91        91         0         0         0           -           0
2021-10-29T00:54:30.326047+0000    76      93        93         0         0         0           -           0
2021-10-29T00:54:31.326161+0000    77      96        96         0         0         0           -           0
2021-10-29T00:54:32.326239+0000    78      98        98         0         0         0           -           0
2021-10-29T00:54:33.326331+0000    79      98        98         0         0         0           -           0
2021-10-29T00:54:34.326430+0000 min lat: 9999 max lat: 0 avg lat: 0
2021-10-29T00:54:34.326430+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-29T00:54:34.326430+0000    80      98        98         0         0         0           -           0
2021-10-29T00:54:35.326569+0000    81      98        98         0         0         0           -           0
2021-10-29T00:54:36.326663+0000    82      99        99         0         0         0           -           0
2021-10-29T00:54:37.326733+0000    83      99        99         0         0         0           -           0
2021-10-29T00:54:38.326802+0000    84      99        99         0         0         0           -           0
2021-10-29T00:54:39.326912+0000    85      99        99         0         0         0           -           0
2021-10-29T00:54:40.326980+0000    86      99        99         0         0         0           -           0
2021-10-29T00:54:41.327069+0000    87      99        99         0         0         0           -           0
2021-10-29T00:54:42.327162+0000    88      99        99         0         0         0           -           0
2021-10-29T00:54:43.327238+0000    89      99        99         0         0         0           -           0
2021-10-29T00:54:44.327346+0000    90     100       100         0         0         0           -           0
2021-10-29T00:54:45.327437+0000    91     100       100         0         0         0           -           0
2021-10-29T00:54:46.327517+0000    92     102       102         0         0         0           -           0
2021-10-29T00:54:47.327610+0000    93     104       104         0         0         0           -           0
2021-10-29T00:54:48.327708+0000    94     105       105         0         0         0           -           0
2021-10-29T00:54:49.327787+0000    95     108       108         0         0         0           -           0
2021-10-29T00:54:50.327888+0000    96     110       110         0         0         0           -           0
2021-10-29T00:54:51.327985+0000    97     113       113         0         0         0           -           0
2021-10-29T00:54:52.328080+0000    98     115       115         0         0         0           -           0
2021-10-29T00:54:53.328155+0000    99     117       117         0         0         0           -           0
2021-10-29T00:54:54.328265+0000 min lat: 9999 max lat: 0 avg lat: 0
2021-10-29T00:54:54.328265+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-29T00:54:54.328265+0000   100     119       119         0         0         0           -           0
2021-10-29T00:54:55.328380+0000   101     122       122         0         0         0           -           0
2021-10-29T00:54:56.328461+0000   102     124       124         0         0         0           -           0
2021-10-29T00:54:57.328571+0000   103     126       126         0         0         0           -           0
2021-10-29T00:54:58.328666+0000   104     126       126         0         0         0           -           0
2021-10-29T00:54:59.328760+0000   105     126       126         0         0         0           -           0
2021-10-29T00:55:00.328860+0000   106     127       127         0         0         0           -           0
2021-10-29T00:55:01.328947+0000   107       2       128       126   18.8393   18.8411     4.76689     59.3117
2021-10-29T00:55:02.329046+0000   108       1       128       127    18.813        16     5.36891     58.8869
2021-10-29T00:55:03.329181+0000 Total time run:         108.9
Total writes made:      128
Write size:             16777216
Object size:            16777216
Bandwidth (MB/sec):     18.8062
Stddev Bandwidth:       2.36751
Max bandwidth (MB/sec): 18.8411
Min bandwidth (MB/sec): 0
Average IOPS:           1
Stddev IOPS:            0.135445
Max IOPS:               1
Min IOPS:               0
Average Latency(s):     58.4545
Stddev Latency(s):      34.4842
Max latency(s):         106.801
Min latency(s):         3.53234

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:55:05,123398040-07:00] INFO: > Stop system activity collection services[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:55:05,130312999-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:391 - rados_bench() > kill -INT 965976

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:55:05,136565049-07:00] INFO: >> for OSD host[0m
# ./benchmarks/bench-rados:396 - rados_bench() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.5 kill -INT 359871
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:55:05,144759649-07:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.5 kill -INT 359871
[1] 17:55:06 [SUCCESS] ljishen@10.10.2.5

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:55:06,422901278-07:00] INFO: >>> Copy system activity file from OSD host[0m
## ./benchmarks/bench-rados:403 - rados_bench() > basename -- /tmp/bench-rados/sys_activity_16MB_write.dat.osd_host.3
# ./benchmarks/bench-rados:403 - rados_bench() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.5:/tmp/bench-rados/sys_activity_16MB_write.dat.osd_host.3 /mnt/sda8/ceph-research/scripts/output/sys_activity_16MB_write.dat.osd_host.3


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:55:07,779287434-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:55:31,627799615-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 129 objects, 2.0 GiB
    usage:   6.5 GiB used, 393 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:55:31,636368150-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:55:40,660278504-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 129 objects, 2.0 GiB
    usage:   6.4 GiB used, 394 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:55:40,669316112-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:55:49,818239726-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 129 objects, 2.0 GiB
    usage:   6.1 GiB used, 394 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:55:49,826889113-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:55:58,750611833-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 129 objects, 2.0 GiB
    usage:   6.1 GiB used, 394 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:55:58,759350829-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:56:07,825831333-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 129 objects, 2.0 GiB
    usage:   6.1 GiB used, 394 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:56:07,834499896-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:56:07,841308946-07:00] INFO: > The cluster is idle now.[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:56:07,845498770-07:00] STAGE: Start benchmarking of mode seq ...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:56:07,852974265-07:00] INFO: > Start system activity collection services[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:56:07,859058809-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:341 - rados_bench() > rm --force /mnt/sda8/ceph-research/scripts/output/sys_activity_16MB_seq.dat.rados_bench_host.3
# ./benchmarks/bench-rados:342 - rados_bench() > sar_pid_rados_bench_host=969349
# ./benchmarks/bench-rados:342 - rados_bench() > S_TIME_FORMAT=ISO
# ./benchmarks/bench-rados:342 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/output/sys_activity_16MB_seq.dat.rados_bench_host.3 2

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:56:07,867045448-07:00] INFO: >> for OSD host[0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:56:07,876226056-07:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.5 bash
10.10.2.5: b'360624\n'
[1] 17:56:09 [SUCCESS] ljishen@10.10.2.5
360624

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:56:09,181399240-07:00] INFO: > Run rados bench[0m
# ./benchmarks/bench-rados:379 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/output/rados_bench_16MB_seq.log.3
## ./benchmarks/bench-rados:376 - rados_bench() > count_cpus 32-63,96-127
# ./benchmarks/bench-rados:376 - rados_bench() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 128 --show-time
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:56:09,202087501-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:56:09,204950425-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'd345935e-3851-11ec-b51d-53e6e728d2d3', '--', 'rados', 'bench', '99999999', 'seq', '--pool', 'bench_rados', '--concurrent-ios', '128', '--show-time'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid d345935e-3851-11ec-b51d-53e6e728d2d3 -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 128 --show-time
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
hints = 1
2021-10-29T00:56:12.173040+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-29T00:56:12.173040+0000     0       0         0         0         0         0           -           0
2021-10-29T00:56:13.173186+0000     1       9         9         0         0         0           -           0
2021-10-29T00:56:14.173296+0000     2      22        22         0         0         0           -           0
2021-10-29T00:56:15.173406+0000     3      34        34         0         0         0           -           0
2021-10-29T00:56:16.173515+0000     4      43        43         0         0         0           -           0
2021-10-29T00:56:17.173627+0000     5      53        53         0         0         0           -           0
2021-10-29T00:56:18.173743+0000     6      61        61         0         0         0           -           0
2021-10-29T00:56:19.173862+0000     7      73        73         0         0         0           -           0
2021-10-29T00:56:20.173975+0000     8      83        83         0         0         0           -           0
2021-10-29T00:56:21.174084+0000     9      92        92         0         0         0           -           0
2021-10-29T00:56:22.174192+0000    10     101       101         0         0         0           -           0
2021-10-29T00:56:23.174307+0000    11     112       112         0         0         0           -           0
2021-10-29T00:56:24.174390+0000    12     123       123         0         0         0           -           0
2021-10-29T00:56:25.174506+0000    13       3       128       125   153.827   153.846     0.85533     6.85576
2021-10-29T00:56:26.174640+0000 Total time run:       13.2306
Total reads made:     128
Read size:            16777216
Object size:          16777216
Bandwidth (MB/sec):   154.793
Average IOPS:         9
Stddev IOPS:          2.49615
Max IOPS:             9
Min IOPS:             0
Average Latency(s):   6.7147
Max latency(s):       12.6665
Min latency(s):       0.732116

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:56:26,920209821-07:00] INFO: > Stop system activity collection services[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:56:26,926740706-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:391 - rados_bench() > kill -INT 969349

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:56:26,933295145-07:00] INFO: >> for OSD host[0m
# ./benchmarks/bench-rados:396 - rados_bench() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.5 kill -INT 360624
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:56:26,941453997-07:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.5 kill -INT 360624
[1] 17:56:28 [SUCCESS] ljishen@10.10.2.5

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:56:28,037102151-07:00] INFO: >>> Copy system activity file from OSD host[0m
## ./benchmarks/bench-rados:403 - rados_bench() > basename -- /tmp/bench-rados/sys_activity_16MB_seq.dat.osd_host.3
# ./benchmarks/bench-rados:403 - rados_bench() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.5:/tmp/bench-rados/sys_activity_16MB_seq.dat.osd_host.3 /mnt/sda8/ceph-research/scripts/output/sys_activity_16MB_seq.dat.osd_host.3


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:56:29,129190858-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:56:53,057430347-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 129 objects, 2.0 GiB
    usage:   6.1 GiB used, 394 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:56:53,065830985-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:57:02,106899866-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 129 objects, 2.0 GiB
    usage:   6.1 GiB used, 394 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:57:02,115302117-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:57:11,006361057-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 129 objects, 2.0 GiB
    usage:   6.1 GiB used, 394 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:57:11,015368658-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:57:20,131827319-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 129 objects, 2.0 GiB
    usage:   6.1 GiB used, 394 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:57:20,140795796-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:57:29,198223028-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 129 objects, 2.0 GiB
    usage:   6.1 GiB used, 394 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:57:29,206823332-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T17:57:29,213372301-07:00] INFO: > The cluster is idle now.[0m
