[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:21:35,563994637-07:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.3 mkdir --parents /tmp/bench-rados
[1] 21:21:35 [SUCCESS] ljishen@10.10.2.3
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:21:35,745399686-07:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.5 mkdir --parents /tmp/bench-rados
[1] 21:21:36 [SUCCESS] ljishen@10.10.2.5
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:21:36,924427203-07:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.5 bash
[1] 21:21:38 [SUCCESS] ljishen@10.10.2.5


[1;7;39;49m[2021-10-26T21:21:38,063551650-07:00][RUNNING][ROUND 1/1/21] object_size=4KB[0m


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:21:38,066614879-07:00] STAGE: Launch a Ceph cluster (CEPH_HOST: 10.10.2.3, OSD_HOST: 10.10.2.5) ...[0m
# ./benchmarks/bench-rados:172 - launch_ceph_cluster() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.3 bash -euo pipefail
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:21:38,075195328-07:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.3 bash -euo pipefail
10.10.2.3: b'\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T00:21:38,520901656-04:00] INFO: > Remove the existing cluster\x1b[0m\n'
10.10.2.3: b'## bash:13 -  > ls /var/lib/ceph\n'
10.10.2.3: b'## bash:13 -  > tail -n1\n'
10.10.2.3: b'# bash:13 -  > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm --host 10.10.2.3 --host 10.10.2.5 rm-cluster --force --fsid 7e5b096c-36dc-11ec-b561-09a495c6360a\n'
10.10.2.3: b'\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T00:21:45,875773070-04:00] INFO: > Deploy a new cluster\x1b[0m\n'
10.10.2.3: b'## bash:22 -  > /tmp/bench-rados/ceph-research/scripts/parallel-ssh --host 10.10.2.5 uname --nodename\n## bash:22 -  > tail -n1\n'
10.10.2.3: b'# bash:22 -  > osd_hostname=sm1\n# bash:23 -  > /tmp/bench-rados/ceph-research/scripts/ceph-deploy -m 10.10.2.3 -o sm1/10.10.2.5:/dev/nvme0n1\n'
10.10.2.3: b'\n\n\x1b[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T00:21:47,004745469-04:00] STAGE: Verify passwordless SSH to hosts, passwordless sudo via ljishen, and the existence of devices on hosts...\x1b[0m\n'
10.10.2.3: b'\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T00:21:47,008665586-04:00] INFO: > Check the host for the monitor\x1b[0m\n# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:181 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@10.10.2.3 sudo --non-interactive --validate\n'
10.10.2.3: b'\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T00:21:47,160612912-04:00] INFO: > Check hosts for OSDs\x1b[0m\n'
10.10.2.3: b"\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T00:21:47,164525104-04:00] INFO: >> Check host 'sm1'\x1b[0m\n# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:200 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 sudo --non-interactive --validate\n"
10.10.2.3: b"\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T00:21:48,231756296-04:00] INFO: >>> Check device '/dev/nvme0n1'\x1b[0m\n"
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:215 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 test -b /dev/nvme0n1\n'
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:226 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 lsblk --nodeps --noheadings --output TYPE /dev/nvme0n1\n# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:227 - verify_parameters() > grep --word-regexp disk\n'
10.10.2.3: b'\n\n\x1b[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T00:21:50,341140099-04:00] STAGE: Reset devices on hosts...\x1b[0m\n'
10.10.2.3: b"\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T00:21:50,345048765-04:00] INFO: > Resetting device '/dev/nvme0n1' on host 'sm1'\x1b[0m\n"
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:288 - reset_host_devices() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 sudo sh\n'
10.10.2.3: b'  Removing ceph--db45ca59--0d72--4367--ac2f--8e7ecc41f16a-osd--block--2972b616--f3ae--4f6c--9da1--16497c9d8de0 (252:0)\n'
10.10.2.3: b'  Archiving volume group "ceph-db45ca59-0d72-4367-ac2f-8e7ecc41f16a" metadata (seqno 5).\n'
10.10.2.3: b'  Releasing logical volume "osd-block-2972b616-f3ae-4f6c-9da1-16497c9d8de0"\n'
10.10.2.3: b'  Creating volume group backup "/etc/lvm/backup/ceph-db45ca59-0d72-4367-ac2f-8e7ecc41f16a" (seqno 6).\n'
10.10.2.3: b'  Logical volume "osd-block-2972b616-f3ae-4f6c-9da1-16497c9d8de0" successfully removed\n  Removing physical volume "/dev/nvme0n1" from volume group "ceph-db45ca59-0d72-4367-ac2f-8e7ecc41f16a"\n'
10.10.2.3: b'  Volume group "ceph-db45ca59-0d72-4367-ac2f-8e7ecc41f16a" successfully removed\n'
10.10.2.3: b'\n\n\x1b[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T00:21:52,652372615-04:00] STAGE: Bootstrap a new cluster...\x1b[0m\n# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:312 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.3 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults\n'
10.10.2.3: b'\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T00:21:52,660724664-04:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.3: b"\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T00:21:52,663506225-04:00] DEBUG: CEPHADM_PARAMS: ['bootstrap', '--config', '/tmp/bench-rados/ceph-research/scripts/ceph.conf', '--mon-ip', '10.10.2.3', '--output-dir', '/tmp/bench-rados/deployment_data_root/etc/ceph', '--ssh-user', 'ljishen', '--skip-dashboard', '--no-minimize-config', '--allow-overwrite', '--allow-fqdn-hostname', '--orphan-initial-daemons', '--skip-monitoring-stack', '--single-host-defaults']\x1b[0m\n"
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.3 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults\n'
10.10.2.3: b'Verifying podman|docker is present...\nVerifying lvm2 is present...\nVerifying time synchronization is in place...\n'
10.10.2.3: b'Unit ntp.service is enabled and running\nRepeating the final host check...\npodman|docker (/usr/bin/docker) is present\n'
10.10.2.3: b'systemctl is present\nlvcreate is present\n'
10.10.2.3: b'Unit ntp.service is enabled and running\nHost looks OK\n'
10.10.2.3: b'Cluster fsid: 699915d6-36dd-11ec-b561-09a495c6360a\n'
10.10.2.3: b'Verifying IP 10.10.2.3 port 3300 ...\nVerifying IP 10.10.2.3 port 6789 ...\n'
10.10.2.3: b'Mon IP `10.10.2.3` is in CIDR network `10.10.2.0/24`\n- internal network (--cluster-network) has not been provided, OSD replication will default to the public_network\n'
10.10.2.3: b'Adjusting default settings to suit single-host cluster...\n'
10.10.2.3: b'Pulling container image quay.io/ceph/ceph:v16...\n'
10.10.2.3: b'Ceph version: ceph version 16.2.6 (ee28fb57e47e9f88813e24bbf4c14496ca299d31) pacific (stable)\nExtracting ceph user uid/gid from container image...\n'
10.10.2.3: b'Creating initial keys...\n'
10.10.2.3: b'Creating initial monmap...\n'
10.10.2.3: b'Creating mon...\n'
10.10.2.3: b'Waiting for mon to start...\nWaiting for mon...\n'
10.10.2.3: b'mon is available\nSetting mon public_network to 10.10.2.0/24\n'
10.10.2.3: b'Wrote config to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf\nWrote keyring to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring\nCreating mgr...\n'
10.10.2.3: b'Verifying port 9283 ...\n'
10.10.2.3: b'Waiting for mgr to start...\nWaiting for mgr...\n'
10.10.2.3: b'mgr not available, waiting (1/15)...\n'
10.10.2.3: b'mgr not available, waiting (2/15)...\n'
10.10.2.3: b'mgr is available\n'
10.10.2.3: b'Enabling cephadm module...\n'
10.10.2.3: b'Waiting for the mgr to restart...\nWaiting for mgr epoch 5...\n'
10.10.2.3: b'mgr epoch 5 is available\nSetting orchestrator backend to cephadm...\n'
10.10.2.3: b'Generating ssh key...\n'
10.10.2.3: b'Wrote public SSH key to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub\nAdding key to ljishen@localhost authorized_keys...\n'
10.10.2.3: b'Adding host node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us...\n'
10.10.2.3: b'Deploying unmanaged mon service...\n'
10.10.2.3: b'Deploying unmanaged mgr service...\n'
10.10.2.3: b'You can access the Ceph CLI with:\n\n\tsudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --fsid 699915d6-36dd-11ec-b561-09a495c6360a -c /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf -k /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring\n\nPlease consider enabling telemetry to help improve Ceph:\n\n\tceph telemetry on\n\nFor more information see:\n\n\thttps://docs.ceph.com/docs/pacific/mgr/telemetry/\n\n'
10.10.2.3: b'Bootstrap complete.\n'
10.10.2.3: b'\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T00:22:55,531508733-04:00] INFO: > Set the default number of monitor daemons to 1\x1b[0m\n# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:330 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1\n'
10.10.2.3: b'\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T00:22:55,540150648-04:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.3: b"\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T00:22:55,542994557-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mon', '1']\x1b[0m\n"
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1\n'
10.10.2.3: b'Inferring fsid 699915d6-36dd-11ec-b561-09a495c6360a\n'
10.10.2.3: b'Inferring config /var/lib/ceph/699915d6-36dd-11ec-b561-09a495c6360a/mon.node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.3: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.3: b'Scheduled mon update...\n'
10.10.2.3: b'\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T00:23:04,970016104-04:00] INFO: > Set the default number of manager daemons to 1\x1b[0m\n# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:335 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1\n'
10.10.2.3: b'\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T00:23:04,978938328-04:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.3: b"\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T00:23:04,982181059-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mgr', '1']\x1b[0m\n"
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1\n'
10.10.2.3: b'Inferring fsid 699915d6-36dd-11ec-b561-09a495c6360a\n'
10.10.2.3: b'Inferring config /var/lib/ceph/699915d6-36dd-11ec-b561-09a495c6360a/mon.node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.3: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.3: b'Scheduled mgr update...\n'
10.10.2.3: b'\n\n\x1b[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T00:23:14,509623139-04:00] STAGE: Add hosts to the cluster...\x1b[0m\n'
10.10.2.3: b"\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T00:23:14,514592214-04:00] INFO: > Adding host 'sm1'\x1b[0m\n# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:362 - add_hosts() > ssh-copy-id -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -f -i /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub ljishen@sm1\n"
10.10.2.3: b'/usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: "/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub"\n'
10.10.2.3: b'\nNumber of key(s) added: 1\n\nNow try logging into the machine, with:   "ssh -o \'GlobalKnownHostsFile=/dev/null\' -o \'LogLevel=ERROR\' -o \'PasswordAuthentication=no\' -o \'StrictHostKeyChecking=no\' -o \'UserKnownHostsFile=/dev/null\' \'ljishen@sm1\'"\nand check to make sure that only the key(s) you wanted were added.\n\n'
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:368 - add_hosts() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add sm1 10.10.2.5\n'
10.10.2.3: b'\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T00:23:15,644194399-04:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.3: b"\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T00:23:15,647253233-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'host', 'add', 'sm1', '10.10.2.5']\x1b[0m\n"
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add sm1 10.10.2.5\n'
10.10.2.3: b'Inferring fsid 699915d6-36dd-11ec-b561-09a495c6360a\n'
10.10.2.3: b'Inferring config /var/lib/ceph/699915d6-36dd-11ec-b561-09a495c6360a/mon.node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.3: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.3: b"Added host 'sm1' with addr '10.10.2.5'\n"
10.10.2.3: b'\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T00:23:26,500218319-04:00] INFO: Sleep for 20 seconds\x1b[0m\n'
10.10.2.3: b'\n\n\x1b[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T00:23:46,504260076-04:00] STAGE: Add OSDs to the cluster...\x1b[0m\n'
10.10.2.3: b"\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T00:23:46,509423266-04:00] INFO: > Adding devices ['/dev/nvme0n1'] on host 'sm1' as OSDs\x1b[0m\n# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:387 - add_host_devices_as_osds() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd sm1:/dev/nvme0n1\n"
10.10.2.3: b'\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T00:23:46,517870503-04:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.3: b"\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T00:23:46,520887559-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'daemon', 'add', 'osd', 'sm1:/dev/nvme0n1']\x1b[0m\n# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd sm1:/dev/nvme0n1\n"
10.10.2.3: b'Inferring fsid 699915d6-36dd-11ec-b561-09a495c6360a\n'
10.10.2.3: b'Inferring config /var/lib/ceph/699915d6-36dd-11ec-b561-09a495c6360a/mon.node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.3: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.3: b"Created osd(s) 0 on host 'sm1'\n"
10.10.2.3: b'\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T00:24:10,232351656-04:00] INFO: > Sleep for 20 seconds\x1b[0m\n'
10.10.2.3: b'\n\n\x1b[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T00:24:30,236685905-04:00] STAGE: Show Cluster Status\x1b[0m\n# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:343 - show_status() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status\n'
10.10.2.3: b'\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T00:24:30,245382012-04:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.3: b"\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T00:24:30,248266127-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', '--status']\x1b[0m\n# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status\n"
10.10.2.3: b'Inferring fsid 699915d6-36dd-11ec-b561-09a495c6360a\n'
10.10.2.3: b'Inferring config /var/lib/ceph/699915d6-36dd-11ec-b561-09a495c6360a/mon.node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.3: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.3: b'  cluster:\n    id:     699915d6-36dd-11ec-b561-09a495c6360a\n    health: HEALTH_WARN\n            1 pool(s) have no replicas configured\n \n  services:\n    mon: 1 daemons, quorum node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us (age 2m)\n    mgr: node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us.wetvnc(active, since 2m)\n    osd: 1 osds: 1 up (since 21s), 1 in (since 40s)\n \n  data:\n    pools:   1 pools, 128 pgs\n    objects: 0 objects, 0 B\n    usage:   4.7 MiB used, 100 GiB / 100 GiB avail\n    pgs:     99.219% pgs unknown\n             127 unknown\n             1   active+clean\n \n  progress:\n \n'
[1] 21:24:39 [SUCCESS] ljishen@10.10.2.3

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T00:21:38,520901656-04:00] INFO: > Remove the existing cluster[0m
## bash:13 -  > ls /var/lib/ceph
## bash:13 -  > tail -n1
# bash:13 -  > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm --host 10.10.2.3 --host 10.10.2.5 rm-cluster --force --fsid 7e5b096c-36dc-11ec-b561-09a495c6360a

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T00:21:45,875773070-04:00] INFO: > Deploy a new cluster[0m
## bash:22 -  > /tmp/bench-rados/ceph-research/scripts/parallel-ssh --host 10.10.2.5 uname --nodename
## bash:22 -  > tail -n1
# bash:22 -  > osd_hostname=sm1
# bash:23 -  > /tmp/bench-rados/ceph-research/scripts/ceph-deploy -m 10.10.2.3 -o sm1/10.10.2.5:/dev/nvme0n1


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T00:21:47,004745469-04:00] STAGE: Verify passwordless SSH to hosts, passwordless sudo via ljishen, and the existence of devices on hosts...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T00:21:47,008665586-04:00] INFO: > Check the host for the monitor[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:181 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@10.10.2.3 sudo --non-interactive --validate

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T00:21:47,160612912-04:00] INFO: > Check hosts for OSDs[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T00:21:47,164525104-04:00] INFO: >> Check host 'sm1'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:200 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 sudo --non-interactive --validate

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T00:21:48,231756296-04:00] INFO: >>> Check device '/dev/nvme0n1'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:215 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 test -b /dev/nvme0n1
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:226 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 lsblk --nodeps --noheadings --output TYPE /dev/nvme0n1
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:227 - verify_parameters() > grep --word-regexp disk


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T00:21:50,341140099-04:00] STAGE: Reset devices on hosts...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T00:21:50,345048765-04:00] INFO: > Resetting device '/dev/nvme0n1' on host 'sm1'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:288 - reset_host_devices() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 sudo sh
  Removing ceph--db45ca59--0d72--4367--ac2f--8e7ecc41f16a-osd--block--2972b616--f3ae--4f6c--9da1--16497c9d8de0 (252:0)
  Archiving volume group "ceph-db45ca59-0d72-4367-ac2f-8e7ecc41f16a" metadata (seqno 5).
  Releasing logical volume "osd-block-2972b616-f3ae-4f6c-9da1-16497c9d8de0"
  Creating volume group backup "/etc/lvm/backup/ceph-db45ca59-0d72-4367-ac2f-8e7ecc41f16a" (seqno 6).
  Logical volume "osd-block-2972b616-f3ae-4f6c-9da1-16497c9d8de0" successfully removed
  Removing physical volume "/dev/nvme0n1" from volume group "ceph-db45ca59-0d72-4367-ac2f-8e7ecc41f16a"
  Volume group "ceph-db45ca59-0d72-4367-ac2f-8e7ecc41f16a" successfully removed


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T00:21:52,652372615-04:00] STAGE: Bootstrap a new cluster...[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:312 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.3 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T00:21:52,660724664-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T00:21:52,663506225-04:00] DEBUG: CEPHADM_PARAMS: ['bootstrap', '--config', '/tmp/bench-rados/ceph-research/scripts/ceph.conf', '--mon-ip', '10.10.2.3', '--output-dir', '/tmp/bench-rados/deployment_data_root/etc/ceph', '--ssh-user', 'ljishen', '--skip-dashboard', '--no-minimize-config', '--allow-overwrite', '--allow-fqdn-hostname', '--orphan-initial-daemons', '--skip-monitoring-stack', '--single-host-defaults'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.3 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults
Verifying podman|docker is present...
Verifying lvm2 is present...
Verifying time synchronization is in place...
Unit ntp.service is enabled and running
Repeating the final host check...
podman|docker (/usr/bin/docker) is present
systemctl is present
lvcreate is present
Unit ntp.service is enabled and running
Host looks OK
Cluster fsid: 699915d6-36dd-11ec-b561-09a495c6360a
Verifying IP 10.10.2.3 port 3300 ...
Verifying IP 10.10.2.3 port 6789 ...
Mon IP `10.10.2.3` is in CIDR network `10.10.2.0/24`
- internal network (--cluster-network) has not been provided, OSD replication will default to the public_network
Adjusting default settings to suit single-host cluster...
Pulling container image quay.io/ceph/ceph:v16...
Ceph version: ceph version 16.2.6 (ee28fb57e47e9f88813e24bbf4c14496ca299d31) pacific (stable)
Extracting ceph user uid/gid from container image...
Creating initial keys...
Creating initial monmap...
Creating mon...
Waiting for mon to start...
Waiting for mon...
mon is available
Setting mon public_network to 10.10.2.0/24
Wrote config to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf
Wrote keyring to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring
Creating mgr...
Verifying port 9283 ...
Waiting for mgr to start...
Waiting for mgr...
mgr not available, waiting (1/15)...
mgr not available, waiting (2/15)...
mgr is available
Enabling cephadm module...
Waiting for the mgr to restart...
Waiting for mgr epoch 5...
mgr epoch 5 is available
Setting orchestrator backend to cephadm...
Generating ssh key...
Wrote public SSH key to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub
Adding key to ljishen@localhost authorized_keys...
Adding host node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us...
Deploying unmanaged mon service...
Deploying unmanaged mgr service...
You can access the Ceph CLI with:

	sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --fsid 699915d6-36dd-11ec-b561-09a495c6360a -c /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf -k /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring

Please consider enabling telemetry to help improve Ceph:

	ceph telemetry on

For more information see:

	https://docs.ceph.com/docs/pacific/mgr/telemetry/

Bootstrap complete.

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T00:22:55,531508733-04:00] INFO: > Set the default number of monitor daemons to 1[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:330 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T00:22:55,540150648-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T00:22:55,542994557-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mon', '1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1
Inferring fsid 699915d6-36dd-11ec-b561-09a495c6360a
Inferring config /var/lib/ceph/699915d6-36dd-11ec-b561-09a495c6360a/mon.node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Scheduled mon update...

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T00:23:04,970016104-04:00] INFO: > Set the default number of manager daemons to 1[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:335 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T00:23:04,978938328-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T00:23:04,982181059-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mgr', '1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1
Inferring fsid 699915d6-36dd-11ec-b561-09a495c6360a
Inferring config /var/lib/ceph/699915d6-36dd-11ec-b561-09a495c6360a/mon.node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Scheduled mgr update...


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T00:23:14,509623139-04:00] STAGE: Add hosts to the cluster...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T00:23:14,514592214-04:00] INFO: > Adding host 'sm1'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:362 - add_hosts() > ssh-copy-id -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -f -i /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub ljishen@sm1
/usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: "/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub"

Number of key(s) added: 1

Now try logging into the machine, with:   "ssh -o 'GlobalKnownHostsFile=/dev/null' -o 'LogLevel=ERROR' -o 'PasswordAuthentication=no' -o 'StrictHostKeyChecking=no' -o 'UserKnownHostsFile=/dev/null' 'ljishen@sm1'"
and check to make sure that only the key(s) you wanted were added.

# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:368 - add_hosts() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add sm1 10.10.2.5
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T00:23:15,644194399-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T00:23:15,647253233-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'host', 'add', 'sm1', '10.10.2.5'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add sm1 10.10.2.5
Inferring fsid 699915d6-36dd-11ec-b561-09a495c6360a
Inferring config /var/lib/ceph/699915d6-36dd-11ec-b561-09a495c6360a/mon.node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Added host 'sm1' with addr '10.10.2.5'

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T00:23:26,500218319-04:00] INFO: Sleep for 20 seconds[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T00:23:46,504260076-04:00] STAGE: Add OSDs to the cluster...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T00:23:46,509423266-04:00] INFO: > Adding devices ['/dev/nvme0n1'] on host 'sm1' as OSDs[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:387 - add_host_devices_as_osds() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd sm1:/dev/nvme0n1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T00:23:46,517870503-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T00:23:46,520887559-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'daemon', 'add', 'osd', 'sm1:/dev/nvme0n1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd sm1:/dev/nvme0n1
Inferring fsid 699915d6-36dd-11ec-b561-09a495c6360a
Inferring config /var/lib/ceph/699915d6-36dd-11ec-b561-09a495c6360a/mon.node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Created osd(s) 0 on host 'sm1'

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T00:24:10,232351656-04:00] INFO: > Sleep for 20 seconds[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T00:24:30,236685905-04:00] STAGE: Show Cluster Status[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:343 - show_status() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T00:24:30,245382012-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T00:24:30,248266127-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', '--status'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status
Inferring fsid 699915d6-36dd-11ec-b561-09a495c6360a
Inferring config /var/lib/ceph/699915d6-36dd-11ec-b561-09a495c6360a/mon.node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
  cluster:
    id:     699915d6-36dd-11ec-b561-09a495c6360a
    health: HEALTH_WARN
            1 pool(s) have no replicas configured
 
  services:
    mon: 1 daemons, quorum node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us (age 2m)
    mgr: node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us.wetvnc(active, since 2m)
    osd: 1 osds: 1 up (since 21s), 1 in (since 40s)
 
  data:
    pools:   1 pools, 128 pgs
    objects: 0 objects, 0 B
    usage:   4.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     99.219% pgs unknown
             127 unknown
             1   active+clean
 
  progress:
 

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:24:39,475624548-07:00] INFO: > Copy configuration files to local[0m
# ./benchmarks/bench-rados:208 - launch_ceph_cluster() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.3:/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf
# ./benchmarks/bench-rados:212 - launch_ceph_cluster() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.3 sudo chmod 644 /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:24:39,631294913-07:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.3 sudo chmod 644 /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring
[1] 21:24:39 [SUCCESS] ljishen@10.10.2.3
# ./benchmarks/bench-rados:214 - launch_ceph_cluster() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.3:/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:24:39,961633722-07:00] STAGE: Update local ceph.conf as a client...[0m


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:24:39,964538582-07:00] STAGE: Configure Ceph pool...[0m
# ./benchmarks/bench-rados:245 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_max_backfills 32
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:24:39,990169142-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:24:39,994031322-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '699915d6-36dd-11ec-b561-09a495c6360a', '--', 'ceph', 'config', 'set', 'osd', 'osd_max_backfills', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 699915d6-36dd-11ec-b561-09a495c6360a -- ceph config set osd osd_max_backfills 32
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:246 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_active 32
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:24:43,120958211-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:24:43,124516209-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '699915d6-36dd-11ec-b561-09a495c6360a', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_active', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 699915d6-36dd-11ec-b561-09a495c6360a -- ceph config set osd osd_recovery_max_active 32
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:247 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_single_start 8
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:24:46,278291798-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:24:46,282120906-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '699915d6-36dd-11ec-b561-09a495c6360a', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_single_start', '8'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 699915d6-36dd-11ec-b561-09a495c6360a -- ceph config set osd osd_recovery_max_single_start 8
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:248 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_op_priority 63
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:24:49,402116977-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:24:49,405900148-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '699915d6-36dd-11ec-b561-09a495c6360a', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_op_priority', '63'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 699915d6-36dd-11ec-b561-09a495c6360a -- ceph config set osd osd_recovery_op_priority 63
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:250 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config show-with-defaults osd.0
# ./benchmarks/bench-rados:251 - configure_ceph_pool() > grep 'osd_max_backfills\|osd_recovery'
# ./benchmarks/bench-rados:251 - configure_ceph_pool() > column -t -s ' '
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
osd_max_backfills                        32         mon
osd_recovery_cost                        20971520   default
osd_recovery_delay_start                 0.000000   default
osd_recovery_max_active                  32         mon
osd_recovery_max_active_hdd              3          default
osd_recovery_max_active_ssd              10         default
osd_recovery_max_chunk                   8388608    default
osd_recovery_max_omap_entries_per_chunk  8096       default
osd_recovery_max_single_start            8          mon
osd_recovery_op_priority                 63         mon
osd_recovery_op_warn_multiple            16         default
osd_recovery_priority                    5          default
osd_recovery_retry_interval              30.000000  default
osd_recovery_sleep                       0.000000   default
osd_recovery_sleep_hdd                   0.100000   default
osd_recovery_sleep_hybrid                0.025000   default
osd_recovery_sleep_ssd                   0.000000   default
# ./benchmarks/bench-rados:253 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool create bench_rados 128 128 replicated --size 1 --pg-num-min 128
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:24:55,693693835-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:24:55,697040987-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '699915d6-36dd-11ec-b561-09a495c6360a', '--', 'ceph', 'osd', 'pool', 'create', 'bench_rados', '128', '128', 'replicated', '--size', '1', '--pg-num-min', '128'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 699915d6-36dd-11ec-b561-09a495c6360a -- ceph osd pool create bench_rados 128 128 replicated --size 1 --pg-num-min 128
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
pool 'bench_rados' created
# ./benchmarks/bench-rados:255 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados min_size 1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:24:59,390389335-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:24:59,394024949-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '699915d6-36dd-11ec-b561-09a495c6360a', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'min_size', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 699915d6-36dd-11ec-b561-09a495c6360a -- ceph osd pool set bench_rados min_size 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 min_size to 1
# ./benchmarks/bench-rados:256 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados pg_autoscale_mode off
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:25:03,847034052-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:25:03,850621776-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '699915d6-36dd-11ec-b561-09a495c6360a', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'pg_autoscale_mode', 'off'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 699915d6-36dd-11ec-b561-09a495c6360a -- ceph osd pool set bench_rados pg_autoscale_mode off
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 pg_autoscale_mode to off
# ./benchmarks/bench-rados:257 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool application enable bench_rados benchmark
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:25:07,281583309-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:25:07,284821546-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '699915d6-36dd-11ec-b561-09a495c6360a', '--', 'ceph', 'osd', 'pool', 'application', 'enable', 'bench_rados', 'benchmark'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 699915d6-36dd-11ec-b561-09a495c6360a -- ceph osd pool application enable bench_rados benchmark
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
enabled application 'benchmark' on pool 'bench_rados'
# ./benchmarks/bench-rados:258 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados noscrub 1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:25:10,902638773-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:25:10,905976628-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '699915d6-36dd-11ec-b561-09a495c6360a', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'noscrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 699915d6-36dd-11ec-b561-09a495c6360a -- ceph osd pool set bench_rados noscrub 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 noscrub to 1
# ./benchmarks/bench-rados:259 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados nodeep-scrub 1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:25:14,526693536-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:25:14,530182644-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '699915d6-36dd-11ec-b561-09a495c6360a', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'nodeep-scrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 699915d6-36dd-11ec-b561-09a495c6360a -- ceph osd pool set bench_rados nodeep-scrub 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 nodeep-scrub to 1
# ./benchmarks/bench-rados:260 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool ls detail
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:25:18,810261869-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:25:18,813580657-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '699915d6-36dd-11ec-b561-09a495c6360a', '--', 'ceph', 'osd', 'pool', 'ls', 'detail'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 699915d6-36dd-11ec-b561-09a495c6360a -- ceph osd pool ls detail
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
pool 1 'device_health_metrics' replicated size 1 min_size 1 crush_rule 0 object_hash rjenkins pg_num 128 pgp_num 128 autoscale_mode on last_change 13 lfor 0/0/11 flags hashpspool stripe_width 0 pg_num_min 1 application mgr_devicehealth
pool 2 'bench_rados' replicated size 1 min_size 1 crush_rule 0 object_hash rjenkins pg_num 128 pgp_num 128 autoscale_mode off last_change 21 flags hashpspool,noscrub,nodeep-scrub stripe_width 0 pg_num_min 128 application benchmark

# ./benchmarks/bench-rados:261 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd df tree
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:25:21,993995320-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:25:21,997302958-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '699915d6-36dd-11ec-b561-09a495c6360a', '--', 'ceph', 'osd', 'df', 'tree'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 699915d6-36dd-11ec-b561-09a495c6360a -- ceph osd df tree
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
ID  CLASS  WEIGHT   REWEIGHT  SIZE     RAW USE  DATA     OMAP  META     AVAIL    %USE  VAR   PGS  STATUS  TYPE NAME    
-1         0.09769         -  100 GiB  6.0 MiB  176 KiB   0 B  5.8 MiB  100 GiB  0.01  1.00    -          root default 
-3         0.09769         -  100 GiB  6.0 MiB  176 KiB   0 B  5.8 MiB  100 GiB  0.01  1.00    -              host sm1 
 0    ssd  0.09769   1.00000  100 GiB  6.0 MiB  176 KiB   0 B  5.8 MiB  100 GiB  0.01  1.00  256      up          osd.0
                       TOTAL  100 GiB  6.0 MiB  176 KiB   0 B  5.8 MiB  100 GiB  0.01                                  
MIN/MAX VAR: 1.00/1.00  STDDEV: 0


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:25:25,241025555-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:25:48,488499557-07:00] INFO: >>   data:
    pools:   2 pools, 256 pgs
    objects: 0 objects, 0 B
    usage:   6.0 MiB used, 100 GiB / 100 GiB avail
    pgs:     256 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:25:48,495479417-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:25:56,694361931-07:00] INFO: >>   data:
    pools:   2 pools, 256 pgs
    objects: 0 objects, 0 B
    usage:   6.0 MiB used, 100 GiB / 100 GiB avail
    pgs:     256 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:25:56,700983648-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:26:05,035137158-07:00] INFO: >>   data:
    pools:   2 pools, 256 pgs
    objects: 0 objects, 0 B
    usage:   6.0 MiB used, 100 GiB / 100 GiB avail
    pgs:     256 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:26:05,041340709-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:26:13,273991848-07:00] INFO: >>   data:
    pools:   2 pools, 256 pgs
    objects: 0 objects, 0 B
    usage:   6.0 MiB used, 100 GiB / 100 GiB avail
    pgs:     256 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:26:13,280383292-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:26:21,378172884-07:00] INFO: >>   data:
    pools:   2 pools, 256 pgs
    objects: 0 objects, 0 B
    usage:   6.0 MiB used, 100 GiB / 100 GiB avail
    pgs:     256 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:26:21,384662352-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:26:21,389495427-07:00] INFO: > The cluster is idle now.[0m


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:26:21,392719929-07:00] STAGE: Start benchmarking of mode write ...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:26:21,397640638-07:00] INFO: > Start system activity collection services[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:26:21,401920473-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:329 - rados_bench() > rm --force /mnt/sda8/ceph-research/scripts/output/sys_activity_4KB_write.dat.rados_bench_host.1
# ./benchmarks/bench-rados:330 - rados_bench() > sar_pid_rados_bench_host=739062
# ./benchmarks/bench-rados:330 - rados_bench() > S_TIME_FORMAT=ISO
# ./benchmarks/bench-rados:330 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/output/sys_activity_4KB_write.dat.rados_bench_host.1 2

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:26:21,407508287-07:00] INFO: >> for OSD host[0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:26:21,416162114-07:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.5 bash
10.10.2.5: b'34376\n'
[1] 21:26:22 [SUCCESS] ljishen@10.10.2.5
34376

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:26:22,519723500-07:00] INFO: > Run rados bench[0m
# ./benchmarks/bench-rados:358 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/output/rados_bench_4KB_write.log.1
## ./benchmarks/bench-rados:349 - rados_bench() > count_cpus 32-63,96-127
# ./benchmarks/bench-rados:349 - rados_bench() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 30 write --pool bench_rados -b 4096 -O 4096 --concurrent-ios 128 --show-time --write-object --write-omap --write-xattr --no-cleanup
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:26:22,541371804-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:26:22,544547344-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '699915d6-36dd-11ec-b561-09a495c6360a', '--', 'rados', 'bench', '30', 'write', '--pool', 'bench_rados', '-b', '4096', '-O', '4096', '--concurrent-ios', '128', '--show-time', '--write-object', '--write-omap', '--write-xattr', '--no-cleanup'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 699915d6-36dd-11ec-b561-09a495c6360a -- rados bench 30 write --pool bench_rados -b 4096 -O 4096 --concurrent-ios 128 --show-time --write-object --write-omap --write-xattr --no-cleanup
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
hints = 1
2021-10-27T04:26:24.825853+0000 Maintaining 128 concurrent writes of 4096 bytes to objects of size 4096 for up to 30 seconds or 0 objects
2021-10-27T04:26:24.825861+0000 Object prefix: benchmark_data_node-0.bluefield2.ucsc-cmps10_7
2021-10-27T04:26:24.826348+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-27T04:26:24.826348+0000     0       0         0         0         0         0           -           0
2021-10-27T04:26:25.826448+0000     1     128      6347      6219   24.2904    24.293   0.0168096   0.0204131
2021-10-27T04:26:26.826545+0000     2     128     12489     12361   24.1401   23.9922   0.0220731   0.0205721
2021-10-27T04:26:27.826614+0000     3     127     18784     18657   24.2908   24.5938   0.0345704    0.020506
2021-10-27T04:26:28.826713+0000     4     128     24790     24662   24.0817    23.457   0.0205703   0.0207206
2021-10-27T04:26:29.826809+0000     5     128     30718     30590   23.8962   23.1562   0.0227296   0.0208708
2021-10-27T04:26:30.826904+0000     6     128     36722     36594    23.822   23.4531   0.0208826   0.0209553
2021-10-27T04:26:31.826981+0000     7     128     42671     42543   23.7383   23.2383   0.0191713    0.021037
2021-10-27T04:26:32.827085+0000     8     128     48597     48469   23.6643   23.1484   0.0193152   0.0211083
2021-10-27T04:26:33.827178+0000     9     128     54417     54289   23.5607   22.7344    0.023485   0.0211986
2021-10-27T04:26:34.827273+0000    10     128     60193     60065   23.4607   22.5625   0.0179738   0.0212866
2021-10-27T04:26:35.827342+0000    11     127     66016     65889    23.396     22.75   0.0254743   0.0213449
2021-10-27T04:26:36.827438+0000    12     128     71788     71660   23.3247    22.543   0.0204606   0.0214101
2021-10-27T04:26:37.827533+0000    13     128     77443     77315   23.2295   22.0898   0.0180639   0.0215079
2021-10-27T04:26:38.827628+0000    14     127     83112     82985   23.1522   22.1484   0.0217611   0.0215759
2021-10-27T04:26:39.827700+0000    15     128     88875     88747   23.1091   22.5078   0.0162148   0.0216227
2021-10-27T04:26:40.827797+0000    16     128     94543     94415   23.0484   22.1406   0.0208214   0.0216813
2021-10-27T04:26:41.827895+0000    17     128    100091     99963   22.9673   21.6719   0.0206489   0.0217546
2021-10-27T04:26:42.827989+0000    18     128    105623    105495   22.8918   21.6094   0.0239158   0.0218304
2021-10-27T04:26:43.828057+0000    19     128    109427    109299    22.469   14.8594   0.0426642   0.0222316
2021-10-27T04:26:44.828162+0000 min lat: 0.00761641 max lat: 0.0584073 avg lat: 0.0227344
2021-10-27T04:26:44.828162+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-27T04:26:44.828162+0000    20     128    112627    112499   21.9705      12.5   0.0365466   0.0227344
2021-10-27T04:26:45.828276+0000    21     128    115699    115571   21.4956        12   0.0466676   0.0232252
2021-10-27T04:26:46.828371+0000    22     128    118757    118629   21.0614   11.9453   0.0405929   0.0237215
2021-10-27T04:26:47.828440+0000    23     128    121715    121587   20.6481   11.5547   0.0481189   0.0241785
2021-10-27T04:26:48.828536+0000    24     128    124659    124531   20.2669      11.5   0.0556552   0.0246376
2021-10-27T04:26:49.828628+0000    25     128    127475    127347   19.8961        11   0.0462252   0.0250988
2021-10-27T04:26:50.828733+0000    26     128    130360    130232   19.5643   11.2695   0.0418832   0.0255316
2021-10-27T04:26:51.828811+0000    27     128    134852    134724   19.4895   17.5469   0.0237202   0.0256416
2021-10-27T04:26:52.828915+0000    28     128    139262    139134   19.4087   17.2266   0.0208778   0.0257542
2021-10-27T04:26:53.829009+0000    29     128    145098    144970   19.5254   22.7969   0.0162247   0.0256002
2021-10-27T04:26:54.829106+0000    30      87    150695    150608   19.6086   22.0234   0.0245682     0.02549
2021-10-27T04:26:55.829364+0000 Total time run:         30.014
Total writes made:      150695
Write size:             4096
Object size:            4096
Bandwidth (MB/sec):     19.6126
Stddev Bandwidth:       4.90053
Max bandwidth (MB/sec): 24.5938
Min bandwidth (MB/sec): 11
Average IOPS:           5020
Stddev IOPS:            1254.53
Max IOPS:               6296
Min IOPS:               2816
Average Latency(s):     0.0254866
Stddev Latency(s):      0.0110097
Max latency(s):         0.287802
Min latency(s):         0.00761641

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:26:56,359429480-07:00] INFO: > Stop system activity collection services[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:26:56,364274577-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:379 - rados_bench() > kill -INT 739062

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:26:56,369042009-07:00] INFO: >> for OSD host[0m
# ./benchmarks/bench-rados:384 - rados_bench() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.5 kill -INT 34376
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:26:56,377341560-07:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.5 kill -INT 34376
[1] 21:26:57 [SUCCESS] ljishen@10.10.2.5

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:26:57,506019108-07:00] INFO: >>> Copy system activity file from OSD host[0m
## ./benchmarks/bench-rados:391 - rados_bench() > basename -- /tmp/bench-rados/sys_activity_4KB_write.dat.osd_host.1
# ./benchmarks/bench-rados:391 - rados_bench() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.5:/tmp/bench-rados/sys_activity_4KB_write.dat.osd_host.1 /mnt/sda8/ceph-research/scripts/output/sys_activity_4KB_write.dat.osd_host.1


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:26:58,608025581-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:27:21,679241087-07:00] INFO: >>   data:
    pools:   2 pools, 256 pgs
    objects: 150.70k objects, 589 MiB
    usage:   1.9 GiB used, 98 GiB / 100 GiB avail
    pgs:     256 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:27:21,685488931-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:27:29,883466259-07:00] INFO: >>   data:
    pools:   2 pools, 256 pgs
    objects: 150.70k objects, 589 MiB
    usage:   1.9 GiB used, 98 GiB / 100 GiB avail
    pgs:     256 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:27:29,889832135-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:27:38,039441592-07:00] INFO: >>   data:
    pools:   2 pools, 256 pgs
    objects: 150.70k objects, 589 MiB
    usage:   1.9 GiB used, 98 GiB / 100 GiB avail
    pgs:     256 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:27:38,045861951-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:27:46,162637497-07:00] INFO: >>   data:
    pools:   2 pools, 256 pgs
    objects: 150.70k objects, 589 MiB
    usage:   1.9 GiB used, 98 GiB / 100 GiB avail
    pgs:     256 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:27:46,168899448-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:27:54,370944441-07:00] INFO: >>   data:
    pools:   2 pools, 256 pgs
    objects: 150.70k objects, 589 MiB
    usage:   1.9 GiB used, 98 GiB / 100 GiB avail
    pgs:     256 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:27:54,377292974-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:27:54,381796680-07:00] INFO: > The cluster is idle now.[0m


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:27:54,385129255-07:00] STAGE: Start benchmarking of mode seq ...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:27:54,390615718-07:00] INFO: > Start system activity collection services[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:27:54,395047669-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:329 - rados_bench() > rm --force /mnt/sda8/ceph-research/scripts/output/sys_activity_4KB_seq.dat.rados_bench_host.1
# ./benchmarks/bench-rados:330 - rados_bench() > sar_pid_rados_bench_host=741534
# ./benchmarks/bench-rados:330 - rados_bench() > S_TIME_FORMAT=ISO
# ./benchmarks/bench-rados:330 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/output/sys_activity_4KB_seq.dat.rados_bench_host.1 2

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:27:54,401255298-07:00] INFO: >> for OSD host[0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:27:54,409120463-07:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.5 bash
10.10.2.5: b'34778\n'
[1] 21:27:55 [SUCCESS] ljishen@10.10.2.5
34778

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:27:55,539116519-07:00] INFO: > Run rados bench[0m
# ./benchmarks/bench-rados:367 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/output/rados_bench_4KB_seq.log.1
## ./benchmarks/bench-rados:364 - rados_bench() > count_cpus 32-63,96-127
# ./benchmarks/bench-rados:364 - rados_bench() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 128 --show-time
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:27:55,560042785-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:27:55,563208256-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '699915d6-36dd-11ec-b561-09a495c6360a', '--', 'rados', 'bench', '99999999', 'seq', '--pool', 'bench_rados', '--concurrent-ios', '128', '--show-time'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 699915d6-36dd-11ec-b561-09a495c6360a -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 128 --show-time
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
hints = 1
2021-10-27T04:27:57.919260+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-27T04:27:57.919260+0000     0       0         0         0         0         0           -           0
2021-10-27T04:27:58.919347+0000     1     128     17522     17394   67.9352   67.9453  0.00725002  0.00731901
2021-10-27T04:27:59.919444+0000     2     128     34236     34108    66.609   65.2891  0.00690166  0.00749282
2021-10-27T04:28:00.919538+0000     3     127     50599     50472   65.7113   63.9219  0.00571105  0.00759518
2021-10-27T04:28:01.919644+0000     4     128     69094     68966   67.3421   72.2422  0.00732886  0.00741306
2021-10-27T04:28:02.919773+0000     5     127     84952     84825   66.2619   61.9492  0.00812982  0.00753532
2021-10-27T04:28:03.919850+0000     6     127    100187    100060   65.1362   59.5117  0.00845645  0.00766778
2021-10-27T04:28:04.919935+0000     7     127    117158    117031   65.3006    66.293  0.00804696  0.00764983
2021-10-27T04:28:05.920042+0000     8     128    133375    133247   65.0551   63.3438   0.0139089  0.00767843
2021-10-27T04:28:06.920526+0000     9     127    150017    149890   65.0468   65.0117  0.00637776  0.00768071
2021-10-27T04:28:07.920686+0000 Total time run:       9.04143
Total reads made:     150695
Read size:            4096
Object size:          4096
Bandwidth (MB/sec):   65.1061
Average IOPS:         16667
Stddev IOPS:          932.667
Max IOPS:             18494
Min IOPS:             15235
Average Latency(s):   0.00767434
Max latency(s):       0.0418361
Min latency(s):       0.00100215

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:28:08,491017481-07:00] INFO: > Stop system activity collection services[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:28:08,496393898-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:379 - rados_bench() > kill -INT 741534

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:28:08,502253743-07:00] INFO: >> for OSD host[0m
# ./benchmarks/bench-rados:384 - rados_bench() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.5 kill -INT 34778
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:28:08,510480948-07:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.5 kill -INT 34778
[1] 21:28:09 [SUCCESS] ljishen@10.10.2.5

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:28:09,603086479-07:00] INFO: >>> Copy system activity file from OSD host[0m
## ./benchmarks/bench-rados:391 - rados_bench() > basename -- /tmp/bench-rados/sys_activity_4KB_seq.dat.osd_host.1
# ./benchmarks/bench-rados:391 - rados_bench() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.5:/tmp/bench-rados/sys_activity_4KB_seq.dat.osd_host.1 /mnt/sda8/ceph-research/scripts/output/sys_activity_4KB_seq.dat.osd_host.1


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:28:10,645443404-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:28:33,775281968-07:00] INFO: >>   data:
    pools:   2 pools, 256 pgs
    objects: 150.70k objects, 589 MiB
    usage:   1.9 GiB used, 98 GiB / 100 GiB avail
    pgs:     256 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:28:33,782963728-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:28:41,864515619-07:00] INFO: >>   data:
    pools:   2 pools, 256 pgs
    objects: 150.70k objects, 589 MiB
    usage:   1.9 GiB used, 98 GiB / 100 GiB avail
    pgs:     256 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:28:41,872772270-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:28:49,961637499-07:00] INFO: >>   data:
    pools:   2 pools, 256 pgs
    objects: 150.70k objects, 589 MiB
    usage:   1.9 GiB used, 98 GiB / 100 GiB avail
    pgs:     256 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:28:49,968610577-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:28:58,135446101-07:00] INFO: >>   data:
    pools:   2 pools, 256 pgs
    objects: 150.70k objects, 589 MiB
    usage:   1.9 GiB used, 98 GiB / 100 GiB avail
    pgs:     256 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:28:58,143830041-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:29:06,247830429-07:00] INFO: >>   data:
    pools:   2 pools, 256 pgs
    objects: 150.70k objects, 589 MiB
    usage:   1.9 GiB used, 98 GiB / 100 GiB avail
    pgs:     256 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:29:06,255550571-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:29:06,261521004-07:00] INFO: > The cluster is idle now.[0m


[1;7;39;49m[2021-10-26T21:29:06,263559065-07:00][RUNNING][ROUND 2/1/21] object_size=4KB[0m


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:29:06,266982060-07:00] STAGE: Launch a Ceph cluster (CEPH_HOST: 10.10.2.3, OSD_HOST: 10.10.2.5) ...[0m
# ./benchmarks/bench-rados:172 - launch_ceph_cluster() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.3 bash -euo pipefail
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:29:06,275083258-07:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.3 bash -euo pipefail
10.10.2.3: b'\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T00:29:06,696673818-04:00] INFO: > Remove the existing cluster\x1b[0m\n'
10.10.2.3: b'## bash:13 -  > ls /var/lib/ceph\n'
10.10.2.3: b'## bash:13 -  > tail -n1\n'
10.10.2.3: b'# bash:13 -  > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm --host 10.10.2.3 --host 10.10.2.5 rm-cluster --force --fsid 699915d6-36dd-11ec-b561-09a495c6360a\n'
10.10.2.3: b'\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T00:29:14,378381852-04:00] INFO: > Deploy a new cluster\x1b[0m\n'
10.10.2.3: b'## bash:22 -  > /tmp/bench-rados/ceph-research/scripts/parallel-ssh --host 10.10.2.5 uname --nodename\n'
10.10.2.3: b'## bash:22 -  > tail -n1\n'
10.10.2.3: b'# bash:22 -  > osd_hostname=sm1\n# bash:23 -  > /tmp/bench-rados/ceph-research/scripts/ceph-deploy -m 10.10.2.3 -o sm1/10.10.2.5:/dev/nvme0n1\n'
10.10.2.3: b'\n\n\x1b[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T00:29:15,507950390-04:00] STAGE: Verify passwordless SSH to hosts, passwordless sudo via ljishen, and the existence of devices on hosts...\x1b[0m\n'
10.10.2.3: b'\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T00:29:15,511607982-04:00] INFO: > Check the host for the monitor\x1b[0m\n# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:181 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@10.10.2.3 sudo --non-interactive --validate\n'
10.10.2.3: b'\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T00:29:15,664376865-04:00] INFO: > Check hosts for OSDs\x1b[0m\n'
10.10.2.3: b"\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T00:29:15,668094431-04:00] INFO: >> Check host 'sm1'\x1b[0m\n# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:200 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 sudo --non-interactive --validate\n"
10.10.2.3: b"\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T00:29:16,711242595-04:00] INFO: >>> Check device '/dev/nvme0n1'\x1b[0m\n"
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:215 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 test -b /dev/nvme0n1\n'
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:226 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 lsblk --nodeps --noheadings --output TYPE /dev/nvme0n1\n'
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:227 - verify_parameters() > grep --word-regexp disk\n'
10.10.2.3: b'\n\n\x1b[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T00:29:18,849027869-04:00] STAGE: Reset devices on hosts...\x1b[0m\n'
10.10.2.3: b"\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T00:29:18,853059948-04:00] INFO: > Resetting device '/dev/nvme0n1' on host 'sm1'\x1b[0m\n"
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:288 - reset_host_devices() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 sudo sh\n'
10.10.2.3: b'  Removing ceph--35385b5d--db1d--439d--9a8c--6abb347814f8-osd--block--0d7cdbd9--bd8a--471e--ac4c--9d059bbdbcc5 (252:0)\n'
10.10.2.3: b'  Archiving volume group "ceph-35385b5d-db1d-439d-9a8c-6abb347814f8" metadata (seqno 5).\n'
10.10.2.3: b'  Releasing logical volume "osd-block-0d7cdbd9-bd8a-471e-ac4c-9d059bbdbcc5"\n'
10.10.2.3: b'  Creating volume group backup "/etc/lvm/backup/ceph-35385b5d-db1d-439d-9a8c-6abb347814f8" (seqno 6).\n'
10.10.2.3: b'  Logical volume "osd-block-0d7cdbd9-bd8a-471e-ac4c-9d059bbdbcc5" successfully removed\n  Removing physical volume "/dev/nvme0n1" from volume group "ceph-35385b5d-db1d-439d-9a8c-6abb347814f8"\n'
10.10.2.3: b'  Volume group "ceph-35385b5d-db1d-439d-9a8c-6abb347814f8" successfully removed\n'
10.10.2.3: b'\n\n\x1b[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T00:29:21,160938223-04:00] STAGE: Bootstrap a new cluster...\x1b[0m\n'
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:312 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.3 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults\n'
10.10.2.3: b'\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T00:29:21,169505177-04:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.3: b"\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T00:29:21,172251543-04:00] DEBUG: CEPHADM_PARAMS: ['bootstrap', '--config', '/tmp/bench-rados/ceph-research/scripts/ceph.conf', '--mon-ip', '10.10.2.3', '--output-dir', '/tmp/bench-rados/deployment_data_root/etc/ceph', '--ssh-user', 'ljishen', '--skip-dashboard', '--no-minimize-config', '--allow-overwrite', '--allow-fqdn-hostname', '--orphan-initial-daemons', '--skip-monitoring-stack', '--single-host-defaults']\x1b[0m\n"
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.3 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults\n'
10.10.2.3: b'Verifying podman|docker is present...\nVerifying lvm2 is present...\nVerifying time synchronization is in place...\n'
10.10.2.3: b'Unit ntp.service is enabled and running\nRepeating the final host check...\npodman|docker (/usr/bin/docker) is present\nsystemctl is present\n'
10.10.2.3: b'lvcreate is present\n'
10.10.2.3: b'Unit ntp.service is enabled and running\nHost looks OK\n'
10.10.2.3: b'Cluster fsid: 74eef5da-36de-11ec-b561-09a495c6360a\n'
10.10.2.3: b'Verifying IP 10.10.2.3 port 3300 ...\nVerifying IP 10.10.2.3 port 6789 ...\n'
10.10.2.3: b'Mon IP `10.10.2.3` is in CIDR network `10.10.2.0/24`\n- internal network (--cluster-network) has not been provided, OSD replication will default to the public_network\n'
10.10.2.3: b'Adjusting default settings to suit single-host cluster...\n'
10.10.2.3: b'Pulling container image quay.io/ceph/ceph:v16...\n'
10.10.2.3: b'Ceph version: ceph version 16.2.6 (ee28fb57e47e9f88813e24bbf4c14496ca299d31) pacific (stable)\nExtracting ceph user uid/gid from container image...\n'
10.10.2.3: b'Creating initial keys...\n'
10.10.2.3: b'Creating initial monmap...\n'
10.10.2.3: b'Creating mon...\n'
10.10.2.3: b'Waiting for mon to start...\nWaiting for mon...\n'
10.10.2.3: b'mon is available\nSetting mon public_network to 10.10.2.0/24\n'
10.10.2.3: b'Wrote config to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf\n'
10.10.2.3: b'Wrote keyring to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring\nCreating mgr...\n'
10.10.2.3: b'Verifying port 9283 ...\n'
10.10.2.3: b'Waiting for mgr to start...\nWaiting for mgr...\n'
10.10.2.3: b'mgr not available, waiting (1/15)...\n'
10.10.2.3: b'mgr not available, waiting (2/15)...\n'
10.10.2.3: b'mgr is available\n'
10.10.2.3: b'Enabling cephadm module...\n'
10.10.2.3: b'Waiting for the mgr to restart...\nWaiting for mgr epoch 5...\n'
10.10.2.3: b'mgr epoch 5 is available\nSetting orchestrator backend to cephadm...\n'
10.10.2.3: b'Generating ssh key...\n'
10.10.2.3: b'Wrote public SSH key to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub\nAdding key to ljishen@localhost authorized_keys...\n'
10.10.2.3: b'Adding host node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us...\n'
10.10.2.3: b'Deploying unmanaged mon service...\n'
10.10.2.3: b'Deploying unmanaged mgr service...\n'
10.10.2.3: b'You can access the Ceph CLI with:\n\n\tsudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --fsid 74eef5da-36de-11ec-b561-09a495c6360a -c /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf -k /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring\n\nPlease consider enabling telemetry to help improve Ceph:\n\n\tceph telemetry on\n\nFor more information see:\n\n\thttps://docs.ceph.com/docs/pacific/mgr/telemetry/\n\nBootstrap complete.\n'
10.10.2.3: b'\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T00:30:26,056159703-04:00] INFO: > Set the default number of monitor daemons to 1\x1b[0m\n'
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:330 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1\n'
10.10.2.3: b'\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T00:30:26,065372143-04:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.3: b"\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T00:30:26,068297656-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mon', '1']\x1b[0m\n# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1\n"
10.10.2.3: b'Inferring fsid 74eef5da-36de-11ec-b561-09a495c6360a\n'
10.10.2.3: b'Inferring config /var/lib/ceph/74eef5da-36de-11ec-b561-09a495c6360a/mon.node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.3: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.3: b'Scheduled mon update...\n'
10.10.2.3: b'\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T00:30:35,561913908-04:00] INFO: > Set the default number of manager daemons to 1\x1b[0m\n# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:335 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1\n'
10.10.2.3: b'\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T00:30:35,571211759-04:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.3: b"\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T00:30:35,574278729-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mgr', '1']\x1b[0m\n# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1\n"
10.10.2.3: b'Inferring fsid 74eef5da-36de-11ec-b561-09a495c6360a\n'
10.10.2.3: b'Inferring config /var/lib/ceph/74eef5da-36de-11ec-b561-09a495c6360a/mon.node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.3: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.3: b'Scheduled mgr update...\n'
10.10.2.3: b'\n\n\x1b[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T00:30:45,210866376-04:00] STAGE: Add hosts to the cluster...\x1b[0m\n'
10.10.2.3: b"\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T00:30:45,215817366-04:00] INFO: > Adding host 'sm1'\x1b[0m\n# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:362 - add_hosts() > ssh-copy-id -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -f -i /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub ljishen@sm1\n"
10.10.2.3: b'/usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: "/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub"\n'
10.10.2.3: b'\nNumber of key(s) added: 1\n\nNow try logging into the machine, with:   "ssh -o \'GlobalKnownHostsFile=/dev/null\' -o \'LogLevel=ERROR\' -o \'PasswordAuthentication=no\' -o \'StrictHostKeyChecking=no\' -o \'UserKnownHostsFile=/dev/null\' \'ljishen@sm1\'"\nand check to make sure that only the key(s) you wanted were added.\n\n'
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:368 - add_hosts() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add sm1 10.10.2.5\n'
10.10.2.3: b'\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T00:30:46,356402403-04:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.3: b"\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T00:30:46,359399862-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'host', 'add', 'sm1', '10.10.2.5']\x1b[0m\n"
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add sm1 10.10.2.5\n'
10.10.2.3: b'Inferring fsid 74eef5da-36de-11ec-b561-09a495c6360a\n'
10.10.2.3: b'Inferring config /var/lib/ceph/74eef5da-36de-11ec-b561-09a495c6360a/mon.node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.3: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.3: b"Added host 'sm1' with addr '10.10.2.5'\n"
10.10.2.3: b'\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T00:30:57,512471251-04:00] INFO: Sleep for 20 seconds\x1b[0m\n'
10.10.2.3: b'\n\n\x1b[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T00:31:17,516207038-04:00] STAGE: Add OSDs to the cluster...\x1b[0m\n'
10.10.2.3: b"\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T00:31:17,521140995-04:00] INFO: > Adding devices ['/dev/nvme0n1'] on host 'sm1' as OSDs\x1b[0m\n"
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:387 - add_host_devices_as_osds() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd sm1:/dev/nvme0n1\n'
10.10.2.3: b'\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T00:31:17,529429034-04:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.3: b"\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T00:31:17,532220253-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'daemon', 'add', 'osd', 'sm1:/dev/nvme0n1']\x1b[0m\n"
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd sm1:/dev/nvme0n1\n'
10.10.2.3: b'Inferring fsid 74eef5da-36de-11ec-b561-09a495c6360a\n'
10.10.2.3: b'Inferring config /var/lib/ceph/74eef5da-36de-11ec-b561-09a495c6360a/mon.node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.3: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.3: b"Created osd(s) 0 on host 'sm1'\n"
10.10.2.3: b'\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T00:31:40,880359882-04:00] INFO: > Sleep for 20 seconds\x1b[0m\n'
10.10.2.3: b'\n\n\x1b[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T00:32:00,885028654-04:00] STAGE: Show Cluster Status\x1b[0m\n# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:343 - show_status() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status\n'
10.10.2.3: b'\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T00:32:00,893399578-04:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.3: b"\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T00:32:00,896155621-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', '--status']\x1b[0m\n"
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status\n'
10.10.2.3: b'Inferring fsid 74eef5da-36de-11ec-b561-09a495c6360a\n'
10.10.2.3: b'Inferring config /var/lib/ceph/74eef5da-36de-11ec-b561-09a495c6360a/mon.node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.3: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.3: b'  cluster:\n    id:     74eef5da-36de-11ec-b561-09a495c6360a\n    health: HEALTH_WARN\n            1 pool(s) have no replicas configured\n \n  services:\n    mon: 1 daemons, quorum node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us (age 2m)\n    mgr: node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us.panmtg(active, since 2m)\n    osd: 1 osds: 1 up (since 20s), 1 in (since 39s)\n \n  data:\n    pools:   1 pools, 128 pgs\n    objects: 0 objects, 0 B\n    usage:   4.7 MiB used, 100 GiB / 100 GiB avail\n    pgs:     99.219% pgs unknown\n             127 unknown\n             1   active+clean\n \n  progress:\n \n'
[1] 21:32:09 [SUCCESS] ljishen@10.10.2.3

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T00:29:06,696673818-04:00] INFO: > Remove the existing cluster[0m
## bash:13 -  > ls /var/lib/ceph
## bash:13 -  > tail -n1
# bash:13 -  > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm --host 10.10.2.3 --host 10.10.2.5 rm-cluster --force --fsid 699915d6-36dd-11ec-b561-09a495c6360a

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T00:29:14,378381852-04:00] INFO: > Deploy a new cluster[0m
## bash:22 -  > /tmp/bench-rados/ceph-research/scripts/parallel-ssh --host 10.10.2.5 uname --nodename
## bash:22 -  > tail -n1
# bash:22 -  > osd_hostname=sm1
# bash:23 -  > /tmp/bench-rados/ceph-research/scripts/ceph-deploy -m 10.10.2.3 -o sm1/10.10.2.5:/dev/nvme0n1


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T00:29:15,507950390-04:00] STAGE: Verify passwordless SSH to hosts, passwordless sudo via ljishen, and the existence of devices on hosts...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T00:29:15,511607982-04:00] INFO: > Check the host for the monitor[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:181 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@10.10.2.3 sudo --non-interactive --validate

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T00:29:15,664376865-04:00] INFO: > Check hosts for OSDs[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T00:29:15,668094431-04:00] INFO: >> Check host 'sm1'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:200 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 sudo --non-interactive --validate

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T00:29:16,711242595-04:00] INFO: >>> Check device '/dev/nvme0n1'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:215 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 test -b /dev/nvme0n1
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:226 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 lsblk --nodeps --noheadings --output TYPE /dev/nvme0n1
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:227 - verify_parameters() > grep --word-regexp disk


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T00:29:18,849027869-04:00] STAGE: Reset devices on hosts...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T00:29:18,853059948-04:00] INFO: > Resetting device '/dev/nvme0n1' on host 'sm1'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:288 - reset_host_devices() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 sudo sh
  Removing ceph--35385b5d--db1d--439d--9a8c--6abb347814f8-osd--block--0d7cdbd9--bd8a--471e--ac4c--9d059bbdbcc5 (252:0)
  Archiving volume group "ceph-35385b5d-db1d-439d-9a8c-6abb347814f8" metadata (seqno 5).
  Releasing logical volume "osd-block-0d7cdbd9-bd8a-471e-ac4c-9d059bbdbcc5"
  Creating volume group backup "/etc/lvm/backup/ceph-35385b5d-db1d-439d-9a8c-6abb347814f8" (seqno 6).
  Logical volume "osd-block-0d7cdbd9-bd8a-471e-ac4c-9d059bbdbcc5" successfully removed
  Removing physical volume "/dev/nvme0n1" from volume group "ceph-35385b5d-db1d-439d-9a8c-6abb347814f8"
  Volume group "ceph-35385b5d-db1d-439d-9a8c-6abb347814f8" successfully removed


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T00:29:21,160938223-04:00] STAGE: Bootstrap a new cluster...[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:312 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.3 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T00:29:21,169505177-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T00:29:21,172251543-04:00] DEBUG: CEPHADM_PARAMS: ['bootstrap', '--config', '/tmp/bench-rados/ceph-research/scripts/ceph.conf', '--mon-ip', '10.10.2.3', '--output-dir', '/tmp/bench-rados/deployment_data_root/etc/ceph', '--ssh-user', 'ljishen', '--skip-dashboard', '--no-minimize-config', '--allow-overwrite', '--allow-fqdn-hostname', '--orphan-initial-daemons', '--skip-monitoring-stack', '--single-host-defaults'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.3 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults
Verifying podman|docker is present...
Verifying lvm2 is present...
Verifying time synchronization is in place...
Unit ntp.service is enabled and running
Repeating the final host check...
podman|docker (/usr/bin/docker) is present
systemctl is present
lvcreate is present
Unit ntp.service is enabled and running
Host looks OK
Cluster fsid: 74eef5da-36de-11ec-b561-09a495c6360a
Verifying IP 10.10.2.3 port 3300 ...
Verifying IP 10.10.2.3 port 6789 ...
Mon IP `10.10.2.3` is in CIDR network `10.10.2.0/24`
- internal network (--cluster-network) has not been provided, OSD replication will default to the public_network
Adjusting default settings to suit single-host cluster...
Pulling container image quay.io/ceph/ceph:v16...
Ceph version: ceph version 16.2.6 (ee28fb57e47e9f88813e24bbf4c14496ca299d31) pacific (stable)
Extracting ceph user uid/gid from container image...
Creating initial keys...
Creating initial monmap...
Creating mon...
Waiting for mon to start...
Waiting for mon...
mon is available
Setting mon public_network to 10.10.2.0/24
Wrote config to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf
Wrote keyring to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring
Creating mgr...
Verifying port 9283 ...
Waiting for mgr to start...
Waiting for mgr...
mgr not available, waiting (1/15)...
mgr not available, waiting (2/15)...
mgr is available
Enabling cephadm module...
Waiting for the mgr to restart...
Waiting for mgr epoch 5...
mgr epoch 5 is available
Setting orchestrator backend to cephadm...
Generating ssh key...
Wrote public SSH key to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub
Adding key to ljishen@localhost authorized_keys...
Adding host node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us...
Deploying unmanaged mon service...
Deploying unmanaged mgr service...
You can access the Ceph CLI with:

	sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --fsid 74eef5da-36de-11ec-b561-09a495c6360a -c /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf -k /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring

Please consider enabling telemetry to help improve Ceph:

	ceph telemetry on

For more information see:

	https://docs.ceph.com/docs/pacific/mgr/telemetry/

Bootstrap complete.

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T00:30:26,056159703-04:00] INFO: > Set the default number of monitor daemons to 1[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:330 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T00:30:26,065372143-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T00:30:26,068297656-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mon', '1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1
Inferring fsid 74eef5da-36de-11ec-b561-09a495c6360a
Inferring config /var/lib/ceph/74eef5da-36de-11ec-b561-09a495c6360a/mon.node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Scheduled mon update...

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T00:30:35,561913908-04:00] INFO: > Set the default number of manager daemons to 1[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:335 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T00:30:35,571211759-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T00:30:35,574278729-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mgr', '1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1
Inferring fsid 74eef5da-36de-11ec-b561-09a495c6360a
Inferring config /var/lib/ceph/74eef5da-36de-11ec-b561-09a495c6360a/mon.node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Scheduled mgr update...


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T00:30:45,210866376-04:00] STAGE: Add hosts to the cluster...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T00:30:45,215817366-04:00] INFO: > Adding host 'sm1'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:362 - add_hosts() > ssh-copy-id -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -f -i /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub ljishen@sm1
/usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: "/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub"

Number of key(s) added: 1

Now try logging into the machine, with:   "ssh -o 'GlobalKnownHostsFile=/dev/null' -o 'LogLevel=ERROR' -o 'PasswordAuthentication=no' -o 'StrictHostKeyChecking=no' -o 'UserKnownHostsFile=/dev/null' 'ljishen@sm1'"
and check to make sure that only the key(s) you wanted were added.

# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:368 - add_hosts() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add sm1 10.10.2.5
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T00:30:46,356402403-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T00:30:46,359399862-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'host', 'add', 'sm1', '10.10.2.5'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add sm1 10.10.2.5
Inferring fsid 74eef5da-36de-11ec-b561-09a495c6360a
Inferring config /var/lib/ceph/74eef5da-36de-11ec-b561-09a495c6360a/mon.node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Added host 'sm1' with addr '10.10.2.5'

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T00:30:57,512471251-04:00] INFO: Sleep for 20 seconds[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T00:31:17,516207038-04:00] STAGE: Add OSDs to the cluster...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T00:31:17,521140995-04:00] INFO: > Adding devices ['/dev/nvme0n1'] on host 'sm1' as OSDs[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:387 - add_host_devices_as_osds() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd sm1:/dev/nvme0n1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T00:31:17,529429034-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T00:31:17,532220253-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'daemon', 'add', 'osd', 'sm1:/dev/nvme0n1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd sm1:/dev/nvme0n1
Inferring fsid 74eef5da-36de-11ec-b561-09a495c6360a
Inferring config /var/lib/ceph/74eef5da-36de-11ec-b561-09a495c6360a/mon.node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Created osd(s) 0 on host 'sm1'

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T00:31:40,880359882-04:00] INFO: > Sleep for 20 seconds[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T00:32:00,885028654-04:00] STAGE: Show Cluster Status[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:343 - show_status() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T00:32:00,893399578-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T00:32:00,896155621-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', '--status'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status
Inferring fsid 74eef5da-36de-11ec-b561-09a495c6360a
Inferring config /var/lib/ceph/74eef5da-36de-11ec-b561-09a495c6360a/mon.node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
  cluster:
    id:     74eef5da-36de-11ec-b561-09a495c6360a
    health: HEALTH_WARN
            1 pool(s) have no replicas configured
 
  services:
    mon: 1 daemons, quorum node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us (age 2m)
    mgr: node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us.panmtg(active, since 2m)
    osd: 1 osds: 1 up (since 20s), 1 in (since 39s)
 
  data:
    pools:   1 pools, 128 pgs
    objects: 0 objects, 0 B
    usage:   4.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     99.219% pgs unknown
             127 unknown
             1   active+clean
 
  progress:
 

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:32:09,926548809-07:00] INFO: > Copy configuration files to local[0m
# ./benchmarks/bench-rados:208 - launch_ceph_cluster() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.3:/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf
# ./benchmarks/bench-rados:212 - launch_ceph_cluster() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.3 sudo chmod 644 /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:32:10,082878362-07:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.3 sudo chmod 644 /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring
[1] 21:32:10 [SUCCESS] ljishen@10.10.2.3
# ./benchmarks/bench-rados:214 - launch_ceph_cluster() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.3:/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:32:10,409667899-07:00] STAGE: Update local ceph.conf as a client...[0m


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:32:10,413251305-07:00] STAGE: Configure Ceph pool...[0m
# ./benchmarks/bench-rados:245 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_max_backfills 32
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:32:10,437959320-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:32:10,441476471-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '74eef5da-36de-11ec-b561-09a495c6360a', '--', 'ceph', 'config', 'set', 'osd', 'osd_max_backfills', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 74eef5da-36de-11ec-b561-09a495c6360a -- ceph config set osd osd_max_backfills 32
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:246 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_active 32
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:32:13,582064694-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:32:13,585919389-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '74eef5da-36de-11ec-b561-09a495c6360a', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_active', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 74eef5da-36de-11ec-b561-09a495c6360a -- ceph config set osd osd_recovery_max_active 32
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:247 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_single_start 8
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:32:16,897003102-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:32:16,900516657-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '74eef5da-36de-11ec-b561-09a495c6360a', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_single_start', '8'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 74eef5da-36de-11ec-b561-09a495c6360a -- ceph config set osd osd_recovery_max_single_start 8
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:248 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_op_priority 63
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:32:20,071268850-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:32:20,075003750-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '74eef5da-36de-11ec-b561-09a495c6360a', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_op_priority', '63'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 74eef5da-36de-11ec-b561-09a495c6360a -- ceph config set osd osd_recovery_op_priority 63
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:250 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config show-with-defaults osd.0
# ./benchmarks/bench-rados:251 - configure_ceph_pool() > grep 'osd_max_backfills\|osd_recovery'
# ./benchmarks/bench-rados:251 - configure_ceph_pool() > column -t -s ' '
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
osd_max_backfills                        32         mon
osd_recovery_cost                        20971520   default
osd_recovery_delay_start                 0.000000   default
osd_recovery_max_active                  32         mon
osd_recovery_max_active_hdd              3          default
osd_recovery_max_active_ssd              10         default
osd_recovery_max_chunk                   8388608    default
osd_recovery_max_omap_entries_per_chunk  8096       default
osd_recovery_max_single_start            8          mon
osd_recovery_op_priority                 63         mon
osd_recovery_op_warn_multiple            16         default
osd_recovery_priority                    5          default
osd_recovery_retry_interval              30.000000  default
osd_recovery_sleep                       0.000000   default
osd_recovery_sleep_hdd                   0.100000   default
osd_recovery_sleep_hybrid                0.025000   default
osd_recovery_sleep_ssd                   0.000000   default
# ./benchmarks/bench-rados:253 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool create bench_rados 128 128 replicated --size 1 --pg-num-min 128
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:32:26,287623593-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:32:26,290913447-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '74eef5da-36de-11ec-b561-09a495c6360a', '--', 'ceph', 'osd', 'pool', 'create', 'bench_rados', '128', '128', 'replicated', '--size', '1', '--pg-num-min', '128'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 74eef5da-36de-11ec-b561-09a495c6360a -- ceph osd pool create bench_rados 128 128 replicated --size 1 --pg-num-min 128
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
pool 'bench_rados' created
# ./benchmarks/bench-rados:255 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados min_size 1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:32:30,142125754-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:32:30,145560280-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '74eef5da-36de-11ec-b561-09a495c6360a', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'min_size', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 74eef5da-36de-11ec-b561-09a495c6360a -- ceph osd pool set bench_rados min_size 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 min_size to 1
# ./benchmarks/bench-rados:256 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados pg_autoscale_mode off
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:32:34,632756345-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:32:34,636494061-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '74eef5da-36de-11ec-b561-09a495c6360a', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'pg_autoscale_mode', 'off'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 74eef5da-36de-11ec-b561-09a495c6360a -- ceph osd pool set bench_rados pg_autoscale_mode off
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 pg_autoscale_mode to off
# ./benchmarks/bench-rados:257 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool application enable bench_rados benchmark
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:32:38,966299529-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:32:38,969952716-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '74eef5da-36de-11ec-b561-09a495c6360a', '--', 'ceph', 'osd', 'pool', 'application', 'enable', 'bench_rados', 'benchmark'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 74eef5da-36de-11ec-b561-09a495c6360a -- ceph osd pool application enable bench_rados benchmark
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
enabled application 'benchmark' on pool 'bench_rados'
# ./benchmarks/bench-rados:258 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados noscrub 1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:32:42,906406356-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:32:42,909800346-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '74eef5da-36de-11ec-b561-09a495c6360a', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'noscrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 74eef5da-36de-11ec-b561-09a495c6360a -- ceph osd pool set bench_rados noscrub 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 noscrub to 1
# ./benchmarks/bench-rados:259 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados nodeep-scrub 1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:32:47,005280227-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:32:47,008808449-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '74eef5da-36de-11ec-b561-09a495c6360a', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'nodeep-scrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 74eef5da-36de-11ec-b561-09a495c6360a -- ceph osd pool set bench_rados nodeep-scrub 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 nodeep-scrub to 1
# ./benchmarks/bench-rados:260 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool ls detail
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:32:51,369251217-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:32:51,372654915-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '74eef5da-36de-11ec-b561-09a495c6360a', '--', 'ceph', 'osd', 'pool', 'ls', 'detail'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 74eef5da-36de-11ec-b561-09a495c6360a -- ceph osd pool ls detail
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
pool 1 'device_health_metrics' replicated size 1 min_size 1 crush_rule 0 object_hash rjenkins pg_num 128 pgp_num 128 autoscale_mode on last_change 13 lfor 0/0/11 flags hashpspool stripe_width 0 pg_num_min 1 application mgr_devicehealth
pool 2 'bench_rados' replicated size 1 min_size 1 crush_rule 0 object_hash rjenkins pg_num 128 pgp_num 128 autoscale_mode off last_change 21 flags hashpspool,noscrub,nodeep-scrub stripe_width 0 pg_num_min 128 application benchmark

# ./benchmarks/bench-rados:261 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd df tree
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:32:54,646069820-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:32:54,649839836-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '74eef5da-36de-11ec-b561-09a495c6360a', '--', 'ceph', 'osd', 'df', 'tree'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 74eef5da-36de-11ec-b561-09a495c6360a -- ceph osd df tree
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
ID  CLASS  WEIGHT   REWEIGHT  SIZE     RAW USE  DATA     OMAP  META     AVAIL    %USE  VAR   PGS  STATUS  TYPE NAME    
-1         0.09769         -  100 GiB  6.0 MiB  176 KiB   0 B  5.8 MiB  100 GiB  0.01  1.00    -          root default 
-3         0.09769         -  100 GiB  6.0 MiB  176 KiB   0 B  5.8 MiB  100 GiB  0.01  1.00    -              host sm1 
 0    ssd  0.09769   1.00000  100 GiB  6.0 MiB  176 KiB   0 B  5.8 MiB  100 GiB  0.01  1.00  256      up          osd.0
                       TOTAL  100 GiB  6.0 MiB  176 KiB   0 B  5.8 MiB  100 GiB  0.01                                  
MIN/MAX VAR: 1.00/1.00  STDDEV: 0


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:32:57,870812401-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:33:20,970146839-07:00] INFO: >>   data:
    pools:   2 pools, 256 pgs
    objects: 0 objects, 0 B
    usage:   6.0 MiB used, 100 GiB / 100 GiB avail
    pgs:     256 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:33:20,977612163-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:33:29,094430284-07:00] INFO: >>   data:
    pools:   2 pools, 256 pgs
    objects: 0 objects, 0 B
    usage:   6.0 MiB used, 100 GiB / 100 GiB avail
    pgs:     256 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:33:29,102612855-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:33:37,293393990-07:00] INFO: >>   data:
    pools:   2 pools, 256 pgs
    objects: 0 objects, 0 B
    usage:   6.0 MiB used, 100 GiB / 100 GiB avail
    pgs:     256 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:33:37,300795313-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:33:45,450489504-07:00] INFO: >>   data:
    pools:   2 pools, 256 pgs
    objects: 0 objects, 0 B
    usage:   6.0 MiB used, 100 GiB / 100 GiB avail
    pgs:     256 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:33:45,457822307-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:33:53,691910508-07:00] INFO: >>   data:
    pools:   2 pools, 256 pgs
    objects: 0 objects, 0 B
    usage:   6.0 MiB used, 100 GiB / 100 GiB avail
    pgs:     256 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:33:53,698960320-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:33:53,704906628-07:00] INFO: > The cluster is idle now.[0m


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:33:53,707982209-07:00] STAGE: Start benchmarking of mode write ...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:33:53,714040568-07:00] INFO: > Start system activity collection services[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:33:53,718704385-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:329 - rados_bench() > rm --force /mnt/sda8/ceph-research/scripts/output/sys_activity_4KB_write.dat.rados_bench_host.2
# ./benchmarks/bench-rados:330 - rados_bench() > sar_pid_rados_bench_host=748994
# ./benchmarks/bench-rados:330 - rados_bench() > S_TIME_FORMAT=ISO
# ./benchmarks/bench-rados:330 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/output/sys_activity_4KB_write.dat.rados_bench_host.2 2

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:33:53,724843114-07:00] INFO: >> for OSD host[0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:33:53,733245278-07:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.5 bash
10.10.2.5: b'39510\n'
[1] 21:33:54 [SUCCESS] ljishen@10.10.2.5
39510

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:33:54,808721358-07:00] INFO: > Run rados bench[0m
# ./benchmarks/bench-rados:358 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/output/rados_bench_4KB_write.log.2
## ./benchmarks/bench-rados:349 - rados_bench() > count_cpus 32-63,96-127
# ./benchmarks/bench-rados:349 - rados_bench() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 30 write --pool bench_rados -b 4096 -O 4096 --concurrent-ios 128 --show-time --write-object --write-omap --write-xattr --no-cleanup
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:33:54,828759805-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:33:54,831615493-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '74eef5da-36de-11ec-b561-09a495c6360a', '--', 'rados', 'bench', '30', 'write', '--pool', 'bench_rados', '-b', '4096', '-O', '4096', '--concurrent-ios', '128', '--show-time', '--write-object', '--write-omap', '--write-xattr', '--no-cleanup'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 74eef5da-36de-11ec-b561-09a495c6360a -- rados bench 30 write --pool bench_rados -b 4096 -O 4096 --concurrent-ios 128 --show-time --write-object --write-omap --write-xattr --no-cleanup
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
hints = 1
2021-10-27T04:33:57.389050+0000 Maintaining 128 concurrent writes of 4096 bytes to objects of size 4096 for up to 30 seconds or 0 objects
2021-10-27T04:33:57.389058+0000 Object prefix: benchmark_data_node-0.bluefield2.ucsc-cmps10_7
2021-10-27T04:33:57.389527+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-27T04:33:57.389527+0000     0       0         0         0         0         0           -           0
2021-10-27T04:33:58.389622+0000     1     128      6458      6330    24.724   24.7266   0.0215182   0.0200686
2021-10-27T04:33:59.389725+0000     2     128     12510     12382   24.1811   23.6406    0.016948    0.020546
2021-10-27T04:34:00.389793+0000     3     128     18540     18412   23.9718   23.5547   0.0201214   0.0207718
2021-10-27T04:34:01.389899+0000     4     128     24572     24444   23.8688   23.5625    0.015563   0.0208686
2021-10-27T04:34:02.389995+0000     5     128     30563     30435   23.7751   23.4023   0.0180023    0.020982
2021-10-27T04:34:03.390081+0000     6     128     36495     36367   23.6742   23.1719   0.0233553   0.0210863
2021-10-27T04:34:04.390195+0000     7     128     42291     42163   23.5262   22.6406   0.0213755    0.021228
2021-10-27T04:34:05.390310+0000     8     127     48149     48022   23.4459   22.8867   0.0216683   0.0212922
2021-10-27T04:34:06.390397+0000     9     128     53914     53786   23.3423   22.5156   0.0239283   0.0213895
2021-10-27T04:34:07.390476+0000    10     128     59683     59555   23.2614   22.5352   0.0198637   0.0214762
2021-10-27T04:34:08.390545+0000    11     128     65421     65293   23.1843   22.4141   0.0264427   0.0215409
2021-10-27T04:34:09.390650+0000    12     128     71288     71160   23.1619    22.918   0.0207787   0.0215615
2021-10-27T04:34:10.390722+0000    13     128     76921     76793   23.0727   22.0039   0.0251064   0.0216424
2021-10-27T04:34:11.390792+0000    14     128     82813     82685   23.0685   23.0156   0.0218356    0.021658
2021-10-27T04:34:12.390874+0000    15     128     88433     88305    22.994   21.9531   0.0211106   0.0217235
2021-10-27T04:34:13.390981+0000    16     127     94188     94061    22.962   22.4844   0.0209014   0.0217606
2021-10-27T04:34:14.391088+0000    17     128     99891     99763   22.9214   22.2734   0.0211718   0.0218005
2021-10-27T04:34:15.391156+0000    18     128    105540    105412   22.8738   22.0664   0.0260015   0.0218436
2021-10-27T04:34:16.391226+0000    19     127    109417    109290   22.4671   15.1484   0.0416812   0.0222394
2021-10-27T04:34:17.391342+0000 min lat: 0.00868608 max lat: 0.0583682 avg lat: 0.022728
2021-10-27T04:34:17.391342+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-27T04:34:17.391342+0000    20     127    112668    112541   21.9787   12.6992   0.0448111    0.022728
2021-10-27T04:34:18.391430+0000    21     128    115817    115689   21.5176   12.2969   0.0472064   0.0232137
2021-10-27T04:34:19.391531+0000    22     128    118951    118823   21.0959   12.2422   0.0491925   0.0236774
2021-10-27T04:34:20.391601+0000    23     128    121876    121748   20.6754   11.4258   0.0477323    0.024144
2021-10-27T04:34:21.391707+0000    24     128    124871    124743   20.3014   11.6992   0.0419897    0.024606
2021-10-27T04:34:22.391791+0000    25     128    127749    127621    19.939   11.2422   0.0447555   0.0250537
2021-10-27T04:34:23.391863+0000    26     128    130565    130437   19.5951        11   0.0504024   0.0254858
2021-10-27T04:34:24.391976+0000    27     128    133703    133575   19.3233   12.2578   0.0275319   0.0258646
2021-10-27T04:34:25.392090+0000    28     128    139341    139213   19.4197   22.0234   0.0248272   0.0257331
2021-10-27T04:34:26.392198+0000    29     128    145159    145031   19.5336   22.7266   0.0257274   0.0255854
2021-10-27T04:34:27.392298+0000    30      76    150847    150771   19.6298   22.4219   0.0229946   0.0254628
2021-10-27T04:34:28.392549+0000 Total time run:         30.009
Total writes made:      150847
Write size:             4096
Object size:            4096
Bandwidth (MB/sec):     19.6356
Stddev Bandwidth:       5.00773
Max bandwidth (MB/sec): 24.7266
Min bandwidth (MB/sec): 11
Average IOPS:           5026
Stddev IOPS:            1281.98
Max IOPS:               6330
Min IOPS:               2816
Average Latency(s):     0.0254587
Stddev Latency(s):      0.0115229
Max latency(s):         0.318697
Min latency(s):         0.00868608

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:34:28,907329621-07:00] INFO: > Stop system activity collection services[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:34:28,913135024-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:379 - rados_bench() > kill -INT 748994

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:34:28,918418336-07:00] INFO: >> for OSD host[0m
# ./benchmarks/bench-rados:384 - rados_bench() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.5 kill -INT 39510
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:34:28,926799360-07:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.5 kill -INT 39510
[1] 21:34:29 [SUCCESS] ljishen@10.10.2.5

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:34:30,006233963-07:00] INFO: >>> Copy system activity file from OSD host[0m
## ./benchmarks/bench-rados:391 - rados_bench() > basename -- /tmp/bench-rados/sys_activity_4KB_write.dat.osd_host.2
# ./benchmarks/bench-rados:391 - rados_bench() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.5:/tmp/bench-rados/sys_activity_4KB_write.dat.osd_host.2 /mnt/sda8/ceph-research/scripts/output/sys_activity_4KB_write.dat.osd_host.2


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:34:31,084414067-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:34:54,136057455-07:00] INFO: >>   data:
    pools:   2 pools, 256 pgs
    objects: 150.85k objects, 589 MiB
    usage:   1.9 GiB used, 98 GiB / 100 GiB avail
    pgs:     256 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:34:54,143650889-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:35:02,277664380-07:00] INFO: >>   data:
    pools:   2 pools, 256 pgs
    objects: 150.85k objects, 589 MiB
    usage:   1.9 GiB used, 98 GiB / 100 GiB avail
    pgs:     256 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:35:02,285643109-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:35:10,476245623-07:00] INFO: >>   data:
    pools:   2 pools, 256 pgs
    objects: 150.85k objects, 589 MiB
    usage:   1.9 GiB used, 98 GiB / 100 GiB avail
    pgs:     256 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:35:10,484332394-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:35:18,543374216-07:00] INFO: >>   data:
    pools:   2 pools, 256 pgs
    objects: 150.85k objects, 589 MiB
    usage:   1.9 GiB used, 98 GiB / 100 GiB avail
    pgs:     256 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:35:18,551811125-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:35:26,875623281-07:00] INFO: >>   data:
    pools:   2 pools, 256 pgs
    objects: 150.85k objects, 589 MiB
    usage:   1.9 GiB used, 98 GiB / 100 GiB avail
    pgs:     256 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:35:26,882737014-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:35:26,887893206-07:00] INFO: > The cluster is idle now.[0m


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:35:26,891005216-07:00] STAGE: Start benchmarking of mode seq ...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:35:26,897722393-07:00] INFO: > Start system activity collection services[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:35:26,902941393-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:329 - rados_bench() > rm --force /mnt/sda8/ceph-research/scripts/output/sys_activity_4KB_seq.dat.rados_bench_host.2
# ./benchmarks/bench-rados:330 - rados_bench() > sar_pid_rados_bench_host=750791
# ./benchmarks/bench-rados:330 - rados_bench() > S_TIME_FORMAT=ISO
# ./benchmarks/bench-rados:330 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/output/sys_activity_4KB_seq.dat.rados_bench_host.2 2

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:35:26,909322238-07:00] INFO: >> for OSD host[0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:35:26,917653458-07:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.5 bash
10.10.2.5: b'40049\n'
[1] 21:35:28 [SUCCESS] ljishen@10.10.2.5
40049

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:35:28,023809999-07:00] INFO: > Run rados bench[0m
# ./benchmarks/bench-rados:367 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/output/rados_bench_4KB_seq.log.2
## ./benchmarks/bench-rados:364 - rados_bench() > count_cpus 32-63,96-127
# ./benchmarks/bench-rados:364 - rados_bench() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 128 --show-time
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:35:28,044657308-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:35:28,047958924-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '74eef5da-36de-11ec-b561-09a495c6360a', '--', 'rados', 'bench', '99999999', 'seq', '--pool', 'bench_rados', '--concurrent-ios', '128', '--show-time'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 74eef5da-36de-11ec-b561-09a495c6360a -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 128 --show-time
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
hints = 1
2021-10-27T04:35:30.459997+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-27T04:35:30.459997+0000     0       0         0         0         0         0           -           0
2021-10-27T04:35:31.460097+0000     1     128     17571     17443   68.1253   68.1367  0.00942548  0.00728271
2021-10-27T04:35:32.460169+0000     2     128     35386     35258    68.855   69.5898  0.00714756   0.0072437
2021-10-27T04:35:33.460245+0000     3     127     53344     53217   69.2857   70.1523  0.00765402  0.00720207
2021-10-27T04:35:34.460316+0000     4     128     70645     70517   68.8576   67.5781   0.0055895  0.00725099
2021-10-27T04:35:35.460386+0000     5     128     88011     87883   68.6523   67.8359  0.00765518  0.00727583
2021-10-27T04:35:36.460458+0000     6     128    105578    105450   68.6463   68.6211  0.00783287  0.00727628
2021-10-27T04:35:37.460530+0000     7     128    124833    124705   69.5839   75.2148  0.00542271  0.00717575
2021-10-27T04:35:38.460608+0000     8     128    143089    142961   69.7993   71.3125  0.00545179  0.00715656
2021-10-27T04:35:39.460745+0000 Total time run:       8.43282
Total reads made:     150847
Read size:            4096
Object size:          4096
Bandwidth (MB/sec):   69.8754
Average IOPS:         17888
Stddev IOPS:          647.106
Max IOPS:             19255
Min IOPS:             17300
Average Latency(s):   0.00714996
Max latency(s):       0.0430718
Min latency(s):       0.000977508

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:35:39,972849210-07:00] INFO: > Stop system activity collection services[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:35:39,978790358-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:379 - rados_bench() > kill -INT 750791

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:35:39,984804754-07:00] INFO: >> for OSD host[0m
# ./benchmarks/bench-rados:384 - rados_bench() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.5 kill -INT 40049
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:35:39,993417634-07:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.5 kill -INT 40049
[1] 21:35:41 [SUCCESS] ljishen@10.10.2.5

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:35:41,081713130-07:00] INFO: >>> Copy system activity file from OSD host[0m
## ./benchmarks/bench-rados:391 - rados_bench() > basename -- /tmp/bench-rados/sys_activity_4KB_seq.dat.osd_host.2
# ./benchmarks/bench-rados:391 - rados_bench() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.5:/tmp/bench-rados/sys_activity_4KB_seq.dat.osd_host.2 /mnt/sda8/ceph-research/scripts/output/sys_activity_4KB_seq.dat.osd_host.2


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:35:42,153989635-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:36:05,217551632-07:00] INFO: >>   data:
    pools:   2 pools, 256 pgs
    objects: 150.85k objects, 589 MiB
    usage:   1.9 GiB used, 98 GiB / 100 GiB avail
    pgs:     256 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:36:05,225065136-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:36:13,308678074-07:00] INFO: >>   data:
    pools:   2 pools, 256 pgs
    objects: 150.85k objects, 589 MiB
    usage:   1.9 GiB used, 98 GiB / 100 GiB avail
    pgs:     256 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:36:13,316099715-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:36:21,567388533-07:00] INFO: >>   data:
    pools:   2 pools, 256 pgs
    objects: 150.85k objects, 589 MiB
    usage:   1.9 GiB used, 98 GiB / 100 GiB avail
    pgs:     256 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:36:21,574958362-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:36:29,900586371-07:00] INFO: >>   data:
    pools:   2 pools, 256 pgs
    objects: 150.85k objects, 589 MiB
    usage:   1.9 GiB used, 98 GiB / 100 GiB avail
    pgs:     256 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:36:29,907818786-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:36:38,148768268-07:00] INFO: >>   data:
    pools:   2 pools, 256 pgs
    objects: 150.85k objects, 589 MiB
    usage:   1.9 GiB used, 98 GiB / 100 GiB avail
    pgs:     256 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:36:38,156525159-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:36:38,163142809-07:00] INFO: > The cluster is idle now.[0m


[1;7;39;49m[2021-10-26T21:36:38,165377800-07:00][RUNNING][ROUND 3/1/21] object_size=4KB[0m


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:36:38,168871738-07:00] STAGE: Launch a Ceph cluster (CEPH_HOST: 10.10.2.3, OSD_HOST: 10.10.2.5) ...[0m
# ./benchmarks/bench-rados:172 - launch_ceph_cluster() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.3 bash -euo pipefail
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:36:38,177039171-07:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.3 bash -euo pipefail
10.10.2.3: b'\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T00:36:38,603350219-04:00] INFO: > Remove the existing cluster\x1b[0m\n'
10.10.2.3: b'## bash:13 -  > ls /var/lib/ceph\n'
10.10.2.3: b'## bash:13 -  > tail -n1\n'
10.10.2.3: b'# bash:13 -  > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm --host 10.10.2.3 --host 10.10.2.5 rm-cluster --force --fsid 74eef5da-36de-11ec-b561-09a495c6360a\n'
10.10.2.3: b'\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T00:36:46,282105721-04:00] INFO: > Deploy a new cluster\x1b[0m\n'
10.10.2.3: b'## bash:22 -  > /tmp/bench-rados/ceph-research/scripts/parallel-ssh --host 10.10.2.5 uname --nodename\n## bash:22 -  > tail -n1\n'
10.10.2.3: b'# bash:22 -  > osd_hostname=sm1\n# bash:23 -  > /tmp/bench-rados/ceph-research/scripts/ceph-deploy -m 10.10.2.3 -o sm1/10.10.2.5:/dev/nvme0n1\n'
10.10.2.3: b'\n\n\x1b[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T00:36:47,403695839-04:00] STAGE: Verify passwordless SSH to hosts, passwordless sudo via ljishen, and the existence of devices on hosts...\x1b[0m\n'
10.10.2.3: b'\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T00:36:47,407580169-04:00] INFO: > Check the host for the monitor\x1b[0m\n# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:181 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@10.10.2.3 sudo --non-interactive --validate\n'
10.10.2.3: b'\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T00:36:47,555811465-04:00] INFO: > Check hosts for OSDs\x1b[0m\n'
10.10.2.3: b"\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T00:36:47,559666689-04:00] INFO: >> Check host 'sm1'\x1b[0m\n# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:200 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 sudo --non-interactive --validate\n"
10.10.2.3: b"\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T00:36:48,615705924-04:00] INFO: >>> Check device '/dev/nvme0n1'\x1b[0m\n# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:215 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 test -b /dev/nvme0n1\n"
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:226 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 lsblk --nodeps --noheadings --output TYPE /dev/nvme0n1\n'
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:227 - verify_parameters() > grep --word-regexp disk\n'
10.10.2.3: b'\n\n\x1b[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T00:36:50,765266774-04:00] STAGE: Reset devices on hosts...\x1b[0m\n'
10.10.2.3: b"\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T00:36:50,768924006-04:00] INFO: > Resetting device '/dev/nvme0n1' on host 'sm1'\x1b[0m\n# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:288 - reset_host_devices() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 sudo sh\n"
10.10.2.3: b'  Removing ceph--8ed30711--1f59--46f5--99de--71559bb9112d-osd--block--98c3839b--3a57--4379--9da5--c686f9b62fdd (252:0)\n'
10.10.2.3: b'  Archiving volume group "ceph-8ed30711-1f59-46f5-99de-71559bb9112d" metadata (seqno 5).\n'
10.10.2.3: b'  Releasing logical volume "osd-block-98c3839b-3a57-4379-9da5-c686f9b62fdd"\n'
10.10.2.3: b'  Creating volume group backup "/etc/lvm/backup/ceph-8ed30711-1f59-46f5-99de-71559bb9112d" (seqno 6).\n'
10.10.2.3: b'  Logical volume "osd-block-98c3839b-3a57-4379-9da5-c686f9b62fdd" successfully removed\n'
10.10.2.3: b'  Removing physical volume "/dev/nvme0n1" from volume group "ceph-8ed30711-1f59-46f5-99de-71559bb9112d"\n'
10.10.2.3: b'  Volume group "ceph-8ed30711-1f59-46f5-99de-71559bb9112d" successfully removed\n'
10.10.2.3: b'\n\n\x1b[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T00:36:53,062963250-04:00] STAGE: Bootstrap a new cluster...\x1b[0m\n'
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:312 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.3 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults\n'
10.10.2.3: b'\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T00:36:53,071264743-04:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.3: b"\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T00:36:53,074182701-04:00] DEBUG: CEPHADM_PARAMS: ['bootstrap', '--config', '/tmp/bench-rados/ceph-research/scripts/ceph.conf', '--mon-ip', '10.10.2.3', '--output-dir', '/tmp/bench-rados/deployment_data_root/etc/ceph', '--ssh-user', 'ljishen', '--skip-dashboard', '--no-minimize-config', '--allow-overwrite', '--allow-fqdn-hostname', '--orphan-initial-daemons', '--skip-monitoring-stack', '--single-host-defaults']\x1b[0m\n"
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.3 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults\n'
10.10.2.3: b'Verifying podman|docker is present...\nVerifying lvm2 is present...\nVerifying time synchronization is in place...\n'
10.10.2.3: b'Unit ntp.service is enabled and running\nRepeating the final host check...\npodman|docker (/usr/bin/docker) is present\n'
10.10.2.3: b'systemctl is present\n'
10.10.2.3: b'lvcreate is present\n'
10.10.2.3: b'Unit ntp.service is enabled and running\nHost looks OK\n'
10.10.2.3: b'Cluster fsid: 82488196-36df-11ec-b561-09a495c6360a\n'
10.10.2.3: b'Verifying IP 10.10.2.3 port 3300 ...\nVerifying IP 10.10.2.3 port 6789 ...\n'
10.10.2.3: b'Mon IP `10.10.2.3` is in CIDR network `10.10.2.0/24`\n- internal network (--cluster-network) has not been provided, OSD replication will default to the public_network\n'
10.10.2.3: b'Adjusting default settings to suit single-host cluster...\n'
10.10.2.3: b'Pulling container image quay.io/ceph/ceph:v16...\n'
10.10.2.3: b'Ceph version: ceph version 16.2.6 (ee28fb57e47e9f88813e24bbf4c14496ca299d31) pacific (stable)\nExtracting ceph user uid/gid from container image...\n'
10.10.2.3: b'Creating initial keys...\n'
10.10.2.3: b'Creating initial monmap...\n'
10.10.2.3: b'Creating mon...\n'
10.10.2.3: b'Waiting for mon to start...\nWaiting for mon...\n'
10.10.2.3: b'mon is available\nSetting mon public_network to 10.10.2.0/24\n'
10.10.2.3: b'Wrote config to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf\n'
10.10.2.3: b'Wrote keyring to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring\nCreating mgr...\n'
10.10.2.3: b'Verifying port 9283 ...\n'
10.10.2.3: b'Waiting for mgr to start...\nWaiting for mgr...\n'
10.10.2.3: b'mgr not available, waiting (1/15)...\n'
10.10.2.3: b'mgr not available, waiting (2/15)...\n'
10.10.2.3: b'mgr is available\n'
10.10.2.3: b'Enabling cephadm module...\n'
10.10.2.3: b'Waiting for the mgr to restart...\nWaiting for mgr epoch 5...\n'
10.10.2.3: b'mgr epoch 5 is available\nSetting orchestrator backend to cephadm...\n'
10.10.2.3: b'Generating ssh key...\n'
10.10.2.3: b'Wrote public SSH key to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub\nAdding key to ljishen@localhost authorized_keys...\n'
10.10.2.3: b'Adding host node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us...\n'
10.10.2.3: b'Deploying unmanaged mon service...\n'
10.10.2.3: b'Deploying unmanaged mgr service...\n'
10.10.2.3: b'You can access the Ceph CLI with:\n\n\tsudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --fsid 82488196-36df-11ec-b561-09a495c6360a -c /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf -k /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring\n\nPlease consider enabling telemetry to help improve Ceph:\n\n\tceph telemetry on\n\nFor more information see:\n\n\thttps://docs.ceph.com/docs/pacific/mgr/telemetry/\n\nBootstrap complete.\n'
10.10.2.3: b'\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T00:37:56,704369759-04:00] INFO: > Set the default number of monitor daemons to 1\x1b[0m\n# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:330 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1\n'
10.10.2.3: b'\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T00:37:56,712841112-04:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.3: b"\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T00:37:56,715667799-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mon', '1']\x1b[0m\n# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1\n"
10.10.2.3: b'Inferring fsid 82488196-36df-11ec-b561-09a495c6360a\n'
10.10.2.3: b'Inferring config /var/lib/ceph/82488196-36df-11ec-b561-09a495c6360a/mon.node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.3: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.3: b'Scheduled mon update...\n'
10.10.2.3: b'\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T00:38:05,963262934-04:00] INFO: > Set the default number of manager daemons to 1\x1b[0m\n# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:335 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1\n'
10.10.2.3: b'\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T00:38:05,972345980-04:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.3: b"\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T00:38:05,975528387-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mgr', '1']\x1b[0m\n"
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1\n'
10.10.2.3: b'Inferring fsid 82488196-36df-11ec-b561-09a495c6360a\n'
10.10.2.3: b'Inferring config /var/lib/ceph/82488196-36df-11ec-b561-09a495c6360a/mon.node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.3: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.3: b'Scheduled mgr update...\n'
10.10.2.3: b'\n\n\x1b[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T00:38:16,108863795-04:00] STAGE: Add hosts to the cluster...\x1b[0m\n'
10.10.2.3: b"\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T00:38:16,113880629-04:00] INFO: > Adding host 'sm1'\x1b[0m\n"
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:362 - add_hosts() > ssh-copy-id -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -f -i /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub ljishen@sm1\n'
10.10.2.3: b'/usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: "/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub"\n'
10.10.2.3: b'\nNumber of key(s) added: 1\n\nNow try logging into the machine, with:   "ssh -o \'GlobalKnownHostsFile=/dev/null\' -o \'LogLevel=ERROR\' -o \'PasswordAuthentication=no\' -o \'StrictHostKeyChecking=no\' -o \'UserKnownHostsFile=/dev/null\' \'ljishen@sm1\'"\nand check to make sure that only the key(s) you wanted were added.\n\n'
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:368 - add_hosts() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add sm1 10.10.2.5\n'
10.10.2.3: b'\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T00:38:17,255977840-04:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.3: b"\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T00:38:17,258966822-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'host', 'add', 'sm1', '10.10.2.5']\x1b[0m\n# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add sm1 10.10.2.5\n"
10.10.2.3: b'Inferring fsid 82488196-36df-11ec-b561-09a495c6360a\n'
10.10.2.3: b'Inferring config /var/lib/ceph/82488196-36df-11ec-b561-09a495c6360a/mon.node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.3: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.3: b"Added host 'sm1' with addr '10.10.2.5'\n"
10.10.2.3: b'\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T00:38:28,212922533-04:00] INFO: Sleep for 20 seconds\x1b[0m\n'
10.10.2.3: b'\n\n\x1b[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T00:38:48,216579670-04:00] STAGE: Add OSDs to the cluster...\x1b[0m\n'
10.10.2.3: b"\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T00:38:48,221924612-04:00] INFO: > Adding devices ['/dev/nvme0n1'] on host 'sm1' as OSDs\x1b[0m\n# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:387 - add_host_devices_as_osds() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd sm1:/dev/nvme0n1\n"
10.10.2.3: b'\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T00:38:48,229909078-04:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.3: b"\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T00:38:48,232909332-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'daemon', 'add', 'osd', 'sm1:/dev/nvme0n1']\x1b[0m\n"
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd sm1:/dev/nvme0n1\n'
10.10.2.3: b'Inferring fsid 82488196-36df-11ec-b561-09a495c6360a\n'
10.10.2.3: b'Inferring config /var/lib/ceph/82488196-36df-11ec-b561-09a495c6360a/mon.node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.3: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.3: b"Created osd(s) 0 on host 'sm1'\n"
10.10.2.3: b'\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T00:39:11,654605872-04:00] INFO: > Sleep for 20 seconds\x1b[0m\n'
10.10.2.3: b'\n\n\x1b[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T00:39:31,658947882-04:00] STAGE: Show Cluster Status\x1b[0m\n# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:343 - show_status() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status\n'
10.10.2.3: b'\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T00:39:31,667188991-04:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.3: b"\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T00:39:31,670013353-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', '--status']\x1b[0m\n"
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status\n'
10.10.2.3: b'Inferring fsid 82488196-36df-11ec-b561-09a495c6360a\n'
10.10.2.3: b'Inferring config /var/lib/ceph/82488196-36df-11ec-b561-09a495c6360a/mon.node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.3: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.3: b'  cluster:\n    id:     82488196-36df-11ec-b561-09a495c6360a\n    health: HEALTH_WARN\n            1 pool(s) have no replicas configured\n \n  services:\n    mon: 1 daemons, quorum node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us (age 2m)\n    mgr: node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us.kteuew(active, since 2m)\n    osd: 1 osds: 1 up (since 21s), 1 in (since 40s)\n \n  data:\n    pools:   1 pools, 128 pgs\n    objects: 0 objects, 0 B\n    usage:   4.7 MiB used, 100 GiB / 100 GiB avail\n    pgs:     99.219% pgs unknown\n             127 unknown\n             1   active+clean\n \n  progress:\n \n'
[1] 21:39:41 [SUCCESS] ljishen@10.10.2.3

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T00:36:38,603350219-04:00] INFO: > Remove the existing cluster[0m
## bash:13 -  > ls /var/lib/ceph
## bash:13 -  > tail -n1
# bash:13 -  > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm --host 10.10.2.3 --host 10.10.2.5 rm-cluster --force --fsid 74eef5da-36de-11ec-b561-09a495c6360a

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T00:36:46,282105721-04:00] INFO: > Deploy a new cluster[0m
## bash:22 -  > /tmp/bench-rados/ceph-research/scripts/parallel-ssh --host 10.10.2.5 uname --nodename
## bash:22 -  > tail -n1
# bash:22 -  > osd_hostname=sm1
# bash:23 -  > /tmp/bench-rados/ceph-research/scripts/ceph-deploy -m 10.10.2.3 -o sm1/10.10.2.5:/dev/nvme0n1


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T00:36:47,403695839-04:00] STAGE: Verify passwordless SSH to hosts, passwordless sudo via ljishen, and the existence of devices on hosts...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T00:36:47,407580169-04:00] INFO: > Check the host for the monitor[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:181 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@10.10.2.3 sudo --non-interactive --validate

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T00:36:47,555811465-04:00] INFO: > Check hosts for OSDs[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T00:36:47,559666689-04:00] INFO: >> Check host 'sm1'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:200 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 sudo --non-interactive --validate

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T00:36:48,615705924-04:00] INFO: >>> Check device '/dev/nvme0n1'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:215 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 test -b /dev/nvme0n1
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:226 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 lsblk --nodeps --noheadings --output TYPE /dev/nvme0n1
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:227 - verify_parameters() > grep --word-regexp disk


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T00:36:50,765266774-04:00] STAGE: Reset devices on hosts...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T00:36:50,768924006-04:00] INFO: > Resetting device '/dev/nvme0n1' on host 'sm1'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:288 - reset_host_devices() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 sudo sh
  Removing ceph--8ed30711--1f59--46f5--99de--71559bb9112d-osd--block--98c3839b--3a57--4379--9da5--c686f9b62fdd (252:0)
  Archiving volume group "ceph-8ed30711-1f59-46f5-99de-71559bb9112d" metadata (seqno 5).
  Releasing logical volume "osd-block-98c3839b-3a57-4379-9da5-c686f9b62fdd"
  Creating volume group backup "/etc/lvm/backup/ceph-8ed30711-1f59-46f5-99de-71559bb9112d" (seqno 6).
  Logical volume "osd-block-98c3839b-3a57-4379-9da5-c686f9b62fdd" successfully removed
  Removing physical volume "/dev/nvme0n1" from volume group "ceph-8ed30711-1f59-46f5-99de-71559bb9112d"
  Volume group "ceph-8ed30711-1f59-46f5-99de-71559bb9112d" successfully removed


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T00:36:53,062963250-04:00] STAGE: Bootstrap a new cluster...[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:312 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.3 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T00:36:53,071264743-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T00:36:53,074182701-04:00] DEBUG: CEPHADM_PARAMS: ['bootstrap', '--config', '/tmp/bench-rados/ceph-research/scripts/ceph.conf', '--mon-ip', '10.10.2.3', '--output-dir', '/tmp/bench-rados/deployment_data_root/etc/ceph', '--ssh-user', 'ljishen', '--skip-dashboard', '--no-minimize-config', '--allow-overwrite', '--allow-fqdn-hostname', '--orphan-initial-daemons', '--skip-monitoring-stack', '--single-host-defaults'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.3 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults
Verifying podman|docker is present...
Verifying lvm2 is present...
Verifying time synchronization is in place...
Unit ntp.service is enabled and running
Repeating the final host check...
podman|docker (/usr/bin/docker) is present
systemctl is present
lvcreate is present
Unit ntp.service is enabled and running
Host looks OK
Cluster fsid: 82488196-36df-11ec-b561-09a495c6360a
Verifying IP 10.10.2.3 port 3300 ...
Verifying IP 10.10.2.3 port 6789 ...
Mon IP `10.10.2.3` is in CIDR network `10.10.2.0/24`
- internal network (--cluster-network) has not been provided, OSD replication will default to the public_network
Adjusting default settings to suit single-host cluster...
Pulling container image quay.io/ceph/ceph:v16...
Ceph version: ceph version 16.2.6 (ee28fb57e47e9f88813e24bbf4c14496ca299d31) pacific (stable)
Extracting ceph user uid/gid from container image...
Creating initial keys...
Creating initial monmap...
Creating mon...
Waiting for mon to start...
Waiting for mon...
mon is available
Setting mon public_network to 10.10.2.0/24
Wrote config to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf
Wrote keyring to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring
Creating mgr...
Verifying port 9283 ...
Waiting for mgr to start...
Waiting for mgr...
mgr not available, waiting (1/15)...
mgr not available, waiting (2/15)...
mgr is available
Enabling cephadm module...
Waiting for the mgr to restart...
Waiting for mgr epoch 5...
mgr epoch 5 is available
Setting orchestrator backend to cephadm...
Generating ssh key...
Wrote public SSH key to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub
Adding key to ljishen@localhost authorized_keys...
Adding host node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us...
Deploying unmanaged mon service...
Deploying unmanaged mgr service...
You can access the Ceph CLI with:

	sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --fsid 82488196-36df-11ec-b561-09a495c6360a -c /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf -k /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring

Please consider enabling telemetry to help improve Ceph:

	ceph telemetry on

For more information see:

	https://docs.ceph.com/docs/pacific/mgr/telemetry/

Bootstrap complete.

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T00:37:56,704369759-04:00] INFO: > Set the default number of monitor daemons to 1[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:330 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T00:37:56,712841112-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T00:37:56,715667799-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mon', '1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1
Inferring fsid 82488196-36df-11ec-b561-09a495c6360a
Inferring config /var/lib/ceph/82488196-36df-11ec-b561-09a495c6360a/mon.node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Scheduled mon update...

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T00:38:05,963262934-04:00] INFO: > Set the default number of manager daemons to 1[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:335 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T00:38:05,972345980-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T00:38:05,975528387-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mgr', '1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1
Inferring fsid 82488196-36df-11ec-b561-09a495c6360a
Inferring config /var/lib/ceph/82488196-36df-11ec-b561-09a495c6360a/mon.node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Scheduled mgr update...


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T00:38:16,108863795-04:00] STAGE: Add hosts to the cluster...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T00:38:16,113880629-04:00] INFO: > Adding host 'sm1'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:362 - add_hosts() > ssh-copy-id -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -f -i /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub ljishen@sm1
/usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: "/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub"

Number of key(s) added: 1

Now try logging into the machine, with:   "ssh -o 'GlobalKnownHostsFile=/dev/null' -o 'LogLevel=ERROR' -o 'PasswordAuthentication=no' -o 'StrictHostKeyChecking=no' -o 'UserKnownHostsFile=/dev/null' 'ljishen@sm1'"
and check to make sure that only the key(s) you wanted were added.

# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:368 - add_hosts() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add sm1 10.10.2.5
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T00:38:17,255977840-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T00:38:17,258966822-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'host', 'add', 'sm1', '10.10.2.5'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add sm1 10.10.2.5
Inferring fsid 82488196-36df-11ec-b561-09a495c6360a
Inferring config /var/lib/ceph/82488196-36df-11ec-b561-09a495c6360a/mon.node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Added host 'sm1' with addr '10.10.2.5'

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T00:38:28,212922533-04:00] INFO: Sleep for 20 seconds[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T00:38:48,216579670-04:00] STAGE: Add OSDs to the cluster...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T00:38:48,221924612-04:00] INFO: > Adding devices ['/dev/nvme0n1'] on host 'sm1' as OSDs[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:387 - add_host_devices_as_osds() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd sm1:/dev/nvme0n1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T00:38:48,229909078-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T00:38:48,232909332-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'daemon', 'add', 'osd', 'sm1:/dev/nvme0n1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd sm1:/dev/nvme0n1
Inferring fsid 82488196-36df-11ec-b561-09a495c6360a
Inferring config /var/lib/ceph/82488196-36df-11ec-b561-09a495c6360a/mon.node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Created osd(s) 0 on host 'sm1'

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T00:39:11,654605872-04:00] INFO: > Sleep for 20 seconds[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T00:39:31,658947882-04:00] STAGE: Show Cluster Status[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:343 - show_status() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T00:39:31,667188991-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T00:39:31,670013353-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', '--status'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status
Inferring fsid 82488196-36df-11ec-b561-09a495c6360a
Inferring config /var/lib/ceph/82488196-36df-11ec-b561-09a495c6360a/mon.node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
  cluster:
    id:     82488196-36df-11ec-b561-09a495c6360a
    health: HEALTH_WARN
            1 pool(s) have no replicas configured
 
  services:
    mon: 1 daemons, quorum node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us (age 2m)
    mgr: node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us.kteuew(active, since 2m)
    osd: 1 osds: 1 up (since 21s), 1 in (since 40s)
 
  data:
    pools:   1 pools, 128 pgs
    objects: 0 objects, 0 B
    usage:   4.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     99.219% pgs unknown
             127 unknown
             1   active+clean
 
  progress:
 

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:39:41,467021134-07:00] INFO: > Copy configuration files to local[0m
# ./benchmarks/bench-rados:208 - launch_ceph_cluster() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.3:/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf
# ./benchmarks/bench-rados:212 - launch_ceph_cluster() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.3 sudo chmod 644 /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:39:41,623222787-07:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.3 sudo chmod 644 /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring
[1] 21:39:41 [SUCCESS] ljishen@10.10.2.3
# ./benchmarks/bench-rados:214 - launch_ceph_cluster() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.3:/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:39:41,950879923-07:00] STAGE: Update local ceph.conf as a client...[0m


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:39:41,954567866-07:00] STAGE: Configure Ceph pool...[0m
# ./benchmarks/bench-rados:245 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_max_backfills 32
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:39:41,977652680-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:39:41,980422226-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '82488196-36df-11ec-b561-09a495c6360a', '--', 'ceph', 'config', 'set', 'osd', 'osd_max_backfills', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 82488196-36df-11ec-b561-09a495c6360a -- ceph config set osd osd_max_backfills 32
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:246 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_active 32
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:39:45,108027886-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:39:45,111074684-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '82488196-36df-11ec-b561-09a495c6360a', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_active', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 82488196-36df-11ec-b561-09a495c6360a -- ceph config set osd osd_recovery_max_active 32
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:247 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_single_start 8
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:39:48,434955525-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:39:48,438480240-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '82488196-36df-11ec-b561-09a495c6360a', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_single_start', '8'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 82488196-36df-11ec-b561-09a495c6360a -- ceph config set osd osd_recovery_max_single_start 8
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:248 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_op_priority 63
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:39:51,533459972-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:39:51,536859783-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '82488196-36df-11ec-b561-09a495c6360a', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_op_priority', '63'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 82488196-36df-11ec-b561-09a495c6360a -- ceph config set osd osd_recovery_op_priority 63
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:250 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config show-with-defaults osd.0
# ./benchmarks/bench-rados:251 - configure_ceph_pool() > grep 'osd_max_backfills\|osd_recovery'
# ./benchmarks/bench-rados:251 - configure_ceph_pool() > column -t -s ' '
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
osd_max_backfills                        32         mon
osd_recovery_cost                        20971520   default
osd_recovery_delay_start                 0.000000   default
osd_recovery_max_active                  32         mon
osd_recovery_max_active_hdd              3          default
osd_recovery_max_active_ssd              10         default
osd_recovery_max_chunk                   8388608    default
osd_recovery_max_omap_entries_per_chunk  8096       default
osd_recovery_max_single_start            8          mon
osd_recovery_op_priority                 63         mon
osd_recovery_op_warn_multiple            16         default
osd_recovery_priority                    5          default
osd_recovery_retry_interval              30.000000  default
osd_recovery_sleep                       0.000000   default
osd_recovery_sleep_hdd                   0.100000   default
osd_recovery_sleep_hybrid                0.025000   default
osd_recovery_sleep_ssd                   0.000000   default
# ./benchmarks/bench-rados:253 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool create bench_rados 128 128 replicated --size 1 --pg-num-min 128
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:39:57,994025601-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:39:57,997752096-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '82488196-36df-11ec-b561-09a495c6360a', '--', 'ceph', 'osd', 'pool', 'create', 'bench_rados', '128', '128', 'replicated', '--size', '1', '--pg-num-min', '128'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 82488196-36df-11ec-b561-09a495c6360a -- ceph osd pool create bench_rados 128 128 replicated --size 1 --pg-num-min 128
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
pool 'bench_rados' created
# ./benchmarks/bench-rados:255 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados min_size 1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:40:01,919989084-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:40:01,922862415-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '82488196-36df-11ec-b561-09a495c6360a', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'min_size', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 82488196-36df-11ec-b561-09a495c6360a -- ceph osd pool set bench_rados min_size 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 min_size to 1
# ./benchmarks/bench-rados:256 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados pg_autoscale_mode off
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:40:06,211459461-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:40:06,214802025-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '82488196-36df-11ec-b561-09a495c6360a', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'pg_autoscale_mode', 'off'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 82488196-36df-11ec-b561-09a495c6360a -- ceph osd pool set bench_rados pg_autoscale_mode off
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 pg_autoscale_mode to off
# ./benchmarks/bench-rados:257 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool application enable bench_rados benchmark
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:40:10,628382761-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:40:10,632002325-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '82488196-36df-11ec-b561-09a495c6360a', '--', 'ceph', 'osd', 'pool', 'application', 'enable', 'bench_rados', 'benchmark'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 82488196-36df-11ec-b561-09a495c6360a -- ceph osd pool application enable bench_rados benchmark
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
enabled application 'benchmark' on pool 'bench_rados'
# ./benchmarks/bench-rados:258 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados noscrub 1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:40:14,145806766-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:40:14,149280576-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '82488196-36df-11ec-b561-09a495c6360a', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'noscrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 82488196-36df-11ec-b561-09a495c6360a -- ceph osd pool set bench_rados noscrub 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 noscrub to 1
# ./benchmarks/bench-rados:259 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados nodeep-scrub 1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:40:18,363656776-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:40:18,367211548-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '82488196-36df-11ec-b561-09a495c6360a', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'nodeep-scrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 82488196-36df-11ec-b561-09a495c6360a -- ceph osd pool set bench_rados nodeep-scrub 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 nodeep-scrub to 1
# ./benchmarks/bench-rados:260 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool ls detail
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:40:21,847120945-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:40:21,850627807-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '82488196-36df-11ec-b561-09a495c6360a', '--', 'ceph', 'osd', 'pool', 'ls', 'detail'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 82488196-36df-11ec-b561-09a495c6360a -- ceph osd pool ls detail
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
pool 1 'device_health_metrics' replicated size 1 min_size 1 crush_rule 0 object_hash rjenkins pg_num 128 pgp_num 128 autoscale_mode on last_change 13 lfor 0/0/11 flags hashpspool stripe_width 0 pg_num_min 1 application mgr_devicehealth
pool 2 'bench_rados' replicated size 1 min_size 1 crush_rule 0 object_hash rjenkins pg_num 128 pgp_num 128 autoscale_mode off last_change 21 flags hashpspool,noscrub,nodeep-scrub stripe_width 0 pg_num_min 128 application benchmark

# ./benchmarks/bench-rados:261 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd df tree
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:40:25,263852432-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:40:25,266967117-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '82488196-36df-11ec-b561-09a495c6360a', '--', 'ceph', 'osd', 'df', 'tree'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 82488196-36df-11ec-b561-09a495c6360a -- ceph osd df tree
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
ID  CLASS  WEIGHT   REWEIGHT  SIZE     RAW USE  DATA     OMAP  META     AVAIL    %USE  VAR   PGS  STATUS  TYPE NAME    
-1         0.09769         -  100 GiB  6.0 MiB  176 KiB   0 B  5.8 MiB  100 GiB  0.01  1.00    -          root default 
-3         0.09769         -  100 GiB  6.0 MiB  176 KiB   0 B  5.8 MiB  100 GiB  0.01  1.00    -              host sm1 
 0    ssd  0.09769   1.00000  100 GiB  6.0 MiB  176 KiB   0 B  5.8 MiB  100 GiB  0.01  1.00  256      up          osd.0
                       TOTAL  100 GiB  6.0 MiB  176 KiB   0 B  5.8 MiB  100 GiB  0.01                                  
MIN/MAX VAR: 1.00/1.00  STDDEV: 0


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:40:28,512948956-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:40:51,637578374-07:00] INFO: >>   data:
    pools:   2 pools, 256 pgs
    objects: 0 objects, 0 B
    usage:   6.0 MiB used, 100 GiB / 100 GiB avail
    pgs:     256 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:40:51,645421557-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:40:59,942875154-07:00] INFO: >>   data:
    pools:   2 pools, 256 pgs
    objects: 0 objects, 0 B
    usage:   6.0 MiB used, 100 GiB / 100 GiB avail
    pgs:     256 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:40:59,949381173-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:41:08,196665493-07:00] INFO: >>   data:
    pools:   2 pools, 256 pgs
    objects: 0 objects, 0 B
    usage:   6.0 MiB used, 100 GiB / 100 GiB avail
    pgs:     256 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:41:08,203571546-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:41:16,339279297-07:00] INFO: >>   data:
    pools:   2 pools, 256 pgs
    objects: 0 objects, 0 B
    usage:   6.0 MiB used, 100 GiB / 100 GiB avail
    pgs:     256 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:41:16,347152938-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:41:24,654617832-07:00] INFO: >>   data:
    pools:   2 pools, 256 pgs
    objects: 0 objects, 0 B
    usage:   6.0 MiB used, 100 GiB / 100 GiB avail
    pgs:     256 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:41:24,662223449-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:41:24,667919577-07:00] INFO: > The cluster is idle now.[0m


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:41:24,671307976-07:00] STAGE: Start benchmarking of mode write ...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:41:24,678160036-07:00] INFO: > Start system activity collection services[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:41:24,683284750-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:329 - rados_bench() > rm --force /mnt/sda8/ceph-research/scripts/output/sys_activity_4KB_write.dat.rados_bench_host.3
# ./benchmarks/bench-rados:330 - rados_bench() > sar_pid_rados_bench_host=757943
# ./benchmarks/bench-rados:330 - rados_bench() > S_TIME_FORMAT=ISO
# ./benchmarks/bench-rados:330 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/output/sys_activity_4KB_write.dat.rados_bench_host.3 2

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:41:24,689891719-07:00] INFO: >> for OSD host[0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:41:24,698292050-07:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.5 bash
10.10.2.5: b'44658\n'
[1] 21:41:25 [SUCCESS] ljishen@10.10.2.5
44658

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:41:25,800859432-07:00] INFO: > Run rados bench[0m
# ./benchmarks/bench-rados:358 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/output/rados_bench_4KB_write.log.3
## ./benchmarks/bench-rados:349 - rados_bench() > count_cpus 32-63,96-127
# ./benchmarks/bench-rados:349 - rados_bench() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 30 write --pool bench_rados -b 4096 -O 4096 --concurrent-ios 128 --show-time --write-object --write-omap --write-xattr --no-cleanup
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:41:25,821972009-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:41:25,825345550-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '82488196-36df-11ec-b561-09a495c6360a', '--', 'rados', 'bench', '30', 'write', '--pool', 'bench_rados', '-b', '4096', '-O', '4096', '--concurrent-ios', '128', '--show-time', '--write-object', '--write-omap', '--write-xattr', '--no-cleanup'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 82488196-36df-11ec-b561-09a495c6360a -- rados bench 30 write --pool bench_rados -b 4096 -O 4096 --concurrent-ios 128 --show-time --write-object --write-omap --write-xattr --no-cleanup
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
hints = 1
2021-10-27T04:41:28.249303+0000 Maintaining 128 concurrent writes of 4096 bytes to objects of size 4096 for up to 30 seconds or 0 objects
2021-10-27T04:41:28.249313+0000 Object prefix: benchmark_data_node-0.bluefield2.ucsc-cmps10_7
2021-10-27T04:41:28.249773+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-27T04:41:28.249773+0000     0       0         0         0         0         0           -           0
2021-10-27T04:41:29.249897+0000     1     128      6509      6381   24.9225   24.9258   0.0162187   0.0197743
2021-10-27T04:41:30.249971+0000     2     128     12672     12544   24.4975   24.0742   0.0179804   0.0203034
2021-10-27T04:41:31.250069+0000     3     128     18832     18704   24.3517   24.0625   0.0179438    0.020437
2021-10-27T04:41:32.250184+0000     4     128     24799     24671   24.0902   23.3086    0.022557   0.0206815
2021-10-27T04:41:33.250279+0000     5     128     30739     30611   23.9124   23.2031    0.020644   0.0208579
2021-10-27T04:41:34.250360+0000     6     128     36618     36490   23.7542   22.9648   0.0210658   0.0210169
2021-10-27T04:41:35.250468+0000     7     127     42519     42392   23.6539   23.0547   0.0211728   0.0211033
2021-10-27T04:41:36.250539+0000     8     128     48273     48145    23.506   22.4727   0.0177194   0.0212443
2021-10-27T04:41:37.250635+0000     9     128     53960     53832   23.3623   22.2148   0.0209043   0.0213688
2021-10-27T04:41:38.250708+0000    10     128     59602     59474   23.2298   22.0391   0.0233224    0.021492
2021-10-27T04:41:39.250803+0000    11     128     65255     65127   23.1253    22.082   0.0250421   0.0215977
2021-10-27T04:41:40.250875+0000    12     128     71017     70889   23.0737   22.5078   0.0184768   0.0216537
2021-10-27T04:41:41.250970+0000    13     128     76620     76492   22.9822   21.8867   0.0192609   0.0217313
2021-10-27T04:41:42.251044+0000    14     128     82329     82201   22.9335   22.3008   0.0205013   0.0217806
2021-10-27T04:41:43.251143+0000    15     128     88055     87927   22.8956   22.3672    0.021285   0.0218268
2021-10-27T04:41:44.251218+0000    16     128     93625     93497   22.8243   21.7578     0.02323   0.0218909
2021-10-27T04:41:45.251329+0000    17     128     99194     99066   22.7612   21.7539   0.0218888   0.0219517
2021-10-27T04:41:46.251411+0000    18     128    104742    104614   22.7006   21.6719   0.0225213   0.0220077
2021-10-27T04:41:47.251508+0000    19     128    108891    108763   22.3588    16.207    0.040092   0.0223331
2021-10-27T04:41:48.251581+0000 min lat: 0.00776528 max lat: 0.051545 avg lat: 0.0228448
2021-10-27T04:41:48.251581+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-27T04:41:48.251581+0000    20     128    112091    111963   21.8658      12.5    0.036558   0.0228448
2021-10-27T04:41:49.251687+0000    21     128    115179    115051   21.3989   12.0625   0.0463194   0.0233328
2021-10-27T04:41:50.251762+0000    22     128    118173    118045   20.9578   11.6953   0.0354853   0.0238393
2021-10-27T04:41:51.251868+0000    23     128    121117    120989   20.5465      11.5   0.0325801   0.0243178
2021-10-27T04:41:52.251968+0000    24     128    124038    123910   20.1658   11.4102   0.0416547   0.0247687
2021-10-27T04:41:53.252066+0000    25     128    126898    126770    19.806   11.1719   0.0480944    0.025215
2021-10-27T04:41:54.252143+0000    26     128    129755    129627   19.4734   11.1602   0.0396417    0.025651
2021-10-27T04:41:55.252387+0000    27     128    133002    132874   19.2218   12.6836   0.0116798   0.0257668
2021-10-27T04:41:56.252460+0000    28     128    138708    138580   19.3313   22.2891   0.0235342   0.0258549
2021-10-27T04:41:57.252560+0000    29     128    144542    144414   19.4504   22.7891   0.0179813   0.0256933
2021-10-27T04:41:58.252634+0000    30     108    150359    150251   19.5621   22.8008   0.0204379   0.0255489
2021-10-27T04:41:59.252783+0000 Total time run:         30.0128
Total writes made:      150359
Write size:             4096
Object size:            4096
Bandwidth (MB/sec):     19.5696
Stddev Bandwidth:       4.98039
Max bandwidth (MB/sec): 24.9258
Min bandwidth (MB/sec): 11.1602
Average IOPS:           5009
Stddev IOPS:            1274.98
Max IOPS:               6381
Min IOPS:               2857
Average Latency(s):     0.0255437
Stddev Latency(s):      0.0110579
Max latency(s):         0.296115
Min latency(s):         0.00776528

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:41:59,769182681-07:00] INFO: > Stop system activity collection services[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:41:59,774704290-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:379 - rados_bench() > kill -INT 757943

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:41:59,780746688-07:00] INFO: >> for OSD host[0m
# ./benchmarks/bench-rados:384 - rados_bench() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.5 kill -INT 44658
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:41:59,789033085-07:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.5 kill -INT 44658
[1] 21:42:00 [SUCCESS] ljishen@10.10.2.5

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:42:00,878345160-07:00] INFO: >>> Copy system activity file from OSD host[0m
## ./benchmarks/bench-rados:391 - rados_bench() > basename -- /tmp/bench-rados/sys_activity_4KB_write.dat.osd_host.3
# ./benchmarks/bench-rados:391 - rados_bench() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.5:/tmp/bench-rados/sys_activity_4KB_write.dat.osd_host.3 /mnt/sda8/ceph-research/scripts/output/sys_activity_4KB_write.dat.osd_host.3


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:42:01,928576994-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:42:24,964905319-07:00] INFO: >>   data:
    pools:   2 pools, 256 pgs
    objects: 150.36k objects, 587 MiB
    usage:   1.9 GiB used, 98 GiB / 100 GiB avail
    pgs:     256 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:42:24,971787847-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:42:33,104715660-07:00] INFO: >>   data:
    pools:   2 pools, 256 pgs
    objects: 150.36k objects, 587 MiB
    usage:   1.9 GiB used, 98 GiB / 100 GiB avail
    pgs:     256 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:42:33,112019129-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:42:41,345477757-07:00] INFO: >>   data:
    pools:   2 pools, 256 pgs
    objects: 150.36k objects, 587 MiB
    usage:   1.9 GiB used, 98 GiB / 100 GiB avail
    pgs:     256 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:42:41,352941978-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:42:49,460035369-07:00] INFO: >>   data:
    pools:   2 pools, 256 pgs
    objects: 150.36k objects, 587 MiB
    usage:   1.9 GiB used, 98 GiB / 100 GiB avail
    pgs:     256 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:42:49,468004488-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:42:57,758518536-07:00] INFO: >>   data:
    pools:   2 pools, 256 pgs
    objects: 150.36k objects, 587 MiB
    usage:   1.9 GiB used, 98 GiB / 100 GiB avail
    pgs:     256 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:42:57,766048280-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:42:57,771721845-07:00] INFO: > The cluster is idle now.[0m


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:42:57,775014875-07:00] STAGE: Start benchmarking of mode seq ...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:42:57,781830287-07:00] INFO: > Start system activity collection services[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:42:57,786998822-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:329 - rados_bench() > rm --force /mnt/sda8/ceph-research/scripts/output/sys_activity_4KB_seq.dat.rados_bench_host.3
# ./benchmarks/bench-rados:330 - rados_bench() > sar_pid_rados_bench_host=759738
# ./benchmarks/bench-rados:330 - rados_bench() > S_TIME_FORMAT=ISO
# ./benchmarks/bench-rados:330 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/output/sys_activity_4KB_seq.dat.rados_bench_host.3 2

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:42:57,794014510-07:00] INFO: >> for OSD host[0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:42:57,802506803-07:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.5 bash
10.10.2.5: b'45191\n'
[1] 21:42:58 [SUCCESS] ljishen@10.10.2.5
45191

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:42:58,908210792-07:00] INFO: > Run rados bench[0m
# ./benchmarks/bench-rados:367 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/output/rados_bench_4KB_seq.log.3
## ./benchmarks/bench-rados:364 - rados_bench() > count_cpus 32-63,96-127
# ./benchmarks/bench-rados:364 - rados_bench() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 128 --show-time
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:42:58,928578458-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:42:58,931411433-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '82488196-36df-11ec-b561-09a495c6360a', '--', 'rados', 'bench', '99999999', 'seq', '--pool', 'bench_rados', '--concurrent-ios', '128', '--show-time'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 82488196-36df-11ec-b561-09a495c6360a -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 128 --show-time
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
hints = 1
2021-10-27T04:43:01.254147+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-27T04:43:01.254147+0000     0       0         0         0         0         0           -           0
2021-10-27T04:43:02.254252+0000     1     128     16732     16604   64.8487   64.8594  0.00957533  0.00766992
2021-10-27T04:43:03.254361+0000     2     128     35773     35645   69.6096   74.3789  0.00675126  0.00716227
2021-10-27T04:43:04.254464+0000     3     128     53699     53571   69.7452   70.0234  0.00869495  0.00715577
2021-10-27T04:43:05.254568+0000     4     128     70477     70349   68.6919   65.5391   0.0092209  0.00726635
2021-10-27T04:43:06.254677+0000     5     128     87848     87720   68.5232   67.8555  0.00784367  0.00729019
2021-10-27T04:43:07.254779+0000     6     127    103961    103834   67.5925   62.9453  0.00708339  0.00738889
2021-10-27T04:43:08.254931+0000     7     128    121229    121101   67.5705   67.4492  0.00823498  0.00739192
2021-10-27T04:43:09.255054+0000     8     128    138631    138503   67.6203   67.9766  0.00728257  0.00738716
2021-10-27T04:43:10.255239+0000 Total time run:       8.71185
Total reads made:     150359
Read size:            4096
Object size:          4096
Bandwidth (MB/sec):   67.4185
Average IOPS:         17259
Stddev IOPS:          894.56
Max IOPS:             19041
Min IOPS:             16114
Average Latency(s):   0.00741155
Max latency(s):       0.0266503
Min latency(s):       0.000998688

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:43:10,762538594-07:00] INFO: > Stop system activity collection services[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:43:10,767897577-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:379 - rados_bench() > kill -INT 759738

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:43:10,773534283-07:00] INFO: >> for OSD host[0m
# ./benchmarks/bench-rados:384 - rados_bench() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.5 kill -INT 45191
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:43:10,781933431-07:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.5 kill -INT 45191
[1] 21:43:11 [SUCCESS] ljishen@10.10.2.5

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:43:11,866632966-07:00] INFO: >>> Copy system activity file from OSD host[0m
## ./benchmarks/bench-rados:391 - rados_bench() > basename -- /tmp/bench-rados/sys_activity_4KB_seq.dat.osd_host.3
# ./benchmarks/bench-rados:391 - rados_bench() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.5:/tmp/bench-rados/sys_activity_4KB_seq.dat.osd_host.3 /mnt/sda8/ceph-research/scripts/output/sys_activity_4KB_seq.dat.osd_host.3


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:43:12,929809523-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:43:35,949453638-07:00] INFO: >>   data:
    pools:   2 pools, 256 pgs
    objects: 150.36k objects, 587 MiB
    usage:   1.9 GiB used, 98 GiB / 100 GiB avail
    pgs:     256 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:43:35,956864008-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:43:44,073857627-07:00] INFO: >>   data:
    pools:   2 pools, 256 pgs
    objects: 150.36k objects, 587 MiB
    usage:   1.9 GiB used, 98 GiB / 100 GiB avail
    pgs:     256 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:43:44,081605181-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:43:52,296125533-07:00] INFO: >>   data:
    pools:   2 pools, 256 pgs
    objects: 150.36k objects, 587 MiB
    usage:   1.9 GiB used, 98 GiB / 100 GiB avail
    pgs:     256 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:43:52,303473695-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:44:00,453744985-07:00] INFO: >>   data:
    pools:   2 pools, 256 pgs
    objects: 150.36k objects, 587 MiB
    usage:   1.9 GiB used, 98 GiB / 100 GiB avail
    pgs:     256 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:44:00,460918940-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:44:08,801571398-07:00] INFO: >>   data:
    pools:   2 pools, 256 pgs
    objects: 150.36k objects, 587 MiB
    usage:   1.9 GiB used, 98 GiB / 100 GiB avail
    pgs:     256 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:44:08,809482859-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:44:08,815351852-07:00] INFO: > The cluster is idle now.[0m


[1;7;39;49m[2021-10-26T21:44:08,819111058-07:00][RUNNING][ROUND 1/2/21] object_size=16KB[0m


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:44:08,822207488-07:00] STAGE: Launch a Ceph cluster (CEPH_HOST: 10.10.2.3, OSD_HOST: 10.10.2.5) ...[0m
# ./benchmarks/bench-rados:172 - launch_ceph_cluster() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.3 bash -euo pipefail
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:44:08,830617077-07:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.3 bash -euo pipefail
10.10.2.3: b'\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T00:44:09,236580372-04:00] INFO: > Remove the existing cluster\x1b[0m\n'
10.10.2.3: b'## bash:13 -  > ls /var/lib/ceph\n'
10.10.2.3: b'## bash:13 -  > tail -n1\n'
10.10.2.3: b'# bash:13 -  > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm --host 10.10.2.3 --host 10.10.2.5 rm-cluster --force --fsid 82488196-36df-11ec-b561-09a495c6360a\n'
10.10.2.3: b'\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T00:44:16,923113923-04:00] INFO: > Deploy a new cluster\x1b[0m\n'
10.10.2.3: b'## bash:22 -  > /tmp/bench-rados/ceph-research/scripts/parallel-ssh --host 10.10.2.5 uname --nodename\n'
10.10.2.3: b'## bash:22 -  > tail -n1\n'
10.10.2.3: b'# bash:22 -  > osd_hostname=sm1\n# bash:23 -  > /tmp/bench-rados/ceph-research/scripts/ceph-deploy -m 10.10.2.3 -o sm1/10.10.2.5:/dev/nvme0n1\n'
10.10.2.3: b'\n\n\x1b[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T00:44:18,041983445-04:00] STAGE: Verify passwordless SSH to hosts, passwordless sudo via ljishen, and the existence of devices on hosts...\x1b[0m\n'
10.10.2.3: b'\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T00:44:18,045689279-04:00] INFO: > Check the host for the monitor\x1b[0m\n'
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:181 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@10.10.2.3 sudo --non-interactive --validate\n'
10.10.2.3: b'\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T00:44:18,195829832-04:00] INFO: > Check hosts for OSDs\x1b[0m\n'
10.10.2.3: b"\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T00:44:18,199761451-04:00] INFO: >> Check host 'sm1'\x1b[0m\n# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:200 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 sudo --non-interactive --validate\n"
10.10.2.3: b"\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T00:44:19,268174933-04:00] INFO: >>> Check device '/dev/nvme0n1'\x1b[0m\n# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:215 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 test -b /dev/nvme0n1\n"
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:226 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 lsblk --nodeps --noheadings --output TYPE /dev/nvme0n1\n'
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:227 - verify_parameters() > grep --word-regexp disk\n'
10.10.2.3: b'\n\n\x1b[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T00:44:21,405063244-04:00] STAGE: Reset devices on hosts...\x1b[0m\n'
10.10.2.3: b"\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T00:44:21,408798853-04:00] INFO: > Resetting device '/dev/nvme0n1' on host 'sm1'\x1b[0m\n"
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:288 - reset_host_devices() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 sudo sh\n'
10.10.2.3: b'  Removing ceph--2cb6416f--f30a--42d7--b854--02c0fd372277-osd--block--a82947df--09cb--471b--9f46--d8afb07edc46 (252:0)\n'
10.10.2.3: b'  Archiving volume group "ceph-2cb6416f-f30a-42d7-b854-02c0fd372277" metadata (seqno 5).\n'
10.10.2.3: b'  Releasing logical volume "osd-block-a82947df-09cb-471b-9f46-d8afb07edc46"\n'
10.10.2.3: b'  Creating volume group backup "/etc/lvm/backup/ceph-2cb6416f-f30a-42d7-b854-02c0fd372277" (seqno 6).\n'
10.10.2.3: b'  Logical volume "osd-block-a82947df-09cb-471b-9f46-d8afb07edc46" successfully removed\n'
10.10.2.3: b'  Removing physical volume "/dev/nvme0n1" from volume group "ceph-2cb6416f-f30a-42d7-b854-02c0fd372277"\n'
10.10.2.3: b'  Volume group "ceph-2cb6416f-f30a-42d7-b854-02c0fd372277" successfully removed\n'
10.10.2.3: b'\n\n\x1b[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T00:44:23,664264451-04:00] STAGE: Bootstrap a new cluster...\x1b[0m\n'
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:312 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.3 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults\n'
10.10.2.3: b'\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T00:44:23,672749359-04:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.3: b"\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T00:44:23,675528045-04:00] DEBUG: CEPHADM_PARAMS: ['bootstrap', '--config', '/tmp/bench-rados/ceph-research/scripts/ceph.conf', '--mon-ip', '10.10.2.3', '--output-dir', '/tmp/bench-rados/deployment_data_root/etc/ceph', '--ssh-user', 'ljishen', '--skip-dashboard', '--no-minimize-config', '--allow-overwrite', '--allow-fqdn-hostname', '--orphan-initial-daemons', '--skip-monitoring-stack', '--single-host-defaults']\x1b[0m\n"
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.3 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults\n'
10.10.2.3: b'Verifying podman|docker is present...\nVerifying lvm2 is present...\nVerifying time synchronization is in place...\n'
10.10.2.3: b'Unit ntp.service is enabled and running\nRepeating the final host check...\n'
10.10.2.3: b'podman|docker (/usr/bin/docker) is present\n'
10.10.2.3: b'systemctl is present\n'
10.10.2.3: b'lvcreate is present\n'
10.10.2.3: b'Unit ntp.service is enabled and running\nHost looks OK\n'
10.10.2.3: b'Cluster fsid: 8edd14de-36e0-11ec-b561-09a495c6360a\n'
10.10.2.3: b'Verifying IP 10.10.2.3 port 3300 ...\n'
10.10.2.3: b'Verifying IP 10.10.2.3 port 6789 ...\n'
10.10.2.3: b'Mon IP `10.10.2.3` is in CIDR network `10.10.2.0/24`\n- internal network (--cluster-network) has not been provided, OSD replication will default to the public_network\n'
10.10.2.3: b'Adjusting default settings to suit single-host cluster...\nPulling container image quay.io/ceph/ceph:v16...\n'
10.10.2.3: b'Ceph version: ceph version 16.2.6 (ee28fb57e47e9f88813e24bbf4c14496ca299d31) pacific (stable)\n'
10.10.2.3: b'Extracting ceph user uid/gid from container image...\n'
10.10.2.3: b'Creating initial keys...\n'
10.10.2.3: b'Creating initial monmap...\n'
10.10.2.3: b'Creating mon...\n'
10.10.2.3: b'Waiting for mon to start...\nWaiting for mon...\n'
10.10.2.3: b'mon is available\nSetting mon public_network to 10.10.2.0/24\n'
10.10.2.3: b'Wrote config to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf\nWrote keyring to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring\nCreating mgr...\n'
10.10.2.3: b'Verifying port 9283 ...\n'
10.10.2.3: b'Waiting for mgr to start...\nWaiting for mgr...\n'
10.10.2.3: b'mgr not available, waiting (1/15)...\n'
10.10.2.3: b'mgr not available, waiting (2/15)...\n'
10.10.2.3: b'mgr is available\n'
10.10.2.3: b'Enabling cephadm module...\n'
10.10.2.3: b'Waiting for the mgr to restart...\nWaiting for mgr epoch 5...\n'
10.10.2.3: b'mgr epoch 5 is available\n'
10.10.2.3: b'Setting orchestrator backend to cephadm...\n'
10.10.2.3: b'Generating ssh key...\n'
10.10.2.3: b'Wrote public SSH key to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub\n'
10.10.2.3: b'Adding key to ljishen@localhost authorized_keys...\n'
10.10.2.3: b'Adding host node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us...\n'
10.10.2.3: b'Deploying unmanaged mon service...\n'
10.10.2.3: b'Deploying unmanaged mgr service...\n'
10.10.2.3: b'You can access the Ceph CLI with:\n\n\tsudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --fsid 8edd14de-36e0-11ec-b561-09a495c6360a -c /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf -k /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring\n\n'
10.10.2.3: b'Please consider enabling telemetry to help improve Ceph:\n\n\tceph telemetry on\n\nFor more information see:\n\n\thttps://docs.ceph.com/docs/pacific/mgr/telemetry/\n\n'
10.10.2.3: b'Bootstrap complete.\n'
10.10.2.3: b'\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T00:45:28,752071949-04:00] INFO: > Set the default number of monitor daemons to 1\x1b[0m\n# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:330 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1\n'
10.10.2.3: b'\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T00:45:28,760543453-04:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.3: b"\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T00:45:28,763496097-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mon', '1']\x1b[0m\n# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1\n"
10.10.2.3: b'Inferring fsid 8edd14de-36e0-11ec-b561-09a495c6360a\n'
10.10.2.3: b'Inferring config /var/lib/ceph/8edd14de-36e0-11ec-b561-09a495c6360a/mon.node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.3: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.3: b'Scheduled mon update...\n'
10.10.2.3: b'\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T00:45:38,733141304-04:00] INFO: > Set the default number of manager daemons to 1\x1b[0m\n# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:335 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1\n'
10.10.2.3: b'\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T00:45:38,741780273-04:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.3: b"\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T00:45:38,744809772-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mgr', '1']\x1b[0m\n"
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1\n'
10.10.2.3: b'Inferring fsid 8edd14de-36e0-11ec-b561-09a495c6360a\n'
10.10.2.3: b'Inferring config /var/lib/ceph/8edd14de-36e0-11ec-b561-09a495c6360a/mon.node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.3: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.3: b'Scheduled mgr update...\n'
10.10.2.3: b'\n\n\x1b[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T00:45:48,895154367-04:00] STAGE: Add hosts to the cluster...\x1b[0m\n'
10.10.2.3: b"\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T00:45:48,899738005-04:00] INFO: > Adding host 'sm1'\x1b[0m\n"
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:362 - add_hosts() > ssh-copy-id -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -f -i /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub ljishen@sm1\n'
10.10.2.3: b'/usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: "/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub"\n'
10.10.2.3: b'\nNumber of key(s) added: 1\n\nNow try logging into the machine, with:   "ssh -o \'GlobalKnownHostsFile=/dev/null\' -o \'LogLevel=ERROR\' -o \'PasswordAuthentication=no\' -o \'StrictHostKeyChecking=no\' -o \'UserKnownHostsFile=/dev/null\' \'ljishen@sm1\'"\nand check to make sure that only the key(s) you wanted were added.\n\n'
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:368 - add_hosts() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add sm1 10.10.2.5\n'
10.10.2.3: b'\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T00:45:50,032256355-04:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.3: b"\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T00:45:50,035124049-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'host', 'add', 'sm1', '10.10.2.5']\x1b[0m\n"
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add sm1 10.10.2.5\n'
10.10.2.3: b'Inferring fsid 8edd14de-36e0-11ec-b561-09a495c6360a\n'
10.10.2.3: b'Inferring config /var/lib/ceph/8edd14de-36e0-11ec-b561-09a495c6360a/mon.node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.3: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.3: b"Added host 'sm1' with addr '10.10.2.5'\n"
10.10.2.3: b'\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T00:46:01,065210773-04:00] INFO: Sleep for 20 seconds\x1b[0m\n'
10.10.2.3: b'\n\n\x1b[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T00:46:21,068741651-04:00] STAGE: Add OSDs to the cluster...\x1b[0m\n'
10.10.2.3: b"\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T00:46:21,073693722-04:00] INFO: > Adding devices ['/dev/nvme0n1'] on host 'sm1' as OSDs\x1b[0m\n# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:387 - add_host_devices_as_osds() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd sm1:/dev/nvme0n1\n"
10.10.2.3: b'\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T00:46:21,081895338-04:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.3: b"\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T00:46:21,084640781-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'daemon', 'add', 'osd', 'sm1:/dev/nvme0n1']\x1b[0m\n"
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd sm1:/dev/nvme0n1\n'
10.10.2.3: b'Inferring fsid 8edd14de-36e0-11ec-b561-09a495c6360a\n'
10.10.2.3: b'Inferring config /var/lib/ceph/8edd14de-36e0-11ec-b561-09a495c6360a/mon.node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.3: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.3: b"Created osd(s) 0 on host 'sm1'\n"
10.10.2.3: b'\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T00:46:44,537740988-04:00] INFO: > Sleep for 20 seconds\x1b[0m\n'
10.10.2.3: b'\n\n\x1b[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T00:47:04,542373191-04:00] STAGE: Show Cluster Status\x1b[0m\n# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:343 - show_status() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status\n'
10.10.2.3: b'\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T00:47:04,550561422-04:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.3: b"\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T00:47:04,553665501-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', '--status']\x1b[0m\n"
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status\n'
10.10.2.3: b'Inferring fsid 8edd14de-36e0-11ec-b561-09a495c6360a\n'
10.10.2.3: b'Inferring config /var/lib/ceph/8edd14de-36e0-11ec-b561-09a495c6360a/mon.node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.3: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.3: b'  cluster:\n    id:     8edd14de-36e0-11ec-b561-09a495c6360a\n    health: HEALTH_WARN\n            1 pool(s) have no replicas configured\n \n  services:\n    mon: 1 daemons, quorum node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us (age 2m)\n    mgr: node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us.ldwrqg(active, since 2m)\n    osd: 1 osds: 1 up (since 20s), 1 in (since 40s)\n \n  data:\n    pools:   1 pools, 128 pgs\n    objects: 0 objects, 0 B\n    usage:   4.7 MiB used, 100 GiB / 100 GiB avail\n    pgs:     99.219% pgs unknown\n             127 unknown\n             1   active+clean\n \n  progress:\n    Global Recovery Event (0s)\n      [............................] \n \n'
[1] 21:47:13 [SUCCESS] ljishen@10.10.2.3

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T00:44:09,236580372-04:00] INFO: > Remove the existing cluster[0m
## bash:13 -  > ls /var/lib/ceph
## bash:13 -  > tail -n1
# bash:13 -  > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm --host 10.10.2.3 --host 10.10.2.5 rm-cluster --force --fsid 82488196-36df-11ec-b561-09a495c6360a

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T00:44:16,923113923-04:00] INFO: > Deploy a new cluster[0m
## bash:22 -  > /tmp/bench-rados/ceph-research/scripts/parallel-ssh --host 10.10.2.5 uname --nodename
## bash:22 -  > tail -n1
# bash:22 -  > osd_hostname=sm1
# bash:23 -  > /tmp/bench-rados/ceph-research/scripts/ceph-deploy -m 10.10.2.3 -o sm1/10.10.2.5:/dev/nvme0n1


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T00:44:18,041983445-04:00] STAGE: Verify passwordless SSH to hosts, passwordless sudo via ljishen, and the existence of devices on hosts...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T00:44:18,045689279-04:00] INFO: > Check the host for the monitor[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:181 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@10.10.2.3 sudo --non-interactive --validate

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T00:44:18,195829832-04:00] INFO: > Check hosts for OSDs[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T00:44:18,199761451-04:00] INFO: >> Check host 'sm1'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:200 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 sudo --non-interactive --validate

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T00:44:19,268174933-04:00] INFO: >>> Check device '/dev/nvme0n1'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:215 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 test -b /dev/nvme0n1
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:226 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 lsblk --nodeps --noheadings --output TYPE /dev/nvme0n1
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:227 - verify_parameters() > grep --word-regexp disk


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T00:44:21,405063244-04:00] STAGE: Reset devices on hosts...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T00:44:21,408798853-04:00] INFO: > Resetting device '/dev/nvme0n1' on host 'sm1'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:288 - reset_host_devices() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 sudo sh
  Removing ceph--2cb6416f--f30a--42d7--b854--02c0fd372277-osd--block--a82947df--09cb--471b--9f46--d8afb07edc46 (252:0)
  Archiving volume group "ceph-2cb6416f-f30a-42d7-b854-02c0fd372277" metadata (seqno 5).
  Releasing logical volume "osd-block-a82947df-09cb-471b-9f46-d8afb07edc46"
  Creating volume group backup "/etc/lvm/backup/ceph-2cb6416f-f30a-42d7-b854-02c0fd372277" (seqno 6).
  Logical volume "osd-block-a82947df-09cb-471b-9f46-d8afb07edc46" successfully removed
  Removing physical volume "/dev/nvme0n1" from volume group "ceph-2cb6416f-f30a-42d7-b854-02c0fd372277"
  Volume group "ceph-2cb6416f-f30a-42d7-b854-02c0fd372277" successfully removed


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T00:44:23,664264451-04:00] STAGE: Bootstrap a new cluster...[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:312 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.3 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T00:44:23,672749359-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T00:44:23,675528045-04:00] DEBUG: CEPHADM_PARAMS: ['bootstrap', '--config', '/tmp/bench-rados/ceph-research/scripts/ceph.conf', '--mon-ip', '10.10.2.3', '--output-dir', '/tmp/bench-rados/deployment_data_root/etc/ceph', '--ssh-user', 'ljishen', '--skip-dashboard', '--no-minimize-config', '--allow-overwrite', '--allow-fqdn-hostname', '--orphan-initial-daemons', '--skip-monitoring-stack', '--single-host-defaults'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.3 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults
Verifying podman|docker is present...
Verifying lvm2 is present...
Verifying time synchronization is in place...
Unit ntp.service is enabled and running
Repeating the final host check...
podman|docker (/usr/bin/docker) is present
systemctl is present
lvcreate is present
Unit ntp.service is enabled and running
Host looks OK
Cluster fsid: 8edd14de-36e0-11ec-b561-09a495c6360a
Verifying IP 10.10.2.3 port 3300 ...
Verifying IP 10.10.2.3 port 6789 ...
Mon IP `10.10.2.3` is in CIDR network `10.10.2.0/24`
- internal network (--cluster-network) has not been provided, OSD replication will default to the public_network
Adjusting default settings to suit single-host cluster...
Pulling container image quay.io/ceph/ceph:v16...
Ceph version: ceph version 16.2.6 (ee28fb57e47e9f88813e24bbf4c14496ca299d31) pacific (stable)
Extracting ceph user uid/gid from container image...
Creating initial keys...
Creating initial monmap...
Creating mon...
Waiting for mon to start...
Waiting for mon...
mon is available
Setting mon public_network to 10.10.2.0/24
Wrote config to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf
Wrote keyring to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring
Creating mgr...
Verifying port 9283 ...
Waiting for mgr to start...
Waiting for mgr...
mgr not available, waiting (1/15)...
mgr not available, waiting (2/15)...
mgr is available
Enabling cephadm module...
Waiting for the mgr to restart...
Waiting for mgr epoch 5...
mgr epoch 5 is available
Setting orchestrator backend to cephadm...
Generating ssh key...
Wrote public SSH key to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub
Adding key to ljishen@localhost authorized_keys...
Adding host node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us...
Deploying unmanaged mon service...
Deploying unmanaged mgr service...
You can access the Ceph CLI with:

	sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --fsid 8edd14de-36e0-11ec-b561-09a495c6360a -c /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf -k /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring

Please consider enabling telemetry to help improve Ceph:

	ceph telemetry on

For more information see:

	https://docs.ceph.com/docs/pacific/mgr/telemetry/

Bootstrap complete.

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T00:45:28,752071949-04:00] INFO: > Set the default number of monitor daemons to 1[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:330 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T00:45:28,760543453-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T00:45:28,763496097-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mon', '1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1
Inferring fsid 8edd14de-36e0-11ec-b561-09a495c6360a
Inferring config /var/lib/ceph/8edd14de-36e0-11ec-b561-09a495c6360a/mon.node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Scheduled mon update...

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T00:45:38,733141304-04:00] INFO: > Set the default number of manager daemons to 1[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:335 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T00:45:38,741780273-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T00:45:38,744809772-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mgr', '1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1
Inferring fsid 8edd14de-36e0-11ec-b561-09a495c6360a
Inferring config /var/lib/ceph/8edd14de-36e0-11ec-b561-09a495c6360a/mon.node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Scheduled mgr update...


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T00:45:48,895154367-04:00] STAGE: Add hosts to the cluster...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T00:45:48,899738005-04:00] INFO: > Adding host 'sm1'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:362 - add_hosts() > ssh-copy-id -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -f -i /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub ljishen@sm1
/usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: "/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub"

Number of key(s) added: 1

Now try logging into the machine, with:   "ssh -o 'GlobalKnownHostsFile=/dev/null' -o 'LogLevel=ERROR' -o 'PasswordAuthentication=no' -o 'StrictHostKeyChecking=no' -o 'UserKnownHostsFile=/dev/null' 'ljishen@sm1'"
and check to make sure that only the key(s) you wanted were added.

# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:368 - add_hosts() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add sm1 10.10.2.5
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T00:45:50,032256355-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T00:45:50,035124049-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'host', 'add', 'sm1', '10.10.2.5'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add sm1 10.10.2.5
Inferring fsid 8edd14de-36e0-11ec-b561-09a495c6360a
Inferring config /var/lib/ceph/8edd14de-36e0-11ec-b561-09a495c6360a/mon.node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Added host 'sm1' with addr '10.10.2.5'

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T00:46:01,065210773-04:00] INFO: Sleep for 20 seconds[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T00:46:21,068741651-04:00] STAGE: Add OSDs to the cluster...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T00:46:21,073693722-04:00] INFO: > Adding devices ['/dev/nvme0n1'] on host 'sm1' as OSDs[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:387 - add_host_devices_as_osds() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd sm1:/dev/nvme0n1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T00:46:21,081895338-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T00:46:21,084640781-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'daemon', 'add', 'osd', 'sm1:/dev/nvme0n1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd sm1:/dev/nvme0n1
Inferring fsid 8edd14de-36e0-11ec-b561-09a495c6360a
Inferring config /var/lib/ceph/8edd14de-36e0-11ec-b561-09a495c6360a/mon.node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Created osd(s) 0 on host 'sm1'

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T00:46:44,537740988-04:00] INFO: > Sleep for 20 seconds[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T00:47:04,542373191-04:00] STAGE: Show Cluster Status[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:343 - show_status() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T00:47:04,550561422-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T00:47:04,553665501-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', '--status'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status
Inferring fsid 8edd14de-36e0-11ec-b561-09a495c6360a
Inferring config /var/lib/ceph/8edd14de-36e0-11ec-b561-09a495c6360a/mon.node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
  cluster:
    id:     8edd14de-36e0-11ec-b561-09a495c6360a
    health: HEALTH_WARN
            1 pool(s) have no replicas configured
 
  services:
    mon: 1 daemons, quorum node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us (age 2m)
    mgr: node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us.ldwrqg(active, since 2m)
    osd: 1 osds: 1 up (since 20s), 1 in (since 40s)
 
  data:
    pools:   1 pools, 128 pgs
    objects: 0 objects, 0 B
    usage:   4.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     99.219% pgs unknown
             127 unknown
             1   active+clean
 
  progress:
    Global Recovery Event (0s)
      [............................] 
 

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:47:13,974123198-07:00] INFO: > Copy configuration files to local[0m
# ./benchmarks/bench-rados:208 - launch_ceph_cluster() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.3:/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf
# ./benchmarks/bench-rados:212 - launch_ceph_cluster() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.3 sudo chmod 644 /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:47:14,131349845-07:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.3 sudo chmod 644 /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring
[1] 21:47:14 [SUCCESS] ljishen@10.10.2.3
# ./benchmarks/bench-rados:214 - launch_ceph_cluster() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.3:/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:47:14,457956794-07:00] STAGE: Update local ceph.conf as a client...[0m


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:47:14,461638184-07:00] STAGE: Configure Ceph pool...[0m
# ./benchmarks/bench-rados:245 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_max_backfills 32
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:47:14,485937320-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:47:14,489449603-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '8edd14de-36e0-11ec-b561-09a495c6360a', '--', 'ceph', 'config', 'set', 'osd', 'osd_max_backfills', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 8edd14de-36e0-11ec-b561-09a495c6360a -- ceph config set osd osd_max_backfills 32
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:246 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_active 32
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:47:17,585240307-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:47:17,588576488-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '8edd14de-36e0-11ec-b561-09a495c6360a', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_active', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 8edd14de-36e0-11ec-b561-09a495c6360a -- ceph config set osd osd_recovery_max_active 32
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:247 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_single_start 8
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:47:20,917827870-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:47:20,921672247-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '8edd14de-36e0-11ec-b561-09a495c6360a', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_single_start', '8'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 8edd14de-36e0-11ec-b561-09a495c6360a -- ceph config set osd osd_recovery_max_single_start 8
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:248 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_op_priority 63
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:47:24,184725974-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:47:24,187956396-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '8edd14de-36e0-11ec-b561-09a495c6360a', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_op_priority', '63'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 8edd14de-36e0-11ec-b561-09a495c6360a -- ceph config set osd osd_recovery_op_priority 63
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:250 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config show-with-defaults osd.0
# ./benchmarks/bench-rados:251 - configure_ceph_pool() > grep 'osd_max_backfills\|osd_recovery'
# ./benchmarks/bench-rados:251 - configure_ceph_pool() > column -t -s ' '
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
osd_max_backfills                        32         mon
osd_recovery_cost                        20971520   default
osd_recovery_delay_start                 0.000000   default
osd_recovery_max_active                  32         mon
osd_recovery_max_active_hdd              3          default
osd_recovery_max_active_ssd              10         default
osd_recovery_max_chunk                   8388608    default
osd_recovery_max_omap_entries_per_chunk  8096       default
osd_recovery_max_single_start            8          mon
osd_recovery_op_priority                 63         mon
osd_recovery_op_warn_multiple            16         default
osd_recovery_priority                    5          default
osd_recovery_retry_interval              30.000000  default
osd_recovery_sleep                       0.000000   default
osd_recovery_sleep_hdd                   0.100000   default
osd_recovery_sleep_hybrid                0.025000   default
osd_recovery_sleep_ssd                   0.000000   default
# ./benchmarks/bench-rados:253 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool create bench_rados 128 128 replicated --size 1 --pg-num-min 128
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:47:30,618057773-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:47:30,621942204-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '8edd14de-36e0-11ec-b561-09a495c6360a', '--', 'ceph', 'osd', 'pool', 'create', 'bench_rados', '128', '128', 'replicated', '--size', '1', '--pg-num-min', '128'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 8edd14de-36e0-11ec-b561-09a495c6360a -- ceph osd pool create bench_rados 128 128 replicated --size 1 --pg-num-min 128
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
pool 'bench_rados' created
# ./benchmarks/bench-rados:255 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados min_size 1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:47:34,145398232-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:47:34,148805167-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '8edd14de-36e0-11ec-b561-09a495c6360a', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'min_size', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 8edd14de-36e0-11ec-b561-09a495c6360a -- ceph osd pool set bench_rados min_size 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 min_size to 1
# ./benchmarks/bench-rados:256 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados pg_autoscale_mode off
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:47:38,375458931-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:47:38,378693461-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '8edd14de-36e0-11ec-b561-09a495c6360a', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'pg_autoscale_mode', 'off'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 8edd14de-36e0-11ec-b561-09a495c6360a -- ceph osd pool set bench_rados pg_autoscale_mode off
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 pg_autoscale_mode to off
# ./benchmarks/bench-rados:257 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool application enable bench_rados benchmark
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:47:41,934685760-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:47:41,938214402-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '8edd14de-36e0-11ec-b561-09a495c6360a', '--', 'ceph', 'osd', 'pool', 'application', 'enable', 'bench_rados', 'benchmark'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 8edd14de-36e0-11ec-b561-09a495c6360a -- ceph osd pool application enable bench_rados benchmark
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
enabled application 'benchmark' on pool 'bench_rados'
# ./benchmarks/bench-rados:258 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados noscrub 1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:47:46,107974932-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:47:46,110938002-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '8edd14de-36e0-11ec-b561-09a495c6360a', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'noscrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 8edd14de-36e0-11ec-b561-09a495c6360a -- ceph osd pool set bench_rados noscrub 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 noscrub to 1
# ./benchmarks/bench-rados:259 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados nodeep-scrub 1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:47:50,356948762-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:47:50,359824257-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '8edd14de-36e0-11ec-b561-09a495c6360a', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'nodeep-scrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 8edd14de-36e0-11ec-b561-09a495c6360a -- ceph osd pool set bench_rados nodeep-scrub 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 nodeep-scrub to 1
# ./benchmarks/bench-rados:260 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool ls detail
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:47:54,683575461-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:47:54,686811654-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '8edd14de-36e0-11ec-b561-09a495c6360a', '--', 'ceph', 'osd', 'pool', 'ls', 'detail'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 8edd14de-36e0-11ec-b561-09a495c6360a -- ceph osd pool ls detail
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
pool 1 'device_health_metrics' replicated size 1 min_size 1 crush_rule 0 object_hash rjenkins pg_num 128 pgp_num 128 autoscale_mode on last_change 13 lfor 0/0/11 flags hashpspool stripe_width 0 pg_num_min 1 application mgr_devicehealth
pool 2 'bench_rados' replicated size 1 min_size 1 crush_rule 0 object_hash rjenkins pg_num 128 pgp_num 128 autoscale_mode off last_change 21 flags hashpspool,noscrub,nodeep-scrub stripe_width 0 pg_num_min 128 application benchmark

# ./benchmarks/bench-rados:261 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd df tree
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:47:57,958168479-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:47:57,961885055-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '8edd14de-36e0-11ec-b561-09a495c6360a', '--', 'ceph', 'osd', 'df', 'tree'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 8edd14de-36e0-11ec-b561-09a495c6360a -- ceph osd df tree
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
ID  CLASS  WEIGHT   REWEIGHT  SIZE     RAW USE  DATA     OMAP  META     AVAIL    %USE  VAR   PGS  STATUS  TYPE NAME    
-1         0.09769         -  100 GiB  6.0 MiB  176 KiB   0 B  5.8 MiB  100 GiB  0.01  1.00    -          root default 
-3         0.09769         -  100 GiB  6.0 MiB  176 KiB   0 B  5.8 MiB  100 GiB  0.01  1.00    -              host sm1 
 0    ssd  0.09769   1.00000  100 GiB  6.0 MiB  176 KiB   0 B  5.8 MiB  100 GiB  0.01  1.00  256      up          osd.0
                       TOTAL  100 GiB  6.0 MiB  176 KiB   0 B  5.8 MiB  100 GiB  0.01                                  
MIN/MAX VAR: 1.00/1.00  STDDEV: 0


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:48:01,099435177-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:48:24,363530672-07:00] INFO: >>   data:
    pools:   2 pools, 256 pgs
    objects: 0 objects, 0 B
    usage:   6.0 MiB used, 100 GiB / 100 GiB avail
    pgs:     256 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:48:24,371069042-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:48:32,620844872-07:00] INFO: >>   data:
    pools:   2 pools, 256 pgs
    objects: 0 objects, 0 B
    usage:   6.0 MiB used, 100 GiB / 100 GiB avail
    pgs:     256 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:48:32,628723442-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:48:40,869641361-07:00] INFO: >>   data:
    pools:   2 pools, 256 pgs
    objects: 0 objects, 0 B
    usage:   6.0 MiB used, 100 GiB / 100 GiB avail
    pgs:     256 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:48:40,877524790-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:48:49,101447452-07:00] INFO: >>   data:
    pools:   2 pools, 256 pgs
    objects: 0 objects, 0 B
    usage:   6.0 MiB used, 100 GiB / 100 GiB avail
    pgs:     256 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:48:49,109414679-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:48:57,340778929-07:00] INFO: >>   data:
    pools:   2 pools, 256 pgs
    objects: 0 objects, 0 B
    usage:   6.0 MiB used, 100 GiB / 100 GiB avail
    pgs:     256 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:48:57,348267496-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:48:57,354214274-07:00] INFO: > The cluster is idle now.[0m


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:48:57,358021200-07:00] STAGE: Start benchmarking of mode write ...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:48:57,364614645-07:00] INFO: > Start system activity collection services[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:48:57,369895231-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:329 - rados_bench() > rm --force /mnt/sda8/ceph-research/scripts/output/sys_activity_16KB_write.dat.rados_bench_host.1
# ./benchmarks/bench-rados:330 - rados_bench() > sar_pid_rados_bench_host=766515
# ./benchmarks/bench-rados:330 - rados_bench() > S_TIME_FORMAT=ISO
# ./benchmarks/bench-rados:330 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/output/sys_activity_16KB_write.dat.rados_bench_host.1 2

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:48:57,375915698-07:00] INFO: >> for OSD host[0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:48:57,384460730-07:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.5 bash
10.10.2.5: b'50096\n'
[1] 21:48:58 [SUCCESS] ljishen@10.10.2.5
50096

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:48:58,491387977-07:00] INFO: > Run rados bench[0m
# ./benchmarks/bench-rados:358 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/output/rados_bench_16KB_write.log.1
## ./benchmarks/bench-rados:349 - rados_bench() > count_cpus 32-63,96-127
# ./benchmarks/bench-rados:349 - rados_bench() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 30 write --pool bench_rados -b 16384 -O 16384 --concurrent-ios 128 --show-time --write-object --write-omap --write-xattr --no-cleanup
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:48:58,511285609-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:48:58,514390696-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '8edd14de-36e0-11ec-b561-09a495c6360a', '--', 'rados', 'bench', '30', 'write', '--pool', 'bench_rados', '-b', '16384', '-O', '16384', '--concurrent-ios', '128', '--show-time', '--write-object', '--write-omap', '--write-xattr', '--no-cleanup'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 8edd14de-36e0-11ec-b561-09a495c6360a -- rados bench 30 write --pool bench_rados -b 16384 -O 16384 --concurrent-ios 128 --show-time --write-object --write-omap --write-xattr --no-cleanup
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
hints = 1
2021-10-27T04:49:01.035487+0000 Maintaining 128 concurrent writes of 16384 bytes to objects of size 16384 for up to 30 seconds or 0 objects
2021-10-27T04:49:01.035498+0000 Object prefix: benchmark_data_node-0.bluefield2.ucsc-cmps10_7
2021-10-27T04:49:01.036459+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-27T04:49:01.036459+0000     0       0         0         0         0         0           -           0
2021-10-27T04:49:02.036573+0000     1     128      3263      3135   48.9781   48.9844   0.0412873   0.0401231
2021-10-27T04:49:03.036670+0000     2     127      6335      6208   48.4946   48.0156   0.0421704   0.0406359
2021-10-27T04:49:04.036774+0000     3     128      9308      9180   47.8073   46.4375    0.036521   0.0415333
2021-10-27T04:49:05.036861+0000     4     128     12245     12117   47.3271   45.8906   0.0441066   0.0419863
2021-10-27T04:49:06.036970+0000     5     128     15295     15167   47.3919   47.6562   0.0393135   0.0420488
2021-10-27T04:49:07.037080+0000     6     128     18239     18111   47.1591        46   0.0420477   0.0422436
2021-10-27T04:49:08.037168+0000     7     128     21182     21054   46.9907   45.9844   0.0399506   0.0423954
2021-10-27T04:49:09.037275+0000     8     128     24156     24028   46.9248   46.4688   0.0442479   0.0424934
2021-10-27T04:49:10.037366+0000     9     128     27135     27007   46.8824   46.5469   0.0398264   0.0425399
2021-10-27T04:49:11.037451+0000    10     128     30079     29951   46.7937        46   0.0491021    0.042653
2021-10-27T04:49:12.037558+0000    11     128     32196     32068   45.5465   33.0781    0.084849   0.0437805
2021-10-27T04:49:13.037637+0000    12     128     33670     33542   43.6701   23.0312   0.0885536   0.0456374
2021-10-27T04:49:14.037726+0000    13     128     35185     35057   42.1317   23.6719   0.0727999   0.0473312
2021-10-27T04:49:15.037804+0000    14     128     36676     36548   40.7862   23.2969   0.0791204   0.0489342
2021-10-27T04:49:16.037895+0000    15     128     38084     37956   39.5337        22   0.0967203   0.0503875
2021-10-27T04:49:17.037990+0000    16     128     39430     39302   38.3772   21.0312   0.0916553   0.0518978
2021-10-27T04:49:18.038082+0000    17     128     40838     40710   37.4137        22   0.0785859   0.0532911
2021-10-27T04:49:19.038340+0000    18     128     42246     42118   36.5569        22   0.0890382   0.0545356
2021-10-27T04:49:20.038433+0000    19     128     44216     44088   36.2528   30.7812   0.0449687   0.0551187
2021-10-27T04:49:21.038503+0000 min lat: 0.0224169 max lat: 0.261186 avg lat: 0.0542987
2021-10-27T04:49:21.038503+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-27T04:49:21.038503+0000    20     128     47234     47106   36.7978   47.1562   0.0436602   0.0542987
2021-10-27T04:49:22.038602+0000    21     128     50244     50116   37.2849   47.0312   0.0412184   0.0535786
2021-10-27T04:49:23.038703+0000    22     128     53188     53060   37.6808        46   0.0437683   0.0530355
2021-10-27T04:49:24.038793+0000    23     128     56132     56004   38.0423        46   0.0434796   0.0525319
2021-10-27T04:49:25.038861+0000    24     128     59010     58882   38.3308   44.9688   0.0395716   0.0521127
2021-10-27T04:49:26.038954+0000    25     128     61969     61841   38.6467   46.2344   0.0413744   0.0516957
2021-10-27T04:49:27.039050+0000    26     128     63498     63370   38.0791   23.8906   0.0872006   0.0524094
2021-10-27T04:49:28.039287+0000    27     128     64369     64241   37.1726   13.6094     0.29888   0.0533902
2021-10-27T04:49:29.039398+0000    28     128     64881     64753   36.1307         8    0.253102   0.0550556
2021-10-27T04:49:30.039505+0000    29     128     65393     65265   35.1606         8    0.259538   0.0564794
2021-10-27T04:49:31.039710+0000    30     128     65905     65777   34.2551         8    0.228082   0.0581316
2021-10-27T04:49:32.039859+0000 Total time run:         30.1964
Total writes made:      65905
Write size:             16384
Object size:            16384
Bandwidth (MB/sec):     34.1022
Stddev Bandwidth:       14.4388
Max bandwidth (MB/sec): 48.9844
Min bandwidth (MB/sec): 8
Average IOPS:           2182
Stddev IOPS:            924.081
Max IOPS:               3135
Min IOPS:               512
Average Latency(s):     0.0585036
Stddev Latency(s):      0.0396829
Max latency(s):         0.310039
Min latency(s):         0.0224169

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:49:32,590358521-07:00] INFO: > Stop system activity collection services[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:49:32,596138205-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:379 - rados_bench() > kill -INT 766515

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:49:32,601778297-07:00] INFO: >> for OSD host[0m
# ./benchmarks/bench-rados:384 - rados_bench() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.5 kill -INT 50096
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:49:32,609640956-07:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.5 kill -INT 50096
[1] 21:49:33 [SUCCESS] ljishen@10.10.2.5

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:49:33,973433147-07:00] INFO: >>> Copy system activity file from OSD host[0m
## ./benchmarks/bench-rados:391 - rados_bench() > basename -- /tmp/bench-rados/sys_activity_16KB_write.dat.osd_host.1
# ./benchmarks/bench-rados:391 - rados_bench() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.5:/tmp/bench-rados/sys_activity_16KB_write.dat.osd_host.1 /mnt/sda8/ceph-research/scripts/output/sys_activity_16KB_write.dat.osd_host.1


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:49:35,370507168-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:49:58,601469484-07:00] INFO: >>   data:
    pools:   2 pools, 256 pgs
    objects: 65.91k objects, 1.0 GiB
    usage:   3.1 GiB used, 97 GiB / 100 GiB avail
    pgs:     256 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:49:58,608960485-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:50:06,948633859-07:00] INFO: >>   data:
    pools:   2 pools, 256 pgs
    objects: 65.91k objects, 1.0 GiB
    usage:   3.1 GiB used, 97 GiB / 100 GiB avail
    pgs:     256 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:50:06,955919534-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:50:15,255843038-07:00] INFO: >>   data:
    pools:   2 pools, 256 pgs
    objects: 65.91k objects, 1.0 GiB
    usage:   3.1 GiB used, 97 GiB / 100 GiB avail
    pgs:     256 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:50:15,263647879-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:50:23,437183576-07:00] INFO: >>   data:
    pools:   2 pools, 256 pgs
    objects: 65.91k objects, 1.0 GiB
    usage:   3.1 GiB used, 97 GiB / 100 GiB avail
    pgs:     256 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:50:23,445138610-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:50:31,636879757-07:00] INFO: >>   data:
    pools:   2 pools, 256 pgs
    objects: 65.91k objects, 1.0 GiB
    usage:   3.1 GiB used, 97 GiB / 100 GiB avail
    pgs:     256 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:50:31,644228971-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:50:31,650083045-07:00] INFO: > The cluster is idle now.[0m


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:50:31,653594266-07:00] STAGE: Start benchmarking of mode seq ...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:50:31,660236031-07:00] INFO: > Start system activity collection services[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:50:31,665981320-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:329 - rados_bench() > rm --force /mnt/sda8/ceph-research/scripts/output/sys_activity_16KB_seq.dat.rados_bench_host.1
# ./benchmarks/bench-rados:330 - rados_bench() > sar_pid_rados_bench_host=768492
# ./benchmarks/bench-rados:330 - rados_bench() > S_TIME_FORMAT=ISO
# ./benchmarks/bench-rados:330 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/output/sys_activity_16KB_seq.dat.rados_bench_host.1 2

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:50:31,672812761-07:00] INFO: >> for OSD host[0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:50:31,681152258-07:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.5 bash
10.10.2.5: b'50494\n'
[1] 21:50:32 [SUCCESS] ljishen@10.10.2.5
50494

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:50:32,803515160-07:00] INFO: > Run rados bench[0m
# ./benchmarks/bench-rados:367 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/output/rados_bench_16KB_seq.log.1
## ./benchmarks/bench-rados:364 - rados_bench() > count_cpus 32-63,96-127
# ./benchmarks/bench-rados:364 - rados_bench() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 128 --show-time
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:50:32,823432610-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:50:32,826978024-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '8edd14de-36e0-11ec-b561-09a495c6360a', '--', 'rados', 'bench', '99999999', 'seq', '--pool', 'bench_rados', '--concurrent-ios', '128', '--show-time'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 8edd14de-36e0-11ec-b561-09a495c6360a -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 128 --show-time
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
hints = 1
2021-10-27T04:50:35.214436+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-27T04:50:35.214436+0000     0       0         0         0         0         0           -           0
2021-10-27T04:50:36.214758+0000     1     128     14364     14236   222.352   222.438   0.0082567  0.00894836
2021-10-27T04:50:37.214834+0000     2     128     29643     29515   230.533   238.734  0.00647415  0.00864897
2021-10-27T04:50:38.214923+0000     3     128     44001     43873   228.463   224.344   0.0104425   0.0087355
2021-10-27T04:50:39.214991+0000     4     128     58623     58495   228.461   228.469  0.00845084  0.00874296
2021-10-27T04:50:40.215122+0000 Total time run:       4.50767
Total reads made:     65905
Read size:            16384
Object size:          16384
Bandwidth (MB/sec):   228.447
Average IOPS:         14620
Stddev IOPS:          465.592
Max IOPS:             15279
Min IOPS:             14236
Average Latency(s):   0.0087435
Max latency(s):       0.0192121
Min latency(s):       0.00229314

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:50:40,766915053-07:00] INFO: > Stop system activity collection services[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:50:40,772797391-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:379 - rados_bench() > kill -INT 768492

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:50:40,777989430-07:00] INFO: >> for OSD host[0m
# ./benchmarks/bench-rados:384 - rados_bench() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.5 kill -INT 50494
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:50:40,785978087-07:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.5 kill -INT 50494
[1] 21:50:41 [SUCCESS] ljishen@10.10.2.5

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:50:41,930750473-07:00] INFO: >>> Copy system activity file from OSD host[0m
## ./benchmarks/bench-rados:391 - rados_bench() > basename -- /tmp/bench-rados/sys_activity_16KB_seq.dat.osd_host.1
# ./benchmarks/bench-rados:391 - rados_bench() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.5:/tmp/bench-rados/sys_activity_16KB_seq.dat.osd_host.1 /mnt/sda8/ceph-research/scripts/output/sys_activity_16KB_seq.dat.osd_host.1


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:50:43,029312325-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:51:06,286600739-07:00] INFO: >>   data:
    pools:   2 pools, 256 pgs
    objects: 65.91k objects, 1.0 GiB
    usage:   3.1 GiB used, 97 GiB / 100 GiB avail
    pgs:     256 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:51:06,294123199-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:51:14,625000167-07:00] INFO: >>   data:
    pools:   2 pools, 256 pgs
    objects: 65.91k objects, 1.0 GiB
    usage:   3.1 GiB used, 97 GiB / 100 GiB avail
    pgs:     256 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:51:14,632679432-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:51:22,900357812-07:00] INFO: >>   data:
    pools:   2 pools, 256 pgs
    objects: 65.91k objects, 1.0 GiB
    usage:   3.1 GiB used, 97 GiB / 100 GiB avail
    pgs:     256 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:51:22,907821893-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:51:31,022936352-07:00] INFO: >>   data:
    pools:   2 pools, 256 pgs
    objects: 65.91k objects, 1.0 GiB
    usage:   3.1 GiB used, 97 GiB / 100 GiB avail
    pgs:     256 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:51:31,030257924-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:51:39,188134685-07:00] INFO: >>   data:
    pools:   2 pools, 256 pgs
    objects: 65.91k objects, 1.0 GiB
    usage:   3.1 GiB used, 97 GiB / 100 GiB avail
    pgs:     256 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:51:39,195944625-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:51:39,202113812-07:00] INFO: > The cluster is idle now.[0m


[1;7;39;49m[2021-10-26T21:51:39,204488346-07:00][RUNNING][ROUND 2/2/21] object_size=16KB[0m


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:51:39,208210823-07:00] STAGE: Launch a Ceph cluster (CEPH_HOST: 10.10.2.3, OSD_HOST: 10.10.2.5) ...[0m
# ./benchmarks/bench-rados:172 - launch_ceph_cluster() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.3 bash -euo pipefail
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:51:39,217568112-07:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.3 bash -euo pipefail
10.10.2.3: b'\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T00:51:39,637489159-04:00] INFO: > Remove the existing cluster\x1b[0m\n'
10.10.2.3: b'## bash:13 -  > ls /var/lib/ceph\n'
10.10.2.3: b'## bash:13 -  > tail -n1\n'
10.10.2.3: b'# bash:13 -  > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm --host 10.10.2.3 --host 10.10.2.5 rm-cluster --force --fsid 8edd14de-36e0-11ec-b561-09a495c6360a\n'
10.10.2.3: b'\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T00:51:48,386709005-04:00] INFO: > Deploy a new cluster\x1b[0m\n'
10.10.2.3: b'## bash:22 -  > /tmp/bench-rados/ceph-research/scripts/parallel-ssh --host 10.10.2.5 uname --nodename\n'
10.10.2.3: b'## bash:22 -  > tail -n1\n'
10.10.2.3: b'# bash:22 -  > osd_hostname=sm1\n# bash:23 -  > /tmp/bench-rados/ceph-research/scripts/ceph-deploy -m 10.10.2.3 -o sm1/10.10.2.5:/dev/nvme0n1\n'
10.10.2.3: b'\n\n\x1b[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T00:51:49,508644895-04:00] STAGE: Verify passwordless SSH to hosts, passwordless sudo via ljishen, and the existence of devices on hosts...\x1b[0m\n'
10.10.2.3: b'\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T00:51:49,512784255-04:00] INFO: > Check the host for the monitor\x1b[0m\n'
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:181 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@10.10.2.3 sudo --non-interactive --validate\n'
10.10.2.3: b'\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T00:51:49,663532834-04:00] INFO: > Check hosts for OSDs\x1b[0m\n'
10.10.2.3: b"\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T00:51:49,667177352-04:00] INFO: >> Check host 'sm1'\x1b[0m\n"
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:200 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 sudo --non-interactive --validate\n'
10.10.2.3: b"\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T00:51:50,739968374-04:00] INFO: >>> Check device '/dev/nvme0n1'\x1b[0m\n"
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:215 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 test -b /dev/nvme0n1\n'
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:226 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 lsblk --nodeps --noheadings --output TYPE /dev/nvme0n1\n'
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:227 - verify_parameters() > grep --word-regexp disk\n'
10.10.2.3: b'\n\n\x1b[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T00:51:52,865398965-04:00] STAGE: Reset devices on hosts...\x1b[0m\n'
10.10.2.3: b"\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T00:51:52,869177134-04:00] INFO: > Resetting device '/dev/nvme0n1' on host 'sm1'\x1b[0m\n# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:288 - reset_host_devices() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 sudo sh\n"
10.10.2.3: b'  Removing ceph--1d7a60d4--66a2--4551--948d--17b9332e9389-osd--block--65dcd4a2--93e1--440b--a016--2c3e53411ac0 (252:0)\n'
10.10.2.3: b'  Archiving volume group "ceph-1d7a60d4-66a2-4551-948d-17b9332e9389" metadata (seqno 5).\n'
10.10.2.3: b'  Releasing logical volume "osd-block-65dcd4a2-93e1-440b-a016-2c3e53411ac0"\n'
10.10.2.3: b'  Creating volume group backup "/etc/lvm/backup/ceph-1d7a60d4-66a2-4551-948d-17b9332e9389" (seqno 6).\n'
10.10.2.3: b'  Logical volume "osd-block-65dcd4a2-93e1-440b-a016-2c3e53411ac0" successfully removed\n'
10.10.2.3: b'  Removing physical volume "/dev/nvme0n1" from volume group "ceph-1d7a60d4-66a2-4551-948d-17b9332e9389"\n'
10.10.2.3: b'  Volume group "ceph-1d7a60d4-66a2-4551-948d-17b9332e9389" successfully removed\n'
10.10.2.3: b'\n\n\x1b[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T00:51:55,174450659-04:00] STAGE: Bootstrap a new cluster...\x1b[0m\n# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:312 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.3 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults\n'
10.10.2.3: b'\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T00:51:55,182660760-04:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.3: b"\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T00:51:55,185579460-04:00] DEBUG: CEPHADM_PARAMS: ['bootstrap', '--config', '/tmp/bench-rados/ceph-research/scripts/ceph.conf', '--mon-ip', '10.10.2.3', '--output-dir', '/tmp/bench-rados/deployment_data_root/etc/ceph', '--ssh-user', 'ljishen', '--skip-dashboard', '--no-minimize-config', '--allow-overwrite', '--allow-fqdn-hostname', '--orphan-initial-daemons', '--skip-monitoring-stack', '--single-host-defaults']\x1b[0m\n"
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.3 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults\n'
10.10.2.3: b'Verifying podman|docker is present...\nVerifying lvm2 is present...\nVerifying time synchronization is in place...\n'
10.10.2.3: b'Unit ntp.service is enabled and running\nRepeating the final host check...\npodman|docker (/usr/bin/docker) is present\n'
10.10.2.3: b'systemctl is present\nlvcreate is present\n'
10.10.2.3: b'Unit ntp.service is enabled and running\nHost looks OK\n'
10.10.2.3: b'Cluster fsid: 9bfbc1c8-36e1-11ec-b561-09a495c6360a\n'
10.10.2.3: b'Verifying IP 10.10.2.3 port 3300 ...\nVerifying IP 10.10.2.3 port 6789 ...\n'
10.10.2.3: b'Mon IP `10.10.2.3` is in CIDR network `10.10.2.0/24`\n- internal network (--cluster-network) has not been provided, OSD replication will default to the public_network\n'
10.10.2.3: b'Adjusting default settings to suit single-host cluster...\nPulling container image quay.io/ceph/ceph:v16...\n'
10.10.2.3: b'Ceph version: ceph version 16.2.6 (ee28fb57e47e9f88813e24bbf4c14496ca299d31) pacific (stable)\nExtracting ceph user uid/gid from container image...\n'
10.10.2.3: b'Creating initial keys...\n'
10.10.2.3: b'Creating initial monmap...\n'
10.10.2.3: b'Creating mon...\n'
10.10.2.3: b'Waiting for mon to start...\nWaiting for mon...\n'
10.10.2.3: b'mon is available\nSetting mon public_network to 10.10.2.0/24\n'
10.10.2.3: b'Wrote config to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf\nWrote keyring to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring\n'
10.10.2.3: b'Creating mgr...\n'
10.10.2.3: b'Verifying port 9283 ...\n'
10.10.2.3: b'Waiting for mgr to start...\nWaiting for mgr...\n'
10.10.2.3: b'mgr not available, waiting (1/15)...\n'
10.10.2.3: b'mgr not available, waiting (2/15)...\n'
10.10.2.3: b'mgr is available\n'
10.10.2.3: b'Enabling cephadm module...\n'
10.10.2.3: b'Waiting for the mgr to restart...\nWaiting for mgr epoch 5...\n'
10.10.2.3: b'mgr epoch 5 is available\nSetting orchestrator backend to cephadm...\n'
10.10.2.3: b'Generating ssh key...\n'
10.10.2.3: b'Wrote public SSH key to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub\nAdding key to ljishen@localhost authorized_keys...\n'
10.10.2.3: b'Adding host node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us...\n'
10.10.2.3: b'Deploying unmanaged mon service...\n'
10.10.2.3: b'Deploying unmanaged mgr service...\n'
10.10.2.3: b'You can access the Ceph CLI with:\n\n\tsudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --fsid 9bfbc1c8-36e1-11ec-b561-09a495c6360a -c /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf -k /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring\n\nPlease consider enabling telemetry to help improve Ceph:\n\n\tceph telemetry on\n\nFor more information see:\n\n\thttps://docs.ceph.com/docs/pacific/mgr/telemetry/\n\nBootstrap complete.\n'
10.10.2.3: b'\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T00:52:59,242548725-04:00] INFO: > Set the default number of monitor daemons to 1\x1b[0m\n# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:330 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1\n'
10.10.2.3: b'\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T00:52:59,251574343-04:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.3: b"\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T00:52:59,254451324-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mon', '1']\x1b[0m\n"
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1\n'
10.10.2.3: b'Inferring fsid 9bfbc1c8-36e1-11ec-b561-09a495c6360a\n'
10.10.2.3: b'Inferring config /var/lib/ceph/9bfbc1c8-36e1-11ec-b561-09a495c6360a/mon.node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.3: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.3: b'Scheduled mon update...\n'
10.10.2.3: b'\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T00:53:09,055831496-04:00] INFO: > Set the default number of manager daemons to 1\x1b[0m\n# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:335 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1\n'
10.10.2.3: b'\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T00:53:09,064228018-04:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.3: b"\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T00:53:09,067159212-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mgr', '1']\x1b[0m\n"
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1\n'
10.10.2.3: b'Inferring fsid 9bfbc1c8-36e1-11ec-b561-09a495c6360a\n'
10.10.2.3: b'Inferring config /var/lib/ceph/9bfbc1c8-36e1-11ec-b561-09a495c6360a/mon.node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.3: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.3: b'Scheduled mgr update...\n'
10.10.2.3: b'\n\n\x1b[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T00:53:18,530587038-04:00] STAGE: Add hosts to the cluster...\x1b[0m\n'
10.10.2.3: b"\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T00:53:18,536015838-04:00] INFO: > Adding host 'sm1'\x1b[0m\n# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:362 - add_hosts() > ssh-copy-id -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -f -i /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub ljishen@sm1\n"
10.10.2.3: b'/usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: "/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub"\n'
10.10.2.3: b'\nNumber of key(s) added: 1\n\nNow try logging into the machine, with:   "ssh -o \'GlobalKnownHostsFile=/dev/null\' -o \'LogLevel=ERROR\' -o \'PasswordAuthentication=no\' -o \'StrictHostKeyChecking=no\' -o \'UserKnownHostsFile=/dev/null\' \'ljishen@sm1\'"\nand check to make sure that only the key(s) you wanted were added.\n\n'
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:368 - add_hosts() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add sm1 10.10.2.5\n'
10.10.2.3: b'\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T00:53:19,672252517-04:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.3: b"\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T00:53:19,675021936-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'host', 'add', 'sm1', '10.10.2.5']\x1b[0m\n"
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add sm1 10.10.2.5\n'
10.10.2.3: b'Inferring fsid 9bfbc1c8-36e1-11ec-b561-09a495c6360a\n'
10.10.2.3: b'Inferring config /var/lib/ceph/9bfbc1c8-36e1-11ec-b561-09a495c6360a/mon.node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.3: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.3: b"Added host 'sm1' with addr '10.10.2.5'\n"
10.10.2.3: b'\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T00:53:30,801768722-04:00] INFO: Sleep for 20 seconds\x1b[0m\n'
10.10.2.3: b'\n\n\x1b[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T00:53:50,805382311-04:00] STAGE: Add OSDs to the cluster...\x1b[0m\n'
10.10.2.3: b"\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T00:53:50,810445942-04:00] INFO: > Adding devices ['/dev/nvme0n1'] on host 'sm1' as OSDs\x1b[0m\n# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:387 - add_host_devices_as_osds() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd sm1:/dev/nvme0n1\n"
10.10.2.3: b'\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T00:53:50,818791498-04:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.3: b"\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T00:53:50,821748020-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'daemon', 'add', 'osd', 'sm1:/dev/nvme0n1']\x1b[0m\n"
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd sm1:/dev/nvme0n1\n'
10.10.2.3: b'Inferring fsid 9bfbc1c8-36e1-11ec-b561-09a495c6360a\n'
10.10.2.3: b'Inferring config /var/lib/ceph/9bfbc1c8-36e1-11ec-b561-09a495c6360a/mon.node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.3: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.3: b"Created osd(s) 0 on host 'sm1'\n"
10.10.2.3: b'\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T00:54:14,373012151-04:00] INFO: > Sleep for 20 seconds\x1b[0m\n'
10.10.2.3: b'\n\n\x1b[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T00:54:34,377243643-04:00] STAGE: Show Cluster Status\x1b[0m\n'
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:343 - show_status() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status\n'
10.10.2.3: b'\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T00:54:34,385507485-04:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.3: b"\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T00:54:34,388187034-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', '--status']\x1b[0m\n"
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status\n'
10.10.2.3: b'Inferring fsid 9bfbc1c8-36e1-11ec-b561-09a495c6360a\n'
10.10.2.3: b'Inferring config /var/lib/ceph/9bfbc1c8-36e1-11ec-b561-09a495c6360a/mon.node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.3: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.3: b'  cluster:\n    id:     9bfbc1c8-36e1-11ec-b561-09a495c6360a\n    health: HEALTH_WARN\n            1 pool(s) have no replicas configured\n \n  services:\n    mon: 1 daemons, quorum node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us (age 2m)\n    mgr: node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us.aajzzq(active, since 2m)\n    osd: 1 osds: 1 up (since 20s), 1 in (since 39s)\n \n  data:\n    pools:   1 pools, 128 pgs\n    objects: 0 objects, 0 B\n    usage:   4.7 MiB used, 100 GiB / 100 GiB avail\n    pgs:     99.219% pgs unknown\n             127 unknown\n             1   active+clean\n \n  progress:\n \n'
[1] 21:54:43 [SUCCESS] ljishen@10.10.2.3

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T00:51:39,637489159-04:00] INFO: > Remove the existing cluster[0m
## bash:13 -  > ls /var/lib/ceph
## bash:13 -  > tail -n1
# bash:13 -  > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm --host 10.10.2.3 --host 10.10.2.5 rm-cluster --force --fsid 8edd14de-36e0-11ec-b561-09a495c6360a

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T00:51:48,386709005-04:00] INFO: > Deploy a new cluster[0m
## bash:22 -  > /tmp/bench-rados/ceph-research/scripts/parallel-ssh --host 10.10.2.5 uname --nodename
## bash:22 -  > tail -n1
# bash:22 -  > osd_hostname=sm1
# bash:23 -  > /tmp/bench-rados/ceph-research/scripts/ceph-deploy -m 10.10.2.3 -o sm1/10.10.2.5:/dev/nvme0n1


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T00:51:49,508644895-04:00] STAGE: Verify passwordless SSH to hosts, passwordless sudo via ljishen, and the existence of devices on hosts...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T00:51:49,512784255-04:00] INFO: > Check the host for the monitor[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:181 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@10.10.2.3 sudo --non-interactive --validate

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T00:51:49,663532834-04:00] INFO: > Check hosts for OSDs[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T00:51:49,667177352-04:00] INFO: >> Check host 'sm1'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:200 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 sudo --non-interactive --validate

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T00:51:50,739968374-04:00] INFO: >>> Check device '/dev/nvme0n1'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:215 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 test -b /dev/nvme0n1
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:226 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 lsblk --nodeps --noheadings --output TYPE /dev/nvme0n1
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:227 - verify_parameters() > grep --word-regexp disk


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T00:51:52,865398965-04:00] STAGE: Reset devices on hosts...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T00:51:52,869177134-04:00] INFO: > Resetting device '/dev/nvme0n1' on host 'sm1'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:288 - reset_host_devices() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 sudo sh
  Removing ceph--1d7a60d4--66a2--4551--948d--17b9332e9389-osd--block--65dcd4a2--93e1--440b--a016--2c3e53411ac0 (252:0)
  Archiving volume group "ceph-1d7a60d4-66a2-4551-948d-17b9332e9389" metadata (seqno 5).
  Releasing logical volume "osd-block-65dcd4a2-93e1-440b-a016-2c3e53411ac0"
  Creating volume group backup "/etc/lvm/backup/ceph-1d7a60d4-66a2-4551-948d-17b9332e9389" (seqno 6).
  Logical volume "osd-block-65dcd4a2-93e1-440b-a016-2c3e53411ac0" successfully removed
  Removing physical volume "/dev/nvme0n1" from volume group "ceph-1d7a60d4-66a2-4551-948d-17b9332e9389"
  Volume group "ceph-1d7a60d4-66a2-4551-948d-17b9332e9389" successfully removed


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T00:51:55,174450659-04:00] STAGE: Bootstrap a new cluster...[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:312 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.3 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T00:51:55,182660760-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T00:51:55,185579460-04:00] DEBUG: CEPHADM_PARAMS: ['bootstrap', '--config', '/tmp/bench-rados/ceph-research/scripts/ceph.conf', '--mon-ip', '10.10.2.3', '--output-dir', '/tmp/bench-rados/deployment_data_root/etc/ceph', '--ssh-user', 'ljishen', '--skip-dashboard', '--no-minimize-config', '--allow-overwrite', '--allow-fqdn-hostname', '--orphan-initial-daemons', '--skip-monitoring-stack', '--single-host-defaults'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.3 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults
Verifying podman|docker is present...
Verifying lvm2 is present...
Verifying time synchronization is in place...
Unit ntp.service is enabled and running
Repeating the final host check...
podman|docker (/usr/bin/docker) is present
systemctl is present
lvcreate is present
Unit ntp.service is enabled and running
Host looks OK
Cluster fsid: 9bfbc1c8-36e1-11ec-b561-09a495c6360a
Verifying IP 10.10.2.3 port 3300 ...
Verifying IP 10.10.2.3 port 6789 ...
Mon IP `10.10.2.3` is in CIDR network `10.10.2.0/24`
- internal network (--cluster-network) has not been provided, OSD replication will default to the public_network
Adjusting default settings to suit single-host cluster...
Pulling container image quay.io/ceph/ceph:v16...
Ceph version: ceph version 16.2.6 (ee28fb57e47e9f88813e24bbf4c14496ca299d31) pacific (stable)
Extracting ceph user uid/gid from container image...
Creating initial keys...
Creating initial monmap...
Creating mon...
Waiting for mon to start...
Waiting for mon...
mon is available
Setting mon public_network to 10.10.2.0/24
Wrote config to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf
Wrote keyring to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring
Creating mgr...
Verifying port 9283 ...
Waiting for mgr to start...
Waiting for mgr...
mgr not available, waiting (1/15)...
mgr not available, waiting (2/15)...
mgr is available
Enabling cephadm module...
Waiting for the mgr to restart...
Waiting for mgr epoch 5...
mgr epoch 5 is available
Setting orchestrator backend to cephadm...
Generating ssh key...
Wrote public SSH key to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub
Adding key to ljishen@localhost authorized_keys...
Adding host node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us...
Deploying unmanaged mon service...
Deploying unmanaged mgr service...
You can access the Ceph CLI with:

	sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --fsid 9bfbc1c8-36e1-11ec-b561-09a495c6360a -c /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf -k /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring

Please consider enabling telemetry to help improve Ceph:

	ceph telemetry on

For more information see:

	https://docs.ceph.com/docs/pacific/mgr/telemetry/

Bootstrap complete.

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T00:52:59,242548725-04:00] INFO: > Set the default number of monitor daemons to 1[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:330 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T00:52:59,251574343-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T00:52:59,254451324-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mon', '1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1
Inferring fsid 9bfbc1c8-36e1-11ec-b561-09a495c6360a
Inferring config /var/lib/ceph/9bfbc1c8-36e1-11ec-b561-09a495c6360a/mon.node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Scheduled mon update...

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T00:53:09,055831496-04:00] INFO: > Set the default number of manager daemons to 1[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:335 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T00:53:09,064228018-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T00:53:09,067159212-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mgr', '1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1
Inferring fsid 9bfbc1c8-36e1-11ec-b561-09a495c6360a
Inferring config /var/lib/ceph/9bfbc1c8-36e1-11ec-b561-09a495c6360a/mon.node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Scheduled mgr update...


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T00:53:18,530587038-04:00] STAGE: Add hosts to the cluster...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T00:53:18,536015838-04:00] INFO: > Adding host 'sm1'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:362 - add_hosts() > ssh-copy-id -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -f -i /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub ljishen@sm1
/usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: "/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub"

Number of key(s) added: 1

Now try logging into the machine, with:   "ssh -o 'GlobalKnownHostsFile=/dev/null' -o 'LogLevel=ERROR' -o 'PasswordAuthentication=no' -o 'StrictHostKeyChecking=no' -o 'UserKnownHostsFile=/dev/null' 'ljishen@sm1'"
and check to make sure that only the key(s) you wanted were added.

# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:368 - add_hosts() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add sm1 10.10.2.5
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T00:53:19,672252517-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T00:53:19,675021936-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'host', 'add', 'sm1', '10.10.2.5'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add sm1 10.10.2.5
Inferring fsid 9bfbc1c8-36e1-11ec-b561-09a495c6360a
Inferring config /var/lib/ceph/9bfbc1c8-36e1-11ec-b561-09a495c6360a/mon.node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Added host 'sm1' with addr '10.10.2.5'

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T00:53:30,801768722-04:00] INFO: Sleep for 20 seconds[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T00:53:50,805382311-04:00] STAGE: Add OSDs to the cluster...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T00:53:50,810445942-04:00] INFO: > Adding devices ['/dev/nvme0n1'] on host 'sm1' as OSDs[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:387 - add_host_devices_as_osds() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd sm1:/dev/nvme0n1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T00:53:50,818791498-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T00:53:50,821748020-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'daemon', 'add', 'osd', 'sm1:/dev/nvme0n1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd sm1:/dev/nvme0n1
Inferring fsid 9bfbc1c8-36e1-11ec-b561-09a495c6360a
Inferring config /var/lib/ceph/9bfbc1c8-36e1-11ec-b561-09a495c6360a/mon.node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Created osd(s) 0 on host 'sm1'

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T00:54:14,373012151-04:00] INFO: > Sleep for 20 seconds[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T00:54:34,377243643-04:00] STAGE: Show Cluster Status[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:343 - show_status() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T00:54:34,385507485-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T00:54:34,388187034-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', '--status'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status
Inferring fsid 9bfbc1c8-36e1-11ec-b561-09a495c6360a
Inferring config /var/lib/ceph/9bfbc1c8-36e1-11ec-b561-09a495c6360a/mon.node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
  cluster:
    id:     9bfbc1c8-36e1-11ec-b561-09a495c6360a
    health: HEALTH_WARN
            1 pool(s) have no replicas configured
 
  services:
    mon: 1 daemons, quorum node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us (age 2m)
    mgr: node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us.aajzzq(active, since 2m)
    osd: 1 osds: 1 up (since 20s), 1 in (since 39s)
 
  data:
    pools:   1 pools, 128 pgs
    objects: 0 objects, 0 B
    usage:   4.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     99.219% pgs unknown
             127 unknown
             1   active+clean
 
  progress:
 

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:54:43,111774697-07:00] INFO: > Copy configuration files to local[0m
# ./benchmarks/bench-rados:208 - launch_ceph_cluster() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.3:/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf
# ./benchmarks/bench-rados:212 - launch_ceph_cluster() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.3 sudo chmod 644 /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:54:43,267062259-07:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.3 sudo chmod 644 /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring
[1] 21:54:43 [SUCCESS] ljishen@10.10.2.3
# ./benchmarks/bench-rados:214 - launch_ceph_cluster() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.3:/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:54:43,594612852-07:00] STAGE: Update local ceph.conf as a client...[0m


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:54:43,598583717-07:00] STAGE: Configure Ceph pool...[0m
# ./benchmarks/bench-rados:245 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_max_backfills 32
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:54:43,622159814-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:54:43,625437685-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '9bfbc1c8-36e1-11ec-b561-09a495c6360a', '--', 'ceph', 'config', 'set', 'osd', 'osd_max_backfills', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 9bfbc1c8-36e1-11ec-b561-09a495c6360a -- ceph config set osd osd_max_backfills 32
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:246 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_active 32
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:54:46,795116799-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:54:46,798424016-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '9bfbc1c8-36e1-11ec-b561-09a495c6360a', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_active', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 9bfbc1c8-36e1-11ec-b561-09a495c6360a -- ceph config set osd osd_recovery_max_active 32
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:247 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_single_start 8
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:54:50,063136299-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:54:50,066682024-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '9bfbc1c8-36e1-11ec-b561-09a495c6360a', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_single_start', '8'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 9bfbc1c8-36e1-11ec-b561-09a495c6360a -- ceph config set osd osd_recovery_max_single_start 8
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:248 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_op_priority 63
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:54:53,312178646-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:54:53,315089047-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '9bfbc1c8-36e1-11ec-b561-09a495c6360a', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_op_priority', '63'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 9bfbc1c8-36e1-11ec-b561-09a495c6360a -- ceph config set osd osd_recovery_op_priority 63
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:250 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config show-with-defaults osd.0
# ./benchmarks/bench-rados:251 - configure_ceph_pool() > grep 'osd_max_backfills\|osd_recovery'
# ./benchmarks/bench-rados:251 - configure_ceph_pool() > column -t -s ' '
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
osd_max_backfills                        32         mon
osd_recovery_cost                        20971520   default
osd_recovery_delay_start                 0.000000   default
osd_recovery_max_active                  32         mon
osd_recovery_max_active_hdd              3          default
osd_recovery_max_active_ssd              10         default
osd_recovery_max_chunk                   8388608    default
osd_recovery_max_omap_entries_per_chunk  8096       default
osd_recovery_max_single_start            8          mon
osd_recovery_op_priority                 63         mon
osd_recovery_op_warn_multiple            16         default
osd_recovery_priority                    5          default
osd_recovery_retry_interval              30.000000  default
osd_recovery_sleep                       0.000000   default
osd_recovery_sleep_hdd                   0.100000   default
osd_recovery_sleep_hybrid                0.025000   default
osd_recovery_sleep_ssd                   0.000000   default
# ./benchmarks/bench-rados:253 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool create bench_rados 128 128 replicated --size 1 --pg-num-min 128
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:54:59,628283123-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:54:59,632146887-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '9bfbc1c8-36e1-11ec-b561-09a495c6360a', '--', 'ceph', 'osd', 'pool', 'create', 'bench_rados', '128', '128', 'replicated', '--size', '1', '--pg-num-min', '128'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 9bfbc1c8-36e1-11ec-b561-09a495c6360a -- ceph osd pool create bench_rados 128 128 replicated --size 1 --pg-num-min 128
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
pool 'bench_rados' created
# ./benchmarks/bench-rados:255 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados min_size 1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:55:03,253618678-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:55:03,257213786-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '9bfbc1c8-36e1-11ec-b561-09a495c6360a', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'min_size', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 9bfbc1c8-36e1-11ec-b561-09a495c6360a -- ceph osd pool set bench_rados min_size 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 min_size to 1
# ./benchmarks/bench-rados:256 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados pg_autoscale_mode off
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:55:07,427510211-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:55:07,430618283-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '9bfbc1c8-36e1-11ec-b561-09a495c6360a', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'pg_autoscale_mode', 'off'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 9bfbc1c8-36e1-11ec-b561-09a495c6360a -- ceph osd pool set bench_rados pg_autoscale_mode off
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 pg_autoscale_mode to off
# ./benchmarks/bench-rados:257 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool application enable bench_rados benchmark
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:55:11,746876539-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:55:11,750631759-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '9bfbc1c8-36e1-11ec-b561-09a495c6360a', '--', 'ceph', 'osd', 'pool', 'application', 'enable', 'bench_rados', 'benchmark'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 9bfbc1c8-36e1-11ec-b561-09a495c6360a -- ceph osd pool application enable bench_rados benchmark
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
enabled application 'benchmark' on pool 'bench_rados'
# ./benchmarks/bench-rados:258 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados noscrub 1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:55:15,245389439-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:55:15,248913714-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '9bfbc1c8-36e1-11ec-b561-09a495c6360a', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'noscrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 9bfbc1c8-36e1-11ec-b561-09a495c6360a -- ceph osd pool set bench_rados noscrub 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 noscrub to 1
# ./benchmarks/bench-rados:259 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados nodeep-scrub 1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:55:19,551262764-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:55:19,554798981-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '9bfbc1c8-36e1-11ec-b561-09a495c6360a', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'nodeep-scrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 9bfbc1c8-36e1-11ec-b561-09a495c6360a -- ceph osd pool set bench_rados nodeep-scrub 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 nodeep-scrub to 1
# ./benchmarks/bench-rados:260 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool ls detail
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:55:23,643717436-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:55:23,646916720-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '9bfbc1c8-36e1-11ec-b561-09a495c6360a', '--', 'ceph', 'osd', 'pool', 'ls', 'detail'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 9bfbc1c8-36e1-11ec-b561-09a495c6360a -- ceph osd pool ls detail
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
pool 1 'device_health_metrics' replicated size 1 min_size 1 crush_rule 0 object_hash rjenkins pg_num 128 pgp_num 128 autoscale_mode on last_change 13 lfor 0/0/11 flags hashpspool stripe_width 0 pg_num_min 1 application mgr_devicehealth
pool 2 'bench_rados' replicated size 1 min_size 1 crush_rule 0 object_hash rjenkins pg_num 128 pgp_num 128 autoscale_mode off last_change 21 flags hashpspool,noscrub,nodeep-scrub stripe_width 0 pg_num_min 128 application benchmark

# ./benchmarks/bench-rados:261 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd df tree
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:55:27,085228035-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:55:27,088505135-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '9bfbc1c8-36e1-11ec-b561-09a495c6360a', '--', 'ceph', 'osd', 'df', 'tree'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 9bfbc1c8-36e1-11ec-b561-09a495c6360a -- ceph osd df tree
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
ID  CLASS  WEIGHT   REWEIGHT  SIZE     RAW USE  DATA     OMAP  META     AVAIL    %USE  VAR   PGS  STATUS  TYPE NAME    
-1         0.09769         -  100 GiB  6.0 MiB  176 KiB   0 B  5.8 MiB  100 GiB  0.01  1.00    -          root default 
-3         0.09769         -  100 GiB  6.0 MiB  176 KiB   0 B  5.8 MiB  100 GiB  0.01  1.00    -              host sm1 
 0    ssd  0.09769   1.00000  100 GiB  6.0 MiB  176 KiB   0 B  5.8 MiB  100 GiB  0.01  1.00  256      up          osd.0
                       TOTAL  100 GiB  6.0 MiB  176 KiB   0 B  5.8 MiB  100 GiB  0.01                                  
MIN/MAX VAR: 1.00/1.00  STDDEV: 0


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:55:30,337385378-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:55:53,569060237-07:00] INFO: >>   data:
    pools:   2 pools, 256 pgs
    objects: 0 objects, 0 B
    usage:   6.0 MiB used, 100 GiB / 100 GiB avail
    pgs:     256 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:55:53,576669540-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:56:01,842369950-07:00] INFO: >>   data:
    pools:   2 pools, 256 pgs
    objects: 0 objects, 0 B
    usage:   6.0 MiB used, 100 GiB / 100 GiB avail
    pgs:     256 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:56:01,850199618-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:56:10,217685589-07:00] INFO: >>   data:
    pools:   2 pools, 256 pgs
    objects: 0 objects, 0 B
    usage:   6.0 MiB used, 100 GiB / 100 GiB avail
    pgs:     256 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:56:10,225449543-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:56:18,508061702-07:00] INFO: >>   data:
    pools:   2 pools, 256 pgs
    objects: 0 objects, 0 B
    usage:   6.0 MiB used, 100 GiB / 100 GiB avail
    pgs:     256 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:56:18,515571028-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:56:26,690821865-07:00] INFO: >>   data:
    pools:   2 pools, 256 pgs
    objects: 0 objects, 0 B
    usage:   6.0 MiB used, 100 GiB / 100 GiB avail
    pgs:     256 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:56:26,698085599-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:56:26,703774362-07:00] INFO: > The cluster is idle now.[0m


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:56:26,707054949-07:00] STAGE: Start benchmarking of mode write ...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:56:26,714003721-07:00] INFO: > Start system activity collection services[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:56:26,719639776-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:329 - rados_bench() > rm --force /mnt/sda8/ceph-research/scripts/output/sys_activity_16KB_write.dat.rados_bench_host.2
# ./benchmarks/bench-rados:330 - rados_bench() > sar_pid_rados_bench_host=775421
# ./benchmarks/bench-rados:330 - rados_bench() > S_TIME_FORMAT=ISO
# ./benchmarks/bench-rados:330 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/output/sys_activity_16KB_write.dat.rados_bench_host.2 2

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:56:26,727161454-07:00] INFO: >> for OSD host[0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:56:26,736060833-07:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.5 bash
10.10.2.5: b'55285\n'
[1] 21:56:27 [SUCCESS] ljishen@10.10.2.5
55285

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:56:27,852619063-07:00] INFO: > Run rados bench[0m
# ./benchmarks/bench-rados:358 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/output/rados_bench_16KB_write.log.2
## ./benchmarks/bench-rados:349 - rados_bench() > count_cpus 32-63,96-127
# ./benchmarks/bench-rados:349 - rados_bench() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 30 write --pool bench_rados -b 16384 -O 16384 --concurrent-ios 128 --show-time --write-object --write-omap --write-xattr --no-cleanup
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:56:27,873712243-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:56:27,876781943-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '9bfbc1c8-36e1-11ec-b561-09a495c6360a', '--', 'rados', 'bench', '30', 'write', '--pool', 'bench_rados', '-b', '16384', '-O', '16384', '--concurrent-ios', '128', '--show-time', '--write-object', '--write-omap', '--write-xattr', '--no-cleanup'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 9bfbc1c8-36e1-11ec-b561-09a495c6360a -- rados bench 30 write --pool bench_rados -b 16384 -O 16384 --concurrent-ios 128 --show-time --write-object --write-omap --write-xattr --no-cleanup
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
hints = 1
2021-10-27T04:56:30.338082+0000 Maintaining 128 concurrent writes of 16384 bytes to objects of size 16384 for up to 30 seconds or 0 objects
2021-10-27T04:56:30.338090+0000 Object prefix: benchmark_data_node-0.bluefield2.ucsc-cmps10_7
2021-10-27T04:56:30.339022+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-27T04:56:30.339022+0000     0       0         0         0         0         0           -           0
2021-10-27T04:56:31.339163+0000     1     128      3061      2933   45.8206   45.8281   0.0363513   0.0429549
2021-10-27T04:56:32.339250+0000     2     128      6005      5877   45.9083        46    0.041753   0.0429683
2021-10-27T04:56:33.339347+0000     3     128      9018      8890   46.2967   47.0781    0.044371   0.0429835
2021-10-27T04:56:34.339432+0000     4     127     12005     11878   46.3934   46.6875   0.0415835   0.0429478
2021-10-27T04:56:35.339529+0000     5     128     14965     14837   46.3607   46.2344   0.0440284   0.0429261
2021-10-27T04:56:36.339612+0000     6     127     17944     17817   46.3937   46.5625   0.0403884   0.0429664
2021-10-27T04:56:37.339712+0000     7     128     20656     20528   45.8168   42.3594   0.0464064   0.0435096
2021-10-27T04:56:38.339799+0000     8     127     23611     23484   45.8626   46.1875   0.0412493   0.0434584
2021-10-27T04:56:39.339880+0000     9     128     26544     26416   45.8566   45.8125   0.0450286   0.0435166
2021-10-27T04:56:40.339980+0000    10     128     29488     29360   45.8705        46    0.042824   0.0435147
2021-10-27T04:56:41.340098+0000    11     128     31792     31664   44.9728        36   0.0879654   0.0442602
2021-10-27T04:56:42.340177+0000    12     128     33269     33141   43.1481   23.0781   0.0887789   0.0461827
2021-10-27T04:56:43.340268+0000    13     128     34711     34583   41.5621   22.5312   0.0935686    0.047956
2021-10-27T04:56:44.340352+0000    14     128     36144     36016   40.1925   22.3906   0.0917052   0.0495246
2021-10-27T04:56:45.340448+0000    15     128     37552     37424   38.9796        22   0.0943287    0.051121
2021-10-27T04:56:46.340776+0000    16     128     38901     38773   37.8601   21.0781   0.0841695   0.0526848
2021-10-27T04:56:47.340861+0000    17     128     40195     40067   36.8223   20.2188   0.0951861   0.0541205
2021-10-27T04:56:48.340942+0000    18     128     41520     41392   35.9267   20.7031    0.104693   0.0555115
2021-10-27T04:56:49.341051+0000    19     128     43440     43312   35.6146        30   0.0527978   0.0560857
2021-10-27T04:56:50.341136+0000 min lat: 0.0303902 max lat: 0.243986 avg lat: 0.0559937
2021-10-27T04:56:50.341136+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-27T04:56:50.341136+0000    20     128     45813     45685   35.6876   37.0781   0.0483716   0.0559937
2021-10-27T04:56:51.341242+0000    21     128     48757     48629   36.1784        46   0.0436627   0.0552089
2021-10-27T04:56:52.341327+0000    22     128     51744     51616   36.6552   46.6719   0.0421453   0.0545155
2021-10-27T04:56:53.341403+0000    23     128     54704     54576   37.0722     46.25   0.0476119   0.0538993
2021-10-27T04:56:54.341500+0000    24     128     57589     57461   37.4056   45.0781   0.0455866   0.0534081
2021-10-27T04:56:55.341609+0000    25     128     60464     60336   37.7061   44.9219   0.0477701   0.0529877
2021-10-27T04:56:56.341716+0000    26     128     62768     62640   37.6403        36   0.0734248    0.053051
2021-10-27T04:56:57.341824+0000    27     128     64116     63988   37.0262   21.0625     0.10921   0.0538241
2021-10-27T04:56:58.342005+0000    28     128     64560     64432   35.9515    6.9375     0.27313   0.0551796
2021-10-27T04:56:59.342081+0000    29     128     65140     65012   35.0243    9.0625    0.205205   0.0569386
2021-10-27T04:57:00.342174+0000    30     128     65584     65456   34.0881    6.9375    0.271259   0.0584298
2021-10-27T04:57:01.342329+0000 Total time run:         30.1956
Total writes made:      65584
Write size:             16384
Object size:            16384
Bandwidth (MB/sec):     33.937
Stddev Bandwidth:       13.7371
Max bandwidth (MB/sec): 47.0781
Min bandwidth (MB/sec): 6.9375
Average IOPS:           2171
Stddev IOPS:            879.176
Max IOPS:               3013
Min IOPS:               444
Average Latency(s):     0.0588186
Stddev Latency(s):      0.0383846
Max latency(s):         0.311627
Min latency(s):         0.0286968

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:57:01,905689648-07:00] INFO: > Stop system activity collection services[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:57:01,911962520-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:379 - rados_bench() > kill -INT 775421

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:57:01,918115486-07:00] INFO: >> for OSD host[0m
# ./benchmarks/bench-rados:384 - rados_bench() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.5 kill -INT 55285
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:57:01,926570409-07:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.5 kill -INT 55285
[1] 21:57:03 [SUCCESS] ljishen@10.10.2.5

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:57:03,204472445-07:00] INFO: >>> Copy system activity file from OSD host[0m
## ./benchmarks/bench-rados:391 - rados_bench() > basename -- /tmp/bench-rados/sys_activity_16KB_write.dat.osd_host.2
# ./benchmarks/bench-rados:391 - rados_bench() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.5:/tmp/bench-rados/sys_activity_16KB_write.dat.osd_host.2 /mnt/sda8/ceph-research/scripts/output/sys_activity_16KB_write.dat.osd_host.2


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:57:04,536339462-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:57:27,667628639-07:00] INFO: >>   data:
    pools:   2 pools, 256 pgs
    objects: 65.58k objects, 1.0 GiB
    usage:   3.1 GiB used, 97 GiB / 100 GiB avail
    pgs:     256 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:57:27,675786403-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:57:35,843302690-07:00] INFO: >>   data:
    pools:   2 pools, 256 pgs
    objects: 65.58k objects, 1.0 GiB
    usage:   3.1 GiB used, 97 GiB / 100 GiB avail
    pgs:     256 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:57:35,850638199-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:57:44,107785830-07:00] INFO: >>   data:
    pools:   2 pools, 256 pgs
    objects: 65.58k objects, 1.0 GiB
    usage:   3.1 GiB used, 97 GiB / 100 GiB avail
    pgs:     256 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:57:44,115649953-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:57:52,416710670-07:00] INFO: >>   data:
    pools:   2 pools, 256 pgs
    objects: 65.58k objects, 1.0 GiB
    usage:   3.1 GiB used, 97 GiB / 100 GiB avail
    pgs:     256 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:57:52,424561648-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:58:00,699954857-07:00] INFO: >>   data:
    pools:   2 pools, 256 pgs
    objects: 65.58k objects, 1.0 GiB
    usage:   3.1 GiB used, 97 GiB / 100 GiB avail
    pgs:     256 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:58:00,707496503-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:58:00,713564990-07:00] INFO: > The cluster is idle now.[0m


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:58:00,717052205-07:00] STAGE: Start benchmarking of mode seq ...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:58:00,723945343-07:00] INFO: > Start system activity collection services[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:58:00,729639997-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:329 - rados_bench() > rm --force /mnt/sda8/ceph-research/scripts/output/sys_activity_16KB_seq.dat.rados_bench_host.2
# ./benchmarks/bench-rados:330 - rados_bench() > sar_pid_rados_bench_host=777387
# ./benchmarks/bench-rados:330 - rados_bench() > S_TIME_FORMAT=ISO
# ./benchmarks/bench-rados:330 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/output/sys_activity_16KB_seq.dat.rados_bench_host.2 2

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:58:00,736468944-07:00] INFO: >> for OSD host[0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:58:00,744961298-07:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.5 bash
10.10.2.5: b'55819\n'
[1] 21:58:01 [SUCCESS] ljishen@10.10.2.5
55819

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:58:01,868087213-07:00] INFO: > Run rados bench[0m
# ./benchmarks/bench-rados:367 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/output/rados_bench_16KB_seq.log.2
## ./benchmarks/bench-rados:364 - rados_bench() > count_cpus 32-63,96-127
# ./benchmarks/bench-rados:364 - rados_bench() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 128 --show-time
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:58:01,889911117-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:58:01,893297292-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '9bfbc1c8-36e1-11ec-b561-09a495c6360a', '--', 'rados', 'bench', '99999999', 'seq', '--pool', 'bench_rados', '--concurrent-ios', '128', '--show-time'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 9bfbc1c8-36e1-11ec-b561-09a495c6360a -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 128 --show-time
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
hints = 1
2021-10-27T04:58:04.247079+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-27T04:58:04.247079+0000     0       1         1         0         0         0           -           0
2021-10-27T04:58:05.247174+0000     1     128     15190     15062   235.305   235.344   0.0126042  0.00845489
2021-10-27T04:58:06.247246+0000     2     128     30285     30157   235.574   235.859  0.00769466  0.00846629
2021-10-27T04:58:07.247336+0000     3     127     45349     45222   235.506   235.391  0.00895506  0.00847636
2021-10-27T04:58:08.247446+0000     4     128     60149     60021   234.431   231.234  0.00886511  0.00851504
2021-10-27T04:58:09.247587+0000 Total time run:       4.36076
Total reads made:     65584
Read size:            16384
Object size:          16384
Bandwidth (MB/sec):   234.993
Average IOPS:         15039
Stddev IOPS:          138.305
Max IOPS:             15095
Min IOPS:             14799
Average Latency(s):   0.00849975
Max latency(s):       0.0281894
Min latency(s):       0.00191428

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:58:09,781521189-07:00] INFO: > Stop system activity collection services[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:58:09,787639139-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:379 - rados_bench() > kill -INT 777387

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:58:09,793762600-07:00] INFO: >> for OSD host[0m
# ./benchmarks/bench-rados:384 - rados_bench() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.5 kill -INT 55819
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:58:09,802158442-07:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.5 kill -INT 55819
[1] 21:58:10 [SUCCESS] ljishen@10.10.2.5

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:58:10,891507836-07:00] INFO: >>> Copy system activity file from OSD host[0m
## ./benchmarks/bench-rados:391 - rados_bench() > basename -- /tmp/bench-rados/sys_activity_16KB_seq.dat.osd_host.2
# ./benchmarks/bench-rados:391 - rados_bench() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.5:/tmp/bench-rados/sys_activity_16KB_seq.dat.osd_host.2 /mnt/sda8/ceph-research/scripts/output/sys_activity_16KB_seq.dat.osd_host.2


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:58:11,937097483-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:58:34,987746315-07:00] INFO: >>   data:
    pools:   2 pools, 256 pgs
    objects: 65.58k objects, 1.0 GiB
    usage:   3.1 GiB used, 97 GiB / 100 GiB avail
    pgs:     256 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:58:34,995638681-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:58:43,285266376-07:00] INFO: >>   data:
    pools:   2 pools, 256 pgs
    objects: 65.58k objects, 1.0 GiB
    usage:   3.1 GiB used, 97 GiB / 100 GiB avail
    pgs:     256 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:58:43,293447935-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:58:51,525647530-07:00] INFO: >>   data:
    pools:   2 pools, 256 pgs
    objects: 65.58k objects, 1.0 GiB
    usage:   3.1 GiB used, 97 GiB / 100 GiB avail
    pgs:     256 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:58:51,532768145-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:58:59,741377025-07:00] INFO: >>   data:
    pools:   2 pools, 256 pgs
    objects: 65.58k objects, 1.0 GiB
    usage:   3.1 GiB used, 97 GiB / 100 GiB avail
    pgs:     256 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:58:59,748537575-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:59:07,956199626-07:00] INFO: >>   data:
    pools:   2 pools, 256 pgs
    objects: 65.58k objects, 1.0 GiB
    usage:   3.1 GiB used, 97 GiB / 100 GiB avail
    pgs:     256 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:59:07,964540514-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:59:07,970435345-07:00] INFO: > The cluster is idle now.[0m


[1;7;39;49m[2021-10-26T21:59:07,972829967-07:00][RUNNING][ROUND 3/2/21] object_size=16KB[0m


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:59:07,976046944-07:00] STAGE: Launch a Ceph cluster (CEPH_HOST: 10.10.2.3, OSD_HOST: 10.10.2.5) ...[0m
# ./benchmarks/bench-rados:172 - launch_ceph_cluster() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.3 bash -euo pipefail
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T21:59:07,985711531-07:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.3 bash -euo pipefail
10.10.2.3: b'\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T00:59:08,398900463-04:00] INFO: > Remove the existing cluster\x1b[0m\n'
10.10.2.3: b'## bash:13 -  > ls /var/lib/ceph\n## bash:13 -  > tail -n1\n'
10.10.2.3: b'# bash:13 -  > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm --host 10.10.2.3 --host 10.10.2.5 rm-cluster --force --fsid 9bfbc1c8-36e1-11ec-b561-09a495c6360a\n'
10.10.2.3: b'\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T00:59:17,259569604-04:00] INFO: > Deploy a new cluster\x1b[0m\n'
10.10.2.3: b'## bash:22 -  > /tmp/bench-rados/ceph-research/scripts/parallel-ssh --host 10.10.2.5 uname --nodename\n'
10.10.2.3: b'## bash:22 -  > tail -n1\n'
10.10.2.3: b'# bash:22 -  > osd_hostname=sm1\n# bash:23 -  > /tmp/bench-rados/ceph-research/scripts/ceph-deploy -m 10.10.2.3 -o sm1/10.10.2.5:/dev/nvme0n1\n'
10.10.2.3: b'\n\n\x1b[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T00:59:18,363916119-04:00] STAGE: Verify passwordless SSH to hosts, passwordless sudo via ljishen, and the existence of devices on hosts...\x1b[0m\n'
10.10.2.3: b'\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T00:59:18,367898834-04:00] INFO: > Check the host for the monitor\x1b[0m\n# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:181 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@10.10.2.3 sudo --non-interactive --validate\n'
10.10.2.3: b'\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T00:59:18,515468333-04:00] INFO: > Check hosts for OSDs\x1b[0m\n'
10.10.2.3: b"\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T00:59:18,519287380-04:00] INFO: >> Check host 'sm1'\x1b[0m\n"
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:200 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 sudo --non-interactive --validate\n'
10.10.2.3: b"\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T00:59:19,584178556-04:00] INFO: >>> Check device '/dev/nvme0n1'\x1b[0m\n# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:215 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 test -b /dev/nvme0n1\n"
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:226 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 lsblk --nodeps --noheadings --output TYPE /dev/nvme0n1\n# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:227 - verify_parameters() > grep --word-regexp disk\n'
10.10.2.3: b'\n\n\x1b[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T00:59:21,681236741-04:00] STAGE: Reset devices on hosts...\x1b[0m\n'
10.10.2.3: b"\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T00:59:21,685018267-04:00] INFO: > Resetting device '/dev/nvme0n1' on host 'sm1'\x1b[0m\n"
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:288 - reset_host_devices() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 sudo sh\n'
10.10.2.3: b'  Removing ceph--43df53ec--fdb6--4194--8335--e5f96e498a73-osd--block--b7213ab8--f24d--41d2--8338--7eaf9f42e672 (252:0)\n'
10.10.2.3: b'  Archiving volume group "ceph-43df53ec-fdb6-4194-8335-e5f96e498a73" metadata (seqno 5).\n'
10.10.2.3: b'  Releasing logical volume "osd-block-b7213ab8-f24d-41d2-8338-7eaf9f42e672"\n'
10.10.2.3: b'  Creating volume group backup "/etc/lvm/backup/ceph-43df53ec-fdb6-4194-8335-e5f96e498a73" (seqno 6).\n'
10.10.2.3: b'  Logical volume "osd-block-b7213ab8-f24d-41d2-8338-7eaf9f42e672" successfully removed\n  Removing physical volume "/dev/nvme0n1" from volume group "ceph-43df53ec-fdb6-4194-8335-e5f96e498a73"\n'
10.10.2.3: b'  Volume group "ceph-43df53ec-fdb6-4194-8335-e5f96e498a73" successfully removed\n'
10.10.2.3: b'\n\n\x1b[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T00:59:23,979383609-04:00] STAGE: Bootstrap a new cluster...\x1b[0m\n# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:312 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.3 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults\n'
10.10.2.3: b'\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T00:59:23,987789909-04:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.3: b"\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T00:59:23,990723116-04:00] DEBUG: CEPHADM_PARAMS: ['bootstrap', '--config', '/tmp/bench-rados/ceph-research/scripts/ceph.conf', '--mon-ip', '10.10.2.3', '--output-dir', '/tmp/bench-rados/deployment_data_root/etc/ceph', '--ssh-user', 'ljishen', '--skip-dashboard', '--no-minimize-config', '--allow-overwrite', '--allow-fqdn-hostname', '--orphan-initial-daemons', '--skip-monitoring-stack', '--single-host-defaults']\x1b[0m\n"
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.3 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults\n'
10.10.2.3: b'Verifying podman|docker is present...\nVerifying lvm2 is present...\nVerifying time synchronization is in place...\n'
10.10.2.3: b'Unit ntp.service is enabled and running\nRepeating the final host check...\npodman|docker (/usr/bin/docker) is present\n'
10.10.2.3: b'systemctl is present\nlvcreate is present\n'
10.10.2.3: b'Unit ntp.service is enabled and running\nHost looks OK\n'
10.10.2.3: b'Cluster fsid: a77e0adc-36e2-11ec-b561-09a495c6360a\n'
10.10.2.3: b'Verifying IP 10.10.2.3 port 3300 ...\n'
10.10.2.3: b'Verifying IP 10.10.2.3 port 6789 ...\n'
10.10.2.3: b'Mon IP `10.10.2.3` is in CIDR network `10.10.2.0/24`\n- internal network (--cluster-network) has not been provided, OSD replication will default to the public_network\n'
10.10.2.3: b'Adjusting default settings to suit single-host cluster...\nPulling container image quay.io/ceph/ceph:v16...\n'
10.10.2.3: b'Ceph version: ceph version 16.2.6 (ee28fb57e47e9f88813e24bbf4c14496ca299d31) pacific (stable)\nExtracting ceph user uid/gid from container image...\n'
10.10.2.3: b'Creating initial keys...\n'
10.10.2.3: b'Creating initial monmap...\n'
10.10.2.3: b'Creating mon...\n'
10.10.2.3: b'Waiting for mon to start...\nWaiting for mon...\n'
10.10.2.3: b'mon is available\nSetting mon public_network to 10.10.2.0/24\n'
10.10.2.3: b'Wrote config to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf\n'
10.10.2.3: b'Wrote keyring to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring\nCreating mgr...\n'
10.10.2.3: b'Verifying port 9283 ...\n'
10.10.2.3: b'Waiting for mgr to start...\nWaiting for mgr...\n'
10.10.2.3: b'mgr not available, waiting (1/15)...\n'
10.10.2.3: b'mgr not available, waiting (2/15)...\n'
10.10.2.3: b'mgr is available\n'
10.10.2.3: b'Enabling cephadm module...\n'
10.10.2.3: b'Waiting for the mgr to restart...\nWaiting for mgr epoch 5...\n'
10.10.2.3: b'mgr epoch 5 is available\nSetting orchestrator backend to cephadm...\n'
10.10.2.3: b'Generating ssh key...\n'
10.10.2.3: b'Wrote public SSH key to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub\nAdding key to ljishen@localhost authorized_keys...\n'
10.10.2.3: b'Adding host node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us...\n'
10.10.2.3: b'Deploying unmanaged mon service...\n'
10.10.2.3: b'Deploying unmanaged mgr service...\n'
10.10.2.3: b'You can access the Ceph CLI with:\n\n\tsudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --fsid a77e0adc-36e2-11ec-b561-09a495c6360a -c /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf -k /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring\n\nPlease consider enabling telemetry to help improve Ceph:\n\n\tceph telemetry on\n\nFor more information see:\n\n\thttps://docs.ceph.com/docs/pacific/mgr/telemetry/\n\nBootstrap complete.\n'
10.10.2.3: b'\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T01:00:28,167309885-04:00] INFO: > Set the default number of monitor daemons to 1\x1b[0m\n# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:330 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1\n'
10.10.2.3: b'\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T01:00:28,176004690-04:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.3: b"\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T01:00:28,179009773-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mon', '1']\x1b[0m\n# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1\n"
10.10.2.3: b'Inferring fsid a77e0adc-36e2-11ec-b561-09a495c6360a\n'
10.10.2.3: b'Inferring config /var/lib/ceph/a77e0adc-36e2-11ec-b561-09a495c6360a/mon.node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.3: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.3: b'Scheduled mon update...\n'
10.10.2.3: b'\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T01:00:37,377578820-04:00] INFO: > Set the default number of manager daemons to 1\x1b[0m\n# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:335 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1\n'
10.10.2.3: b'\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T01:00:37,386203813-04:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.3: b"\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T01:00:37,389341265-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mgr', '1']\x1b[0m\n"
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1\n'
10.10.2.3: b'Inferring fsid a77e0adc-36e2-11ec-b561-09a495c6360a\n'
10.10.2.3: b'Inferring config /var/lib/ceph/a77e0adc-36e2-11ec-b561-09a495c6360a/mon.node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.3: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.3: b'Scheduled mgr update...\n'
10.10.2.3: b'\n\n\x1b[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T01:00:46,642962194-04:00] STAGE: Add hosts to the cluster...\x1b[0m\n'
10.10.2.3: b"\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T01:00:46,647953559-04:00] INFO: > Adding host 'sm1'\x1b[0m\n"
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:362 - add_hosts() > ssh-copy-id -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -f -i /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub ljishen@sm1\n'
10.10.2.3: b'/usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: "/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub"\n'
10.10.2.3: b'\nNumber of key(s) added: 1\n\nNow try logging into the machine, with:   "ssh -o \'GlobalKnownHostsFile=/dev/null\' -o \'LogLevel=ERROR\' -o \'PasswordAuthentication=no\' -o \'StrictHostKeyChecking=no\' -o \'UserKnownHostsFile=/dev/null\' \'ljishen@sm1\'"\nand check to make sure that only the key(s) you wanted were added.\n\n'
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:368 - add_hosts() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add sm1 10.10.2.5\n'
10.10.2.3: b'\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T01:00:47,784576836-04:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.3: b"\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T01:00:47,787551021-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'host', 'add', 'sm1', '10.10.2.5']\x1b[0m\n"
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add sm1 10.10.2.5\n'
10.10.2.3: b'Inferring fsid a77e0adc-36e2-11ec-b561-09a495c6360a\n'
10.10.2.3: b'Inferring config /var/lib/ceph/a77e0adc-36e2-11ec-b561-09a495c6360a/mon.node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.3: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.3: b"Added host 'sm1' with addr '10.10.2.5'\n"
10.10.2.3: b'\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T01:00:58,714724566-04:00] INFO: Sleep for 20 seconds\x1b[0m\n'
10.10.2.3: b'\n\n\x1b[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T01:01:18,718317431-04:00] STAGE: Add OSDs to the cluster...\x1b[0m\n'
10.10.2.3: b"\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T01:01:18,723201645-04:00] INFO: > Adding devices ['/dev/nvme0n1'] on host 'sm1' as OSDs\x1b[0m\n# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:387 - add_host_devices_as_osds() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd sm1:/dev/nvme0n1\n"
10.10.2.3: b'\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T01:01:18,731191070-04:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.3: b"\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T01:01:18,733864678-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'daemon', 'add', 'osd', 'sm1:/dev/nvme0n1']\x1b[0m\n"
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd sm1:/dev/nvme0n1\n'
10.10.2.3: b'Inferring fsid a77e0adc-36e2-11ec-b561-09a495c6360a\n'
10.10.2.3: b'Inferring config /var/lib/ceph/a77e0adc-36e2-11ec-b561-09a495c6360a/mon.node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.3: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.3: b"Created osd(s) 0 on host 'sm1'\n"
10.10.2.3: b'\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T01:01:42,509776512-04:00] INFO: > Sleep for 20 seconds\x1b[0m\n'
10.10.2.3: b'\n\n\x1b[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T01:02:02,514375002-04:00] STAGE: Show Cluster Status\x1b[0m\n# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:343 - show_status() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status\n'
10.10.2.3: b'\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T01:02:02,522742219-04:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.3: b"\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T01:02:02,525434222-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', '--status']\x1b[0m\n"
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status\n'
10.10.2.3: b'Inferring fsid a77e0adc-36e2-11ec-b561-09a495c6360a\n'
10.10.2.3: b'Inferring config /var/lib/ceph/a77e0adc-36e2-11ec-b561-09a495c6360a/mon.node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.3: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.3: b'  cluster:\n    id:     a77e0adc-36e2-11ec-b561-09a495c6360a\n    health: HEALTH_WARN\n            1 pool(s) have no replicas configured\n \n  services:\n    mon: 1 daemons, quorum node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us (age 2m)\n    mgr: node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us.xnfytb(active, since 2m)\n    osd: 1 osds: 1 up (since 20s), 1 in (since 40s)\n \n  data:\n    pools:   1 pools, 128 pgs\n    objects: 0 objects, 0 B\n    usage:   4.7 MiB used, 100 GiB / 100 GiB avail\n    pgs:     99.219% pgs unknown\n             127 unknown\n             1   active+clean\n \n  progress:\n \n'
[1] 22:02:11 [SUCCESS] ljishen@10.10.2.3

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T00:59:08,398900463-04:00] INFO: > Remove the existing cluster[0m
## bash:13 -  > ls /var/lib/ceph
## bash:13 -  > tail -n1
# bash:13 -  > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm --host 10.10.2.3 --host 10.10.2.5 rm-cluster --force --fsid 9bfbc1c8-36e1-11ec-b561-09a495c6360a

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T00:59:17,259569604-04:00] INFO: > Deploy a new cluster[0m
## bash:22 -  > /tmp/bench-rados/ceph-research/scripts/parallel-ssh --host 10.10.2.5 uname --nodename
## bash:22 -  > tail -n1
# bash:22 -  > osd_hostname=sm1
# bash:23 -  > /tmp/bench-rados/ceph-research/scripts/ceph-deploy -m 10.10.2.3 -o sm1/10.10.2.5:/dev/nvme0n1


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T00:59:18,363916119-04:00] STAGE: Verify passwordless SSH to hosts, passwordless sudo via ljishen, and the existence of devices on hosts...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T00:59:18,367898834-04:00] INFO: > Check the host for the monitor[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:181 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@10.10.2.3 sudo --non-interactive --validate

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T00:59:18,515468333-04:00] INFO: > Check hosts for OSDs[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T00:59:18,519287380-04:00] INFO: >> Check host 'sm1'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:200 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 sudo --non-interactive --validate

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T00:59:19,584178556-04:00] INFO: >>> Check device '/dev/nvme0n1'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:215 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 test -b /dev/nvme0n1
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:226 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 lsblk --nodeps --noheadings --output TYPE /dev/nvme0n1
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:227 - verify_parameters() > grep --word-regexp disk


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T00:59:21,681236741-04:00] STAGE: Reset devices on hosts...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T00:59:21,685018267-04:00] INFO: > Resetting device '/dev/nvme0n1' on host 'sm1'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:288 - reset_host_devices() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 sudo sh
  Removing ceph--43df53ec--fdb6--4194--8335--e5f96e498a73-osd--block--b7213ab8--f24d--41d2--8338--7eaf9f42e672 (252:0)
  Archiving volume group "ceph-43df53ec-fdb6-4194-8335-e5f96e498a73" metadata (seqno 5).
  Releasing logical volume "osd-block-b7213ab8-f24d-41d2-8338-7eaf9f42e672"
  Creating volume group backup "/etc/lvm/backup/ceph-43df53ec-fdb6-4194-8335-e5f96e498a73" (seqno 6).
  Logical volume "osd-block-b7213ab8-f24d-41d2-8338-7eaf9f42e672" successfully removed
  Removing physical volume "/dev/nvme0n1" from volume group "ceph-43df53ec-fdb6-4194-8335-e5f96e498a73"
  Volume group "ceph-43df53ec-fdb6-4194-8335-e5f96e498a73" successfully removed


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T00:59:23,979383609-04:00] STAGE: Bootstrap a new cluster...[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:312 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.3 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T00:59:23,987789909-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T00:59:23,990723116-04:00] DEBUG: CEPHADM_PARAMS: ['bootstrap', '--config', '/tmp/bench-rados/ceph-research/scripts/ceph.conf', '--mon-ip', '10.10.2.3', '--output-dir', '/tmp/bench-rados/deployment_data_root/etc/ceph', '--ssh-user', 'ljishen', '--skip-dashboard', '--no-minimize-config', '--allow-overwrite', '--allow-fqdn-hostname', '--orphan-initial-daemons', '--skip-monitoring-stack', '--single-host-defaults'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.3 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults
Verifying podman|docker is present...
Verifying lvm2 is present...
Verifying time synchronization is in place...
Unit ntp.service is enabled and running
Repeating the final host check...
podman|docker (/usr/bin/docker) is present
systemctl is present
lvcreate is present
Unit ntp.service is enabled and running
Host looks OK
Cluster fsid: a77e0adc-36e2-11ec-b561-09a495c6360a
Verifying IP 10.10.2.3 port 3300 ...
Verifying IP 10.10.2.3 port 6789 ...
Mon IP `10.10.2.3` is in CIDR network `10.10.2.0/24`
- internal network (--cluster-network) has not been provided, OSD replication will default to the public_network
Adjusting default settings to suit single-host cluster...
Pulling container image quay.io/ceph/ceph:v16...
Ceph version: ceph version 16.2.6 (ee28fb57e47e9f88813e24bbf4c14496ca299d31) pacific (stable)
Extracting ceph user uid/gid from container image...
Creating initial keys...
Creating initial monmap...
Creating mon...
Waiting for mon to start...
Waiting for mon...
mon is available
Setting mon public_network to 10.10.2.0/24
Wrote config to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf
Wrote keyring to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring
Creating mgr...
Verifying port 9283 ...
Waiting for mgr to start...
Waiting for mgr...
mgr not available, waiting (1/15)...
mgr not available, waiting (2/15)...
mgr is available
Enabling cephadm module...
Waiting for the mgr to restart...
Waiting for mgr epoch 5...
mgr epoch 5 is available
Setting orchestrator backend to cephadm...
Generating ssh key...
Wrote public SSH key to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub
Adding key to ljishen@localhost authorized_keys...
Adding host node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us...
Deploying unmanaged mon service...
Deploying unmanaged mgr service...
You can access the Ceph CLI with:

	sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --fsid a77e0adc-36e2-11ec-b561-09a495c6360a -c /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf -k /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring

Please consider enabling telemetry to help improve Ceph:

	ceph telemetry on

For more information see:

	https://docs.ceph.com/docs/pacific/mgr/telemetry/

Bootstrap complete.

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T01:00:28,167309885-04:00] INFO: > Set the default number of monitor daemons to 1[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:330 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T01:00:28,176004690-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T01:00:28,179009773-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mon', '1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1
Inferring fsid a77e0adc-36e2-11ec-b561-09a495c6360a
Inferring config /var/lib/ceph/a77e0adc-36e2-11ec-b561-09a495c6360a/mon.node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Scheduled mon update...

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T01:00:37,377578820-04:00] INFO: > Set the default number of manager daemons to 1[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:335 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T01:00:37,386203813-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T01:00:37,389341265-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mgr', '1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1
Inferring fsid a77e0adc-36e2-11ec-b561-09a495c6360a
Inferring config /var/lib/ceph/a77e0adc-36e2-11ec-b561-09a495c6360a/mon.node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Scheduled mgr update...


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T01:00:46,642962194-04:00] STAGE: Add hosts to the cluster...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T01:00:46,647953559-04:00] INFO: > Adding host 'sm1'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:362 - add_hosts() > ssh-copy-id -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -f -i /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub ljishen@sm1
/usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: "/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub"

Number of key(s) added: 1

Now try logging into the machine, with:   "ssh -o 'GlobalKnownHostsFile=/dev/null' -o 'LogLevel=ERROR' -o 'PasswordAuthentication=no' -o 'StrictHostKeyChecking=no' -o 'UserKnownHostsFile=/dev/null' 'ljishen@sm1'"
and check to make sure that only the key(s) you wanted were added.

# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:368 - add_hosts() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add sm1 10.10.2.5
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T01:00:47,784576836-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T01:00:47,787551021-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'host', 'add', 'sm1', '10.10.2.5'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add sm1 10.10.2.5
Inferring fsid a77e0adc-36e2-11ec-b561-09a495c6360a
Inferring config /var/lib/ceph/a77e0adc-36e2-11ec-b561-09a495c6360a/mon.node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Added host 'sm1' with addr '10.10.2.5'

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T01:00:58,714724566-04:00] INFO: Sleep for 20 seconds[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T01:01:18,718317431-04:00] STAGE: Add OSDs to the cluster...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T01:01:18,723201645-04:00] INFO: > Adding devices ['/dev/nvme0n1'] on host 'sm1' as OSDs[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:387 - add_host_devices_as_osds() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd sm1:/dev/nvme0n1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T01:01:18,731191070-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T01:01:18,733864678-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'daemon', 'add', 'osd', 'sm1:/dev/nvme0n1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd sm1:/dev/nvme0n1
Inferring fsid a77e0adc-36e2-11ec-b561-09a495c6360a
Inferring config /var/lib/ceph/a77e0adc-36e2-11ec-b561-09a495c6360a/mon.node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Created osd(s) 0 on host 'sm1'

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T01:01:42,509776512-04:00] INFO: > Sleep for 20 seconds[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T01:02:02,514375002-04:00] STAGE: Show Cluster Status[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:343 - show_status() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T01:02:02,522742219-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T01:02:02,525434222-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', '--status'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status
Inferring fsid a77e0adc-36e2-11ec-b561-09a495c6360a
Inferring config /var/lib/ceph/a77e0adc-36e2-11ec-b561-09a495c6360a/mon.node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
  cluster:
    id:     a77e0adc-36e2-11ec-b561-09a495c6360a
    health: HEALTH_WARN
            1 pool(s) have no replicas configured
 
  services:
    mon: 1 daemons, quorum node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us (age 2m)
    mgr: node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us.xnfytb(active, since 2m)
    osd: 1 osds: 1 up (since 20s), 1 in (since 40s)
 
  data:
    pools:   1 pools, 128 pgs
    objects: 0 objects, 0 B
    usage:   4.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     99.219% pgs unknown
             127 unknown
             1   active+clean
 
  progress:
 

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:02:11,813860654-07:00] INFO: > Copy configuration files to local[0m
# ./benchmarks/bench-rados:208 - launch_ceph_cluster() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.3:/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf
# ./benchmarks/bench-rados:212 - launch_ceph_cluster() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.3 sudo chmod 644 /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:02:11,971016337-07:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.3 sudo chmod 644 /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring
[1] 22:02:12 [SUCCESS] ljishen@10.10.2.3
# ./benchmarks/bench-rados:214 - launch_ceph_cluster() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.3:/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:02:12,298768527-07:00] STAGE: Update local ceph.conf as a client...[0m


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:02:12,302306027-07:00] STAGE: Configure Ceph pool...[0m
# ./benchmarks/bench-rados:245 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_max_backfills 32
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:02:12,325651981-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:02:12,328806692-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'a77e0adc-36e2-11ec-b561-09a495c6360a', '--', 'ceph', 'config', 'set', 'osd', 'osd_max_backfills', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid a77e0adc-36e2-11ec-b561-09a495c6360a -- ceph config set osd osd_max_backfills 32
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:246 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_active 32
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:02:15,406040407-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:02:15,409831043-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'a77e0adc-36e2-11ec-b561-09a495c6360a', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_active', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid a77e0adc-36e2-11ec-b561-09a495c6360a -- ceph config set osd osd_recovery_max_active 32
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:247 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_single_start 8
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:02:18,657780931-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:02:18,661406687-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'a77e0adc-36e2-11ec-b561-09a495c6360a', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_single_start', '8'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid a77e0adc-36e2-11ec-b561-09a495c6360a -- ceph config set osd osd_recovery_max_single_start 8
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:248 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_op_priority 63
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:02:21,873661638-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:02:21,877282755-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'a77e0adc-36e2-11ec-b561-09a495c6360a', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_op_priority', '63'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid a77e0adc-36e2-11ec-b561-09a495c6360a -- ceph config set osd osd_recovery_op_priority 63
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:250 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config show-with-defaults osd.0
# ./benchmarks/bench-rados:251 - configure_ceph_pool() > grep 'osd_max_backfills\|osd_recovery'
# ./benchmarks/bench-rados:251 - configure_ceph_pool() > column -t -s ' '
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
osd_max_backfills                        32         mon
osd_recovery_cost                        20971520   default
osd_recovery_delay_start                 0.000000   default
osd_recovery_max_active                  32         mon
osd_recovery_max_active_hdd              3          default
osd_recovery_max_active_ssd              10         default
osd_recovery_max_chunk                   8388608    default
osd_recovery_max_omap_entries_per_chunk  8096       default
osd_recovery_max_single_start            8          mon
osd_recovery_op_priority                 63         mon
osd_recovery_op_warn_multiple            16         default
osd_recovery_priority                    5          default
osd_recovery_retry_interval              30.000000  default
osd_recovery_sleep                       0.000000   default
osd_recovery_sleep_hdd                   0.100000   default
osd_recovery_sleep_hybrid                0.025000   default
osd_recovery_sleep_ssd                   0.000000   default
# ./benchmarks/bench-rados:253 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool create bench_rados 128 128 replicated --size 1 --pg-num-min 128
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:02:28,326416523-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:02:28,329953793-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'a77e0adc-36e2-11ec-b561-09a495c6360a', '--', 'ceph', 'osd', 'pool', 'create', 'bench_rados', '128', '128', 'replicated', '--size', '1', '--pg-num-min', '128'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid a77e0adc-36e2-11ec-b561-09a495c6360a -- ceph osd pool create bench_rados 128 128 replicated --size 1 --pg-num-min 128
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
pool 'bench_rados' created
# ./benchmarks/bench-rados:255 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados min_size 1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:02:32,686031369-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:02:32,689873982-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'a77e0adc-36e2-11ec-b561-09a495c6360a', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'min_size', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid a77e0adc-36e2-11ec-b561-09a495c6360a -- ceph osd pool set bench_rados min_size 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 min_size to 1
# ./benchmarks/bench-rados:256 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados pg_autoscale_mode off
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:02:37,018131243-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:02:37,022213356-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'a77e0adc-36e2-11ec-b561-09a495c6360a', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'pg_autoscale_mode', 'off'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid a77e0adc-36e2-11ec-b561-09a495c6360a -- ceph osd pool set bench_rados pg_autoscale_mode off
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 pg_autoscale_mode to off
# ./benchmarks/bench-rados:257 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool application enable bench_rados benchmark
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:02:40,352982955-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:02:40,356147073-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'a77e0adc-36e2-11ec-b561-09a495c6360a', '--', 'ceph', 'osd', 'pool', 'application', 'enable', 'bench_rados', 'benchmark'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid a77e0adc-36e2-11ec-b561-09a495c6360a -- ceph osd pool application enable bench_rados benchmark
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
enabled application 'benchmark' on pool 'bench_rados'
# ./benchmarks/bench-rados:258 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados noscrub 1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:02:44,093735217-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:02:44,097027525-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'a77e0adc-36e2-11ec-b561-09a495c6360a', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'noscrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid a77e0adc-36e2-11ec-b561-09a495c6360a -- ceph osd pool set bench_rados noscrub 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 noscrub to 1
# ./benchmarks/bench-rados:259 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados nodeep-scrub 1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:02:48,162137426-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:02:48,165955834-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'a77e0adc-36e2-11ec-b561-09a495c6360a', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'nodeep-scrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid a77e0adc-36e2-11ec-b561-09a495c6360a -- ceph osd pool set bench_rados nodeep-scrub 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 nodeep-scrub to 1
# ./benchmarks/bench-rados:260 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool ls detail
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:02:51,828942126-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:02:51,832088961-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'a77e0adc-36e2-11ec-b561-09a495c6360a', '--', 'ceph', 'osd', 'pool', 'ls', 'detail'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid a77e0adc-36e2-11ec-b561-09a495c6360a -- ceph osd pool ls detail
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
pool 1 'device_health_metrics' replicated size 1 min_size 1 crush_rule 0 object_hash rjenkins pg_num 128 pgp_num 128 autoscale_mode on last_change 13 lfor 0/0/11 flags hashpspool stripe_width 0 pg_num_min 1 application mgr_devicehealth
pool 2 'bench_rados' replicated size 1 min_size 1 crush_rule 0 object_hash rjenkins pg_num 128 pgp_num 128 autoscale_mode off last_change 21 flags hashpspool,noscrub,nodeep-scrub stripe_width 0 pg_num_min 128 application benchmark

# ./benchmarks/bench-rados:261 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd df tree
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:02:55,080920497-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:02:55,084253442-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'a77e0adc-36e2-11ec-b561-09a495c6360a', '--', 'ceph', 'osd', 'df', 'tree'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid a77e0adc-36e2-11ec-b561-09a495c6360a -- ceph osd df tree
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
ID  CLASS  WEIGHT   REWEIGHT  SIZE     RAW USE  DATA     OMAP  META     AVAIL    %USE  VAR   PGS  STATUS  TYPE NAME    
-1         0.09769         -  100 GiB  6.0 MiB  168 KiB   0 B  5.8 MiB  100 GiB  0.01  1.00    -          root default 
-3         0.09769         -  100 GiB  6.0 MiB  168 KiB   0 B  5.8 MiB  100 GiB  0.01  1.00    -              host sm1 
 0    ssd  0.09769   1.00000  100 GiB  6.0 MiB  168 KiB   0 B  5.8 MiB  100 GiB  0.01  1.00  256      up          osd.0
                       TOTAL  100 GiB  6.0 MiB  168 KiB   0 B  5.8 MiB  100 GiB  0.01                                  
MIN/MAX VAR: 1.00/1.00  STDDEV: 0


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:02:58,178586280-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:03:21,283727831-07:00] INFO: >>   data:
    pools:   2 pools, 256 pgs
    objects: 0 objects, 0 B
    usage:   6.0 MiB used, 100 GiB / 100 GiB avail
    pgs:     256 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:03:21,291149752-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:03:29,674898985-07:00] INFO: >>   data:
    pools:   2 pools, 256 pgs
    objects: 0 objects, 0 B
    usage:   6.0 MiB used, 100 GiB / 100 GiB avail
    pgs:     256 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:03:29,682599841-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:03:37,864545655-07:00] INFO: >>   data:
    pools:   2 pools, 256 pgs
    objects: 0 objects, 0 B
    usage:   6.0 MiB used, 100 GiB / 100 GiB avail
    pgs:     256 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:03:37,872420067-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:03:46,228204383-07:00] INFO: >>   data:
    pools:   2 pools, 256 pgs
    objects: 0 objects, 0 B
    usage:   6.0 MiB used, 100 GiB / 100 GiB avail
    pgs:     256 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:03:46,236118590-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:03:54,432547292-07:00] INFO: >>   data:
    pools:   2 pools, 256 pgs
    objects: 0 objects, 0 B
    usage:   6.0 MiB used, 100 GiB / 100 GiB avail
    pgs:     256 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:03:54,440326656-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:03:54,446109766-07:00] INFO: > The cluster is idle now.[0m


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:03:54,449247123-07:00] STAGE: Start benchmarking of mode write ...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:03:54,455464901-07:00] INFO: > Start system activity collection services[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:03:54,460743654-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:329 - rados_bench() > rm --force /mnt/sda8/ceph-research/scripts/output/sys_activity_16KB_write.dat.rados_bench_host.3
# ./benchmarks/bench-rados:330 - rados_bench() > sar_pid_rados_bench_host=784081
# ./benchmarks/bench-rados:330 - rados_bench() > S_TIME_FORMAT=ISO
# ./benchmarks/bench-rados:330 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/output/sys_activity_16KB_write.dat.rados_bench_host.3 2

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:03:54,466911247-07:00] INFO: >> for OSD host[0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:03:54,475777804-07:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.5 bash
10.10.2.5: b'60593\n'
[1] 22:03:55 [SUCCESS] ljishen@10.10.2.5
60593

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:03:55,564655468-07:00] INFO: > Run rados bench[0m
# ./benchmarks/bench-rados:358 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/output/rados_bench_16KB_write.log.3
## ./benchmarks/bench-rados:349 - rados_bench() > count_cpus 32-63,96-127
# ./benchmarks/bench-rados:349 - rados_bench() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 30 write --pool bench_rados -b 16384 -O 16384 --concurrent-ios 128 --show-time --write-object --write-omap --write-xattr --no-cleanup
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:03:55,585650514-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:03:55,589169148-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'a77e0adc-36e2-11ec-b561-09a495c6360a', '--', 'rados', 'bench', '30', 'write', '--pool', 'bench_rados', '-b', '16384', '-O', '16384', '--concurrent-ios', '128', '--show-time', '--write-object', '--write-omap', '--write-xattr', '--no-cleanup'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid a77e0adc-36e2-11ec-b561-09a495c6360a -- rados bench 30 write --pool bench_rados -b 16384 -O 16384 --concurrent-ios 128 --show-time --write-object --write-omap --write-xattr --no-cleanup
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
hints = 1
2021-10-27T05:03:58.020458+0000 Maintaining 128 concurrent writes of 16384 bytes to objects of size 16384 for up to 30 seconds or 0 objects
2021-10-27T05:03:58.020468+0000 Object prefix: benchmark_data_node-0.bluefield2.ucsc-cmps10_7
2021-10-27T05:03:58.021427+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-27T05:03:58.021427+0000     0       0         0         0         0         0           -           0
2021-10-27T05:03:59.021521+0000     1     128      3188      3060   47.8072   47.8125   0.0344531   0.0412502
2021-10-27T05:04:00.021560+0000     2     128      6260      6132   47.9027        48   0.0451595   0.0413152
2021-10-27T05:04:01.021617+0000     3     128      9332      9204   47.9342        48   0.0477844   0.0414611
2021-10-27T05:04:02.021698+0000     4     128     12341     12213   47.7036   47.0156   0.0406341   0.0416534
2021-10-27T05:04:03.021748+0000     5     128     15348     15220   47.5593   46.9844   0.0448239   0.0418899
2021-10-27T05:04:04.021783+0000     6     128     18250     18122   47.1898   45.3438   0.0433814   0.0421909
2021-10-27T05:04:05.021846+0000     7     128     21173     21045   46.9725   45.6719   0.0426162   0.0424428
2021-10-27T05:04:06.021932+0000     8     128     24117     23989   46.8505        46   0.0467927   0.0425795
2021-10-27T05:04:07.022000+0000     9     128     27096     26968   46.8164   46.5469   0.0443709   0.0425957
2021-10-27T05:04:08.022068+0000    10     128     30005     29877   46.6797   45.4531   0.0523678   0.0427554
2021-10-27T05:04:09.022144+0000    11     128     32053     31925    45.345        32   0.0842225   0.0439913
2021-10-27T05:04:10.022233+0000    12     128     33524     33396   43.4814   22.9844    0.095996   0.0458684
2021-10-27T05:04:11.022301+0000    13     128     34946     34818   41.8457   22.2188   0.0880294   0.0475813
2021-10-27T05:04:12.022370+0000    14     128     36405     36277    40.485   22.7969   0.0941344   0.0492565
2021-10-27T05:04:13.022438+0000    15     128     37813     37685   39.2525        22   0.0916893   0.0508772
2021-10-27T05:04:14.022496+0000    16     128     39093     38965   38.0492        20   0.0959026   0.0524363
2021-10-27T05:04:15.022554+0000    17     128     40436     40308   37.0453   20.9844   0.0930206   0.0538478
2021-10-27T05:04:16.022628+0000    18     128     41781     41653   36.1547   21.0156    0.103892   0.0551662
2021-10-27T05:04:17.022767+0000    19     128     43445     43317     35.62        26   0.0476533   0.0554983
2021-10-27T05:04:18.022825+0000 min lat: 0.0296488 max lat: 0.243125 avg lat: 0.0553393
2021-10-27T05:04:18.022825+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-27T05:04:18.022825+0000    20     128     46324     46196   36.0881   44.9844   0.0490295   0.0553393
2021-10-27T05:04:19.022888+0000    21     128     49268     49140   36.5599        46   0.0426196   0.0546625
2021-10-27T05:04:20.022935+0000    22     128     52149     52021   36.9442   45.0156   0.0446877   0.0540634
2021-10-27T05:04:21.022993+0000    23     128     55093     54965   37.3378        46   0.0408109   0.0535151
2021-10-27T05:04:22.023074+0000    24     128     57972     57844   37.6562   44.9844   0.0509115   0.0530668
2021-10-27T05:04:23.023132+0000    25     128     60878     60750   37.9661   45.4062   0.0435505   0.0526287
2021-10-27T05:04:24.023191+0000    26     128     62901     62773   37.7216   31.6094   0.0901283   0.0528834
2021-10-27T05:04:25.023263+0000    27     128     64116     63988   37.0276   18.9844    0.143227   0.0536222
2021-10-27T05:04:26.023344+0000    28     128     64565     64437   35.9557   7.01562    0.295583   0.0551717
2021-10-27T05:04:27.023413+0000    29     128     65077     64949   34.9917         8    0.271271    0.056872
2021-10-27T05:04:28.023484+0000    30     128     65524     65396   34.0581   6.98438    0.297187   0.0584476
2021-10-27T05:04:29.023762+0000 Total time run:         30.1868
Total writes made:      65524
Write size:             16384
Object size:            16384
Bandwidth (MB/sec):     33.916
Stddev Bandwidth:       14.2584
Max bandwidth (MB/sec): 48
Min bandwidth (MB/sec): 6.98438
Average IOPS:           2170
Stddev IOPS:            912.54
Max IOPS:               3072
Min IOPS:               447
Average Latency(s):     0.0588339
Stddev Latency(s):      0.0399815
Max latency(s):         0.326275
Min latency(s):         0.0296488

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:04:29,597408840-07:00] INFO: > Stop system activity collection services[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:04:29,603350388-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:379 - rados_bench() > kill -INT 784081

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:04:29,608982104-07:00] INFO: >> for OSD host[0m
# ./benchmarks/bench-rados:384 - rados_bench() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.5 kill -INT 60593
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:04:29,617081809-07:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.5 kill -INT 60593
[1] 22:04:31 [SUCCESS] ljishen@10.10.2.5

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:04:31,074358158-07:00] INFO: >>> Copy system activity file from OSD host[0m
## ./benchmarks/bench-rados:391 - rados_bench() > basename -- /tmp/bench-rados/sys_activity_16KB_write.dat.osd_host.3
# ./benchmarks/bench-rados:391 - rados_bench() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.5:/tmp/bench-rados/sys_activity_16KB_write.dat.osd_host.3 /mnt/sda8/ceph-research/scripts/output/sys_activity_16KB_write.dat.osd_host.3


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:04:32,380250883-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:04:55,533106005-07:00] INFO: >>   data:
    pools:   2 pools, 256 pgs
    objects: 65.53k objects, 1024 MiB
    usage:   3.1 GiB used, 97 GiB / 100 GiB avail
    pgs:     256 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:04:55,540716741-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:05:03,963440326-07:00] INFO: >>   data:
    pools:   2 pools, 256 pgs
    objects: 65.53k objects, 1024 MiB
    usage:   3.1 GiB used, 97 GiB / 100 GiB avail
    pgs:     256 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:05:03,971255246-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:05:12,071588149-07:00] INFO: >>   data:
    pools:   2 pools, 256 pgs
    objects: 65.53k objects, 1024 MiB
    usage:   3.1 GiB used, 97 GiB / 100 GiB avail
    pgs:     256 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:05:12,080383884-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:05:20,217654791-07:00] INFO: >>   data:
    pools:   2 pools, 256 pgs
    objects: 65.53k objects, 1024 MiB
    usage:   3.1 GiB used, 97 GiB / 100 GiB avail
    pgs:     256 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:05:20,224921550-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:05:28,466715844-07:00] INFO: >>   data:
    pools:   2 pools, 256 pgs
    objects: 65.53k objects, 1024 MiB
    usage:   3.1 GiB used, 97 GiB / 100 GiB avail
    pgs:     256 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:05:28,474553017-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:05:28,480607928-07:00] INFO: > The cluster is idle now.[0m


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:05:28,484127875-07:00] STAGE: Start benchmarking of mode seq ...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:05:28,491554926-07:00] INFO: > Start system activity collection services[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:05:28,497685380-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:329 - rados_bench() > rm --force /mnt/sda8/ceph-research/scripts/output/sys_activity_16KB_seq.dat.rados_bench_host.3
# ./benchmarks/bench-rados:330 - rados_bench() > sar_pid_rados_bench_host=785858
# ./benchmarks/bench-rados:330 - rados_bench() > S_TIME_FORMAT=ISO
# ./benchmarks/bench-rados:330 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/output/sys_activity_16KB_seq.dat.rados_bench_host.3 2

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:05:28,504468730-07:00] INFO: >> for OSD host[0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:05:28,513209621-07:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.5 bash
10.10.2.5: b'60994\n'
[1] 22:05:29 [SUCCESS] ljishen@10.10.2.5
60994

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:05:29,636414079-07:00] INFO: > Run rados bench[0m
# ./benchmarks/bench-rados:367 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/output/rados_bench_16KB_seq.log.3
## ./benchmarks/bench-rados:364 - rados_bench() > count_cpus 32-63,96-127
# ./benchmarks/bench-rados:364 - rados_bench() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 128 --show-time
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:05:29,656722183-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:05:29,659875841-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'a77e0adc-36e2-11ec-b561-09a495c6360a', '--', 'rados', 'bench', '99999999', 'seq', '--pool', 'bench_rados', '--concurrent-ios', '128', '--show-time'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid a77e0adc-36e2-11ec-b561-09a495c6360a -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 128 --show-time
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
hints = 1
2021-10-27T05:05:32.043563+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-27T05:05:32.043563+0000     0       0         0         0         0         0           -           0
2021-10-27T05:05:33.043686+0000     1     127     15999     15872   247.956       248  0.00532066  0.00803022
2021-10-27T05:05:34.043800+0000     2     128     31536     31408   245.339    242.75  0.00830546  0.00812481
2021-10-27T05:05:35.043909+0000     3     128     46577     46449    241.89   235.016  0.00864252  0.00825279
2021-10-27T05:05:36.043999+0000     4     128     62089     61961   242.006   242.375   0.0108722  0.00825194
2021-10-27T05:05:37.044116+0000 Total time run:       4.21691
Total reads made:     65524
Read size:            16384
Object size:          16384
Bandwidth (MB/sec):   242.788
Average IOPS:         15538
Stddev IOPS:          341.626
Max IOPS:             15872
Min IOPS:             15041
Average Latency(s):   0.00822716
Max latency(s):       0.0389776
Min latency(s):       0.000737626

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:05:37,595494364-07:00] INFO: > Stop system activity collection services[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:05:37,601795969-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:379 - rados_bench() > kill -INT 785858

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:05:37,607788293-07:00] INFO: >> for OSD host[0m
# ./benchmarks/bench-rados:384 - rados_bench() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.5 kill -INT 60994
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:05:37,616718660-07:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.5 kill -INT 60994
[1] 22:05:38 [SUCCESS] ljishen@10.10.2.5

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:05:38,719391243-07:00] INFO: >>> Copy system activity file from OSD host[0m
## ./benchmarks/bench-rados:391 - rados_bench() > basename -- /tmp/bench-rados/sys_activity_16KB_seq.dat.osd_host.3
# ./benchmarks/bench-rados:391 - rados_bench() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.5:/tmp/bench-rados/sys_activity_16KB_seq.dat.osd_host.3 /mnt/sda8/ceph-research/scripts/output/sys_activity_16KB_seq.dat.osd_host.3


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:05:39,801988510-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:06:02,855966577-07:00] INFO: >>   data:
    pools:   2 pools, 256 pgs
    objects: 65.53k objects, 1024 MiB
    usage:   3.1 GiB used, 97 GiB / 100 GiB avail
    pgs:     256 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:06:02,864267081-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:06:10,986858704-07:00] INFO: >>   data:
    pools:   2 pools, 256 pgs
    objects: 65.53k objects, 1024 MiB
    usage:   3.1 GiB used, 97 GiB / 100 GiB avail
    pgs:     256 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:06:10,994298298-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:06:19,151599477-07:00] INFO: >>   data:
    pools:   2 pools, 256 pgs
    objects: 65.53k objects, 1024 MiB
    usage:   3.1 GiB used, 97 GiB / 100 GiB avail
    pgs:     256 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:06:19,159677010-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:06:27,273778735-07:00] INFO: >>   data:
    pools:   2 pools, 256 pgs
    objects: 65.53k objects, 1024 MiB
    usage:   3.1 GiB used, 97 GiB / 100 GiB avail
    pgs:     256 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:06:27,281381727-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:06:35,506138405-07:00] INFO: >>   data:
    pools:   2 pools, 256 pgs
    objects: 65.53k objects, 1024 MiB
    usage:   3.1 GiB used, 97 GiB / 100 GiB avail
    pgs:     256 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:06:35,513824202-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:06:35,520037552-07:00] INFO: > The cluster is idle now.[0m


[1;7;39;49m[2021-10-26T22:06:35,524268274-07:00][RUNNING][ROUND 1/3/21] object_size=64KB[0m


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:06:35,527299142-07:00] STAGE: Launch a Ceph cluster (CEPH_HOST: 10.10.2.3, OSD_HOST: 10.10.2.5) ...[0m
# ./benchmarks/bench-rados:172 - launch_ceph_cluster() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.3 bash -euo pipefail
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:06:35,536267961-07:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.3 bash -euo pipefail
10.10.2.3: b'\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T01:06:35,965343859-04:00] INFO: > Remove the existing cluster\x1b[0m\n'
10.10.2.3: b'## bash:13 -  > ls /var/lib/ceph\n## bash:13 -  > tail -n1\n'
10.10.2.3: b'# bash:13 -  > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm --host 10.10.2.3 --host 10.10.2.5 rm-cluster --force --fsid a77e0adc-36e2-11ec-b561-09a495c6360a\n'
10.10.2.3: b'\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T01:06:44,811965231-04:00] INFO: > Deploy a new cluster\x1b[0m\n'
10.10.2.3: b'## bash:22 -  > /tmp/bench-rados/ceph-research/scripts/parallel-ssh --host 10.10.2.5 uname --nodename\n## bash:22 -  > tail -n1\n'
10.10.2.3: b'# bash:22 -  > osd_hostname=sm1\n# bash:23 -  > /tmp/bench-rados/ceph-research/scripts/ceph-deploy -m 10.10.2.3 -o sm1/10.10.2.5:/dev/nvme0n1\n'
10.10.2.3: b'\n\n\x1b[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T01:06:45,925549313-04:00] STAGE: Verify passwordless SSH to hosts, passwordless sudo via ljishen, and the existence of devices on hosts...\x1b[0m\n'
10.10.2.3: b'\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T01:06:45,929372007-04:00] INFO: > Check the host for the monitor\x1b[0m\n# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:181 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@10.10.2.3 sudo --non-interactive --validate\n'
10.10.2.3: b'\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T01:06:46,079684054-04:00] INFO: > Check hosts for OSDs\x1b[0m\n'
10.10.2.3: b"\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T01:06:46,083593070-04:00] INFO: >> Check host 'sm1'\x1b[0m\n# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:200 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 sudo --non-interactive --validate\n"
10.10.2.3: b"\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T01:06:47,159942979-04:00] INFO: >>> Check device '/dev/nvme0n1'\x1b[0m\n"
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:215 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 test -b /dev/nvme0n1\n'
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:226 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 lsblk --nodeps --noheadings --output TYPE /dev/nvme0n1\n'
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:227 - verify_parameters() > grep --word-regexp disk\n'
10.10.2.3: b'\n\n\x1b[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T01:06:49,297215266-04:00] STAGE: Reset devices on hosts...\x1b[0m\n'
10.10.2.3: b"\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T01:06:49,300982445-04:00] INFO: > Resetting device '/dev/nvme0n1' on host 'sm1'\x1b[0m\n"
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:288 - reset_host_devices() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 sudo sh\n'
10.10.2.3: b'  Removing ceph--33ec804e--6dc4--4aed--b3dc--e97cf0ba2543-osd--block--dbfd8b68--ace4--4bb2--aa9b--bdf91cbe8a9f (252:0)\n'
10.10.2.3: b'  Archiving volume group "ceph-33ec804e-6dc4-4aed-b3dc-e97cf0ba2543" metadata (seqno 5).\n'
10.10.2.3: b'  Releasing logical volume "osd-block-dbfd8b68-ace4-4bb2-aa9b-bdf91cbe8a9f"\n'
10.10.2.3: b'  Creating volume group backup "/etc/lvm/backup/ceph-33ec804e-6dc4-4aed-b3dc-e97cf0ba2543" (seqno 6).\n'
10.10.2.3: b'  Logical volume "osd-block-dbfd8b68-ace4-4bb2-aa9b-bdf91cbe8a9f" successfully removed\n'
10.10.2.3: b'  Removing physical volume "/dev/nvme0n1" from volume group "ceph-33ec804e-6dc4-4aed-b3dc-e97cf0ba2543"\n'
10.10.2.3: b'  Volume group "ceph-33ec804e-6dc4-4aed-b3dc-e97cf0ba2543" successfully removed\n'
10.10.2.3: b'\n\n\x1b[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T01:06:51,592915412-04:00] STAGE: Bootstrap a new cluster...\x1b[0m\n'
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:312 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.3 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults\n'
10.10.2.3: b'\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T01:06:51,601368752-04:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.3: b"\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T01:06:51,604163107-04:00] DEBUG: CEPHADM_PARAMS: ['bootstrap', '--config', '/tmp/bench-rados/ceph-research/scripts/ceph.conf', '--mon-ip', '10.10.2.3', '--output-dir', '/tmp/bench-rados/deployment_data_root/etc/ceph', '--ssh-user', 'ljishen', '--skip-dashboard', '--no-minimize-config', '--allow-overwrite', '--allow-fqdn-hostname', '--orphan-initial-daemons', '--skip-monitoring-stack', '--single-host-defaults']\x1b[0m\n# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.3 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults\n"
10.10.2.3: b'Verifying podman|docker is present...\nVerifying lvm2 is present...\nVerifying time synchronization is in place...\n'
10.10.2.3: b'Unit ntp.service is enabled and running\nRepeating the final host check...\npodman|docker (/usr/bin/docker) is present\n'
10.10.2.3: b'systemctl is present\nlvcreate is present\n'
10.10.2.3: b'Unit ntp.service is enabled and running\nHost looks OK\n'
10.10.2.3: b'Cluster fsid: b24a55dc-36e3-11ec-b561-09a495c6360a\n'
10.10.2.3: b'Verifying IP 10.10.2.3 port 3300 ...\nVerifying IP 10.10.2.3 port 6789 ...\n'
10.10.2.3: b'Mon IP `10.10.2.3` is in CIDR network `10.10.2.0/24`\n- internal network (--cluster-network) has not been provided, OSD replication will default to the public_network\n'
10.10.2.3: b'Adjusting default settings to suit single-host cluster...\nPulling container image quay.io/ceph/ceph:v16...\n'
10.10.2.3: b'Ceph version: ceph version 16.2.6 (ee28fb57e47e9f88813e24bbf4c14496ca299d31) pacific (stable)\nExtracting ceph user uid/gid from container image...\n'
10.10.2.3: b'Creating initial keys...\n'
10.10.2.3: b'Creating initial monmap...\n'
10.10.2.3: b'Creating mon...\n'
10.10.2.3: b'Waiting for mon to start...\nWaiting for mon...\n'
10.10.2.3: b'mon is available\nSetting mon public_network to 10.10.2.0/24\n'
10.10.2.3: b'Wrote config to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf\nWrote keyring to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring\nCreating mgr...\n'
10.10.2.3: b'Verifying port 9283 ...\n'
10.10.2.3: b'Waiting for mgr to start...\nWaiting for mgr...\n'
10.10.2.3: b'mgr not available, waiting (1/15)...\n'
10.10.2.3: b'mgr not available, waiting (2/15)...\n'
10.10.2.3: b'mgr is available\n'
10.10.2.3: b'Enabling cephadm module...\n'
10.10.2.3: b'Waiting for the mgr to restart...\nWaiting for mgr epoch 5...\n'
10.10.2.3: b'mgr epoch 5 is available\nSetting orchestrator backend to cephadm...\n'
10.10.2.3: b'Generating ssh key...\n'
10.10.2.3: b'Wrote public SSH key to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub\nAdding key to ljishen@localhost authorized_keys...\n'
10.10.2.3: b'Adding host node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us...\n'
10.10.2.3: b'Deploying unmanaged mon service...\n'
10.10.2.3: b'Deploying unmanaged mgr service...\n'
10.10.2.3: b'You can access the Ceph CLI with:\n\n\tsudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --fsid b24a55dc-36e3-11ec-b561-09a495c6360a -c /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf -k /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring\n\nPlease consider enabling telemetry to help improve Ceph:\n\n\tceph telemetry on\n\nFor more information see:\n\n\thttps://docs.ceph.com/docs/pacific/mgr/telemetry/\n\nBootstrap complete.\n'
10.10.2.3: b'\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T01:07:55,141271201-04:00] INFO: > Set the default number of monitor daemons to 1\x1b[0m\n# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:330 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1\n'
10.10.2.3: b'\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T01:07:55,149842924-04:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.3: b"\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T01:07:55,152776121-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mon', '1']\x1b[0m\n"
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1\n'
10.10.2.3: b'Inferring fsid b24a55dc-36e3-11ec-b561-09a495c6360a\n'
10.10.2.3: b'Inferring config /var/lib/ceph/b24a55dc-36e3-11ec-b561-09a495c6360a/mon.node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.3: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.3: b'Scheduled mon update...\n'
10.10.2.3: b'\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T01:08:04,264709748-04:00] INFO: > Set the default number of manager daemons to 1\x1b[0m\n# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:335 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1\n'
10.10.2.3: b'\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T01:08:04,273729344-04:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.3: b"\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T01:08:04,276717035-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mgr', '1']\x1b[0m\n# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1\n"
10.10.2.3: b'Inferring fsid b24a55dc-36e3-11ec-b561-09a495c6360a\n'
10.10.2.3: b'Inferring config /var/lib/ceph/b24a55dc-36e3-11ec-b561-09a495c6360a/mon.node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.3: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.3: b'Scheduled mgr update...\n'
10.10.2.3: b'\n\n\x1b[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T01:08:13,662269145-04:00] STAGE: Add hosts to the cluster...\x1b[0m\n'
10.10.2.3: b"\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T01:08:13,667051938-04:00] INFO: > Adding host 'sm1'\x1b[0m\n# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:362 - add_hosts() > ssh-copy-id -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -f -i /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub ljishen@sm1\n"
10.10.2.3: b'/usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: "/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub"\n'
10.10.2.3: b'\nNumber of key(s) added: 1\n\nNow try logging into the machine, with:   "ssh -o \'GlobalKnownHostsFile=/dev/null\' -o \'LogLevel=ERROR\' -o \'PasswordAuthentication=no\' -o \'StrictHostKeyChecking=no\' -o \'UserKnownHostsFile=/dev/null\' \'ljishen@sm1\'"\nand check to make sure that only the key(s) you wanted were added.\n\n'
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:368 - add_hosts() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add sm1 10.10.2.5\n'
10.10.2.3: b'\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T01:08:14,804273483-04:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.3: b"\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T01:08:14,807172496-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'host', 'add', 'sm1', '10.10.2.5']\x1b[0m\n"
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add sm1 10.10.2.5\n'
10.10.2.3: b'Inferring fsid b24a55dc-36e3-11ec-b561-09a495c6360a\n'
10.10.2.3: b'Inferring config /var/lib/ceph/b24a55dc-36e3-11ec-b561-09a495c6360a/mon.node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.3: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.3: b"Added host 'sm1' with addr '10.10.2.5'\n"
10.10.2.3: b'\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T01:08:25,499432540-04:00] INFO: Sleep for 20 seconds\x1b[0m\n'
10.10.2.3: b'\n\n\x1b[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T01:08:45,503373954-04:00] STAGE: Add OSDs to the cluster...\x1b[0m\n'
10.10.2.3: b"\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T01:08:45,508313923-04:00] INFO: > Adding devices ['/dev/nvme0n1'] on host 'sm1' as OSDs\x1b[0m\n# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:387 - add_host_devices_as_osds() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd sm1:/dev/nvme0n1\n"
10.10.2.3: b'\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T01:08:45,516138878-04:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.3: b"\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T01:08:45,518828937-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'daemon', 'add', 'osd', 'sm1:/dev/nvme0n1']\x1b[0m\n# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd sm1:/dev/nvme0n1\n"
10.10.2.3: b'Inferring fsid b24a55dc-36e3-11ec-b561-09a495c6360a\n'
10.10.2.3: b'Inferring config /var/lib/ceph/b24a55dc-36e3-11ec-b561-09a495c6360a/mon.node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.3: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.3: b"Created osd(s) 0 on host 'sm1'\n"
10.10.2.3: b'\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T01:09:09,082415449-04:00] INFO: > Sleep for 20 seconds\x1b[0m\n'
10.10.2.3: b'\n\n\x1b[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T01:09:29,086528840-04:00] STAGE: Show Cluster Status\x1b[0m\n# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:343 - show_status() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status\n'
10.10.2.3: b'\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T01:09:29,095011855-04:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.3: b"\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T01:09:29,097770243-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', '--status']\x1b[0m\n"
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status\n'
10.10.2.3: b'Inferring fsid b24a55dc-36e3-11ec-b561-09a495c6360a\n'
10.10.2.3: b'Inferring config /var/lib/ceph/b24a55dc-36e3-11ec-b561-09a495c6360a/mon.node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.3: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.3: b'  cluster:\n    id:     b24a55dc-36e3-11ec-b561-09a495c6360a\n    health: HEALTH_WARN\n            1 pool(s) have no replicas configured\n \n  services:\n    mon: 1 daemons, quorum node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us (age 2m)\n    mgr: node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us.phvbne(active, since 2m)\n    osd: 1 osds: 1 up (since 21s), 1 in (since 40s)\n \n  data:\n    pools:   1 pools, 128 pgs\n    objects: 0 objects, 0 B\n    usage:   4.7 MiB used, 100 GiB / 100 GiB avail\n    pgs:     99.219% pgs unknown\n             127 unknown\n             1   active+clean\n \n  progress:\n \n'
[1] 22:09:39 [SUCCESS] ljishen@10.10.2.3

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T01:06:35,965343859-04:00] INFO: > Remove the existing cluster[0m
## bash:13 -  > ls /var/lib/ceph
## bash:13 -  > tail -n1
# bash:13 -  > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm --host 10.10.2.3 --host 10.10.2.5 rm-cluster --force --fsid a77e0adc-36e2-11ec-b561-09a495c6360a

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T01:06:44,811965231-04:00] INFO: > Deploy a new cluster[0m
## bash:22 -  > /tmp/bench-rados/ceph-research/scripts/parallel-ssh --host 10.10.2.5 uname --nodename
## bash:22 -  > tail -n1
# bash:22 -  > osd_hostname=sm1
# bash:23 -  > /tmp/bench-rados/ceph-research/scripts/ceph-deploy -m 10.10.2.3 -o sm1/10.10.2.5:/dev/nvme0n1


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T01:06:45,925549313-04:00] STAGE: Verify passwordless SSH to hosts, passwordless sudo via ljishen, and the existence of devices on hosts...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T01:06:45,929372007-04:00] INFO: > Check the host for the monitor[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:181 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@10.10.2.3 sudo --non-interactive --validate

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T01:06:46,079684054-04:00] INFO: > Check hosts for OSDs[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T01:06:46,083593070-04:00] INFO: >> Check host 'sm1'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:200 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 sudo --non-interactive --validate

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T01:06:47,159942979-04:00] INFO: >>> Check device '/dev/nvme0n1'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:215 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 test -b /dev/nvme0n1
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:226 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 lsblk --nodeps --noheadings --output TYPE /dev/nvme0n1
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:227 - verify_parameters() > grep --word-regexp disk


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T01:06:49,297215266-04:00] STAGE: Reset devices on hosts...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T01:06:49,300982445-04:00] INFO: > Resetting device '/dev/nvme0n1' on host 'sm1'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:288 - reset_host_devices() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 sudo sh
  Removing ceph--33ec804e--6dc4--4aed--b3dc--e97cf0ba2543-osd--block--dbfd8b68--ace4--4bb2--aa9b--bdf91cbe8a9f (252:0)
  Archiving volume group "ceph-33ec804e-6dc4-4aed-b3dc-e97cf0ba2543" metadata (seqno 5).
  Releasing logical volume "osd-block-dbfd8b68-ace4-4bb2-aa9b-bdf91cbe8a9f"
  Creating volume group backup "/etc/lvm/backup/ceph-33ec804e-6dc4-4aed-b3dc-e97cf0ba2543" (seqno 6).
  Logical volume "osd-block-dbfd8b68-ace4-4bb2-aa9b-bdf91cbe8a9f" successfully removed
  Removing physical volume "/dev/nvme0n1" from volume group "ceph-33ec804e-6dc4-4aed-b3dc-e97cf0ba2543"
  Volume group "ceph-33ec804e-6dc4-4aed-b3dc-e97cf0ba2543" successfully removed


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T01:06:51,592915412-04:00] STAGE: Bootstrap a new cluster...[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:312 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.3 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T01:06:51,601368752-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T01:06:51,604163107-04:00] DEBUG: CEPHADM_PARAMS: ['bootstrap', '--config', '/tmp/bench-rados/ceph-research/scripts/ceph.conf', '--mon-ip', '10.10.2.3', '--output-dir', '/tmp/bench-rados/deployment_data_root/etc/ceph', '--ssh-user', 'ljishen', '--skip-dashboard', '--no-minimize-config', '--allow-overwrite', '--allow-fqdn-hostname', '--orphan-initial-daemons', '--skip-monitoring-stack', '--single-host-defaults'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.3 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults
Verifying podman|docker is present...
Verifying lvm2 is present...
Verifying time synchronization is in place...
Unit ntp.service is enabled and running
Repeating the final host check...
podman|docker (/usr/bin/docker) is present
systemctl is present
lvcreate is present
Unit ntp.service is enabled and running
Host looks OK
Cluster fsid: b24a55dc-36e3-11ec-b561-09a495c6360a
Verifying IP 10.10.2.3 port 3300 ...
Verifying IP 10.10.2.3 port 6789 ...
Mon IP `10.10.2.3` is in CIDR network `10.10.2.0/24`
- internal network (--cluster-network) has not been provided, OSD replication will default to the public_network
Adjusting default settings to suit single-host cluster...
Pulling container image quay.io/ceph/ceph:v16...
Ceph version: ceph version 16.2.6 (ee28fb57e47e9f88813e24bbf4c14496ca299d31) pacific (stable)
Extracting ceph user uid/gid from container image...
Creating initial keys...
Creating initial monmap...
Creating mon...
Waiting for mon to start...
Waiting for mon...
mon is available
Setting mon public_network to 10.10.2.0/24
Wrote config to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf
Wrote keyring to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring
Creating mgr...
Verifying port 9283 ...
Waiting for mgr to start...
Waiting for mgr...
mgr not available, waiting (1/15)...
mgr not available, waiting (2/15)...
mgr is available
Enabling cephadm module...
Waiting for the mgr to restart...
Waiting for mgr epoch 5...
mgr epoch 5 is available
Setting orchestrator backend to cephadm...
Generating ssh key...
Wrote public SSH key to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub
Adding key to ljishen@localhost authorized_keys...
Adding host node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us...
Deploying unmanaged mon service...
Deploying unmanaged mgr service...
You can access the Ceph CLI with:

	sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --fsid b24a55dc-36e3-11ec-b561-09a495c6360a -c /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf -k /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring

Please consider enabling telemetry to help improve Ceph:

	ceph telemetry on

For more information see:

	https://docs.ceph.com/docs/pacific/mgr/telemetry/

Bootstrap complete.

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T01:07:55,141271201-04:00] INFO: > Set the default number of monitor daemons to 1[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:330 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T01:07:55,149842924-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T01:07:55,152776121-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mon', '1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1
Inferring fsid b24a55dc-36e3-11ec-b561-09a495c6360a
Inferring config /var/lib/ceph/b24a55dc-36e3-11ec-b561-09a495c6360a/mon.node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Scheduled mon update...

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T01:08:04,264709748-04:00] INFO: > Set the default number of manager daemons to 1[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:335 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T01:08:04,273729344-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T01:08:04,276717035-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mgr', '1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1
Inferring fsid b24a55dc-36e3-11ec-b561-09a495c6360a
Inferring config /var/lib/ceph/b24a55dc-36e3-11ec-b561-09a495c6360a/mon.node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Scheduled mgr update...


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T01:08:13,662269145-04:00] STAGE: Add hosts to the cluster...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T01:08:13,667051938-04:00] INFO: > Adding host 'sm1'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:362 - add_hosts() > ssh-copy-id -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -f -i /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub ljishen@sm1
/usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: "/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub"

Number of key(s) added: 1

Now try logging into the machine, with:   "ssh -o 'GlobalKnownHostsFile=/dev/null' -o 'LogLevel=ERROR' -o 'PasswordAuthentication=no' -o 'StrictHostKeyChecking=no' -o 'UserKnownHostsFile=/dev/null' 'ljishen@sm1'"
and check to make sure that only the key(s) you wanted were added.

# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:368 - add_hosts() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add sm1 10.10.2.5
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T01:08:14,804273483-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T01:08:14,807172496-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'host', 'add', 'sm1', '10.10.2.5'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add sm1 10.10.2.5
Inferring fsid b24a55dc-36e3-11ec-b561-09a495c6360a
Inferring config /var/lib/ceph/b24a55dc-36e3-11ec-b561-09a495c6360a/mon.node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Added host 'sm1' with addr '10.10.2.5'

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T01:08:25,499432540-04:00] INFO: Sleep for 20 seconds[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T01:08:45,503373954-04:00] STAGE: Add OSDs to the cluster...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T01:08:45,508313923-04:00] INFO: > Adding devices ['/dev/nvme0n1'] on host 'sm1' as OSDs[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:387 - add_host_devices_as_osds() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd sm1:/dev/nvme0n1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T01:08:45,516138878-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T01:08:45,518828937-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'daemon', 'add', 'osd', 'sm1:/dev/nvme0n1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd sm1:/dev/nvme0n1
Inferring fsid b24a55dc-36e3-11ec-b561-09a495c6360a
Inferring config /var/lib/ceph/b24a55dc-36e3-11ec-b561-09a495c6360a/mon.node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Created osd(s) 0 on host 'sm1'

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T01:09:09,082415449-04:00] INFO: > Sleep for 20 seconds[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T01:09:29,086528840-04:00] STAGE: Show Cluster Status[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:343 - show_status() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T01:09:29,095011855-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T01:09:29,097770243-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', '--status'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status
Inferring fsid b24a55dc-36e3-11ec-b561-09a495c6360a
Inferring config /var/lib/ceph/b24a55dc-36e3-11ec-b561-09a495c6360a/mon.node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
  cluster:
    id:     b24a55dc-36e3-11ec-b561-09a495c6360a
    health: HEALTH_WARN
            1 pool(s) have no replicas configured
 
  services:
    mon: 1 daemons, quorum node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us (age 2m)
    mgr: node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us.phvbne(active, since 2m)
    osd: 1 osds: 1 up (since 21s), 1 in (since 40s)
 
  data:
    pools:   1 pools, 128 pgs
    objects: 0 objects, 0 B
    usage:   4.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     99.219% pgs unknown
             127 unknown
             1   active+clean
 
  progress:
 

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:09:39,092103110-07:00] INFO: > Copy configuration files to local[0m
# ./benchmarks/bench-rados:208 - launch_ceph_cluster() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.3:/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf
# ./benchmarks/bench-rados:212 - launch_ceph_cluster() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.3 sudo chmod 644 /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:09:39,247318094-07:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.3 sudo chmod 644 /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring
[1] 22:09:39 [SUCCESS] ljishen@10.10.2.3
# ./benchmarks/bench-rados:214 - launch_ceph_cluster() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.3:/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:09:39,574022977-07:00] STAGE: Update local ceph.conf as a client...[0m


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:09:39,577849900-07:00] STAGE: Configure Ceph pool...[0m
# ./benchmarks/bench-rados:245 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_max_backfills 32
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:09:39,602274081-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:09:39,605534811-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'b24a55dc-36e3-11ec-b561-09a495c6360a', '--', 'ceph', 'config', 'set', 'osd', 'osd_max_backfills', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid b24a55dc-36e3-11ec-b561-09a495c6360a -- ceph config set osd osd_max_backfills 32
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:246 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_active 32
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:09:42,661106798-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:09:42,664584925-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'b24a55dc-36e3-11ec-b561-09a495c6360a', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_active', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid b24a55dc-36e3-11ec-b561-09a495c6360a -- ceph config set osd osd_recovery_max_active 32
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:247 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_single_start 8
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:09:46,037769324-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:09:46,041407893-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'b24a55dc-36e3-11ec-b561-09a495c6360a', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_single_start', '8'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid b24a55dc-36e3-11ec-b561-09a495c6360a -- ceph config set osd osd_recovery_max_single_start 8
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:248 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_op_priority 63
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:09:49,126475725-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:09:49,129799823-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'b24a55dc-36e3-11ec-b561-09a495c6360a', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_op_priority', '63'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid b24a55dc-36e3-11ec-b561-09a495c6360a -- ceph config set osd osd_recovery_op_priority 63
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:250 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config show-with-defaults osd.0
# ./benchmarks/bench-rados:251 - configure_ceph_pool() > grep 'osd_max_backfills\|osd_recovery'
# ./benchmarks/bench-rados:251 - configure_ceph_pool() > column -t -s ' '
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
osd_max_backfills                        32         mon
osd_recovery_cost                        20971520   default
osd_recovery_delay_start                 0.000000   default
osd_recovery_max_active                  32         mon
osd_recovery_max_active_hdd              3          default
osd_recovery_max_active_ssd              10         default
osd_recovery_max_chunk                   8388608    default
osd_recovery_max_omap_entries_per_chunk  8096       default
osd_recovery_max_single_start            8          mon
osd_recovery_op_priority                 63         mon
osd_recovery_op_warn_multiple            16         default
osd_recovery_priority                    5          default
osd_recovery_retry_interval              30.000000  default
osd_recovery_sleep                       0.000000   default
osd_recovery_sleep_hdd                   0.100000   default
osd_recovery_sleep_hybrid                0.025000   default
osd_recovery_sleep_ssd                   0.000000   default
# ./benchmarks/bench-rados:253 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool create bench_rados 128 128 replicated --size 1 --pg-num-min 128
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:09:55,533667753-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:09:55,537391283-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'b24a55dc-36e3-11ec-b561-09a495c6360a', '--', 'ceph', 'osd', 'pool', 'create', 'bench_rados', '128', '128', 'replicated', '--size', '1', '--pg-num-min', '128'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid b24a55dc-36e3-11ec-b561-09a495c6360a -- ceph osd pool create bench_rados 128 128 replicated --size 1 --pg-num-min 128
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
pool 'bench_rados' created
# ./benchmarks/bench-rados:255 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados min_size 1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:09:58,998886072-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:09:59,002476891-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'b24a55dc-36e3-11ec-b561-09a495c6360a', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'min_size', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid b24a55dc-36e3-11ec-b561-09a495c6360a -- ceph osd pool set bench_rados min_size 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 min_size to 1
# ./benchmarks/bench-rados:256 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados pg_autoscale_mode off
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:10:03,365868066-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:10:03,369308793-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'b24a55dc-36e3-11ec-b561-09a495c6360a', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'pg_autoscale_mode', 'off'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid b24a55dc-36e3-11ec-b561-09a495c6360a -- ceph osd pool set bench_rados pg_autoscale_mode off
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 pg_autoscale_mode to off
# ./benchmarks/bench-rados:257 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool application enable bench_rados benchmark
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:10:07,489005764-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:10:07,491810025-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'b24a55dc-36e3-11ec-b561-09a495c6360a', '--', 'ceph', 'osd', 'pool', 'application', 'enable', 'bench_rados', 'benchmark'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid b24a55dc-36e3-11ec-b561-09a495c6360a -- ceph osd pool application enable bench_rados benchmark
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
enabled application 'benchmark' on pool 'bench_rados'
# ./benchmarks/bench-rados:258 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados noscrub 1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:10:10,994768354-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:10:10,998482005-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'b24a55dc-36e3-11ec-b561-09a495c6360a', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'noscrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid b24a55dc-36e3-11ec-b561-09a495c6360a -- ceph osd pool set bench_rados noscrub 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 noscrub to 1
# ./benchmarks/bench-rados:259 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados nodeep-scrub 1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:10:15,018389488-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:10:15,021723124-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'b24a55dc-36e3-11ec-b561-09a495c6360a', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'nodeep-scrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid b24a55dc-36e3-11ec-b561-09a495c6360a -- ceph osd pool set bench_rados nodeep-scrub 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 nodeep-scrub to 1
# ./benchmarks/bench-rados:260 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool ls detail
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:10:19,191855912-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:10:19,194933918-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'b24a55dc-36e3-11ec-b561-09a495c6360a', '--', 'ceph', 'osd', 'pool', 'ls', 'detail'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid b24a55dc-36e3-11ec-b561-09a495c6360a -- ceph osd pool ls detail
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
pool 1 'device_health_metrics' replicated size 1 min_size 1 crush_rule 0 object_hash rjenkins pg_num 128 pgp_num 128 autoscale_mode on last_change 13 lfor 0/0/11 flags hashpspool stripe_width 0 pg_num_min 1 application mgr_devicehealth
pool 2 'bench_rados' replicated size 1 min_size 1 crush_rule 0 object_hash rjenkins pg_num 128 pgp_num 128 autoscale_mode off last_change 21 flags hashpspool,noscrub,nodeep-scrub stripe_width 0 pg_num_min 128 application benchmark

# ./benchmarks/bench-rados:261 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd df tree
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:10:22,414311247-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:10:22,417632891-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'b24a55dc-36e3-11ec-b561-09a495c6360a', '--', 'ceph', 'osd', 'df', 'tree'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid b24a55dc-36e3-11ec-b561-09a495c6360a -- ceph osd df tree
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
ID  CLASS  WEIGHT   REWEIGHT  SIZE     RAW USE  DATA     OMAP  META     AVAIL    %USE  VAR   PGS  STATUS  TYPE NAME    
-1         0.09769         -  100 GiB  6.0 MiB  176 KiB   0 B  5.8 MiB  100 GiB  0.01  1.00    -          root default 
-3         0.09769         -  100 GiB  6.0 MiB  176 KiB   0 B  5.8 MiB  100 GiB  0.01  1.00    -              host sm1 
 0    ssd  0.09769   1.00000  100 GiB  6.0 MiB  176 KiB   0 B  5.8 MiB  100 GiB  0.01  1.00  256      up          osd.0
                       TOTAL  100 GiB  6.0 MiB  176 KiB   0 B  5.8 MiB  100 GiB  0.01                                  
MIN/MAX VAR: 1.00/1.00  STDDEV: 0


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:10:25,733449011-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:10:48,972192750-07:00] INFO: >>   data:
    pools:   2 pools, 256 pgs
    objects: 0 objects, 0 B
    usage:   6.0 MiB used, 100 GiB / 100 GiB avail
    pgs:     256 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:10:48,979526516-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:10:57,227709058-07:00] INFO: >>   data:
    pools:   2 pools, 256 pgs
    objects: 0 objects, 0 B
    usage:   6.0 MiB used, 100 GiB / 100 GiB avail
    pgs:     256 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:10:57,235736668-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:11:05,492939977-07:00] INFO: >>   data:
    pools:   2 pools, 256 pgs
    objects: 0 objects, 0 B
    usage:   6.0 MiB used, 100 GiB / 100 GiB avail
    pgs:     256 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:11:05,500286107-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:11:13,839226320-07:00] INFO: >>   data:
    pools:   2 pools, 256 pgs
    objects: 0 objects, 0 B
    usage:   6.0 MiB used, 100 GiB / 100 GiB avail
    pgs:     256 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:11:13,846546019-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:11:22,096351412-07:00] INFO: >>   data:
    pools:   2 pools, 256 pgs
    objects: 0 objects, 0 B
    usage:   6.0 MiB used, 100 GiB / 100 GiB avail
    pgs:     256 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:11:22,104608052-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:11:22,110350687-07:00] INFO: > The cluster is idle now.[0m


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:11:22,114321892-07:00] STAGE: Start benchmarking of mode write ...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:11:22,121395529-07:00] INFO: > Start system activity collection services[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:11:22,126891119-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:329 - rados_bench() > rm --force /mnt/sda8/ceph-research/scripts/output/sys_activity_64KB_write.dat.rados_bench_host.1
# ./benchmarks/bench-rados:330 - rados_bench() > sar_pid_rados_bench_host=792578
# ./benchmarks/bench-rados:330 - rados_bench() > S_TIME_FORMAT=ISO
# ./benchmarks/bench-rados:330 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/output/sys_activity_64KB_write.dat.rados_bench_host.1 2

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:11:22,133303532-07:00] INFO: >> for OSD host[0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:11:22,141559923-07:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.5 bash
10.10.2.5: b'65737\n'
[1] 22:11:23 [SUCCESS] ljishen@10.10.2.5
65737

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:11:23,246110957-07:00] INFO: > Run rados bench[0m
# ./benchmarks/bench-rados:358 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/output/rados_bench_64KB_write.log.1
## ./benchmarks/bench-rados:349 - rados_bench() > count_cpus 32-63,96-127
# ./benchmarks/bench-rados:349 - rados_bench() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 30 write --pool bench_rados -b 65536 -O 65536 --concurrent-ios 128 --show-time --write-object --write-omap --write-xattr --no-cleanup
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:11:23,266761515-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:11:23,270547301-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'b24a55dc-36e3-11ec-b561-09a495c6360a', '--', 'rados', 'bench', '30', 'write', '--pool', 'bench_rados', '-b', '65536', '-O', '65536', '--concurrent-ios', '128', '--show-time', '--write-object', '--write-omap', '--write-xattr', '--no-cleanup'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid b24a55dc-36e3-11ec-b561-09a495c6360a -- rados bench 30 write --pool bench_rados -b 65536 -O 65536 --concurrent-ios 128 --show-time --write-object --write-omap --write-xattr --no-cleanup
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
hints = 1
2021-10-27T05:11:25.653861+0000 Maintaining 128 concurrent writes of 65536 bytes to objects of size 65536 for up to 30 seconds or 0 objects
2021-10-27T05:11:25.653870+0000 Object prefix: benchmark_data_node-0.bluefield2.ucsc-cmps10_7
2021-10-27T05:11:25.656304+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-27T05:11:25.656304+0000     0       0         0         0         0         0           -           0
2021-10-27T05:11:26.656410+0000     1     128      1101       973   60.8051   60.8125    0.126474    0.126437
2021-10-27T05:11:27.656488+0000     2     128      2050      1922   60.0565   59.3125    0.137478    0.127843
2021-10-27T05:11:28.656559+0000     3     128      3074      2946   61.3694        64     0.13102    0.128051
2021-10-27T05:11:29.656671+0000     4     128      4040      3912   61.1191    60.375    0.161306    0.128458
2021-10-27T05:11:30.656746+0000     5     128      5064      4936   61.6943        64    0.125676    0.128718
2021-10-27T05:11:31.656818+0000     6     128      6018      5890   61.3487    59.625    0.120749    0.128513
2021-10-27T05:11:32.656889+0000     7     128      7042      6914   61.7268        64    0.106431    0.128402
2021-10-27T05:11:33.656995+0000     8     128      8023      7895   61.6742   61.3125    0.128898    0.128511
2021-10-27T05:11:34.657067+0000     9     128      8578      8450   58.6755   34.6875    0.258146    0.134257
2021-10-27T05:11:35.657136+0000    10     128      9090      8962   56.0077        32    0.257023    0.140996
2021-10-27T05:11:36.657311+0000    11     128      9602      9474   53.8245        32    0.230221    0.147285
2021-10-27T05:11:37.657550+0000    12     128     10056      9928   51.7029    28.375    0.286158    0.152695
2021-10-27T05:11:38.657621+0000    13     128     10498     10370   49.8507    27.625    0.298851    0.158101
2021-10-27T05:11:39.657691+0000    14     128     11010     10882   48.5755        32    0.269113    0.163362
2021-10-27T05:11:40.657779+0000    15     128     11464     11336   47.2286    28.375    0.259637    0.167555
2021-10-27T05:11:41.657881+0000    16     128     11976     11848   46.2766        32    0.281759    0.171941
2021-10-27T05:11:42.657951+0000    17     128     12802     12674    46.591    51.625    0.121008    0.170483
2021-10-27T05:11:43.658021+0000    18     128     13768     13640   47.3566    60.375    0.122283    0.168284
2021-10-27T05:11:44.658096+0000    19     128     14792     14664   48.2323        64    0.150259    0.165383
2021-10-27T05:11:45.658200+0000 min lat: 0.0820122 max lat: 0.308263 avg lat: 0.163196
2021-10-27T05:11:45.658200+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-27T05:11:45.658200+0000    20     128     15746     15618   48.8016    59.625    0.135901    0.163196
2021-10-27T05:11:46.658283+0000    21     128     16514     16386   48.7632        48    0.288428     0.16329
2021-10-27T05:11:47.658355+0000    22     128     16958     16830    47.808     27.75    0.327424     0.16627
2021-10-27T05:11:48.658423+0000    23     128     17224     17096   46.4522    16.625    0.503431    0.171169
2021-10-27T05:11:49.658526+0000    24     128     17480     17352   45.1833        16    0.476621    0.175992
2021-10-27T05:11:50.658594+0000    25     128     17707     17579   43.9434   14.1875    0.480228    0.180354
2021-10-27T05:11:51.658790+0000    26     128     17922     17794   42.7699   13.4375    0.507936    0.184284
2021-10-27T05:11:52.658919+0000    27     128     18178     18050   41.7783        16    0.487051    0.188744
2021-10-27T05:11:53.659171+0000    28     128     18434     18306   40.8574        16    0.565502    0.193477
2021-10-27T05:11:54.659252+0000    29     128     18690     18562   40.0002        16    0.507824    0.198017
2021-10-27T05:11:55.659362+0000    30     128     18946     18818   39.2001        16    0.498756    0.202169
2021-10-27T05:11:56.659650+0000 Total time run:         30.3723
Total writes made:      18946
Write size:             65536
Object size:            65536
Bandwidth (MB/sec):     38.9869
Stddev Bandwidth:       19.4768
Max bandwidth (MB/sec): 64
Min bandwidth (MB/sec): 13.4375
Average IOPS:           623
Stddev IOPS:            311.629
Max IOPS:               1024
Min IOPS:               215
Average Latency(s):     0.204342
Stddev Latency(s):      0.122739
Max latency(s):         0.565502
Min latency(s):         0.0820122

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:11:57,192770692-07:00] INFO: > Stop system activity collection services[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:11:57,198465256-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:379 - rados_bench() > kill -INT 792578

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:11:57,204643139-07:00] INFO: >> for OSD host[0m
# ./benchmarks/bench-rados:384 - rados_bench() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.5 kill -INT 65737
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:11:57,213307385-07:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.5 kill -INT 65737
[1] 22:11:58 [SUCCESS] ljishen@10.10.2.5

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:11:58,439275413-07:00] INFO: >>> Copy system activity file from OSD host[0m
## ./benchmarks/bench-rados:391 - rados_bench() > basename -- /tmp/bench-rados/sys_activity_64KB_write.dat.osd_host.1
# ./benchmarks/bench-rados:391 - rados_bench() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.5:/tmp/bench-rados/sys_activity_64KB_write.dat.osd_host.1 /mnt/sda8/ceph-research/scripts/output/sys_activity_64KB_write.dat.osd_host.1


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:11:59,689956956-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:12:22,877631298-07:00] INFO: >>   data:
    pools:   2 pools, 256 pgs
    objects: 18.95k objects, 1.2 GiB
    usage:   3.5 GiB used, 96 GiB / 100 GiB avail
    pgs:     256 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:12:22,885361609-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:12:31,233656987-07:00] INFO: >>   data:
    pools:   2 pools, 256 pgs
    objects: 18.95k objects, 1.2 GiB
    usage:   3.5 GiB used, 96 GiB / 100 GiB avail
    pgs:     256 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:12:31,241665091-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:12:39,430577170-07:00] INFO: >>   data:
    pools:   2 pools, 256 pgs
    objects: 18.95k objects, 1.2 GiB
    usage:   3.5 GiB used, 96 GiB / 100 GiB avail
    pgs:     256 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:12:39,438109729-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:12:47,670574796-07:00] INFO: >>   data:
    pools:   2 pools, 256 pgs
    objects: 18.95k objects, 1.2 GiB
    usage:   3.5 GiB used, 96 GiB / 100 GiB avail
    pgs:     256 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:12:47,678448116-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:12:55,902170379-07:00] INFO: >>   data:
    pools:   2 pools, 256 pgs
    objects: 18.95k objects, 1.2 GiB
    usage:   3.5 GiB used, 96 GiB / 100 GiB avail
    pgs:     256 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:12:55,909903555-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:12:55,915888836-07:00] INFO: > The cluster is idle now.[0m


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:12:55,918981319-07:00] STAGE: Start benchmarking of mode seq ...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:12:55,925598709-07:00] INFO: > Start system activity collection services[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:12:55,930976046-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:329 - rados_bench() > rm --force /mnt/sda8/ceph-research/scripts/output/sys_activity_64KB_seq.dat.rados_bench_host.1
# ./benchmarks/bench-rados:330 - rados_bench() > sar_pid_rados_bench_host=794367
# ./benchmarks/bench-rados:330 - rados_bench() > S_TIME_FORMAT=ISO
# ./benchmarks/bench-rados:330 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/output/sys_activity_64KB_seq.dat.rados_bench_host.1 2

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:12:55,937333116-07:00] INFO: >> for OSD host[0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:12:55,945706356-07:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.5 bash
10.10.2.5: b'66268\n'
[1] 22:12:57 [SUCCESS] ljishen@10.10.2.5
66268

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:12:57,047838213-07:00] INFO: > Run rados bench[0m
# ./benchmarks/bench-rados:367 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/output/rados_bench_64KB_seq.log.1
## ./benchmarks/bench-rados:364 - rados_bench() > count_cpus 32-63,96-127
# ./benchmarks/bench-rados:364 - rados_bench() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 128 --show-time
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:12:57,068615689-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:12:57,071674679-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'b24a55dc-36e3-11ec-b561-09a495c6360a', '--', 'rados', 'bench', '99999999', 'seq', '--pool', 'bench_rados', '--concurrent-ios', '128', '--show-time'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid b24a55dc-36e3-11ec-b561-09a495c6360a -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 128 --show-time
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
hints = 1
2021-10-27T05:12:59.419999+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-27T05:12:59.419999+0000     0       0         0         0         0         0           -           0
2021-10-27T05:13:00.420081+0000     1     128      8662      8534   533.305   533.375   0.0143809   0.0148616
2021-10-27T05:13:01.420194+0000     2     128     17882     17754   554.745    576.25   0.0175335   0.0143622
2021-10-27T05:13:02.420366+0000 Total time run:       2.12474
Total reads made:     18946
Read size:            65536
Object size:          65536
Bandwidth (MB/sec):   557.302
Average IOPS:         8916
Stddev IOPS:          485.075
Max IOPS:             9220
Min IOPS:             8534
Average Latency(s):   0.0143101
Max latency(s):       0.0332932
Min latency(s):       0.00347114

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:13:03,040004028-07:00] INFO: > Stop system activity collection services[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:13:03,046400963-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:379 - rados_bench() > kill -INT 794367

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:13:03,052487203-07:00] INFO: >> for OSD host[0m
# ./benchmarks/bench-rados:384 - rados_bench() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.5 kill -INT 66268
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:13:03,061795862-07:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.5 kill -INT 66268
[1] 22:13:04 [SUCCESS] ljishen@10.10.2.5

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:13:04,159282908-07:00] INFO: >>> Copy system activity file from OSD host[0m
## ./benchmarks/bench-rados:391 - rados_bench() > basename -- /tmp/bench-rados/sys_activity_64KB_seq.dat.osd_host.1
# ./benchmarks/bench-rados:391 - rados_bench() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.5:/tmp/bench-rados/sys_activity_64KB_seq.dat.osd_host.1 /mnt/sda8/ceph-research/scripts/output/sys_activity_64KB_seq.dat.osd_host.1


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:13:05,237739962-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:13:28,400615469-07:00] INFO: >>   data:
    pools:   2 pools, 256 pgs
    objects: 18.95k objects, 1.2 GiB
    usage:   3.5 GiB used, 96 GiB / 100 GiB avail
    pgs:     256 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:13:28,408084619-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:13:36,639317690-07:00] INFO: >>   data:
    pools:   2 pools, 256 pgs
    objects: 18.95k objects, 1.2 GiB
    usage:   3.5 GiB used, 96 GiB / 100 GiB avail
    pgs:     256 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:13:36,646621420-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:13:44,837747370-07:00] INFO: >>   data:
    pools:   2 pools, 256 pgs
    objects: 18.95k objects, 1.2 GiB
    usage:   3.5 GiB used, 96 GiB / 100 GiB avail
    pgs:     256 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:13:44,845620640-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:13:53,120218667-07:00] INFO: >>   data:
    pools:   2 pools, 256 pgs
    objects: 18.95k objects, 1.2 GiB
    usage:   3.5 GiB used, 96 GiB / 100 GiB avail
    pgs:     256 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:13:53,128016945-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:14:01,375982430-07:00] INFO: >>   data:
    pools:   2 pools, 256 pgs
    objects: 18.95k objects, 1.2 GiB
    usage:   3.5 GiB used, 96 GiB / 100 GiB avail
    pgs:     256 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:14:01,383642759-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:14:01,390241513-07:00] INFO: > The cluster is idle now.[0m


[1;7;39;49m[2021-10-26T22:14:01,392574067-07:00][RUNNING][ROUND 2/3/21] object_size=64KB[0m


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:14:01,395743075-07:00] STAGE: Launch a Ceph cluster (CEPH_HOST: 10.10.2.3, OSD_HOST: 10.10.2.5) ...[0m
# ./benchmarks/bench-rados:172 - launch_ceph_cluster() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.3 bash -euo pipefail
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:14:01,404871343-07:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.3 bash -euo pipefail
10.10.2.3: b'\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T01:14:01,994704551-04:00] INFO: > Remove the existing cluster\x1b[0m\n'
10.10.2.3: b'## bash:13 -  > ls /var/lib/ceph\n'
10.10.2.3: b'## bash:13 -  > tail -n1\n'
10.10.2.3: b'# bash:13 -  > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm --host 10.10.2.3 --host 10.10.2.5 rm-cluster --force --fsid b24a55dc-36e3-11ec-b561-09a495c6360a\n'
10.10.2.3: b'\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T01:14:10,981094690-04:00] INFO: > Deploy a new cluster\x1b[0m\n'
10.10.2.3: b'## bash:22 -  > /tmp/bench-rados/ceph-research/scripts/parallel-ssh --host 10.10.2.5 uname --nodename\n## bash:22 -  > tail -n1\n'
10.10.2.3: b'# bash:22 -  > osd_hostname=sm1\n# bash:23 -  > /tmp/bench-rados/ceph-research/scripts/ceph-deploy -m 10.10.2.3 -o sm1/10.10.2.5:/dev/nvme0n1\n'
10.10.2.3: b'\n\n\x1b[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T01:14:12,105235547-04:00] STAGE: Verify passwordless SSH to hosts, passwordless sudo via ljishen, and the existence of devices on hosts...\x1b[0m\n'
10.10.2.3: b'\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T01:14:12,108813479-04:00] INFO: > Check the host for the monitor\x1b[0m\n'
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:181 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@10.10.2.3 sudo --non-interactive --validate\n'
10.10.2.3: b'\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T01:14:12,260125219-04:00] INFO: > Check hosts for OSDs\x1b[0m\n'
10.10.2.3: b"\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T01:14:12,263984992-04:00] INFO: >> Check host 'sm1'\x1b[0m\n"
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:200 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 sudo --non-interactive --validate\n'
10.10.2.3: b"\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T01:14:13,332311633-04:00] INFO: >>> Check device '/dev/nvme0n1'\x1b[0m\n# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:215 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 test -b /dev/nvme0n1\n"
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:226 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 lsblk --nodeps --noheadings --output TYPE /dev/nvme0n1\n# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:227 - verify_parameters() > grep --word-regexp disk\n'
10.10.2.3: b'\n\n\x1b[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T01:14:15,469534167-04:00] STAGE: Reset devices on hosts...\x1b[0m\n'
10.10.2.3: b"\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T01:14:15,473105757-04:00] INFO: > Resetting device '/dev/nvme0n1' on host 'sm1'\x1b[0m\n# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:288 - reset_host_devices() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 sudo sh\n"
10.10.2.3: b'  Removing ceph--bc1a1116--deab--4a69--8120--606d45414d29-osd--block--69f2cbf9--6f89--4898--b1c7--1f939f6b4307 (252:0)\n'
10.10.2.3: b'  Archiving volume group "ceph-bc1a1116-deab-4a69-8120-606d45414d29" metadata (seqno 5).\n'
10.10.2.3: b'  Releasing logical volume "osd-block-69f2cbf9-6f89-4898-b1c7-1f939f6b4307"\n'
10.10.2.3: b'  Creating volume group backup "/etc/lvm/backup/ceph-bc1a1116-deab-4a69-8120-606d45414d29" (seqno 6).\n'
10.10.2.3: b'  Logical volume "osd-block-69f2cbf9-6f89-4898-b1c7-1f939f6b4307" successfully removed\n'
10.10.2.3: b'  Removing physical volume "/dev/nvme0n1" from volume group "ceph-bc1a1116-deab-4a69-8120-606d45414d29"\n'
10.10.2.3: b'  Volume group "ceph-bc1a1116-deab-4a69-8120-606d45414d29" successfully removed\n'
10.10.2.3: b'\n\n\x1b[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T01:14:17,731995722-04:00] STAGE: Bootstrap a new cluster...\x1b[0m\n# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:312 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.3 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults\n'
10.10.2.3: b'\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T01:14:17,740143116-04:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.3: b"\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T01:14:17,742964252-04:00] DEBUG: CEPHADM_PARAMS: ['bootstrap', '--config', '/tmp/bench-rados/ceph-research/scripts/ceph.conf', '--mon-ip', '10.10.2.3', '--output-dir', '/tmp/bench-rados/deployment_data_root/etc/ceph', '--ssh-user', 'ljishen', '--skip-dashboard', '--no-minimize-config', '--allow-overwrite', '--allow-fqdn-hostname', '--orphan-initial-daemons', '--skip-monitoring-stack', '--single-host-defaults']\x1b[0m\n"
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.3 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults\n'
10.10.2.3: b'Verifying podman|docker is present...\nVerifying lvm2 is present...\nVerifying time synchronization is in place...\n'
10.10.2.3: b'Unit ntp.service is enabled and running\nRepeating the final host check...\npodman|docker (/usr/bin/docker) is present\nsystemctl is present\n'
10.10.2.3: b'lvcreate is present\n'
10.10.2.3: b'Unit ntp.service is enabled and running\n'
10.10.2.3: b'Host looks OK\n'
10.10.2.3: b'Cluster fsid: bc365c2a-36e4-11ec-b561-09a495c6360a\n'
10.10.2.3: b'Verifying IP 10.10.2.3 port 3300 ...\n'
10.10.2.3: b'Verifying IP 10.10.2.3 port 6789 ...\n'
10.10.2.3: b'Mon IP `10.10.2.3` is in CIDR network `10.10.2.0/24`\n- internal network (--cluster-network) has not been provided, OSD replication will default to the public_network\n'
10.10.2.3: b'Adjusting default settings to suit single-host cluster...\n'
10.10.2.3: b'Pulling container image quay.io/ceph/ceph:v16...\n'
10.10.2.3: b'Ceph version: ceph version 16.2.6 (ee28fb57e47e9f88813e24bbf4c14496ca299d31) pacific (stable)\nExtracting ceph user uid/gid from container image...\n'
10.10.2.3: b'Creating initial keys...\n'
10.10.2.3: b'Creating initial monmap...\n'
10.10.2.3: b'Creating mon...\n'
10.10.2.3: b'Waiting for mon to start...\nWaiting for mon...\n'
10.10.2.3: b'mon is available\nSetting mon public_network to 10.10.2.0/24\n'
10.10.2.3: b'Wrote config to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf\nWrote keyring to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring\n'
10.10.2.3: b'Creating mgr...\n'
10.10.2.3: b'Verifying port 9283 ...\n'
10.10.2.3: b'Waiting for mgr to start...\nWaiting for mgr...\n'
10.10.2.3: b'mgr not available, waiting (1/15)...\n'
10.10.2.3: b'mgr not available, waiting (2/15)...\n'
10.10.2.3: b'mgr is available\n'
10.10.2.3: b'Enabling cephadm module...\n'
10.10.2.3: b'Waiting for the mgr to restart...\nWaiting for mgr epoch 5...\n'
10.10.2.3: b'mgr epoch 5 is available\nSetting orchestrator backend to cephadm...\n'
10.10.2.3: b'Generating ssh key...\n'
10.10.2.3: b'Wrote public SSH key to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub\nAdding key to ljishen@localhost authorized_keys...\n'
10.10.2.3: b'Adding host node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us...\n'
10.10.2.3: b'Deploying unmanaged mon service...\n'
10.10.2.3: b'Deploying unmanaged mgr service...\n'
10.10.2.3: b'You can access the Ceph CLI with:\n\n\tsudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --fsid bc365c2a-36e4-11ec-b561-09a495c6360a -c /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf -k /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring\n\nPlease consider enabling telemetry to help improve Ceph:\n\n\tceph telemetry on\n\nFor more information see:\n\n\thttps://docs.ceph.com/docs/pacific/mgr/telemetry/\n\nBootstrap complete.\n'
10.10.2.3: b'\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T01:15:22,248376400-04:00] INFO: > Set the default number of monitor daemons to 1\x1b[0m\n# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:330 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1\n'
10.10.2.3: b'\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T01:15:22,257151796-04:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.3: b"\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T01:15:22,260246828-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mon', '1']\x1b[0m\n"
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1\n'
10.10.2.3: b'Inferring fsid bc365c2a-36e4-11ec-b561-09a495c6360a\n'
10.10.2.3: b'Inferring config /var/lib/ceph/bc365c2a-36e4-11ec-b561-09a495c6360a/mon.node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.3: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.3: b'Scheduled mon update...\n'
10.10.2.3: b'\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T01:15:31,562105074-04:00] INFO: > Set the default number of manager daemons to 1\x1b[0m\n# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:335 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1\n'
10.10.2.3: b'\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T01:15:31,570911087-04:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.3: b"\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T01:15:31,573904077-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mgr', '1']\x1b[0m\n"
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1\n'
10.10.2.3: b'Inferring fsid bc365c2a-36e4-11ec-b561-09a495c6360a\n'
10.10.2.3: b'Inferring config /var/lib/ceph/bc365c2a-36e4-11ec-b561-09a495c6360a/mon.node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.3: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.3: b'Scheduled mgr update...\n'
10.10.2.3: b'\n\n\x1b[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T01:15:41,335155778-04:00] STAGE: Add hosts to the cluster...\x1b[0m\n'
10.10.2.3: b"\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T01:15:41,340027348-04:00] INFO: > Adding host 'sm1'\x1b[0m\n"
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:362 - add_hosts() > ssh-copy-id -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -f -i /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub ljishen@sm1\n'
10.10.2.3: b'/usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: "/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub"\n'
10.10.2.3: b'\nNumber of key(s) added: 1\n\nNow try logging into the machine, with:   "ssh -o \'GlobalKnownHostsFile=/dev/null\' -o \'LogLevel=ERROR\' -o \'PasswordAuthentication=no\' -o \'StrictHostKeyChecking=no\' -o \'UserKnownHostsFile=/dev/null\' \'ljishen@sm1\'"\nand check to make sure that only the key(s) you wanted were added.\n\n'
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:368 - add_hosts() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add sm1 10.10.2.5\n'
10.10.2.3: b'\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T01:15:42,476115859-04:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.3: b"\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T01:15:42,479105974-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'host', 'add', 'sm1', '10.10.2.5']\x1b[0m\n"
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add sm1 10.10.2.5\n'
10.10.2.3: b'Inferring fsid bc365c2a-36e4-11ec-b561-09a495c6360a\n'
10.10.2.3: b'Inferring config /var/lib/ceph/bc365c2a-36e4-11ec-b561-09a495c6360a/mon.node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.3: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.3: b"Added host 'sm1' with addr '10.10.2.5'\n"
10.10.2.3: b'\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T01:15:53,485342200-04:00] INFO: Sleep for 20 seconds\x1b[0m\n'
10.10.2.3: b'\n\n\x1b[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T01:16:13,489068513-04:00] STAGE: Add OSDs to the cluster...\x1b[0m\n'
10.10.2.3: b"\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T01:16:13,494215622-04:00] INFO: > Adding devices ['/dev/nvme0n1'] on host 'sm1' as OSDs\x1b[0m\n# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:387 - add_host_devices_as_osds() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd sm1:/dev/nvme0n1\n"
10.10.2.3: b'\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T01:16:13,502519560-04:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.3: b"\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T01:16:13,505244935-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'daemon', 'add', 'osd', 'sm1:/dev/nvme0n1']\x1b[0m\n"
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd sm1:/dev/nvme0n1\n'
10.10.2.3: b'Inferring fsid bc365c2a-36e4-11ec-b561-09a495c6360a\n'
10.10.2.3: b'Inferring config /var/lib/ceph/bc365c2a-36e4-11ec-b561-09a495c6360a/mon.node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.3: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.3: b"Created osd(s) 0 on host 'sm1'\n"
10.10.2.3: b'\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T01:16:37,375780516-04:00] INFO: > Sleep for 20 seconds\x1b[0m\n'
10.10.2.3: b'\n\n\x1b[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T01:16:57,380279304-04:00] STAGE: Show Cluster Status\x1b[0m\n# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:343 - show_status() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status\n'
10.10.2.3: b'\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T01:16:57,388509453-04:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.3: b"\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T01:16:57,391530135-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', '--status']\x1b[0m\n"
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status\n'
10.10.2.3: b'Inferring fsid bc365c2a-36e4-11ec-b561-09a495c6360a\n'
10.10.2.3: b'Inferring config /var/lib/ceph/bc365c2a-36e4-11ec-b561-09a495c6360a/mon.node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.3: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.3: b'  cluster:\n    id:     bc365c2a-36e4-11ec-b561-09a495c6360a\n    health: HEALTH_WARN\n            1 pool(s) have no replicas configured\n \n  services:\n    mon: 1 daemons, quorum node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us (age 2m)\n    mgr: node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us.equyjr(active, since 2m)\n    osd: 1 osds: 1 up (since 20s), 1 in (since 40s)\n \n  data:\n    pools:   1 pools, 128 pgs\n    objects: 0 objects, 0 B\n    usage:   4.7 MiB used, 100 GiB / 100 GiB avail\n    pgs:     99.219% pgs unknown\n             127 unknown\n             1   active+clean\n \n  progress:\n \n'
[1] 22:17:06 [SUCCESS] ljishen@10.10.2.3

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T01:14:01,994704551-04:00] INFO: > Remove the existing cluster[0m
## bash:13 -  > ls /var/lib/ceph
## bash:13 -  > tail -n1
# bash:13 -  > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm --host 10.10.2.3 --host 10.10.2.5 rm-cluster --force --fsid b24a55dc-36e3-11ec-b561-09a495c6360a

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T01:14:10,981094690-04:00] INFO: > Deploy a new cluster[0m
## bash:22 -  > /tmp/bench-rados/ceph-research/scripts/parallel-ssh --host 10.10.2.5 uname --nodename
## bash:22 -  > tail -n1
# bash:22 -  > osd_hostname=sm1
# bash:23 -  > /tmp/bench-rados/ceph-research/scripts/ceph-deploy -m 10.10.2.3 -o sm1/10.10.2.5:/dev/nvme0n1


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T01:14:12,105235547-04:00] STAGE: Verify passwordless SSH to hosts, passwordless sudo via ljishen, and the existence of devices on hosts...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T01:14:12,108813479-04:00] INFO: > Check the host for the monitor[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:181 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@10.10.2.3 sudo --non-interactive --validate

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T01:14:12,260125219-04:00] INFO: > Check hosts for OSDs[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T01:14:12,263984992-04:00] INFO: >> Check host 'sm1'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:200 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 sudo --non-interactive --validate

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T01:14:13,332311633-04:00] INFO: >>> Check device '/dev/nvme0n1'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:215 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 test -b /dev/nvme0n1
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:226 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 lsblk --nodeps --noheadings --output TYPE /dev/nvme0n1
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:227 - verify_parameters() > grep --word-regexp disk


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T01:14:15,469534167-04:00] STAGE: Reset devices on hosts...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T01:14:15,473105757-04:00] INFO: > Resetting device '/dev/nvme0n1' on host 'sm1'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:288 - reset_host_devices() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 sudo sh
  Removing ceph--bc1a1116--deab--4a69--8120--606d45414d29-osd--block--69f2cbf9--6f89--4898--b1c7--1f939f6b4307 (252:0)
  Archiving volume group "ceph-bc1a1116-deab-4a69-8120-606d45414d29" metadata (seqno 5).
  Releasing logical volume "osd-block-69f2cbf9-6f89-4898-b1c7-1f939f6b4307"
  Creating volume group backup "/etc/lvm/backup/ceph-bc1a1116-deab-4a69-8120-606d45414d29" (seqno 6).
  Logical volume "osd-block-69f2cbf9-6f89-4898-b1c7-1f939f6b4307" successfully removed
  Removing physical volume "/dev/nvme0n1" from volume group "ceph-bc1a1116-deab-4a69-8120-606d45414d29"
  Volume group "ceph-bc1a1116-deab-4a69-8120-606d45414d29" successfully removed


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T01:14:17,731995722-04:00] STAGE: Bootstrap a new cluster...[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:312 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.3 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T01:14:17,740143116-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T01:14:17,742964252-04:00] DEBUG: CEPHADM_PARAMS: ['bootstrap', '--config', '/tmp/bench-rados/ceph-research/scripts/ceph.conf', '--mon-ip', '10.10.2.3', '--output-dir', '/tmp/bench-rados/deployment_data_root/etc/ceph', '--ssh-user', 'ljishen', '--skip-dashboard', '--no-minimize-config', '--allow-overwrite', '--allow-fqdn-hostname', '--orphan-initial-daemons', '--skip-monitoring-stack', '--single-host-defaults'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.3 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults
Verifying podman|docker is present...
Verifying lvm2 is present...
Verifying time synchronization is in place...
Unit ntp.service is enabled and running
Repeating the final host check...
podman|docker (/usr/bin/docker) is present
systemctl is present
lvcreate is present
Unit ntp.service is enabled and running
Host looks OK
Cluster fsid: bc365c2a-36e4-11ec-b561-09a495c6360a
Verifying IP 10.10.2.3 port 3300 ...
Verifying IP 10.10.2.3 port 6789 ...
Mon IP `10.10.2.3` is in CIDR network `10.10.2.0/24`
- internal network (--cluster-network) has not been provided, OSD replication will default to the public_network
Adjusting default settings to suit single-host cluster...
Pulling container image quay.io/ceph/ceph:v16...
Ceph version: ceph version 16.2.6 (ee28fb57e47e9f88813e24bbf4c14496ca299d31) pacific (stable)
Extracting ceph user uid/gid from container image...
Creating initial keys...
Creating initial monmap...
Creating mon...
Waiting for mon to start...
Waiting for mon...
mon is available
Setting mon public_network to 10.10.2.0/24
Wrote config to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf
Wrote keyring to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring
Creating mgr...
Verifying port 9283 ...
Waiting for mgr to start...
Waiting for mgr...
mgr not available, waiting (1/15)...
mgr not available, waiting (2/15)...
mgr is available
Enabling cephadm module...
Waiting for the mgr to restart...
Waiting for mgr epoch 5...
mgr epoch 5 is available
Setting orchestrator backend to cephadm...
Generating ssh key...
Wrote public SSH key to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub
Adding key to ljishen@localhost authorized_keys...
Adding host node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us...
Deploying unmanaged mon service...
Deploying unmanaged mgr service...
You can access the Ceph CLI with:

	sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --fsid bc365c2a-36e4-11ec-b561-09a495c6360a -c /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf -k /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring

Please consider enabling telemetry to help improve Ceph:

	ceph telemetry on

For more information see:

	https://docs.ceph.com/docs/pacific/mgr/telemetry/

Bootstrap complete.

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T01:15:22,248376400-04:00] INFO: > Set the default number of monitor daemons to 1[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:330 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T01:15:22,257151796-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T01:15:22,260246828-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mon', '1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1
Inferring fsid bc365c2a-36e4-11ec-b561-09a495c6360a
Inferring config /var/lib/ceph/bc365c2a-36e4-11ec-b561-09a495c6360a/mon.node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Scheduled mon update...

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T01:15:31,562105074-04:00] INFO: > Set the default number of manager daemons to 1[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:335 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T01:15:31,570911087-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T01:15:31,573904077-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mgr', '1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1
Inferring fsid bc365c2a-36e4-11ec-b561-09a495c6360a
Inferring config /var/lib/ceph/bc365c2a-36e4-11ec-b561-09a495c6360a/mon.node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Scheduled mgr update...


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T01:15:41,335155778-04:00] STAGE: Add hosts to the cluster...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T01:15:41,340027348-04:00] INFO: > Adding host 'sm1'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:362 - add_hosts() > ssh-copy-id -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -f -i /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub ljishen@sm1
/usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: "/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub"

Number of key(s) added: 1

Now try logging into the machine, with:   "ssh -o 'GlobalKnownHostsFile=/dev/null' -o 'LogLevel=ERROR' -o 'PasswordAuthentication=no' -o 'StrictHostKeyChecking=no' -o 'UserKnownHostsFile=/dev/null' 'ljishen@sm1'"
and check to make sure that only the key(s) you wanted were added.

# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:368 - add_hosts() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add sm1 10.10.2.5
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T01:15:42,476115859-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T01:15:42,479105974-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'host', 'add', 'sm1', '10.10.2.5'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add sm1 10.10.2.5
Inferring fsid bc365c2a-36e4-11ec-b561-09a495c6360a
Inferring config /var/lib/ceph/bc365c2a-36e4-11ec-b561-09a495c6360a/mon.node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Added host 'sm1' with addr '10.10.2.5'

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T01:15:53,485342200-04:00] INFO: Sleep for 20 seconds[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T01:16:13,489068513-04:00] STAGE: Add OSDs to the cluster...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T01:16:13,494215622-04:00] INFO: > Adding devices ['/dev/nvme0n1'] on host 'sm1' as OSDs[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:387 - add_host_devices_as_osds() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd sm1:/dev/nvme0n1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T01:16:13,502519560-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T01:16:13,505244935-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'daemon', 'add', 'osd', 'sm1:/dev/nvme0n1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd sm1:/dev/nvme0n1
Inferring fsid bc365c2a-36e4-11ec-b561-09a495c6360a
Inferring config /var/lib/ceph/bc365c2a-36e4-11ec-b561-09a495c6360a/mon.node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Created osd(s) 0 on host 'sm1'

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T01:16:37,375780516-04:00] INFO: > Sleep for 20 seconds[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T01:16:57,380279304-04:00] STAGE: Show Cluster Status[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:343 - show_status() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T01:16:57,388509453-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T01:16:57,391530135-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', '--status'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status
Inferring fsid bc365c2a-36e4-11ec-b561-09a495c6360a
Inferring config /var/lib/ceph/bc365c2a-36e4-11ec-b561-09a495c6360a/mon.node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
  cluster:
    id:     bc365c2a-36e4-11ec-b561-09a495c6360a
    health: HEALTH_WARN
            1 pool(s) have no replicas configured
 
  services:
    mon: 1 daemons, quorum node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us (age 2m)
    mgr: node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us.equyjr(active, since 2m)
    osd: 1 osds: 1 up (since 20s), 1 in (since 40s)
 
  data:
    pools:   1 pools, 128 pgs
    objects: 0 objects, 0 B
    usage:   4.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     99.219% pgs unknown
             127 unknown
             1   active+clean
 
  progress:
 

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:17:06,796185175-07:00] INFO: > Copy configuration files to local[0m
# ./benchmarks/bench-rados:208 - launch_ceph_cluster() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.3:/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf
# ./benchmarks/bench-rados:212 - launch_ceph_cluster() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.3 sudo chmod 644 /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:17:06,950887176-07:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.3 sudo chmod 644 /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring
[1] 22:17:07 [SUCCESS] ljishen@10.10.2.3
# ./benchmarks/bench-rados:214 - launch_ceph_cluster() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.3:/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:17:07,277962925-07:00] STAGE: Update local ceph.conf as a client...[0m


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:17:07,281690712-07:00] STAGE: Configure Ceph pool...[0m
# ./benchmarks/bench-rados:245 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_max_backfills 32
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:17:07,305737183-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:17:07,309332552-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'bc365c2a-36e4-11ec-b561-09a495c6360a', '--', 'ceph', 'config', 'set', 'osd', 'osd_max_backfills', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid bc365c2a-36e4-11ec-b561-09a495c6360a -- ceph config set osd osd_max_backfills 32
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:246 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_active 32
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:17:10,532706303-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:17:10,536186615-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'bc365c2a-36e4-11ec-b561-09a495c6360a', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_active', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid bc365c2a-36e4-11ec-b561-09a495c6360a -- ceph config set osd osd_recovery_max_active 32
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:247 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_single_start 8
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:17:13,813829679-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:17:13,817463610-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'bc365c2a-36e4-11ec-b561-09a495c6360a', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_single_start', '8'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid bc365c2a-36e4-11ec-b561-09a495c6360a -- ceph config set osd osd_recovery_max_single_start 8
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:248 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_op_priority 63
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:17:16,946705268-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:17:16,949953714-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'bc365c2a-36e4-11ec-b561-09a495c6360a', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_op_priority', '63'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid bc365c2a-36e4-11ec-b561-09a495c6360a -- ceph config set osd osd_recovery_op_priority 63
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:250 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config show-with-defaults osd.0
# ./benchmarks/bench-rados:251 - configure_ceph_pool() > grep 'osd_max_backfills\|osd_recovery'
# ./benchmarks/bench-rados:251 - configure_ceph_pool() > column -t -s ' '
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
osd_max_backfills                        32         mon
osd_recovery_cost                        20971520   default
osd_recovery_delay_start                 0.000000   default
osd_recovery_max_active                  32         mon
osd_recovery_max_active_hdd              3          default
osd_recovery_max_active_ssd              10         default
osd_recovery_max_chunk                   8388608    default
osd_recovery_max_omap_entries_per_chunk  8096       default
osd_recovery_max_single_start            8          mon
osd_recovery_op_priority                 63         mon
osd_recovery_op_warn_multiple            16         default
osd_recovery_priority                    5          default
osd_recovery_retry_interval              30.000000  default
osd_recovery_sleep                       0.000000   default
osd_recovery_sleep_hdd                   0.100000   default
osd_recovery_sleep_hybrid                0.025000   default
osd_recovery_sleep_ssd                   0.000000   default
# ./benchmarks/bench-rados:253 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool create bench_rados 128 128 replicated --size 1 --pg-num-min 128
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:17:23,451751307-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:17:23,455461181-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'bc365c2a-36e4-11ec-b561-09a495c6360a', '--', 'ceph', 'osd', 'pool', 'create', 'bench_rados', '128', '128', 'replicated', '--size', '1', '--pg-num-min', '128'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid bc365c2a-36e4-11ec-b561-09a495c6360a -- ceph osd pool create bench_rados 128 128 replicated --size 1 --pg-num-min 128
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
pool 'bench_rados' created
# ./benchmarks/bench-rados:255 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados min_size 1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:17:26,884711078-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:17:26,888131197-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'bc365c2a-36e4-11ec-b561-09a495c6360a', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'min_size', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid bc365c2a-36e4-11ec-b561-09a495c6360a -- ceph osd pool set bench_rados min_size 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 min_size to 1
# ./benchmarks/bench-rados:256 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados pg_autoscale_mode off
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:17:31,175427822-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:17:31,178752853-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'bc365c2a-36e4-11ec-b561-09a495c6360a', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'pg_autoscale_mode', 'off'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid bc365c2a-36e4-11ec-b561-09a495c6360a -- ceph osd pool set bench_rados pg_autoscale_mode off
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 pg_autoscale_mode to off
# ./benchmarks/bench-rados:257 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool application enable bench_rados benchmark
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:17:35,466855032-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:17:35,470293937-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'bc365c2a-36e4-11ec-b561-09a495c6360a', '--', 'ceph', 'osd', 'pool', 'application', 'enable', 'bench_rados', 'benchmark'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid bc365c2a-36e4-11ec-b561-09a495c6360a -- ceph osd pool application enable bench_rados benchmark
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
enabled application 'benchmark' on pool 'bench_rados'
# ./benchmarks/bench-rados:258 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados noscrub 1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:17:39,113771787-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:17:39,117468345-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'bc365c2a-36e4-11ec-b561-09a495c6360a', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'noscrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid bc365c2a-36e4-11ec-b561-09a495c6360a -- ceph osd pool set bench_rados noscrub 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 noscrub to 1
# ./benchmarks/bench-rados:259 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados nodeep-scrub 1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:17:43,303789132-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:17:43,307077674-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'bc365c2a-36e4-11ec-b561-09a495c6360a', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'nodeep-scrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid bc365c2a-36e4-11ec-b561-09a495c6360a -- ceph osd pool set bench_rados nodeep-scrub 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 nodeep-scrub to 1
# ./benchmarks/bench-rados:260 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool ls detail
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:17:46,918187801-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:17:46,921550873-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'bc365c2a-36e4-11ec-b561-09a495c6360a', '--', 'ceph', 'osd', 'pool', 'ls', 'detail'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid bc365c2a-36e4-11ec-b561-09a495c6360a -- ceph osd pool ls detail
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
pool 1 'device_health_metrics' replicated size 1 min_size 1 crush_rule 0 object_hash rjenkins pg_num 128 pgp_num 128 autoscale_mode on last_change 13 lfor 0/0/11 flags hashpspool stripe_width 0 pg_num_min 1 application mgr_devicehealth
pool 2 'bench_rados' replicated size 1 min_size 1 crush_rule 0 object_hash rjenkins pg_num 128 pgp_num 128 autoscale_mode off last_change 21 flags hashpspool,noscrub,nodeep-scrub stripe_width 0 pg_num_min 128 application benchmark

# ./benchmarks/bench-rados:261 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd df tree
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:17:50,309808394-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:17:50,313429641-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'bc365c2a-36e4-11ec-b561-09a495c6360a', '--', 'ceph', 'osd', 'df', 'tree'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid bc365c2a-36e4-11ec-b561-09a495c6360a -- ceph osd df tree
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
ID  CLASS  WEIGHT   REWEIGHT  SIZE     RAW USE  DATA     OMAP  META     AVAIL    %USE  VAR   PGS  STATUS  TYPE NAME    
-1         0.09769         -  100 GiB  6.0 MiB  176 KiB   0 B  5.8 MiB  100 GiB  0.01  1.00    -          root default 
-3         0.09769         -  100 GiB  6.0 MiB  176 KiB   0 B  5.8 MiB  100 GiB  0.01  1.00    -              host sm1 
 0    ssd  0.09769   1.00000  100 GiB  6.0 MiB  176 KiB   0 B  5.8 MiB  100 GiB  0.01  1.00  256      up          osd.0
                       TOTAL  100 GiB  6.0 MiB  176 KiB   0 B  5.8 MiB  100 GiB  0.01                                  
MIN/MAX VAR: 1.00/1.00  STDDEV: 0


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:17:53,493624097-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:18:16,909026499-07:00] INFO: >>   data:
    pools:   2 pools, 256 pgs
    objects: 0 objects, 0 B
    usage:   6.0 MiB used, 100 GiB / 100 GiB avail
    pgs:     256 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:18:16,916612870-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:18:25,166563395-07:00] INFO: >>   data:
    pools:   2 pools, 256 pgs
    objects: 0 objects, 0 B
    usage:   6.0 MiB used, 100 GiB / 100 GiB avail
    pgs:     256 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:18:25,174430945-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:18:33,390575263-07:00] INFO: >>   data:
    pools:   2 pools, 256 pgs
    objects: 0 objects, 0 B
    usage:   6.0 MiB used, 100 GiB / 100 GiB avail
    pgs:     256 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:18:33,398900693-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:18:41,814255233-07:00] INFO: >>   data:
    pools:   2 pools, 256 pgs
    objects: 0 objects, 0 B
    usage:   6.0 MiB used, 100 GiB / 100 GiB avail
    pgs:     256 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:18:41,821710487-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:18:50,014094017-07:00] INFO: >>   data:
    pools:   2 pools, 256 pgs
    objects: 0 objects, 0 B
    usage:   6.0 MiB used, 100 GiB / 100 GiB avail
    pgs:     256 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:18:50,022091640-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:18:50,028336539-07:00] INFO: > The cluster is idle now.[0m


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:18:50,031972724-07:00] STAGE: Start benchmarking of mode write ...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:18:50,039345783-07:00] INFO: > Start system activity collection services[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:18:50,044958353-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:329 - rados_bench() > rm --force /mnt/sda8/ceph-research/scripts/output/sys_activity_64KB_write.dat.rados_bench_host.2
# ./benchmarks/bench-rados:330 - rados_bench() > sar_pid_rados_bench_host=801173
# ./benchmarks/bench-rados:330 - rados_bench() > S_TIME_FORMAT=ISO
# ./benchmarks/bench-rados:330 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/output/sys_activity_64KB_write.dat.rados_bench_host.2 2

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:18:50,051832796-07:00] INFO: >> for OSD host[0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:18:50,060438332-07:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.5 bash
10.10.2.5: b'71045\n'
[1] 22:18:51 [SUCCESS] ljishen@10.10.2.5
71045

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:18:51,160976452-07:00] INFO: > Run rados bench[0m
# ./benchmarks/bench-rados:358 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/output/rados_bench_64KB_write.log.2
## ./benchmarks/bench-rados:349 - rados_bench() > count_cpus 32-63,96-127
# ./benchmarks/bench-rados:349 - rados_bench() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 30 write --pool bench_rados -b 65536 -O 65536 --concurrent-ios 128 --show-time --write-object --write-omap --write-xattr --no-cleanup
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:18:51,182909050-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:18:51,186727999-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'bc365c2a-36e4-11ec-b561-09a495c6360a', '--', 'rados', 'bench', '30', 'write', '--pool', 'bench_rados', '-b', '65536', '-O', '65536', '--concurrent-ios', '128', '--show-time', '--write-object', '--write-omap', '--write-xattr', '--no-cleanup'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid bc365c2a-36e4-11ec-b561-09a495c6360a -- rados bench 30 write --pool bench_rados -b 65536 -O 65536 --concurrent-ios 128 --show-time --write-object --write-omap --write-xattr --no-cleanup
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
hints = 1
2021-10-27T05:18:53.625232+0000 Maintaining 128 concurrent writes of 65536 bytes to objects of size 65536 for up to 30 seconds or 0 objects
2021-10-27T05:18:53.625242+0000 Object prefix: benchmark_data_node-0.bluefield2.ucsc-cmps10_7
2021-10-27T05:18:53.627504+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-27T05:18:53.627504+0000     0       0         0         0         0         0           -           0
2021-10-27T05:18:54.627694+0000     1     128      1061       933   58.3007   58.3125    0.149959     0.12653
2021-10-27T05:18:55.627764+0000     2     128      2084      1956   61.1167   63.9375    0.123765    0.127424
2021-10-27T05:18:56.627835+0000     3     128      3052      2924   60.9097      60.5    0.116501    0.127899
2021-10-27T05:18:57.627939+0000     4     128      4133      4005   62.5711   67.5625    0.148584    0.126102
2021-10-27T05:18:58.628009+0000     5     128      5116      4988   62.3435   61.4375    0.125022    0.126491
2021-10-27T05:18:59.628082+0000     6     128      6124      5996   62.4522        63    0.157572    0.126516
2021-10-27T05:19:00.628151+0000     7     128      7148      7020   62.6727        64   0.0928465    0.126595
2021-10-27T05:19:01.628256+0000     8     128      8172      8044   62.8377        64    0.118638     0.12637
2021-10-27T05:19:02.628328+0000     9     128      8684      8556   59.4111        32    0.251773    0.133236
2021-10-27T05:19:03.628402+0000    10     128      9125      8997   56.2261   27.5625    0.268985    0.139845
2021-10-27T05:19:04.628473+0000    11     128      9637      9509   54.0236        32      0.2717    0.146752
2021-10-27T05:19:05.628576+0000    12     128     10092      9964   51.8911   28.4375    0.265701    0.152242
2021-10-27T05:19:06.628645+0000    13     127     10533     10406   50.0244    27.625    0.273073    0.157297
2021-10-27T05:19:07.628714+0000    14     128     11045     10917   48.7323   31.9375    0.265632     0.16237
2021-10-27T05:19:08.628943+0000    15     128     11526     11398   47.4871   30.0625    0.257843    0.166895
2021-10-27T05:19:09.629046+0000    16     128     12012     11884   46.4174    30.375    0.245366    0.170903
2021-10-27T05:19:10.629118+0000    17     128     12908     12780   46.9808        56    0.109111    0.169753
2021-10-27T05:19:11.629191+0000    18     128     13832     13704   47.5788     57.75     0.11703    0.167646
2021-10-27T05:19:12.629326+0000    19     128     14821     14693   48.3276   61.8125     0.14203    0.164853
2021-10-27T05:19:13.629431+0000 min lat: 0.0928465 max lat: 0.315109 avg lat: 0.162614
2021-10-27T05:19:13.629431+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-27T05:19:13.629431+0000    20     127     15817     15690   49.0265   62.3125    0.126097    0.162614
2021-10-27T05:19:14.629510+0000    21     128     16549     16421   48.8673   45.6875    0.252747    0.163131
2021-10-27T05:19:15.629580+0000    22     128     16933     16805   47.7369        24    0.276934    0.165569
2021-10-27T05:19:16.629652+0000    23     128     17189     17061   46.3571        16    0.536423    0.170262
2021-10-27T05:19:17.629787+0000    24     128     17445     17317    45.092        16    0.506702    0.175298
2021-10-27T05:19:18.629857+0000    25     128     17701     17573   43.9283        16    0.535552    0.180189
2021-10-27T05:19:19.629958+0000    26     128     17957     17829   42.8541        16    0.512908    0.184911
2021-10-27T05:19:20.630185+0000    27     128     18213     18085   41.8592        16    0.506085    0.189726
2021-10-27T05:19:21.630401+0000    28     128     18469     18341   40.9355        16    0.514356    0.194205
2021-10-27T05:19:22.630593+0000    29     128     18661     18533   39.9375        12    0.527972    0.197642
2021-10-27T05:19:23.630662+0000    30     128     18917     18789   39.1396        16    0.498208    0.201829
2021-10-27T05:19:24.630796+0000 Total time run:         30.2828
Total writes made:      18917
Write size:             65536
Object size:            65536
Bandwidth (MB/sec):     39.0423
Stddev Bandwidth:       20.0704
Max bandwidth (MB/sec): 67.5625
Min bandwidth (MB/sec): 12
Average IOPS:           624
Stddev IOPS:            321.127
Max IOPS:               1081
Min IOPS:               192
Average Latency(s):     0.204016
Stddev Latency(s):      0.123972
Max latency(s):         0.54777
Min latency(s):         0.0928465

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:19:25,147589991-07:00] INFO: > Stop system activity collection services[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:19:25,153411444-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:379 - rados_bench() > kill -INT 801173

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:19:25,159638499-07:00] INFO: >> for OSD host[0m
# ./benchmarks/bench-rados:384 - rados_bench() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.5 kill -INT 71045
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:19:25,168094665-07:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.5 kill -INT 71045
[1] 22:19:26 [SUCCESS] ljishen@10.10.2.5

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:19:26,412679116-07:00] INFO: >>> Copy system activity file from OSD host[0m
## ./benchmarks/bench-rados:391 - rados_bench() > basename -- /tmp/bench-rados/sys_activity_64KB_write.dat.osd_host.2
# ./benchmarks/bench-rados:391 - rados_bench() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.5:/tmp/bench-rados/sys_activity_64KB_write.dat.osd_host.2 /mnt/sda8/ceph-research/scripts/output/sys_activity_64KB_write.dat.osd_host.2


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:19:27,650305647-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:19:50,742202893-07:00] INFO: >>   data:
    pools:   2 pools, 256 pgs
    objects: 18.92k objects, 1.2 GiB
    usage:   3.5 GiB used, 97 GiB / 100 GiB avail
    pgs:     256 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:19:50,749987155-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:19:59,032722427-07:00] INFO: >>   data:
    pools:   2 pools, 256 pgs
    objects: 18.92k objects, 1.2 GiB
    usage:   3.5 GiB used, 97 GiB / 100 GiB avail
    pgs:     256 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:19:59,041626093-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:20:07,357493407-07:00] INFO: >>   data:
    pools:   2 pools, 256 pgs
    objects: 18.92k objects, 1.2 GiB
    usage:   3.5 GiB used, 97 GiB / 100 GiB avail
    pgs:     256 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:20:07,365575018-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:20:15,624490966-07:00] INFO: >>   data:
    pools:   2 pools, 256 pgs
    objects: 18.92k objects, 1.2 GiB
    usage:   3.5 GiB used, 97 GiB / 100 GiB avail
    pgs:     256 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:20:15,631958474-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:20:23,840842276-07:00] INFO: >>   data:
    pools:   2 pools, 256 pgs
    objects: 18.92k objects, 1.2 GiB
    usage:   3.5 GiB used, 97 GiB / 100 GiB avail
    pgs:     256 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:20:23,849214284-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:20:23,856376416-07:00] INFO: > The cluster is idle now.[0m


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:20:23,860494557-07:00] STAGE: Start benchmarking of mode seq ...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:20:23,867609582-07:00] INFO: > Start system activity collection services[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:20:23,873568513-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:329 - rados_bench() > rm --force /mnt/sda8/ceph-research/scripts/output/sys_activity_64KB_seq.dat.rados_bench_host.2
# ./benchmarks/bench-rados:330 - rados_bench() > sar_pid_rados_bench_host=804212
# ./benchmarks/bench-rados:330 - rados_bench() > S_TIME_FORMAT=ISO
# ./benchmarks/bench-rados:330 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/output/sys_activity_64KB_seq.dat.rados_bench_host.2 2

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:20:23,880489643-07:00] INFO: >> for OSD host[0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:20:23,889414800-07:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.5 bash
10.10.2.5: b'71444\n'
[1] 22:20:24 [SUCCESS] ljishen@10.10.2.5
71444

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:20:24,978664913-07:00] INFO: > Run rados bench[0m
# ./benchmarks/bench-rados:367 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/output/rados_bench_64KB_seq.log.2
## ./benchmarks/bench-rados:364 - rados_bench() > count_cpus 32-63,96-127
# ./benchmarks/bench-rados:364 - rados_bench() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 128 --show-time
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:20:24,999337452-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:20:25,002657873-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'bc365c2a-36e4-11ec-b561-09a495c6360a', '--', 'rados', 'bench', '99999999', 'seq', '--pool', 'bench_rados', '--concurrent-ios', '128', '--show-time'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid bc365c2a-36e4-11ec-b561-09a495c6360a -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 128 --show-time
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
hints = 1
2021-10-27T05:20:27.367865+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-27T05:20:27.367865+0000     0       1         1         0         0         0           -           0
2021-10-27T05:20:28.367990+0000     1     128      8899      8771   548.084   548.188    0.015242   0.0145003
2021-10-27T05:20:29.368127+0000     2     127     17856     17729   553.941   559.875   0.0152273   0.0143903
2021-10-27T05:20:30.368416+0000 Total time run:       2.13039
Total reads made:     18917
Read size:            65536
Object size:          65536
Bandwidth (MB/sec):   554.976
Average IOPS:         8879
Stddev IOPS:          132.229
Max IOPS:             8958
Min IOPS:             8771
Average Latency(s):   0.0143673
Max latency(s):       0.0225388
Min latency(s):       0.00336673

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:20:31,047657673-07:00] INFO: > Stop system activity collection services[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:20:31,054103520-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:379 - rados_bench() > kill -INT 804212

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:20:31,060536332-07:00] INFO: >> for OSD host[0m
# ./benchmarks/bench-rados:384 - rados_bench() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.5 kill -INT 71444
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:20:31,068641828-07:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.5 kill -INT 71444
[1] 22:20:32 [SUCCESS] ljishen@10.10.2.5

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:20:32,146982905-07:00] INFO: >>> Copy system activity file from OSD host[0m
## ./benchmarks/bench-rados:391 - rados_bench() > basename -- /tmp/bench-rados/sys_activity_64KB_seq.dat.osd_host.2
# ./benchmarks/bench-rados:391 - rados_bench() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.5:/tmp/bench-rados/sys_activity_64KB_seq.dat.osd_host.2 /mnt/sda8/ceph-research/scripts/output/sys_activity_64KB_seq.dat.osd_host.2


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:20:33,205322234-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:20:56,448872447-07:00] INFO: >>   data:
    pools:   2 pools, 256 pgs
    objects: 18.92k objects, 1.2 GiB
    usage:   3.5 GiB used, 97 GiB / 100 GiB avail
    pgs:     256 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:20:56,457085246-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:21:04,782824915-07:00] INFO: >>   data:
    pools:   2 pools, 256 pgs
    objects: 18.92k objects, 1.2 GiB
    usage:   3.5 GiB used, 97 GiB / 100 GiB avail
    pgs:     256 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:21:04,790633053-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:21:12,925455477-07:00] INFO: >>   data:
    pools:   2 pools, 256 pgs
    objects: 18.92k objects, 1.2 GiB
    usage:   3.5 GiB used, 97 GiB / 100 GiB avail
    pgs:     256 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:21:12,933004007-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:21:21,198456402-07:00] INFO: >>   data:
    pools:   2 pools, 256 pgs
    objects: 18.92k objects, 1.2 GiB
    usage:   3.5 GiB used, 97 GiB / 100 GiB avail
    pgs:     256 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:21:21,206677044-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:21:29,506663745-07:00] INFO: >>   data:
    pools:   2 pools, 256 pgs
    objects: 18.92k objects, 1.2 GiB
    usage:   3.5 GiB used, 97 GiB / 100 GiB avail
    pgs:     256 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:21:29,514340886-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:21:29,520638936-07:00] INFO: > The cluster is idle now.[0m


[1;7;39;49m[2021-10-26T22:21:29,522833461-07:00][RUNNING][ROUND 3/3/21] object_size=64KB[0m


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:21:29,526436293-07:00] STAGE: Launch a Ceph cluster (CEPH_HOST: 10.10.2.3, OSD_HOST: 10.10.2.5) ...[0m
# ./benchmarks/bench-rados:172 - launch_ceph_cluster() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.3 bash -euo pipefail
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:21:29,534963822-07:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.3 bash -euo pipefail
10.10.2.3: b'\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T01:21:29,886279780-04:00] INFO: > Remove the existing cluster\x1b[0m\n'
10.10.2.3: b'## bash:13 -  > ls /var/lib/ceph\n'
10.10.2.3: b'## bash:13 -  > tail -n1\n'
10.10.2.3: b'# bash:13 -  > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm --host 10.10.2.3 --host 10.10.2.5 rm-cluster --force --fsid bc365c2a-36e4-11ec-b561-09a495c6360a\n'
10.10.2.3: b'\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T01:21:38,867937168-04:00] INFO: > Deploy a new cluster\x1b[0m\n'
10.10.2.3: b'## bash:22 -  > /tmp/bench-rados/ceph-research/scripts/parallel-ssh --host 10.10.2.5 uname --nodename\n## bash:22 -  > tail -n1\n'
10.10.2.3: b'# bash:22 -  > osd_hostname=sm1\n'
10.10.2.3: b'# bash:23 -  > /tmp/bench-rados/ceph-research/scripts/ceph-deploy -m 10.10.2.3 -o sm1/10.10.2.5:/dev/nvme0n1\n'
10.10.2.3: b'\n\n\x1b[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T01:21:40,005171439-04:00] STAGE: Verify passwordless SSH to hosts, passwordless sudo via ljishen, and the existence of devices on hosts...\x1b[0m\n'
10.10.2.3: b'\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T01:21:40,009059305-04:00] INFO: > Check the host for the monitor\x1b[0m\n'
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:181 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@10.10.2.3 sudo --non-interactive --validate\n'
10.10.2.3: b'\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T01:21:40,160956709-04:00] INFO: > Check hosts for OSDs\x1b[0m\n'
10.10.2.3: b"\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T01:21:40,165160350-04:00] INFO: >> Check host 'sm1'\x1b[0m\n# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:200 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 sudo --non-interactive --validate\n"
10.10.2.3: b"\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T01:21:41,231989370-04:00] INFO: >>> Check device '/dev/nvme0n1'\x1b[0m\n# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:215 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 test -b /dev/nvme0n1\n"
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:226 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 lsblk --nodeps --noheadings --output TYPE /dev/nvme0n1\n'
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:227 - verify_parameters() > grep --word-regexp disk\n'
10.10.2.3: b'\n\n\x1b[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T01:21:43,353362189-04:00] STAGE: Reset devices on hosts...\x1b[0m\n'
10.10.2.3: b"\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T01:21:43,357081198-04:00] INFO: > Resetting device '/dev/nvme0n1' on host 'sm1'\x1b[0m\n"
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:288 - reset_host_devices() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 sudo sh\n'
10.10.2.3: b'  Removing ceph--9e2f61b1--c173--4302--81e8--8d02581ae8d3-osd--block--d64601e1--5649--4f67--a491--96bfda70a4d2 (252:0)\n'
10.10.2.3: b'  Archiving volume group "ceph-9e2f61b1-c173-4302-81e8-8d02581ae8d3" metadata (seqno 5).\n'
10.10.2.3: b'  Releasing logical volume "osd-block-d64601e1-5649-4f67-a491-96bfda70a4d2"\n'
10.10.2.3: b'  Creating volume group backup "/etc/lvm/backup/ceph-9e2f61b1-c173-4302-81e8-8d02581ae8d3" (seqno 6).\n'
10.10.2.3: b'  Logical volume "osd-block-d64601e1-5649-4f67-a491-96bfda70a4d2" successfully removed\n'
10.10.2.3: b'  Removing physical volume "/dev/nvme0n1" from volume group "ceph-9e2f61b1-c173-4302-81e8-8d02581ae8d3"\n'
10.10.2.3: b'  Volume group "ceph-9e2f61b1-c173-4302-81e8-8d02581ae8d3" successfully removed\n'
10.10.2.3: b'\n\n\x1b[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T01:21:45,555146496-04:00] STAGE: Bootstrap a new cluster...\x1b[0m\n'
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:312 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.3 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults\n'
10.10.2.3: b'\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T01:21:45,563581821-04:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.3: b"\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T01:21:45,566614396-04:00] DEBUG: CEPHADM_PARAMS: ['bootstrap', '--config', '/tmp/bench-rados/ceph-research/scripts/ceph.conf', '--mon-ip', '10.10.2.3', '--output-dir', '/tmp/bench-rados/deployment_data_root/etc/ceph', '--ssh-user', 'ljishen', '--skip-dashboard', '--no-minimize-config', '--allow-overwrite', '--allow-fqdn-hostname', '--orphan-initial-daemons', '--skip-monitoring-stack', '--single-host-defaults']\x1b[0m\n"
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.3 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults\n'
10.10.2.3: b'Verifying podman|docker is present...\nVerifying lvm2 is present...\n'
10.10.2.3: b'Verifying time synchronization is in place...\n'
10.10.2.3: b'Unit ntp.service is enabled and running\nRepeating the final host check...\npodman|docker (/usr/bin/docker) is present\nsystemctl is present\nlvcreate is present\n'
10.10.2.3: b'Unit ntp.service is enabled and running\nHost looks OK\n'
10.10.2.3: b'Cluster fsid: c72300b0-36e5-11ec-b561-09a495c6360a\n'
10.10.2.3: b'Verifying IP 10.10.2.3 port 3300 ...\nVerifying IP 10.10.2.3 port 6789 ...\n'
10.10.2.3: b'Mon IP `10.10.2.3` is in CIDR network `10.10.2.0/24`\n- internal network (--cluster-network) has not been provided, OSD replication will default to the public_network\n'
10.10.2.3: b'Adjusting default settings to suit single-host cluster...\n'
10.10.2.3: b'Pulling container image quay.io/ceph/ceph:v16...\n'
10.10.2.3: b'Ceph version: ceph version 16.2.6 (ee28fb57e47e9f88813e24bbf4c14496ca299d31) pacific (stable)\n'
10.10.2.3: b'Extracting ceph user uid/gid from container image...\n'
10.10.2.3: b'Creating initial keys...\n'
10.10.2.3: b'Creating initial monmap...\n'
10.10.2.3: b'Creating mon...\n'
10.10.2.3: b'Waiting for mon to start...\nWaiting for mon...\n'
10.10.2.3: b'mon is available\nSetting mon public_network to 10.10.2.0/24\n'
10.10.2.3: b'Wrote config to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf\n'
10.10.2.3: b'Wrote keyring to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring\nCreating mgr...\n'
10.10.2.3: b'Verifying port 9283 ...\n'
10.10.2.3: b'Waiting for mgr to start...\nWaiting for mgr...\n'
10.10.2.3: b'mgr not available, waiting (1/15)...\n'
10.10.2.3: b'mgr not available, waiting (2/15)...\n'
10.10.2.3: b'mgr is available\n'
10.10.2.3: b'Enabling cephadm module...\n'
10.10.2.3: b'Waiting for the mgr to restart...\nWaiting for mgr epoch 5...\n'
10.10.2.3: b'mgr epoch 5 is available\nSetting orchestrator backend to cephadm...\n'
10.10.2.3: b'Generating ssh key...\n'
10.10.2.3: b'Wrote public SSH key to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub\n'
10.10.2.3: b'Adding key to ljishen@localhost authorized_keys...\n'
10.10.2.3: b'Adding host node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us...\n'
10.10.2.3: b'Deploying unmanaged mon service...\n'
10.10.2.3: b'Deploying unmanaged mgr service...\n'
10.10.2.3: b'You can access the Ceph CLI with:\n\n\tsudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --fsid c72300b0-36e5-11ec-b561-09a495c6360a -c /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf -k /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring\n\n'
10.10.2.3: b'Please consider enabling telemetry to help improve Ceph:\n\n\tceph telemetry on\n\nFor more information see:\n\n\thttps://docs.ceph.com/docs/pacific/mgr/telemetry/\n\nBootstrap complete.\n'
10.10.2.3: b'\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T01:22:50,276349022-04:00] INFO: > Set the default number of monitor daemons to 1\x1b[0m\n# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:330 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1\n'
10.10.2.3: b'\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T01:22:50,284841625-04:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.3: b"\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T01:22:50,287763161-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mon', '1']\x1b[0m\n"
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1\n'
10.10.2.3: b'Inferring fsid c72300b0-36e5-11ec-b561-09a495c6360a\n'
10.10.2.3: b'Inferring config /var/lib/ceph/c72300b0-36e5-11ec-b561-09a495c6360a/mon.node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.3: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.3: b'Scheduled mon update...\n'
10.10.2.3: b'\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T01:22:59,639603059-04:00] INFO: > Set the default number of manager daemons to 1\x1b[0m\n'
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:335 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1\n'
10.10.2.3: b'\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T01:22:59,648135006-04:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.3: b"\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T01:22:59,651242101-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mgr', '1']\x1b[0m\n"
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1\n'
10.10.2.3: b'Inferring fsid c72300b0-36e5-11ec-b561-09a495c6360a\n'
10.10.2.3: b'Inferring config /var/lib/ceph/c72300b0-36e5-11ec-b561-09a495c6360a/mon.node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.3: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.3: b'Scheduled mgr update...\n'
10.10.2.3: b'\n\n\x1b[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T01:23:08,830173522-04:00] STAGE: Add hosts to the cluster...\x1b[0m\n'
10.10.2.3: b"\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T01:23:08,835122167-04:00] INFO: > Adding host 'sm1'\x1b[0m\n"
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:362 - add_hosts() > ssh-copy-id -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -f -i /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub ljishen@sm1\n'
10.10.2.3: b'/usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: "/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub"\n'
10.10.2.3: b'\nNumber of key(s) added: 1\n\nNow try logging into the machine, with:   "ssh -o \'GlobalKnownHostsFile=/dev/null\' -o \'LogLevel=ERROR\' -o \'PasswordAuthentication=no\' -o \'StrictHostKeyChecking=no\' -o \'UserKnownHostsFile=/dev/null\' \'ljishen@sm1\'"\nand check to make sure that only the key(s) you wanted were added.\n\n'
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:368 - add_hosts() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add sm1 10.10.2.5\n'
10.10.2.3: b'\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T01:23:09,967993326-04:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.3: b"\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T01:23:09,970741625-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'host', 'add', 'sm1', '10.10.2.5']\x1b[0m\n"
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add sm1 10.10.2.5\n'
10.10.2.3: b'Inferring fsid c72300b0-36e5-11ec-b561-09a495c6360a\n'
10.10.2.3: b'Inferring config /var/lib/ceph/c72300b0-36e5-11ec-b561-09a495c6360a/mon.node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.3: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.3: b"Added host 'sm1' with addr '10.10.2.5'\n"
10.10.2.3: b'\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T01:23:20,795740523-04:00] INFO: Sleep for 20 seconds\x1b[0m\n'
10.10.2.3: b'\n\n\x1b[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T01:23:40,799316425-04:00] STAGE: Add OSDs to the cluster...\x1b[0m\n'
10.10.2.3: b"\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T01:23:40,804760614-04:00] INFO: > Adding devices ['/dev/nvme0n1'] on host 'sm1' as OSDs\x1b[0m\n"
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:387 - add_host_devices_as_osds() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd sm1:/dev/nvme0n1\n'
10.10.2.3: b'\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T01:23:40,812486102-04:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.3: b"\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T01:23:40,815408719-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'daemon', 'add', 'osd', 'sm1:/dev/nvme0n1']\x1b[0m\n"
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd sm1:/dev/nvme0n1\n'
10.10.2.3: b'Inferring fsid c72300b0-36e5-11ec-b561-09a495c6360a\n'
10.10.2.3: b'Inferring config /var/lib/ceph/c72300b0-36e5-11ec-b561-09a495c6360a/mon.node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.3: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.3: b"Created osd(s) 0 on host 'sm1'\n"
10.10.2.3: b'\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T01:24:04,219286932-04:00] INFO: > Sleep for 20 seconds\x1b[0m\n'
10.10.2.3: b'\n\n\x1b[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T01:24:24,223617127-04:00] STAGE: Show Cluster Status\x1b[0m\n# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:343 - show_status() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status\n'
10.10.2.3: b'\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T01:24:24,231841846-04:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.3: b"\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T01:24:24,234587139-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', '--status']\x1b[0m\n"
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status\n'
10.10.2.3: b'Inferring fsid c72300b0-36e5-11ec-b561-09a495c6360a\n'
10.10.2.3: b'Inferring config /var/lib/ceph/c72300b0-36e5-11ec-b561-09a495c6360a/mon.node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.3: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.3: b'  cluster:\n    id:     c72300b0-36e5-11ec-b561-09a495c6360a\n    health: HEALTH_WARN\n            1 pool(s) have no replicas configured\n \n  services:\n    mon: 1 daemons, quorum node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us (age 2m)\n    mgr: node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us.pafzxw(active, since 2m)\n    osd: 1 osds: 1 up (since 21s), 1 in (since 40s)\n \n  data:\n    pools:   1 pools, 128 pgs\n    objects: 0 objects, 0 B\n    usage:   4.7 MiB used, 100 GiB / 100 GiB avail\n    pgs:     99.219% pgs unknown\n             127 unknown\n             1   active+clean\n \n  progress:\n \n'
[1] 22:24:34 [SUCCESS] ljishen@10.10.2.3

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T01:21:29,886279780-04:00] INFO: > Remove the existing cluster[0m
## bash:13 -  > ls /var/lib/ceph
## bash:13 -  > tail -n1
# bash:13 -  > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm --host 10.10.2.3 --host 10.10.2.5 rm-cluster --force --fsid bc365c2a-36e4-11ec-b561-09a495c6360a

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T01:21:38,867937168-04:00] INFO: > Deploy a new cluster[0m
## bash:22 -  > /tmp/bench-rados/ceph-research/scripts/parallel-ssh --host 10.10.2.5 uname --nodename
## bash:22 -  > tail -n1
# bash:22 -  > osd_hostname=sm1
# bash:23 -  > /tmp/bench-rados/ceph-research/scripts/ceph-deploy -m 10.10.2.3 -o sm1/10.10.2.5:/dev/nvme0n1


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T01:21:40,005171439-04:00] STAGE: Verify passwordless SSH to hosts, passwordless sudo via ljishen, and the existence of devices on hosts...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T01:21:40,009059305-04:00] INFO: > Check the host for the monitor[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:181 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@10.10.2.3 sudo --non-interactive --validate

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T01:21:40,160956709-04:00] INFO: > Check hosts for OSDs[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T01:21:40,165160350-04:00] INFO: >> Check host 'sm1'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:200 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 sudo --non-interactive --validate

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T01:21:41,231989370-04:00] INFO: >>> Check device '/dev/nvme0n1'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:215 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 test -b /dev/nvme0n1
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:226 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 lsblk --nodeps --noheadings --output TYPE /dev/nvme0n1
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:227 - verify_parameters() > grep --word-regexp disk


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T01:21:43,353362189-04:00] STAGE: Reset devices on hosts...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T01:21:43,357081198-04:00] INFO: > Resetting device '/dev/nvme0n1' on host 'sm1'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:288 - reset_host_devices() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 sudo sh
  Removing ceph--9e2f61b1--c173--4302--81e8--8d02581ae8d3-osd--block--d64601e1--5649--4f67--a491--96bfda70a4d2 (252:0)
  Archiving volume group "ceph-9e2f61b1-c173-4302-81e8-8d02581ae8d3" metadata (seqno 5).
  Releasing logical volume "osd-block-d64601e1-5649-4f67-a491-96bfda70a4d2"
  Creating volume group backup "/etc/lvm/backup/ceph-9e2f61b1-c173-4302-81e8-8d02581ae8d3" (seqno 6).
  Logical volume "osd-block-d64601e1-5649-4f67-a491-96bfda70a4d2" successfully removed
  Removing physical volume "/dev/nvme0n1" from volume group "ceph-9e2f61b1-c173-4302-81e8-8d02581ae8d3"
  Volume group "ceph-9e2f61b1-c173-4302-81e8-8d02581ae8d3" successfully removed


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T01:21:45,555146496-04:00] STAGE: Bootstrap a new cluster...[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:312 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.3 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T01:21:45,563581821-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T01:21:45,566614396-04:00] DEBUG: CEPHADM_PARAMS: ['bootstrap', '--config', '/tmp/bench-rados/ceph-research/scripts/ceph.conf', '--mon-ip', '10.10.2.3', '--output-dir', '/tmp/bench-rados/deployment_data_root/etc/ceph', '--ssh-user', 'ljishen', '--skip-dashboard', '--no-minimize-config', '--allow-overwrite', '--allow-fqdn-hostname', '--orphan-initial-daemons', '--skip-monitoring-stack', '--single-host-defaults'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.3 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults
Verifying podman|docker is present...
Verifying lvm2 is present...
Verifying time synchronization is in place...
Unit ntp.service is enabled and running
Repeating the final host check...
podman|docker (/usr/bin/docker) is present
systemctl is present
lvcreate is present
Unit ntp.service is enabled and running
Host looks OK
Cluster fsid: c72300b0-36e5-11ec-b561-09a495c6360a
Verifying IP 10.10.2.3 port 3300 ...
Verifying IP 10.10.2.3 port 6789 ...
Mon IP `10.10.2.3` is in CIDR network `10.10.2.0/24`
- internal network (--cluster-network) has not been provided, OSD replication will default to the public_network
Adjusting default settings to suit single-host cluster...
Pulling container image quay.io/ceph/ceph:v16...
Ceph version: ceph version 16.2.6 (ee28fb57e47e9f88813e24bbf4c14496ca299d31) pacific (stable)
Extracting ceph user uid/gid from container image...
Creating initial keys...
Creating initial monmap...
Creating mon...
Waiting for mon to start...
Waiting for mon...
mon is available
Setting mon public_network to 10.10.2.0/24
Wrote config to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf
Wrote keyring to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring
Creating mgr...
Verifying port 9283 ...
Waiting for mgr to start...
Waiting for mgr...
mgr not available, waiting (1/15)...
mgr not available, waiting (2/15)...
mgr is available
Enabling cephadm module...
Waiting for the mgr to restart...
Waiting for mgr epoch 5...
mgr epoch 5 is available
Setting orchestrator backend to cephadm...
Generating ssh key...
Wrote public SSH key to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub
Adding key to ljishen@localhost authorized_keys...
Adding host node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us...
Deploying unmanaged mon service...
Deploying unmanaged mgr service...
You can access the Ceph CLI with:

	sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --fsid c72300b0-36e5-11ec-b561-09a495c6360a -c /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf -k /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring

Please consider enabling telemetry to help improve Ceph:

	ceph telemetry on

For more information see:

	https://docs.ceph.com/docs/pacific/mgr/telemetry/

Bootstrap complete.

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T01:22:50,276349022-04:00] INFO: > Set the default number of monitor daemons to 1[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:330 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T01:22:50,284841625-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T01:22:50,287763161-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mon', '1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1
Inferring fsid c72300b0-36e5-11ec-b561-09a495c6360a
Inferring config /var/lib/ceph/c72300b0-36e5-11ec-b561-09a495c6360a/mon.node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Scheduled mon update...

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T01:22:59,639603059-04:00] INFO: > Set the default number of manager daemons to 1[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:335 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T01:22:59,648135006-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T01:22:59,651242101-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mgr', '1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1
Inferring fsid c72300b0-36e5-11ec-b561-09a495c6360a
Inferring config /var/lib/ceph/c72300b0-36e5-11ec-b561-09a495c6360a/mon.node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Scheduled mgr update...


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T01:23:08,830173522-04:00] STAGE: Add hosts to the cluster...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T01:23:08,835122167-04:00] INFO: > Adding host 'sm1'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:362 - add_hosts() > ssh-copy-id -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -f -i /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub ljishen@sm1
/usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: "/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub"

Number of key(s) added: 1

Now try logging into the machine, with:   "ssh -o 'GlobalKnownHostsFile=/dev/null' -o 'LogLevel=ERROR' -o 'PasswordAuthentication=no' -o 'StrictHostKeyChecking=no' -o 'UserKnownHostsFile=/dev/null' 'ljishen@sm1'"
and check to make sure that only the key(s) you wanted were added.

# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:368 - add_hosts() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add sm1 10.10.2.5
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T01:23:09,967993326-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T01:23:09,970741625-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'host', 'add', 'sm1', '10.10.2.5'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add sm1 10.10.2.5
Inferring fsid c72300b0-36e5-11ec-b561-09a495c6360a
Inferring config /var/lib/ceph/c72300b0-36e5-11ec-b561-09a495c6360a/mon.node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Added host 'sm1' with addr '10.10.2.5'

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T01:23:20,795740523-04:00] INFO: Sleep for 20 seconds[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T01:23:40,799316425-04:00] STAGE: Add OSDs to the cluster...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T01:23:40,804760614-04:00] INFO: > Adding devices ['/dev/nvme0n1'] on host 'sm1' as OSDs[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:387 - add_host_devices_as_osds() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd sm1:/dev/nvme0n1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T01:23:40,812486102-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T01:23:40,815408719-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'daemon', 'add', 'osd', 'sm1:/dev/nvme0n1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd sm1:/dev/nvme0n1
Inferring fsid c72300b0-36e5-11ec-b561-09a495c6360a
Inferring config /var/lib/ceph/c72300b0-36e5-11ec-b561-09a495c6360a/mon.node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Created osd(s) 0 on host 'sm1'

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T01:24:04,219286932-04:00] INFO: > Sleep for 20 seconds[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T01:24:24,223617127-04:00] STAGE: Show Cluster Status[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:343 - show_status() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T01:24:24,231841846-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T01:24:24,234587139-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', '--status'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status
Inferring fsid c72300b0-36e5-11ec-b561-09a495c6360a
Inferring config /var/lib/ceph/c72300b0-36e5-11ec-b561-09a495c6360a/mon.node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
  cluster:
    id:     c72300b0-36e5-11ec-b561-09a495c6360a
    health: HEALTH_WARN
            1 pool(s) have no replicas configured
 
  services:
    mon: 1 daemons, quorum node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us (age 2m)
    mgr: node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us.pafzxw(active, since 2m)
    osd: 1 osds: 1 up (since 21s), 1 in (since 40s)
 
  data:
    pools:   1 pools, 128 pgs
    objects: 0 objects, 0 B
    usage:   4.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     99.219% pgs unknown
             127 unknown
             1   active+clean
 
  progress:
 

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:24:34,262663599-07:00] INFO: > Copy configuration files to local[0m
# ./benchmarks/bench-rados:208 - launch_ceph_cluster() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.3:/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf
# ./benchmarks/bench-rados:212 - launch_ceph_cluster() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.3 sudo chmod 644 /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:24:34,418658640-07:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.3 sudo chmod 644 /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring
[1] 22:24:34 [SUCCESS] ljishen@10.10.2.3
# ./benchmarks/bench-rados:214 - launch_ceph_cluster() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.3:/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:24:34,741867690-07:00] STAGE: Update local ceph.conf as a client...[0m


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:24:34,745496181-07:00] STAGE: Configure Ceph pool...[0m
# ./benchmarks/bench-rados:245 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_max_backfills 32
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:24:34,770484653-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:24:34,773947473-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'c72300b0-36e5-11ec-b561-09a495c6360a', '--', 'ceph', 'config', 'set', 'osd', 'osd_max_backfills', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid c72300b0-36e5-11ec-b561-09a495c6360a -- ceph config set osd osd_max_backfills 32
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:246 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_active 32
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:24:37,989878824-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:24:37,993336834-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'c72300b0-36e5-11ec-b561-09a495c6360a', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_active', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid c72300b0-36e5-11ec-b561-09a495c6360a -- ceph config set osd osd_recovery_max_active 32
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:247 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_single_start 8
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:24:41,373290276-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:24:41,376575291-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'c72300b0-36e5-11ec-b561-09a495c6360a', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_single_start', '8'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid c72300b0-36e5-11ec-b561-09a495c6360a -- ceph config set osd osd_recovery_max_single_start 8
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:248 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_op_priority 63
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:24:44,497941004-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:24:44,501778297-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'c72300b0-36e5-11ec-b561-09a495c6360a', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_op_priority', '63'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid c72300b0-36e5-11ec-b561-09a495c6360a -- ceph config set osd osd_recovery_op_priority 63
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:250 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config show-with-defaults osd.0
# ./benchmarks/bench-rados:251 - configure_ceph_pool() > grep 'osd_max_backfills\|osd_recovery'
# ./benchmarks/bench-rados:251 - configure_ceph_pool() > column -t -s ' '
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
osd_max_backfills                        32         mon
osd_recovery_cost                        20971520   default
osd_recovery_delay_start                 0.000000   default
osd_recovery_max_active                  32         mon
osd_recovery_max_active_hdd              3          default
osd_recovery_max_active_ssd              10         default
osd_recovery_max_chunk                   8388608    default
osd_recovery_max_omap_entries_per_chunk  8096       default
osd_recovery_max_single_start            8          mon
osd_recovery_op_priority                 63         mon
osd_recovery_op_warn_multiple            16         default
osd_recovery_priority                    5          default
osd_recovery_retry_interval              30.000000  default
osd_recovery_sleep                       0.000000   default
osd_recovery_sleep_hdd                   0.100000   default
osd_recovery_sleep_hybrid                0.025000   default
osd_recovery_sleep_ssd                   0.000000   default
# ./benchmarks/bench-rados:253 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool create bench_rados 128 128 replicated --size 1 --pg-num-min 128
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:24:50,800392367-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:24:50,803458340-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'c72300b0-36e5-11ec-b561-09a495c6360a', '--', 'ceph', 'osd', 'pool', 'create', 'bench_rados', '128', '128', 'replicated', '--size', '1', '--pg-num-min', '128'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid c72300b0-36e5-11ec-b561-09a495c6360a -- ceph osd pool create bench_rados 128 128 replicated --size 1 --pg-num-min 128
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
pool 'bench_rados' created
# ./benchmarks/bench-rados:255 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados min_size 1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:24:54,324956199-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:24:54,328547149-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'c72300b0-36e5-11ec-b561-09a495c6360a', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'min_size', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid c72300b0-36e5-11ec-b561-09a495c6360a -- ceph osd pool set bench_rados min_size 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 min_size to 1
# ./benchmarks/bench-rados:256 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados pg_autoscale_mode off
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:24:57,607648915-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:24:57,610888905-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'c72300b0-36e5-11ec-b561-09a495c6360a', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'pg_autoscale_mode', 'off'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid c72300b0-36e5-11ec-b561-09a495c6360a -- ceph osd pool set bench_rados pg_autoscale_mode off
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 pg_autoscale_mode to off
# ./benchmarks/bench-rados:257 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool application enable bench_rados benchmark
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:25:00,985340395-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:25:00,988485587-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'c72300b0-36e5-11ec-b561-09a495c6360a', '--', 'ceph', 'osd', 'pool', 'application', 'enable', 'bench_rados', 'benchmark'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid c72300b0-36e5-11ec-b561-09a495c6360a -- ceph osd pool application enable bench_rados benchmark
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
enabled application 'benchmark' on pool 'bench_rados'
# ./benchmarks/bench-rados:258 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados noscrub 1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:25:05,534675075-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:25:05,537969267-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'c72300b0-36e5-11ec-b561-09a495c6360a', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'noscrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid c72300b0-36e5-11ec-b561-09a495c6360a -- ceph osd pool set bench_rados noscrub 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 noscrub to 1
# ./benchmarks/bench-rados:259 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados nodeep-scrub 1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:25:09,243649673-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:25:09,247101702-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'c72300b0-36e5-11ec-b561-09a495c6360a', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'nodeep-scrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid c72300b0-36e5-11ec-b561-09a495c6360a -- ceph osd pool set bench_rados nodeep-scrub 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 nodeep-scrub to 1
# ./benchmarks/bench-rados:260 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool ls detail
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:25:13,178228549-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:25:13,181774645-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'c72300b0-36e5-11ec-b561-09a495c6360a', '--', 'ceph', 'osd', 'pool', 'ls', 'detail'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid c72300b0-36e5-11ec-b561-09a495c6360a -- ceph osd pool ls detail
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
pool 1 'device_health_metrics' replicated size 1 min_size 1 crush_rule 0 object_hash rjenkins pg_num 128 pgp_num 128 autoscale_mode on last_change 13 lfor 0/0/11 flags hashpspool stripe_width 0 pg_num_min 1 application mgr_devicehealth
pool 2 'bench_rados' replicated size 1 min_size 1 crush_rule 0 object_hash rjenkins pg_num 128 pgp_num 128 autoscale_mode off last_change 21 flags hashpspool,noscrub,nodeep-scrub stripe_width 0 pg_num_min 128 application benchmark

# ./benchmarks/bench-rados:261 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd df tree
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:25:16,403784172-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:25:16,407316041-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'c72300b0-36e5-11ec-b561-09a495c6360a', '--', 'ceph', 'osd', 'df', 'tree'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid c72300b0-36e5-11ec-b561-09a495c6360a -- ceph osd df tree
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
ID  CLASS  WEIGHT   REWEIGHT  SIZE     RAW USE  DATA     OMAP  META     AVAIL    %USE  VAR   PGS  STATUS  TYPE NAME    
-1         0.09769         -  100 GiB  6.0 MiB  176 KiB   0 B  5.8 MiB  100 GiB  0.01  1.00    -          root default 
-3         0.09769         -  100 GiB  6.0 MiB  176 KiB   0 B  5.8 MiB  100 GiB  0.01  1.00    -              host sm1 
 0    ssd  0.09769   1.00000  100 GiB  6.0 MiB  176 KiB   0 B  5.8 MiB  100 GiB  0.01  1.00  256      up          osd.0
                       TOTAL  100 GiB  6.0 MiB  176 KiB   0 B  5.8 MiB  100 GiB  0.01                                  
MIN/MAX VAR: 1.00/1.00  STDDEV: 0


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:25:19,869160256-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:25:43,136992315-07:00] INFO: >>   data:
    pools:   2 pools, 256 pgs
    objects: 0 objects, 0 B
    usage:   6.0 MiB used, 100 GiB / 100 GiB avail
    pgs:     256 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:25:43,144654227-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:25:51,226890763-07:00] INFO: >>   data:
    pools:   2 pools, 256 pgs
    objects: 0 objects, 0 B
    usage:   6.0 MiB used, 100 GiB / 100 GiB avail
    pgs:     256 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:25:51,235082862-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:25:59,417771553-07:00] INFO: >>   data:
    pools:   2 pools, 256 pgs
    objects: 0 objects, 0 B
    usage:   6.0 MiB used, 100 GiB / 100 GiB avail
    pgs:     256 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:25:59,425490993-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:26:07,506240204-07:00] INFO: >>   data:
    pools:   2 pools, 256 pgs
    objects: 0 objects, 0 B
    usage:   6.0 MiB used, 100 GiB / 100 GiB avail
    pgs:     256 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:26:07,513994570-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:26:15,697994074-07:00] INFO: >>   data:
    pools:   2 pools, 256 pgs
    objects: 0 objects, 0 B
    usage:   6.0 MiB used, 100 GiB / 100 GiB avail
    pgs:     256 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:26:15,706605401-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:26:15,712360730-07:00] INFO: > The cluster is idle now.[0m


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:26:15,716325052-07:00] STAGE: Start benchmarking of mode write ...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:26:15,724375725-07:00] INFO: > Start system activity collection services[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:26:15,730242252-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:329 - rados_bench() > rm --force /mnt/sda8/ceph-research/scripts/output/sys_activity_64KB_write.dat.rados_bench_host.3
# ./benchmarks/bench-rados:330 - rados_bench() > sar_pid_rados_bench_host=811025
# ./benchmarks/bench-rados:330 - rados_bench() > S_TIME_FORMAT=ISO
# ./benchmarks/bench-rados:330 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/output/sys_activity_64KB_write.dat.rados_bench_host.3 2

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:26:15,737421087-07:00] INFO: >> for OSD host[0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:26:15,746001957-07:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.5 bash
10.10.2.5: b'76239\n'
[1] 22:26:16 [SUCCESS] ljishen@10.10.2.5
76239

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:26:16,856351381-07:00] INFO: > Run rados bench[0m
# ./benchmarks/bench-rados:358 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/output/rados_bench_64KB_write.log.3
## ./benchmarks/bench-rados:349 - rados_bench() > count_cpus 32-63,96-127
# ./benchmarks/bench-rados:349 - rados_bench() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 30 write --pool bench_rados -b 65536 -O 65536 --concurrent-ios 128 --show-time --write-object --write-omap --write-xattr --no-cleanup
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:26:16,878024300-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:26:16,881770102-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'c72300b0-36e5-11ec-b561-09a495c6360a', '--', 'rados', 'bench', '30', 'write', '--pool', 'bench_rados', '-b', '65536', '-O', '65536', '--concurrent-ios', '128', '--show-time', '--write-object', '--write-omap', '--write-xattr', '--no-cleanup'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid c72300b0-36e5-11ec-b561-09a495c6360a -- rados bench 30 write --pool bench_rados -b 65536 -O 65536 --concurrent-ios 128 --show-time --write-object --write-omap --write-xattr --no-cleanup
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
hints = 1
2021-10-27T05:26:19.501749+0000 Maintaining 128 concurrent writes of 65536 bytes to objects of size 65536 for up to 30 seconds or 0 objects
2021-10-27T05:26:19.501757+0000 Object prefix: benchmark_data_node-0.bluefield2.ucsc-cmps10_7
2021-10-27T05:26:19.504235+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-27T05:26:19.504235+0000     0       0         0         0         0         0           -           0
2021-10-27T05:26:20.504316+0000     1     128      1071       943    58.932   58.9375    0.148528    0.127788
2021-10-27T05:26:21.504427+0000     2     128      2063      1935   60.4626        62    0.128232    0.127888
2021-10-27T05:26:22.504680+0000     3     128      3119      2991    62.303        66      0.1334    0.126691
2021-10-27T05:26:23.504750+0000     4     128      4082      3954   61.7731   60.1875     0.14178    0.127775
2021-10-27T05:26:24.504824+0000     5     128      5039      4911   61.3801   59.8125    0.155257    0.128389
2021-10-27T05:26:25.504936+0000     6     128      6002      5874   61.1802   60.1875    0.113533    0.128969
2021-10-27T05:26:26.505046+0000     7     128      6959      6831   60.9839   59.8125    0.138154     0.12955
2021-10-27T05:26:27.505117+0000     8     128      7977      7849   61.3135    63.625    0.113435    0.129528
2021-10-27T05:26:28.505220+0000     9     128      8562      8434    58.563   36.5625    0.249236    0.135063
2021-10-27T05:26:29.505330+0000    10     128      9007      8879   55.4876   27.8125    0.285617    0.141306
2021-10-27T05:26:30.505413+0000    11     128      9519      9391   53.3522        32    0.276339    0.148004
2021-10-27T05:26:31.505464+0000    12     128     10031      9903   51.5728        32    0.250641    0.154113
2021-10-27T05:26:32.505562+0000    13     128     10482     10354   49.7737   28.1875    0.275392    0.158996
2021-10-27T05:26:33.505660+0000    14     128     10984     10856   48.4593    31.375    0.276195    0.164072
2021-10-27T05:26:34.505727+0000    15     128     11439     11311   47.1244   28.4375    0.263493    0.168354
2021-10-27T05:26:35.505914+0000    16     128     11890     11762   45.9405   28.1875    0.255511    0.172153
2021-10-27T05:26:36.505974+0000    17     128     12698     12570   46.2085      50.5    0.132052     0.17247
2021-10-27T05:26:37.506064+0000    18     128     13615     13487   46.8251   57.3125    0.127221    0.170399
2021-10-27T05:26:38.506125+0000    19     128     14639     14511   47.7288        64    0.113628    0.167073
2021-10-27T05:26:39.506195+0000 min lat: 0.0789232 max lat: 0.299394 avg lat: 0.164921
2021-10-27T05:26:39.506195+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-27T05:26:39.506195+0000    20     128     15602     15474   48.3515   60.1875    0.133124    0.164921
2021-10-27T05:26:40.506277+0000    21     128     16431     16303   48.5161   51.8125    0.275187    0.164106
2021-10-27T05:26:41.506389+0000    22     128     16882     16754   47.5919   28.1875    0.280577    0.166939
2021-10-27T05:26:42.506461+0000    23     128     17138     17010   46.2183        16    0.519952    0.170402
2021-10-27T05:26:43.506509+0000    24     128     17416     17288   45.0165    17.375     0.48463    0.175752
2021-10-27T05:26:44.506717+0000    25     128     17711     17583   43.9531   18.4375    0.455594    0.181043
2021-10-27T05:26:45.506826+0000    26     128     17967     17839   42.8779        16    0.460879    0.185579
2021-10-27T05:26:46.506900+0000    27     128     18223     18095   41.8824        16    0.493199      0.1899
2021-10-27T05:26:47.506969+0000    28     128     18418     18290   40.8219   12.1875    0.501685    0.193468
2021-10-27T05:26:48.507039+0000    29     128     18674     18546   39.9659        16    0.517859    0.198094
2021-10-27T05:26:49.507146+0000    30     128     18930     18802    39.167        16    0.502589    0.202427
2021-10-27T05:26:50.507307+0000 Total time run:         30.3254
Total writes made:      18930
Write size:             65536
Object size:            65536
Bandwidth (MB/sec):     39.0143
Stddev Bandwidth:       19.2396
Max bandwidth (MB/sec): 66
Min bandwidth (MB/sec): 12.1875
Average IOPS:           624
Stddev IOPS:            307.834
Max IOPS:               1056
Min IOPS:               195
Average Latency(s):     0.204259
Stddev Latency(s):      0.120033
Max latency(s):         0.556235
Min latency(s):         0.0789232

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:26:51,019360386-07:00] INFO: > Stop system activity collection services[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:26:51,025292788-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:379 - rados_bench() > kill -INT 811025

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:26:51,031744715-07:00] INFO: >> for OSD host[0m
# ./benchmarks/bench-rados:384 - rados_bench() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.5 kill -INT 76239
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:26:51,040388864-07:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.5 kill -INT 76239
[1] 22:26:52 [SUCCESS] ljishen@10.10.2.5

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:26:52,307690669-07:00] INFO: >>> Copy system activity file from OSD host[0m
## ./benchmarks/bench-rados:391 - rados_bench() > basename -- /tmp/bench-rados/sys_activity_64KB_write.dat.osd_host.3
# ./benchmarks/bench-rados:391 - rados_bench() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.5:/tmp/bench-rados/sys_activity_64KB_write.dat.osd_host.3 /mnt/sda8/ceph-research/scripts/output/sys_activity_64KB_write.dat.osd_host.3


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:26:53,590161487-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:27:16,756230767-07:00] INFO: >>   data:
    pools:   2 pools, 256 pgs
    objects: 18.93k objects, 1.2 GiB
    usage:   3.5 GiB used, 96 GiB / 100 GiB avail
    pgs:     256 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:27:16,764368173-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:27:24,930435260-07:00] INFO: >>   data:
    pools:   2 pools, 256 pgs
    objects: 18.93k objects, 1.2 GiB
    usage:   3.5 GiB used, 96 GiB / 100 GiB avail
    pgs:     256 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:27:24,937927423-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:27:33,136387480-07:00] INFO: >>   data:
    pools:   2 pools, 256 pgs
    objects: 18.93k objects, 1.2 GiB
    usage:   3.5 GiB used, 96 GiB / 100 GiB avail
    pgs:     256 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:27:33,145033652-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:27:41,334243030-07:00] INFO: >>   data:
    pools:   2 pools, 256 pgs
    objects: 18.93k objects, 1.2 GiB
    usage:   3.5 GiB used, 96 GiB / 100 GiB avail
    pgs:     256 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:27:41,342196059-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:27:49,650584113-07:00] INFO: >>   data:
    pools:   2 pools, 256 pgs
    objects: 18.93k objects, 1.2 GiB
    usage:   3.5 GiB used, 96 GiB / 100 GiB avail
    pgs:     256 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:27:49,658407329-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:27:49,664513867-07:00] INFO: > The cluster is idle now.[0m


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:27:49,668019006-07:00] STAGE: Start benchmarking of mode seq ...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:27:49,674712428-07:00] INFO: > Start system activity collection services[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:27:49,680036416-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:329 - rados_bench() > rm --force /mnt/sda8/ceph-research/scripts/output/sys_activity_64KB_seq.dat.rados_bench_host.3
# ./benchmarks/bench-rados:330 - rados_bench() > sar_pid_rados_bench_host=812843
# ./benchmarks/bench-rados:330 - rados_bench() > S_TIME_FORMAT=ISO
# ./benchmarks/bench-rados:330 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/output/sys_activity_64KB_seq.dat.rados_bench_host.3 2

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:27:49,686357337-07:00] INFO: >> for OSD host[0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:27:49,695269199-07:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.5 bash
10.10.2.5: b'76635\n'
[1] 22:27:50 [SUCCESS] ljishen@10.10.2.5
76635

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:27:50,817409237-07:00] INFO: > Run rados bench[0m
# ./benchmarks/bench-rados:367 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/output/rados_bench_64KB_seq.log.3
## ./benchmarks/bench-rados:364 - rados_bench() > count_cpus 32-63,96-127
# ./benchmarks/bench-rados:364 - rados_bench() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 128 --show-time
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:27:50,838790719-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:27:50,842237738-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'c72300b0-36e5-11ec-b561-09a495c6360a', '--', 'rados', 'bench', '99999999', 'seq', '--pool', 'bench_rados', '--concurrent-ios', '128', '--show-time'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid c72300b0-36e5-11ec-b561-09a495c6360a -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 128 --show-time
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
hints = 1
2021-10-27T05:27:53.271043+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-27T05:27:53.271043+0000     0       2         2         0         0         0           -           0
2021-10-27T05:27:54.271165+0000     1     128      8503      8375   523.335   523.438   0.0160946   0.0151573
2021-10-27T05:27:55.271283+0000     2     128     17294     17166   536.353   549.438   0.0144864   0.0148652
2021-10-27T05:27:56.271486+0000 Total time run:       2.19255
Total reads made:     18930
Read size:            65536
Object size:          65536
Bandwidth (MB/sec):   539.613
Average IOPS:         8633
Stddev IOPS:          294.156
Max IOPS:             8791
Min IOPS:             8375
Average Latency(s):   0.0147771
Max latency(s):       0.0328243
Min latency(s):       0.00345012

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:27:56,864795082-07:00] INFO: > Stop system activity collection services[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:27:56,871050351-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:379 - rados_bench() > kill -INT 812843

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:27:56,877011957-07:00] INFO: >> for OSD host[0m
# ./benchmarks/bench-rados:384 - rados_bench() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.5 kill -INT 76635
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:27:56,885520540-07:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.5 kill -INT 76635
[1] 22:27:57 [SUCCESS] ljishen@10.10.2.5

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:27:57,975195211-07:00] INFO: >>> Copy system activity file from OSD host[0m
## ./benchmarks/bench-rados:391 - rados_bench() > basename -- /tmp/bench-rados/sys_activity_64KB_seq.dat.osd_host.3
# ./benchmarks/bench-rados:391 - rados_bench() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.5:/tmp/bench-rados/sys_activity_64KB_seq.dat.osd_host.3 /mnt/sda8/ceph-research/scripts/output/sys_activity_64KB_seq.dat.osd_host.3


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:27:59,052789574-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:28:22,141584077-07:00] INFO: >>   data:
    pools:   2 pools, 256 pgs
    objects: 18.93k objects, 1.2 GiB
    usage:   3.5 GiB used, 96 GiB / 100 GiB avail
    pgs:     256 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:28:22,149521869-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:28:30,399596668-07:00] INFO: >>   data:
    pools:   2 pools, 256 pgs
    objects: 18.93k objects, 1.2 GiB
    usage:   3.5 GiB used, 96 GiB / 100 GiB avail
    pgs:     256 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:28:30,407977221-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:28:38,640094875-07:00] INFO: >>   data:
    pools:   2 pools, 256 pgs
    objects: 18.93k objects, 1.2 GiB
    usage:   3.5 GiB used, 96 GiB / 100 GiB avail
    pgs:     256 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:28:38,648440243-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:28:46,870816506-07:00] INFO: >>   data:
    pools:   2 pools, 256 pgs
    objects: 18.93k objects, 1.2 GiB
    usage:   3.5 GiB used, 96 GiB / 100 GiB avail
    pgs:     256 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:28:46,878798780-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:28:55,203030736-07:00] INFO: >>   data:
    pools:   2 pools, 256 pgs
    objects: 18.93k objects, 1.2 GiB
    usage:   3.5 GiB used, 96 GiB / 100 GiB avail
    pgs:     256 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:28:55,211697948-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:28:55,217630119-07:00] INFO: > The cluster is idle now.[0m


[1;7;39;49m[2021-10-26T22:28:55,221293926-07:00][RUNNING][ROUND 1/4/21] object_size=256KB[0m


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:28:55,224728522-07:00] STAGE: Launch a Ceph cluster (CEPH_HOST: 10.10.2.3, OSD_HOST: 10.10.2.5) ...[0m
# ./benchmarks/bench-rados:172 - launch_ceph_cluster() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.3 bash -euo pipefail
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:28:55,232771010-07:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.3 bash -euo pipefail
10.10.2.3: b'\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T01:28:55,718275845-04:00] INFO: > Remove the existing cluster\x1b[0m\n'
10.10.2.3: b'## bash:13 -  > ls /var/lib/ceph\n## bash:13 -  > tail -n1\n'
10.10.2.3: b'# bash:13 -  > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm --host 10.10.2.3 --host 10.10.2.5 rm-cluster --force --fsid c72300b0-36e5-11ec-b561-09a495c6360a\n'
10.10.2.3: b'\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T01:29:04,598608833-04:00] INFO: > Deploy a new cluster\x1b[0m\n'
10.10.2.3: b'## bash:22 -  > /tmp/bench-rados/ceph-research/scripts/parallel-ssh --host 10.10.2.5 uname --nodename\n## bash:22 -  > tail -n1\n'
10.10.2.3: b'# bash:22 -  > osd_hostname=sm1\n# bash:23 -  > /tmp/bench-rados/ceph-research/scripts/ceph-deploy -m 10.10.2.3 -o sm1/10.10.2.5:/dev/nvme0n1\n'
10.10.2.3: b'\n\n\x1b[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T01:29:05,725213991-04:00] STAGE: Verify passwordless SSH to hosts, passwordless sudo via ljishen, and the existence of devices on hosts...\x1b[0m\n'
10.10.2.3: b'\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T01:29:05,729145089-04:00] INFO: > Check the host for the monitor\x1b[0m\n# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:181 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@10.10.2.3 sudo --non-interactive --validate\n'
10.10.2.3: b'\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T01:29:05,879544307-04:00] INFO: > Check hosts for OSDs\x1b[0m\n'
10.10.2.3: b"\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T01:29:05,883545186-04:00] INFO: >> Check host 'sm1'\x1b[0m\n# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:200 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 sudo --non-interactive --validate\n"
10.10.2.3: b"\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T01:29:06,943869356-04:00] INFO: >>> Check device '/dev/nvme0n1'\x1b[0m\n# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:215 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 test -b /dev/nvme0n1\n"
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:226 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 lsblk --nodeps --noheadings --output TYPE /dev/nvme0n1\n# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:227 - verify_parameters() > grep --word-regexp disk\n'
10.10.2.3: b'\n\n\x1b[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T01:29:09,084861561-04:00] STAGE: Reset devices on hosts...\x1b[0m\n'
10.10.2.3: b"\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T01:29:09,088559910-04:00] INFO: > Resetting device '/dev/nvme0n1' on host 'sm1'\x1b[0m\n# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:288 - reset_host_devices() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 sudo sh\n"
10.10.2.3: b'  Removing ceph--d6c33913--4894--4fab--839b--16d974911c77-osd--block--b9e452bc--fc76--4263--b37a--a5579338be34 (252:0)\n'
10.10.2.3: b'  Archiving volume group "ceph-d6c33913-4894-4fab-839b-16d974911c77" metadata (seqno 5).\n'
10.10.2.3: b'  Releasing logical volume "osd-block-b9e452bc-fc76-4263-b37a-a5579338be34"\n'
10.10.2.3: b'  Creating volume group backup "/etc/lvm/backup/ceph-d6c33913-4894-4fab-839b-16d974911c77" (seqno 6).\n'
10.10.2.3: b'  Logical volume "osd-block-b9e452bc-fc76-4263-b37a-a5579338be34" successfully removed\n  Removing physical volume "/dev/nvme0n1" from volume group "ceph-d6c33913-4894-4fab-839b-16d974911c77"\n'
10.10.2.3: b'  Volume group "ceph-d6c33913-4894-4fab-839b-16d974911c77" successfully removed\n'
10.10.2.3: b'\n\n\x1b[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T01:29:11,320786876-04:00] STAGE: Bootstrap a new cluster...\x1b[0m\n# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:312 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.3 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults\n'
10.10.2.3: b'\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T01:29:11,329287013-04:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.3: b"\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T01:29:11,331957475-04:00] DEBUG: CEPHADM_PARAMS: ['bootstrap', '--config', '/tmp/bench-rados/ceph-research/scripts/ceph.conf', '--mon-ip', '10.10.2.3', '--output-dir', '/tmp/bench-rados/deployment_data_root/etc/ceph', '--ssh-user', 'ljishen', '--skip-dashboard', '--no-minimize-config', '--allow-overwrite', '--allow-fqdn-hostname', '--orphan-initial-daemons', '--skip-monitoring-stack', '--single-host-defaults']\x1b[0m\n"
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.3 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults\n'
10.10.2.3: b'Verifying podman|docker is present...\nVerifying lvm2 is present...\nVerifying time synchronization is in place...\n'
10.10.2.3: b'Unit ntp.service is enabled and running\nRepeating the final host check...\npodman|docker (/usr/bin/docker) is present\nsystemctl is present\n'
10.10.2.3: b'lvcreate is present\n'
10.10.2.3: b'Unit ntp.service is enabled and running\nHost looks OK\n'
10.10.2.3: b'Cluster fsid: d0d4d84e-36e6-11ec-b561-09a495c6360a\n'
10.10.2.3: b'Verifying IP 10.10.2.3 port 3300 ...\n'
10.10.2.3: b'Verifying IP 10.10.2.3 port 6789 ...\n'
10.10.2.3: b'Mon IP `10.10.2.3` is in CIDR network `10.10.2.0/24`\n- internal network (--cluster-network) has not been provided, OSD replication will default to the public_network\n'
10.10.2.3: b'Adjusting default settings to suit single-host cluster...\nPulling container image quay.io/ceph/ceph:v16...\n'
10.10.2.3: b'Ceph version: ceph version 16.2.6 (ee28fb57e47e9f88813e24bbf4c14496ca299d31) pacific (stable)\nExtracting ceph user uid/gid from container image...\n'
10.10.2.3: b'Creating initial keys...\n'
10.10.2.3: b'Creating initial monmap...\n'
10.10.2.3: b'Creating mon...\n'
10.10.2.3: b'Waiting for mon to start...\nWaiting for mon...\n'
10.10.2.3: b'mon is available\nSetting mon public_network to 10.10.2.0/24\n'
10.10.2.3: b'Wrote config to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf\nWrote keyring to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring\nCreating mgr...\n'
10.10.2.3: b'Verifying port 9283 ...\n'
10.10.2.3: b'Waiting for mgr to start...\nWaiting for mgr...\n'
10.10.2.3: b'mgr not available, waiting (1/15)...\n'
10.10.2.3: b'mgr not available, waiting (2/15)...\n'
10.10.2.3: b'mgr is available\n'
10.10.2.3: b'Enabling cephadm module...\n'
10.10.2.3: b'Waiting for the mgr to restart...\nWaiting for mgr epoch 5...\n'
10.10.2.3: b'mgr epoch 5 is available\nSetting orchestrator backend to cephadm...\n'
10.10.2.3: b'Generating ssh key...\n'
10.10.2.3: b'Wrote public SSH key to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub\nAdding key to ljishen@localhost authorized_keys...\n'
10.10.2.3: b'Adding host node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us...\n'
10.10.2.3: b'Deploying unmanaged mon service...\n'
10.10.2.3: b'Deploying unmanaged mgr service...\n'
10.10.2.3: b'You can access the Ceph CLI with:\n\n\tsudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --fsid d0d4d84e-36e6-11ec-b561-09a495c6360a -c /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf -k /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring\n\nPlease consider enabling telemetry to help improve Ceph:\n\n\tceph telemetry on\n\nFor more information see:\n\n\thttps://docs.ceph.com/docs/pacific/mgr/telemetry/\n\nBootstrap complete.\n'
10.10.2.3: b'\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T01:30:16,026110920-04:00] INFO: > Set the default number of monitor daemons to 1\x1b[0m\n# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:330 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1\n'
10.10.2.3: b'\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T01:30:16,035119595-04:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.3: b"\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T01:30:16,037969466-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mon', '1']\x1b[0m\n"
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1\n'
10.10.2.3: b'Inferring fsid d0d4d84e-36e6-11ec-b561-09a495c6360a\n'
10.10.2.3: b'Inferring config /var/lib/ceph/d0d4d84e-36e6-11ec-b561-09a495c6360a/mon.node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.3: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.3: b'Scheduled mon update...\n'
10.10.2.3: b'\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T01:30:25,815863079-04:00] INFO: > Set the default number of manager daemons to 1\x1b[0m\n# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:335 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1\n'
10.10.2.3: b'\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T01:30:25,824383214-04:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.3: b"\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T01:30:25,827224027-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mgr', '1']\x1b[0m\n"
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1\n'
10.10.2.3: b'Inferring fsid d0d4d84e-36e6-11ec-b561-09a495c6360a\n'
10.10.2.3: b'Inferring config /var/lib/ceph/d0d4d84e-36e6-11ec-b561-09a495c6360a/mon.node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.3: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.3: b'Scheduled mgr update...\n'
10.10.2.3: b'\n\n\x1b[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T01:30:35,550387469-04:00] STAGE: Add hosts to the cluster...\x1b[0m\n'
10.10.2.3: b"\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T01:30:35,556101647-04:00] INFO: > Adding host 'sm1'\x1b[0m\n# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:362 - add_hosts() > ssh-copy-id -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -f -i /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub ljishen@sm1\n"
10.10.2.3: b'/usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: "/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub"\n'
10.10.2.3: b'\nNumber of key(s) added: 1\n\nNow try logging into the machine, with:   "ssh -o \'GlobalKnownHostsFile=/dev/null\' -o \'LogLevel=ERROR\' -o \'PasswordAuthentication=no\' -o \'StrictHostKeyChecking=no\' -o \'UserKnownHostsFile=/dev/null\' \'ljishen@sm1\'"\nand check to make sure that only the key(s) you wanted were added.\n\n'
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:368 - add_hosts() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add sm1 10.10.2.5\n'
10.10.2.3: b'\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T01:30:36,696208678-04:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.3: b"\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T01:30:36,699224571-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'host', 'add', 'sm1', '10.10.2.5']\x1b[0m\n"
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add sm1 10.10.2.5\n'
10.10.2.3: b'Inferring fsid d0d4d84e-36e6-11ec-b561-09a495c6360a\n'
10.10.2.3: b'Inferring config /var/lib/ceph/d0d4d84e-36e6-11ec-b561-09a495c6360a/mon.node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.3: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.3: b"Added host 'sm1' with addr '10.10.2.5'\n"
10.10.2.3: b'\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T01:30:47,764886376-04:00] INFO: Sleep for 20 seconds\x1b[0m\n'
10.10.2.3: b'\n\n\x1b[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T01:31:07,769126209-04:00] STAGE: Add OSDs to the cluster...\x1b[0m\n'
10.10.2.3: b"\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T01:31:07,774638426-04:00] INFO: > Adding devices ['/dev/nvme0n1'] on host 'sm1' as OSDs\x1b[0m\n# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:387 - add_host_devices_as_osds() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd sm1:/dev/nvme0n1\n"
10.10.2.3: b'\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T01:31:07,783115981-04:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.3: b"\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T01:31:07,786560041-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'daemon', 'add', 'osd', 'sm1:/dev/nvme0n1']\x1b[0m\n"
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd sm1:/dev/nvme0n1\n'
10.10.2.3: b'Inferring fsid d0d4d84e-36e6-11ec-b561-09a495c6360a\n'
10.10.2.3: b'Inferring config /var/lib/ceph/d0d4d84e-36e6-11ec-b561-09a495c6360a/mon.node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.3: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.3: b"Created osd(s) 0 on host 'sm1'\n"
10.10.2.3: b'\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T01:31:32,183322504-04:00] INFO: > Sleep for 20 seconds\x1b[0m\n'
10.10.2.3: b'\n\n\x1b[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T01:31:52,187925937-04:00] STAGE: Show Cluster Status\x1b[0m\n'
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:343 - show_status() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status\n'
10.10.2.3: b'\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T01:31:52,196547433-04:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.3: b"\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T01:31:52,199806014-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', '--status']\x1b[0m\n# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status\n"
10.10.2.3: b'Inferring fsid d0d4d84e-36e6-11ec-b561-09a495c6360a\n'
10.10.2.3: b'Inferring config /var/lib/ceph/d0d4d84e-36e6-11ec-b561-09a495c6360a/mon.node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.3: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.3: b'  cluster:\n    id:     d0d4d84e-36e6-11ec-b561-09a495c6360a\n    health: HEALTH_WARN\n            1 pool(s) have no replicas configured\n \n  services:\n    mon: 1 daemons, quorum node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us (age 2m)\n    mgr: node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us.ldaaxt(active, since 2m)\n    osd: 1 osds: 1 up (since 21s), 1 in (since 40s)\n \n  data:\n    pools:   1 pools, 128 pgs\n    objects: 0 objects, 0 B\n    usage:   4.7 MiB used, 100 GiB / 100 GiB avail\n    pgs:     99.219% pgs unknown\n             0.781% pgs not active\n             127 unknown\n             1   peering\n \n  progress:\n    Global Recovery Event (0s)\n      [............................] \n \n'
[1] 22:32:01 [SUCCESS] ljishen@10.10.2.3

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T01:28:55,718275845-04:00] INFO: > Remove the existing cluster[0m
## bash:13 -  > ls /var/lib/ceph
## bash:13 -  > tail -n1
# bash:13 -  > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm --host 10.10.2.3 --host 10.10.2.5 rm-cluster --force --fsid c72300b0-36e5-11ec-b561-09a495c6360a

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T01:29:04,598608833-04:00] INFO: > Deploy a new cluster[0m
## bash:22 -  > /tmp/bench-rados/ceph-research/scripts/parallel-ssh --host 10.10.2.5 uname --nodename
## bash:22 -  > tail -n1
# bash:22 -  > osd_hostname=sm1
# bash:23 -  > /tmp/bench-rados/ceph-research/scripts/ceph-deploy -m 10.10.2.3 -o sm1/10.10.2.5:/dev/nvme0n1


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T01:29:05,725213991-04:00] STAGE: Verify passwordless SSH to hosts, passwordless sudo via ljishen, and the existence of devices on hosts...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T01:29:05,729145089-04:00] INFO: > Check the host for the monitor[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:181 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@10.10.2.3 sudo --non-interactive --validate

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T01:29:05,879544307-04:00] INFO: > Check hosts for OSDs[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T01:29:05,883545186-04:00] INFO: >> Check host 'sm1'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:200 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 sudo --non-interactive --validate

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T01:29:06,943869356-04:00] INFO: >>> Check device '/dev/nvme0n1'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:215 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 test -b /dev/nvme0n1
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:226 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 lsblk --nodeps --noheadings --output TYPE /dev/nvme0n1
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:227 - verify_parameters() > grep --word-regexp disk


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T01:29:09,084861561-04:00] STAGE: Reset devices on hosts...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T01:29:09,088559910-04:00] INFO: > Resetting device '/dev/nvme0n1' on host 'sm1'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:288 - reset_host_devices() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 sudo sh
  Removing ceph--d6c33913--4894--4fab--839b--16d974911c77-osd--block--b9e452bc--fc76--4263--b37a--a5579338be34 (252:0)
  Archiving volume group "ceph-d6c33913-4894-4fab-839b-16d974911c77" metadata (seqno 5).
  Releasing logical volume "osd-block-b9e452bc-fc76-4263-b37a-a5579338be34"
  Creating volume group backup "/etc/lvm/backup/ceph-d6c33913-4894-4fab-839b-16d974911c77" (seqno 6).
  Logical volume "osd-block-b9e452bc-fc76-4263-b37a-a5579338be34" successfully removed
  Removing physical volume "/dev/nvme0n1" from volume group "ceph-d6c33913-4894-4fab-839b-16d974911c77"
  Volume group "ceph-d6c33913-4894-4fab-839b-16d974911c77" successfully removed


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T01:29:11,320786876-04:00] STAGE: Bootstrap a new cluster...[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:312 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.3 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T01:29:11,329287013-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T01:29:11,331957475-04:00] DEBUG: CEPHADM_PARAMS: ['bootstrap', '--config', '/tmp/bench-rados/ceph-research/scripts/ceph.conf', '--mon-ip', '10.10.2.3', '--output-dir', '/tmp/bench-rados/deployment_data_root/etc/ceph', '--ssh-user', 'ljishen', '--skip-dashboard', '--no-minimize-config', '--allow-overwrite', '--allow-fqdn-hostname', '--orphan-initial-daemons', '--skip-monitoring-stack', '--single-host-defaults'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.3 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults
Verifying podman|docker is present...
Verifying lvm2 is present...
Verifying time synchronization is in place...
Unit ntp.service is enabled and running
Repeating the final host check...
podman|docker (/usr/bin/docker) is present
systemctl is present
lvcreate is present
Unit ntp.service is enabled and running
Host looks OK
Cluster fsid: d0d4d84e-36e6-11ec-b561-09a495c6360a
Verifying IP 10.10.2.3 port 3300 ...
Verifying IP 10.10.2.3 port 6789 ...
Mon IP `10.10.2.3` is in CIDR network `10.10.2.0/24`
- internal network (--cluster-network) has not been provided, OSD replication will default to the public_network
Adjusting default settings to suit single-host cluster...
Pulling container image quay.io/ceph/ceph:v16...
Ceph version: ceph version 16.2.6 (ee28fb57e47e9f88813e24bbf4c14496ca299d31) pacific (stable)
Extracting ceph user uid/gid from container image...
Creating initial keys...
Creating initial monmap...
Creating mon...
Waiting for mon to start...
Waiting for mon...
mon is available
Setting mon public_network to 10.10.2.0/24
Wrote config to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf
Wrote keyring to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring
Creating mgr...
Verifying port 9283 ...
Waiting for mgr to start...
Waiting for mgr...
mgr not available, waiting (1/15)...
mgr not available, waiting (2/15)...
mgr is available
Enabling cephadm module...
Waiting for the mgr to restart...
Waiting for mgr epoch 5...
mgr epoch 5 is available
Setting orchestrator backend to cephadm...
Generating ssh key...
Wrote public SSH key to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub
Adding key to ljishen@localhost authorized_keys...
Adding host node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us...
Deploying unmanaged mon service...
Deploying unmanaged mgr service...
You can access the Ceph CLI with:

	sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --fsid d0d4d84e-36e6-11ec-b561-09a495c6360a -c /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf -k /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring

Please consider enabling telemetry to help improve Ceph:

	ceph telemetry on

For more information see:

	https://docs.ceph.com/docs/pacific/mgr/telemetry/

Bootstrap complete.

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T01:30:16,026110920-04:00] INFO: > Set the default number of monitor daemons to 1[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:330 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T01:30:16,035119595-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T01:30:16,037969466-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mon', '1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1
Inferring fsid d0d4d84e-36e6-11ec-b561-09a495c6360a
Inferring config /var/lib/ceph/d0d4d84e-36e6-11ec-b561-09a495c6360a/mon.node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Scheduled mon update...

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T01:30:25,815863079-04:00] INFO: > Set the default number of manager daemons to 1[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:335 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T01:30:25,824383214-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T01:30:25,827224027-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mgr', '1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1
Inferring fsid d0d4d84e-36e6-11ec-b561-09a495c6360a
Inferring config /var/lib/ceph/d0d4d84e-36e6-11ec-b561-09a495c6360a/mon.node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Scheduled mgr update...


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T01:30:35,550387469-04:00] STAGE: Add hosts to the cluster...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T01:30:35,556101647-04:00] INFO: > Adding host 'sm1'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:362 - add_hosts() > ssh-copy-id -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -f -i /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub ljishen@sm1
/usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: "/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub"

Number of key(s) added: 1

Now try logging into the machine, with:   "ssh -o 'GlobalKnownHostsFile=/dev/null' -o 'LogLevel=ERROR' -o 'PasswordAuthentication=no' -o 'StrictHostKeyChecking=no' -o 'UserKnownHostsFile=/dev/null' 'ljishen@sm1'"
and check to make sure that only the key(s) you wanted were added.

# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:368 - add_hosts() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add sm1 10.10.2.5
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T01:30:36,696208678-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T01:30:36,699224571-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'host', 'add', 'sm1', '10.10.2.5'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add sm1 10.10.2.5
Inferring fsid d0d4d84e-36e6-11ec-b561-09a495c6360a
Inferring config /var/lib/ceph/d0d4d84e-36e6-11ec-b561-09a495c6360a/mon.node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Added host 'sm1' with addr '10.10.2.5'

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T01:30:47,764886376-04:00] INFO: Sleep for 20 seconds[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T01:31:07,769126209-04:00] STAGE: Add OSDs to the cluster...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T01:31:07,774638426-04:00] INFO: > Adding devices ['/dev/nvme0n1'] on host 'sm1' as OSDs[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:387 - add_host_devices_as_osds() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd sm1:/dev/nvme0n1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T01:31:07,783115981-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T01:31:07,786560041-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'daemon', 'add', 'osd', 'sm1:/dev/nvme0n1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd sm1:/dev/nvme0n1
Inferring fsid d0d4d84e-36e6-11ec-b561-09a495c6360a
Inferring config /var/lib/ceph/d0d4d84e-36e6-11ec-b561-09a495c6360a/mon.node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Created osd(s) 0 on host 'sm1'

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T01:31:32,183322504-04:00] INFO: > Sleep for 20 seconds[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T01:31:52,187925937-04:00] STAGE: Show Cluster Status[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:343 - show_status() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T01:31:52,196547433-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T01:31:52,199806014-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', '--status'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status
Inferring fsid d0d4d84e-36e6-11ec-b561-09a495c6360a
Inferring config /var/lib/ceph/d0d4d84e-36e6-11ec-b561-09a495c6360a/mon.node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
  cluster:
    id:     d0d4d84e-36e6-11ec-b561-09a495c6360a
    health: HEALTH_WARN
            1 pool(s) have no replicas configured
 
  services:
    mon: 1 daemons, quorum node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us (age 2m)
    mgr: node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us.ldaaxt(active, since 2m)
    osd: 1 osds: 1 up (since 21s), 1 in (since 40s)
 
  data:
    pools:   1 pools, 128 pgs
    objects: 0 objects, 0 B
    usage:   4.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     99.219% pgs unknown
             0.781% pgs not active
             127 unknown
             1   peering
 
  progress:
    Global Recovery Event (0s)
      [............................] 
 

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:32:01,721370179-07:00] INFO: > Copy configuration files to local[0m
# ./benchmarks/bench-rados:208 - launch_ceph_cluster() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.3:/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf
# ./benchmarks/bench-rados:212 - launch_ceph_cluster() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.3 sudo chmod 644 /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:32:01,875323703-07:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.3 sudo chmod 644 /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring
[1] 22:32:02 [SUCCESS] ljishen@10.10.2.3
# ./benchmarks/bench-rados:214 - launch_ceph_cluster() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.3:/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:32:02,198345140-07:00] STAGE: Update local ceph.conf as a client...[0m


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:32:02,202089268-07:00] STAGE: Configure Ceph pool...[0m
# ./benchmarks/bench-rados:245 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_max_backfills 32
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:32:02,227622414-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:32:02,230680473-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'd0d4d84e-36e6-11ec-b561-09a495c6360a', '--', 'ceph', 'config', 'set', 'osd', 'osd_max_backfills', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid d0d4d84e-36e6-11ec-b561-09a495c6360a -- ceph config set osd osd_max_backfills 32
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:246 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_active 32
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:32:05,484891280-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:32:05,488134486-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'd0d4d84e-36e6-11ec-b561-09a495c6360a', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_active', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid d0d4d84e-36e6-11ec-b561-09a495c6360a -- ceph config set osd osd_recovery_max_active 32
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:247 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_single_start 8
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:32:08,758052600-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:32:08,761748687-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'd0d4d84e-36e6-11ec-b561-09a495c6360a', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_single_start', '8'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid d0d4d84e-36e6-11ec-b561-09a495c6360a -- ceph config set osd osd_recovery_max_single_start 8
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:248 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_op_priority 63
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:32:12,025422222-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:32:12,029025666-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'd0d4d84e-36e6-11ec-b561-09a495c6360a', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_op_priority', '63'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid d0d4d84e-36e6-11ec-b561-09a495c6360a -- ceph config set osd osd_recovery_op_priority 63
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:250 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config show-with-defaults osd.0
# ./benchmarks/bench-rados:251 - configure_ceph_pool() > grep 'osd_max_backfills\|osd_recovery'
# ./benchmarks/bench-rados:251 - configure_ceph_pool() > column -t -s ' '
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
osd_max_backfills                        32         mon
osd_recovery_cost                        20971520   default
osd_recovery_delay_start                 0.000000   default
osd_recovery_max_active                  32         mon
osd_recovery_max_active_hdd              3          default
osd_recovery_max_active_ssd              10         default
osd_recovery_max_chunk                   8388608    default
osd_recovery_max_omap_entries_per_chunk  8096       default
osd_recovery_max_single_start            8          mon
osd_recovery_op_priority                 63         mon
osd_recovery_op_warn_multiple            16         default
osd_recovery_priority                    5          default
osd_recovery_retry_interval              30.000000  default
osd_recovery_sleep                       0.000000   default
osd_recovery_sleep_hdd                   0.100000   default
osd_recovery_sleep_hybrid                0.025000   default
osd_recovery_sleep_ssd                   0.000000   default
# ./benchmarks/bench-rados:253 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool create bench_rados 128 128 replicated --size 1 --pg-num-min 128
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:32:18,599565287-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:32:18,602590423-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'd0d4d84e-36e6-11ec-b561-09a495c6360a', '--', 'ceph', 'osd', 'pool', 'create', 'bench_rados', '128', '128', 'replicated', '--size', '1', '--pg-num-min', '128'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid d0d4d84e-36e6-11ec-b561-09a495c6360a -- ceph osd pool create bench_rados 128 128 replicated --size 1 --pg-num-min 128
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
pool 'bench_rados' created
# ./benchmarks/bench-rados:255 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados min_size 1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:32:21,933049068-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:32:21,936347167-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'd0d4d84e-36e6-11ec-b561-09a495c6360a', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'min_size', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid d0d4d84e-36e6-11ec-b561-09a495c6360a -- ceph osd pool set bench_rados min_size 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 min_size to 1
# ./benchmarks/bench-rados:256 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados pg_autoscale_mode off
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:32:26,305907177-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:32:26,309587705-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'd0d4d84e-36e6-11ec-b561-09a495c6360a', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'pg_autoscale_mode', 'off'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid d0d4d84e-36e6-11ec-b561-09a495c6360a -- ceph osd pool set bench_rados pg_autoscale_mode off
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 pg_autoscale_mode to off
# ./benchmarks/bench-rados:257 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool application enable bench_rados benchmark
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:32:29,914903751-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:32:29,918524738-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'd0d4d84e-36e6-11ec-b561-09a495c6360a', '--', 'ceph', 'osd', 'pool', 'application', 'enable', 'bench_rados', 'benchmark'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid d0d4d84e-36e6-11ec-b561-09a495c6360a -- ceph osd pool application enable bench_rados benchmark
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
enabled application 'benchmark' on pool 'bench_rados'
# ./benchmarks/bench-rados:258 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados noscrub 1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:32:33,238940945-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:32:33,242227192-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'd0d4d84e-36e6-11ec-b561-09a495c6360a', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'noscrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid d0d4d84e-36e6-11ec-b561-09a495c6360a -- ceph osd pool set bench_rados noscrub 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 noscrub to 1
# ./benchmarks/bench-rados:259 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados nodeep-scrub 1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:32:37,522981760-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:32:37,526512136-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'd0d4d84e-36e6-11ec-b561-09a495c6360a', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'nodeep-scrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid d0d4d84e-36e6-11ec-b561-09a495c6360a -- ceph osd pool set bench_rados nodeep-scrub 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 nodeep-scrub to 1
# ./benchmarks/bench-rados:260 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool ls detail
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:32:41,112954384-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:32:41,116160260-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'd0d4d84e-36e6-11ec-b561-09a495c6360a', '--', 'ceph', 'osd', 'pool', 'ls', 'detail'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid d0d4d84e-36e6-11ec-b561-09a495c6360a -- ceph osd pool ls detail
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
pool 1 'device_health_metrics' replicated size 1 min_size 1 crush_rule 0 object_hash rjenkins pg_num 128 pgp_num 128 autoscale_mode on last_change 13 lfor 0/0/11 flags hashpspool stripe_width 0 pg_num_min 1 application mgr_devicehealth
pool 2 'bench_rados' replicated size 1 min_size 1 crush_rule 0 object_hash rjenkins pg_num 128 pgp_num 128 autoscale_mode off last_change 21 flags hashpspool,noscrub,nodeep-scrub stripe_width 0 pg_num_min 128 application benchmark

# ./benchmarks/bench-rados:261 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd df tree
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:32:44,303674014-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:32:44,306763051-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'd0d4d84e-36e6-11ec-b561-09a495c6360a', '--', 'ceph', 'osd', 'df', 'tree'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid d0d4d84e-36e6-11ec-b561-09a495c6360a -- ceph osd df tree
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
ID  CLASS  WEIGHT   REWEIGHT  SIZE     RAW USE  DATA     OMAP  META     AVAIL    %USE  VAR   PGS  STATUS  TYPE NAME    
-1         0.09769         -  100 GiB  6.0 MiB  176 KiB   0 B  5.8 MiB  100 GiB  0.01  1.00    -          root default 
-3         0.09769         -  100 GiB  6.0 MiB  176 KiB   0 B  5.8 MiB  100 GiB  0.01  1.00    -              host sm1 
 0    ssd  0.09769   1.00000  100 GiB  6.0 MiB  176 KiB   0 B  5.8 MiB  100 GiB  0.01  1.00  256      up          osd.0
                       TOTAL  100 GiB  6.0 MiB  176 KiB   0 B  5.8 MiB  100 GiB  0.01                                  
MIN/MAX VAR: 1.00/1.00  STDDEV: 0


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:32:47,488910998-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:33:10,702649104-07:00] INFO: >>   data:
    pools:   2 pools, 256 pgs
    objects: 0 objects, 0 B
    usage:   6.0 MiB used, 100 GiB / 100 GiB avail
    pgs:     256 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:33:10,710708604-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:33:18,952546349-07:00] INFO: >>   data:
    pools:   2 pools, 256 pgs
    objects: 0 objects, 0 B
    usage:   6.0 MiB used, 100 GiB / 100 GiB avail
    pgs:     256 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:33:18,960345279-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:33:27,178485260-07:00] INFO: >>   data:
    pools:   2 pools, 256 pgs
    objects: 0 objects, 0 B
    usage:   6.0 MiB used, 100 GiB / 100 GiB avail
    pgs:     256 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:33:27,186494354-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:33:35,417602558-07:00] INFO: >>   data:
    pools:   2 pools, 256 pgs
    objects: 0 objects, 0 B
    usage:   6.0 MiB used, 100 GiB / 100 GiB avail
    pgs:     256 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:33:35,426414512-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:33:43,606353342-07:00] INFO: >>   data:
    pools:   2 pools, 256 pgs
    objects: 0 objects, 0 B
    usage:   6.0 MiB used, 100 GiB / 100 GiB avail
    pgs:     256 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:33:43,613821650-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:33:43,620093480-07:00] INFO: > The cluster is idle now.[0m


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:33:43,623360521-07:00] STAGE: Start benchmarking of mode write ...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:33:43,630308331-07:00] INFO: > Start system activity collection services[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:33:43,635849557-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:329 - rados_bench() > rm --force /mnt/sda8/ceph-research/scripts/output/sys_activity_256KB_write.dat.rados_bench_host.1
# ./benchmarks/bench-rados:330 - rados_bench() > sar_pid_rados_bench_host=819856
# ./benchmarks/bench-rados:330 - rados_bench() > S_TIME_FORMAT=ISO
# ./benchmarks/bench-rados:330 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/output/sys_activity_256KB_write.dat.rados_bench_host.1 2

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:33:43,642280826-07:00] INFO: >> for OSD host[0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:33:43,651193720-07:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.5 bash
10.10.2.5: b'81376\n'
[1] 22:33:44 [SUCCESS] ljishen@10.10.2.5
81376

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:33:44,752863708-07:00] INFO: > Run rados bench[0m
# ./benchmarks/bench-rados:358 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/output/rados_bench_256KB_write.log.1
## ./benchmarks/bench-rados:349 - rados_bench() > count_cpus 32-63,96-127
# ./benchmarks/bench-rados:349 - rados_bench() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 30 write --pool bench_rados -b 262144 -O 262144 --concurrent-ios 128 --show-time --write-object --write-omap --write-xattr --no-cleanup
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:33:44,774223399-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:33:44,777508995-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'd0d4d84e-36e6-11ec-b561-09a495c6360a', '--', 'rados', 'bench', '30', 'write', '--pool', 'bench_rados', '-b', '262144', '-O', '262144', '--concurrent-ios', '128', '--show-time', '--write-object', '--write-omap', '--write-xattr', '--no-cleanup'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid d0d4d84e-36e6-11ec-b561-09a495c6360a -- rados bench 30 write --pool bench_rados -b 262144 -O 262144 --concurrent-ios 128 --show-time --write-object --write-omap --write-xattr --no-cleanup
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
hints = 1
2021-10-27T05:33:47.209421+0000 Maintaining 128 concurrent writes of 262144 bytes to objects of size 262144 for up to 30 seconds or 0 objects
2021-10-27T05:33:47.209431+0000 Object prefix: benchmark_data_node-0.bluefield2.ucsc-cmps10_7
2021-10-27T05:33:47.217572+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-27T05:33:47.217572+0000     0       0         0         0         0         0           -           0
2021-10-27T05:33:48.217680+0000     1     128       330       202   50.4982      50.5    0.410555    0.450051
2021-10-27T05:33:49.217752+0000     2     128       598       470   58.7468        67    0.532874    0.478787
2021-10-27T05:33:50.217835+0000     3     128       864       736   61.3294      66.5     0.52404    0.482501
2021-10-27T05:33:51.217906+0000     4     128      1117       989   61.8084     63.25    0.487322     0.48049
2021-10-27T05:33:52.217974+0000     5     128      1391      1263   63.1458      68.5    0.514722    0.485312
2021-10-27T05:33:53.218094+0000     6     128      1621      1493   62.2037      57.5    0.344817    0.485976
2021-10-27T05:33:54.218176+0000     7     128      1873      1745   62.3167        63    0.482008    0.488959
2021-10-27T05:33:55.218246+0000     8     128      2154      2026   63.3077     70.25    0.507521    0.489783
2021-10-27T05:33:56.218315+0000     9     128      2297      2169   60.2455     35.75     1.07113    0.512126
2021-10-27T05:33:57.218387+0000    10     128      2437      2309   57.7207        35     1.01343    0.538119
2021-10-27T05:33:58.218467+0000    11     128      2553      2425   55.1095        29    0.955321    0.557562
2021-10-27T05:33:59.218683+0000    12     128      2660      2532   52.7454     26.75    0.822766     0.57664
2021-10-27T05:34:00.218753+0000    13     128      2827      2699   51.8994     41.75    0.697003    0.598445
2021-10-27T05:34:01.218922+0000    14     128      2951      2823   50.4061        31    0.915135    0.613721
2021-10-27T05:34:02.219004+0000    15     128      3100      2972   49.5288     37.25    0.768181     0.63248
2021-10-27T05:34:03.219081+0000    16     128      3198      3070   47.9644      24.5    0.767758    0.642242
2021-10-27T05:34:04.219300+0000    17     128      3469      3341   49.1276     67.75    0.339201    0.641082
2021-10-27T05:34:05.219369+0000    18     128      3739      3611    50.148      67.5    0.507036    0.631914
2021-10-27T05:34:06.219579+0000    19     128      3964      3836   50.4685     56.25    0.338852     0.62419
2021-10-27T05:34:07.219648+0000 min lat: 0.0300625 max lat: 1.55919 avg lat: 0.616618
2021-10-27T05:34:07.219648+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-27T05:34:07.219648+0000    20     128      4201      4073   50.9074     59.25    0.510043    0.616618
2021-10-27T05:34:08.219727+0000    21     128      4356      4228   50.3283     38.75    0.699018    0.623133
2021-10-27T05:34:09.219797+0000    22     128      4426      4298   48.8361      17.5      1.2671    0.631343
2021-10-27T05:34:10.219877+0000    23     128      4550      4422   48.0605        31     1.34253    0.653754
2021-10-27T05:34:11.219947+0000    24     128      4645      4517   47.0476     23.75    0.883518    0.667635
2021-10-27T05:34:12.220016+0000    25     128      4741      4613   46.1256        24     1.20545    0.680063
2021-10-27T05:34:13.220224+0000    26     128      4792      4664   44.8417     12.75     1.75753    0.688047
2021-10-27T05:34:14.220304+0000    27     128      4916      4788    44.329        31     1.29702    0.706254
2021-10-27T05:34:15.220384+0000    28     128      4985      4857   43.3618     17.25     1.45168     0.71665
2021-10-27T05:34:16.220453+0000    29     128      5110      4982   42.9441     31.25     1.28869    0.735318
2021-10-27T05:34:17.220523+0000    30     128      5204      5076   42.2959      23.5     1.04698    0.744933
2021-10-27T05:34:18.220603+0000    31      95      5204      5109   41.1977      8.25     1.19724    0.749262
2021-10-27T05:34:19.220750+0000 Total time run:         31.1198
Total writes made:      5204
Write size:             262144
Object size:            262144
Bandwidth (MB/sec):     41.8061
Stddev Bandwidth:       19.3556
Max bandwidth (MB/sec): 70.25
Min bandwidth (MB/sec): 8.25
Average IOPS:           167
Stddev IOPS:            77.4224
Max IOPS:               281
Min IOPS:               33
Average Latency(s):     0.760353
Stddev Latency(s):      0.384287
Max latency(s):         2.38945
Min latency(s):         0.0300625

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:34:19,814403352-07:00] INFO: > Stop system activity collection services[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:34:19,821464896-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:379 - rados_bench() > kill -INT 819856

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:34:19,828097834-07:00] INFO: >> for OSD host[0m
# ./benchmarks/bench-rados:384 - rados_bench() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.5 kill -INT 81376
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:34:19,836599005-07:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.5 kill -INT 81376
[1] 22:34:21 [SUCCESS] ljishen@10.10.2.5

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:34:21,177838153-07:00] INFO: >>> Copy system activity file from OSD host[0m
## ./benchmarks/bench-rados:391 - rados_bench() > basename -- /tmp/bench-rados/sys_activity_256KB_write.dat.osd_host.1
# ./benchmarks/bench-rados:391 - rados_bench() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.5:/tmp/bench-rados/sys_activity_256KB_write.dat.osd_host.1 /mnt/sda8/ceph-research/scripts/output/sys_activity_256KB_write.dat.osd_host.1


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:34:22,395144298-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:34:45,691585113-07:00] INFO: >>   data:
    pools:   2 pools, 256 pgs
    objects: 5.21k objects, 1.3 GiB
    usage:   3.8 GiB used, 96 GiB / 100 GiB avail
    pgs:     256 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:34:45,700037050-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:34:53,940689778-07:00] INFO: >>   data:
    pools:   2 pools, 256 pgs
    objects: 5.21k objects, 1.3 GiB
    usage:   3.8 GiB used, 96 GiB / 100 GiB avail
    pgs:     256 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:34:53,949533763-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:35:02,462049724-07:00] INFO: >>   data:
    pools:   2 pools, 256 pgs
    objects: 5.21k objects, 1.3 GiB
    usage:   3.8 GiB used, 96 GiB / 100 GiB avail
    pgs:     256 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:35:02,469814270-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:35:10,729715500-07:00] INFO: >>   data:
    pools:   2 pools, 256 pgs
    objects: 5.21k objects, 1.3 GiB
    usage:   3.8 GiB used, 96 GiB / 100 GiB avail
    pgs:     256 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:35:10,738311899-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:35:18,952066074-07:00] INFO: >>   data:
    pools:   2 pools, 256 pgs
    objects: 5.21k objects, 1.3 GiB
    usage:   3.8 GiB used, 96 GiB / 100 GiB avail
    pgs:     256 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:35:18,959845708-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:35:18,965542677-07:00] INFO: > The cluster is idle now.[0m


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:35:18,969305891-07:00] STAGE: Start benchmarking of mode seq ...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:35:18,976599741-07:00] INFO: > Start system activity collection services[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:35:18,982367333-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:329 - rados_bench() > rm --force /mnt/sda8/ceph-research/scripts/output/sys_activity_256KB_seq.dat.rados_bench_host.1
# ./benchmarks/bench-rados:330 - rados_bench() > sar_pid_rados_bench_host=821703
# ./benchmarks/bench-rados:330 - rados_bench() > S_TIME_FORMAT=ISO
# ./benchmarks/bench-rados:330 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/output/sys_activity_256KB_seq.dat.rados_bench_host.1 2

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:35:18,989619244-07:00] INFO: >> for OSD host[0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:35:18,997798670-07:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.5 bash
10.10.2.5: b'81911\n'
[1] 22:35:20 [SUCCESS] ljishen@10.10.2.5
81911

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:35:20,132070271-07:00] INFO: > Run rados bench[0m
# ./benchmarks/bench-rados:367 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/output/rados_bench_256KB_seq.log.1
## ./benchmarks/bench-rados:364 - rados_bench() > count_cpus 32-63,96-127
# ./benchmarks/bench-rados:364 - rados_bench() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 128 --show-time
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:35:20,153568983-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:35:20,157055287-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'd0d4d84e-36e6-11ec-b561-09a495c6360a', '--', 'rados', 'bench', '99999999', 'seq', '--pool', 'bench_rados', '--concurrent-ios', '128', '--show-time'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid d0d4d84e-36e6-11ec-b561-09a495c6360a -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 128 --show-time
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
hints = 1
2021-10-27T05:35:22.519624+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-27T05:35:22.519624+0000     0       1         1         0         0         0           -           0
2021-10-27T05:35:23.519734+0000     1     128      3502      3374   843.353     843.5   0.0306022   0.0371801
2021-10-27T05:35:24.519886+0000 Total time run:       1.53779
Total reads made:     5204
Read size:            262144
Object size:          262144
Bandwidth (MB/sec):   846.017
Average IOPS:         3384
Stddev IOPS:          0
Max IOPS:             3374
Min IOPS:             3374
Average Latency(s):   0.0374056
Max latency(s):       0.0632445
Min latency(s):       0.00343338

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:35:25,099636450-07:00] INFO: > Stop system activity collection services[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:35:25,106115850-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:379 - rados_bench() > kill -INT 821703

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:35:25,111868924-07:00] INFO: >> for OSD host[0m
# ./benchmarks/bench-rados:384 - rados_bench() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.5 kill -INT 81911
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:35:25,120972376-07:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.5 kill -INT 81911
[1] 22:35:26 [SUCCESS] ljishen@10.10.2.5

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:35:26,215583126-07:00] INFO: >>> Copy system activity file from OSD host[0m
## ./benchmarks/bench-rados:391 - rados_bench() > basename -- /tmp/bench-rados/sys_activity_256KB_seq.dat.osd_host.1
# ./benchmarks/bench-rados:391 - rados_bench() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.5:/tmp/bench-rados/sys_activity_256KB_seq.dat.osd_host.1 /mnt/sda8/ceph-research/scripts/output/sys_activity_256KB_seq.dat.osd_host.1


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:35:27,289527678-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:35:50,619569384-07:00] INFO: >>   data:
    pools:   2 pools, 256 pgs
    objects: 5.21k objects, 1.3 GiB
    usage:   3.8 GiB used, 96 GiB / 100 GiB avail
    pgs:     256 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:35:50,627771212-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:35:58,844895753-07:00] INFO: >>   data:
    pools:   2 pools, 256 pgs
    objects: 5.21k objects, 1.3 GiB
    usage:   3.8 GiB used, 96 GiB / 100 GiB avail
    pgs:     256 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:35:58,852889609-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:36:07,025142563-07:00] INFO: >>   data:
    pools:   2 pools, 256 pgs
    objects: 5.21k objects, 1.3 GiB
    usage:   3.8 GiB used, 96 GiB / 100 GiB avail
    pgs:     256 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:36:07,033081918-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:36:15,191673511-07:00] INFO: >>   data:
    pools:   2 pools, 256 pgs
    objects: 5.21k objects, 1.3 GiB
    usage:   3.8 GiB used, 96 GiB / 100 GiB avail
    pgs:     256 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:36:15,200421105-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:36:23,423130634-07:00] INFO: >>   data:
    pools:   2 pools, 256 pgs
    objects: 5.21k objects, 1.3 GiB
    usage:   3.8 GiB used, 96 GiB / 100 GiB avail
    pgs:     256 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:36:23,431574597-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:36:23,437590185-07:00] INFO: > The cluster is idle now.[0m


[1;7;39;49m[2021-10-26T22:36:23,440022397-07:00][RUNNING][ROUND 2/4/21] object_size=256KB[0m


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:36:23,443462273-07:00] STAGE: Launch a Ceph cluster (CEPH_HOST: 10.10.2.3, OSD_HOST: 10.10.2.5) ...[0m
# ./benchmarks/bench-rados:172 - launch_ceph_cluster() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.3 bash -euo pipefail
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:36:23,452288443-07:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.3 bash -euo pipefail
10.10.2.3: b'\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T01:36:23,893968924-04:00] INFO: > Remove the existing cluster\x1b[0m\n'
10.10.2.3: b'## bash:13 -  > ls /var/lib/ceph\n'
10.10.2.3: b'## bash:13 -  > tail -n1\n'
10.10.2.3: b'# bash:13 -  > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm --host 10.10.2.3 --host 10.10.2.5 rm-cluster --force --fsid d0d4d84e-36e6-11ec-b561-09a495c6360a\n'
10.10.2.3: b'\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T01:36:33,046320504-04:00] INFO: > Deploy a new cluster\x1b[0m\n'
10.10.2.3: b'## bash:22 -  > /tmp/bench-rados/ceph-research/scripts/parallel-ssh --host 10.10.2.5 uname --nodename\n## bash:22 -  > tail -n1\n'
10.10.2.3: b'# bash:22 -  > osd_hostname=sm1\n# bash:23 -  > /tmp/bench-rados/ceph-research/scripts/ceph-deploy -m 10.10.2.3 -o sm1/10.10.2.5:/dev/nvme0n1\n'
10.10.2.3: b'\n\n\x1b[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T01:36:34,161019739-04:00] STAGE: Verify passwordless SSH to hosts, passwordless sudo via ljishen, and the existence of devices on hosts...\x1b[0m\n'
10.10.2.3: b'\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T01:36:34,164624902-04:00] INFO: > Check the host for the monitor\x1b[0m\n# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:181 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@10.10.2.3 sudo --non-interactive --validate\n'
10.10.2.3: b'\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T01:36:34,315162111-04:00] INFO: > Check hosts for OSDs\x1b[0m\n'
10.10.2.3: b"\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T01:36:34,319001686-04:00] INFO: >> Check host 'sm1'\x1b[0m\n# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:200 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 sudo --non-interactive --validate\n"
10.10.2.3: b"\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T01:36:35,391558699-04:00] INFO: >>> Check device '/dev/nvme0n1'\x1b[0m\n"
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:215 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 test -b /dev/nvme0n1\n'
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:226 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 lsblk --nodeps --noheadings --output TYPE /dev/nvme0n1\n'
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:227 - verify_parameters() > grep --word-regexp disk\n'
10.10.2.3: b'\n\n\x1b[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T01:36:37,521396258-04:00] STAGE: Reset devices on hosts...\x1b[0m\n'
10.10.2.3: b"\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T01:36:37,525096591-04:00] INFO: > Resetting device '/dev/nvme0n1' on host 'sm1'\x1b[0m\n"
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:288 - reset_host_devices() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 sudo sh\n'
10.10.2.3: b'  Removing ceph--ede52c44--4e97--4881--9512--878f4d1c29b1-osd--block--28e92370--6793--4d6d--be51--4f6f6400ea95 (252:0)\n'
10.10.2.3: b'  Archiving volume group "ceph-ede52c44-4e97-4881-9512-878f4d1c29b1" metadata (seqno 5).\n'
10.10.2.3: b'  Releasing logical volume "osd-block-28e92370-6793-4d6d-be51-4f6f6400ea95"\n'
10.10.2.3: b'  Creating volume group backup "/etc/lvm/backup/ceph-ede52c44-4e97-4881-9512-878f4d1c29b1" (seqno 6).\n'
10.10.2.3: b'  Logical volume "osd-block-28e92370-6793-4d6d-be51-4f6f6400ea95" successfully removed\n'
10.10.2.3: b'  Removing physical volume "/dev/nvme0n1" from volume group "ceph-ede52c44-4e97-4881-9512-878f4d1c29b1"\n'
10.10.2.3: b'  Volume group "ceph-ede52c44-4e97-4881-9512-878f4d1c29b1" successfully removed\n'
10.10.2.3: b'\n\n\x1b[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T01:36:39,801409725-04:00] STAGE: Bootstrap a new cluster...\x1b[0m\n'
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:312 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.3 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults\n'
10.10.2.3: b'\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T01:36:39,809686923-04:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.3: b"\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T01:36:39,812544287-04:00] DEBUG: CEPHADM_PARAMS: ['bootstrap', '--config', '/tmp/bench-rados/ceph-research/scripts/ceph.conf', '--mon-ip', '10.10.2.3', '--output-dir', '/tmp/bench-rados/deployment_data_root/etc/ceph', '--ssh-user', 'ljishen', '--skip-dashboard', '--no-minimize-config', '--allow-overwrite', '--allow-fqdn-hostname', '--orphan-initial-daemons', '--skip-monitoring-stack', '--single-host-defaults']\x1b[0m\n"
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.3 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults\n'
10.10.2.3: b'Verifying podman|docker is present...\nVerifying lvm2 is present...\nVerifying time synchronization is in place...\n'
10.10.2.3: b'Unit ntp.service is enabled and running\nRepeating the final host check...\n'
10.10.2.3: b'podman|docker (/usr/bin/docker) is present\n'
10.10.2.3: b'systemctl is present\nlvcreate is present\n'
10.10.2.3: b'Unit ntp.service is enabled and running\nHost looks OK\n'
10.10.2.3: b'Cluster fsid: dc25683e-36e7-11ec-b561-09a495c6360a\n'
10.10.2.3: b'Verifying IP 10.10.2.3 port 3300 ...\n'
10.10.2.3: b'Verifying IP 10.10.2.3 port 6789 ...\n'
10.10.2.3: b'Mon IP `10.10.2.3` is in CIDR network `10.10.2.0/24`\n- internal network (--cluster-network) has not been provided, OSD replication will default to the public_network\n'
10.10.2.3: b'Adjusting default settings to suit single-host cluster...\n'
10.10.2.3: b'Pulling container image quay.io/ceph/ceph:v16...\n'
10.10.2.3: b'Ceph version: ceph version 16.2.6 (ee28fb57e47e9f88813e24bbf4c14496ca299d31) pacific (stable)\nExtracting ceph user uid/gid from container image...\n'
10.10.2.3: b'Creating initial keys...\n'
10.10.2.3: b'Creating initial monmap...\n'
10.10.2.3: b'Creating mon...\n'
10.10.2.3: b'Waiting for mon to start...\nWaiting for mon...\n'
10.10.2.3: b'mon is available\nSetting mon public_network to 10.10.2.0/24\n'
10.10.2.3: b'Wrote config to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf\nWrote keyring to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring\n'
10.10.2.3: b'Creating mgr...\n'
10.10.2.3: b'Verifying port 9283 ...\n'
10.10.2.3: b'Waiting for mgr to start...\nWaiting for mgr...\n'
10.10.2.3: b'mgr not available, waiting (1/15)...\n'
10.10.2.3: b'mgr not available, waiting (2/15)...\n'
10.10.2.3: b'mgr is available\n'
10.10.2.3: b'Enabling cephadm module...\n'
10.10.2.3: b'Waiting for the mgr to restart...\nWaiting for mgr epoch 5...\n'
10.10.2.3: b'mgr epoch 5 is available\nSetting orchestrator backend to cephadm...\n'
10.10.2.3: b'Generating ssh key...\n'
10.10.2.3: b'Wrote public SSH key to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub\nAdding key to ljishen@localhost authorized_keys...\n'
10.10.2.3: b'Adding host node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us...\n'
10.10.2.3: b'Deploying unmanaged mon service...\n'
10.10.2.3: b'Deploying unmanaged mgr service...\n'
10.10.2.3: b'You can access the Ceph CLI with:\n\n\tsudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --fsid dc25683e-36e7-11ec-b561-09a495c6360a -c /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf -k /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring\n\nPlease consider enabling telemetry to help improve Ceph:\n\n\tceph telemetry on\n\nFor more information see:\n\n\thttps://docs.ceph.com/docs/pacific/mgr/telemetry/\n\nBootstrap complete.\n'
10.10.2.3: b'\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T01:37:43,509756683-04:00] INFO: > Set the default number of monitor daemons to 1\x1b[0m\n# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:330 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1\n'
10.10.2.3: b'\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T01:37:43,518361398-04:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.3: b"\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T01:37:43,521026089-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mon', '1']\x1b[0m\n"
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1\n'
10.10.2.3: b'Inferring fsid dc25683e-36e7-11ec-b561-09a495c6360a\n'
10.10.2.3: b'Inferring config /var/lib/ceph/dc25683e-36e7-11ec-b561-09a495c6360a/mon.node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.3: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.3: b'Scheduled mon update...\n'
10.10.2.3: b'\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T01:37:52,843913396-04:00] INFO: > Set the default number of manager daemons to 1\x1b[0m\n'
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:335 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1\n'
10.10.2.3: b'\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T01:37:52,853435439-04:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.3: b"\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T01:37:52,856803676-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mgr', '1']\x1b[0m\n# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1\n"
10.10.2.3: b'Inferring fsid dc25683e-36e7-11ec-b561-09a495c6360a\n'
10.10.2.3: b'Inferring config /var/lib/ceph/dc25683e-36e7-11ec-b561-09a495c6360a/mon.node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.3: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.3: b'Scheduled mgr update...\n'
10.10.2.3: b'\n\n\x1b[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T01:38:02,052196866-04:00] STAGE: Add hosts to the cluster...\x1b[0m\n'
10.10.2.3: b"\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T01:38:02,057454524-04:00] INFO: > Adding host 'sm1'\x1b[0m\n# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:362 - add_hosts() > ssh-copy-id -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -f -i /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub ljishen@sm1\n"
10.10.2.3: b'/usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: "/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub"\n'
10.10.2.3: b'\nNumber of key(s) added: 1\n\nNow try logging into the machine, with:   "ssh -o \'GlobalKnownHostsFile=/dev/null\' -o \'LogLevel=ERROR\' -o \'PasswordAuthentication=no\' -o \'StrictHostKeyChecking=no\' -o \'UserKnownHostsFile=/dev/null\' \'ljishen@sm1\'"\nand check to make sure that only the key(s) you wanted were added.\n\n'
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:368 - add_hosts() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add sm1 10.10.2.5\n'
10.10.2.3: b'\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T01:38:03,196070216-04:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.3: b"\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T01:38:03,199098943-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'host', 'add', 'sm1', '10.10.2.5']\x1b[0m\n"
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add sm1 10.10.2.5\n'
10.10.2.3: b'Inferring fsid dc25683e-36e7-11ec-b561-09a495c6360a\n'
10.10.2.3: b'Inferring config /var/lib/ceph/dc25683e-36e7-11ec-b561-09a495c6360a/mon.node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.3: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.3: b"Added host 'sm1' with addr '10.10.2.5'\n"
10.10.2.3: b'\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T01:38:13,111148624-04:00] INFO: Sleep for 20 seconds\x1b[0m\n'
10.10.2.3: b'\n\n\x1b[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T01:38:33,114770076-04:00] STAGE: Add OSDs to the cluster...\x1b[0m\n'
10.10.2.3: b"\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T01:38:33,120156447-04:00] INFO: > Adding devices ['/dev/nvme0n1'] on host 'sm1' as OSDs\x1b[0m\n# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:387 - add_host_devices_as_osds() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd sm1:/dev/nvme0n1\n"
10.10.2.3: b'\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T01:38:33,128985093-04:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.3: b"\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T01:38:33,131697314-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'daemon', 'add', 'osd', 'sm1:/dev/nvme0n1']\x1b[0m\n"
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd sm1:/dev/nvme0n1\n'
10.10.2.3: b'Inferring fsid dc25683e-36e7-11ec-b561-09a495c6360a\n'
10.10.2.3: b'Inferring config /var/lib/ceph/dc25683e-36e7-11ec-b561-09a495c6360a/mon.node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.3: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.3: b"Created osd(s) 0 on host 'sm1'\n"
10.10.2.3: b'\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T01:38:57,009365447-04:00] INFO: > Sleep for 20 seconds\x1b[0m\n'
10.10.2.3: b'\n\n\x1b[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T01:39:17,014301440-04:00] STAGE: Show Cluster Status\x1b[0m\n# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:343 - show_status() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status\n'
10.10.2.3: b'\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T01:39:17,023124727-04:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.3: b"\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T01:39:17,026220180-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', '--status']\x1b[0m\n"
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status\n'
10.10.2.3: b'Inferring fsid dc25683e-36e7-11ec-b561-09a495c6360a\n'
10.10.2.3: b'Inferring config /var/lib/ceph/dc25683e-36e7-11ec-b561-09a495c6360a/mon.node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.3: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.3: b'  cluster:\n    id:     dc25683e-36e7-11ec-b561-09a495c6360a\n    health: HEALTH_WARN\n            1 pool(s) have no replicas configured\n \n  services:\n    mon: 1 daemons, quorum node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us (age 2m)\n    mgr: node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us.wqithz(active, since 2m)\n    osd: 1 osds: 1 up (since 20s), 1 in (since 40s)\n \n  data:\n    pools:   1 pools, 1 pgs\n    objects: 0 objects, 0 B\n    usage:   4.7 MiB used, 100 GiB / 100 GiB avail\n    pgs:     1 active+clean\n \n  progress:\n \n'
[1] 22:39:26 [SUCCESS] ljishen@10.10.2.3

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T01:36:23,893968924-04:00] INFO: > Remove the existing cluster[0m
## bash:13 -  > ls /var/lib/ceph
## bash:13 -  > tail -n1
# bash:13 -  > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm --host 10.10.2.3 --host 10.10.2.5 rm-cluster --force --fsid d0d4d84e-36e6-11ec-b561-09a495c6360a

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T01:36:33,046320504-04:00] INFO: > Deploy a new cluster[0m
## bash:22 -  > /tmp/bench-rados/ceph-research/scripts/parallel-ssh --host 10.10.2.5 uname --nodename
## bash:22 -  > tail -n1
# bash:22 -  > osd_hostname=sm1
# bash:23 -  > /tmp/bench-rados/ceph-research/scripts/ceph-deploy -m 10.10.2.3 -o sm1/10.10.2.5:/dev/nvme0n1


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T01:36:34,161019739-04:00] STAGE: Verify passwordless SSH to hosts, passwordless sudo via ljishen, and the existence of devices on hosts...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T01:36:34,164624902-04:00] INFO: > Check the host for the monitor[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:181 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@10.10.2.3 sudo --non-interactive --validate

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T01:36:34,315162111-04:00] INFO: > Check hosts for OSDs[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T01:36:34,319001686-04:00] INFO: >> Check host 'sm1'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:200 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 sudo --non-interactive --validate

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T01:36:35,391558699-04:00] INFO: >>> Check device '/dev/nvme0n1'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:215 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 test -b /dev/nvme0n1
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:226 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 lsblk --nodeps --noheadings --output TYPE /dev/nvme0n1
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:227 - verify_parameters() > grep --word-regexp disk


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T01:36:37,521396258-04:00] STAGE: Reset devices on hosts...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T01:36:37,525096591-04:00] INFO: > Resetting device '/dev/nvme0n1' on host 'sm1'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:288 - reset_host_devices() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 sudo sh
  Removing ceph--ede52c44--4e97--4881--9512--878f4d1c29b1-osd--block--28e92370--6793--4d6d--be51--4f6f6400ea95 (252:0)
  Archiving volume group "ceph-ede52c44-4e97-4881-9512-878f4d1c29b1" metadata (seqno 5).
  Releasing logical volume "osd-block-28e92370-6793-4d6d-be51-4f6f6400ea95"
  Creating volume group backup "/etc/lvm/backup/ceph-ede52c44-4e97-4881-9512-878f4d1c29b1" (seqno 6).
  Logical volume "osd-block-28e92370-6793-4d6d-be51-4f6f6400ea95" successfully removed
  Removing physical volume "/dev/nvme0n1" from volume group "ceph-ede52c44-4e97-4881-9512-878f4d1c29b1"
  Volume group "ceph-ede52c44-4e97-4881-9512-878f4d1c29b1" successfully removed


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T01:36:39,801409725-04:00] STAGE: Bootstrap a new cluster...[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:312 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.3 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T01:36:39,809686923-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T01:36:39,812544287-04:00] DEBUG: CEPHADM_PARAMS: ['bootstrap', '--config', '/tmp/bench-rados/ceph-research/scripts/ceph.conf', '--mon-ip', '10.10.2.3', '--output-dir', '/tmp/bench-rados/deployment_data_root/etc/ceph', '--ssh-user', 'ljishen', '--skip-dashboard', '--no-minimize-config', '--allow-overwrite', '--allow-fqdn-hostname', '--orphan-initial-daemons', '--skip-monitoring-stack', '--single-host-defaults'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.3 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults
Verifying podman|docker is present...
Verifying lvm2 is present...
Verifying time synchronization is in place...
Unit ntp.service is enabled and running
Repeating the final host check...
podman|docker (/usr/bin/docker) is present
systemctl is present
lvcreate is present
Unit ntp.service is enabled and running
Host looks OK
Cluster fsid: dc25683e-36e7-11ec-b561-09a495c6360a
Verifying IP 10.10.2.3 port 3300 ...
Verifying IP 10.10.2.3 port 6789 ...
Mon IP `10.10.2.3` is in CIDR network `10.10.2.0/24`
- internal network (--cluster-network) has not been provided, OSD replication will default to the public_network
Adjusting default settings to suit single-host cluster...
Pulling container image quay.io/ceph/ceph:v16...
Ceph version: ceph version 16.2.6 (ee28fb57e47e9f88813e24bbf4c14496ca299d31) pacific (stable)
Extracting ceph user uid/gid from container image...
Creating initial keys...
Creating initial monmap...
Creating mon...
Waiting for mon to start...
Waiting for mon...
mon is available
Setting mon public_network to 10.10.2.0/24
Wrote config to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf
Wrote keyring to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring
Creating mgr...
Verifying port 9283 ...
Waiting for mgr to start...
Waiting for mgr...
mgr not available, waiting (1/15)...
mgr not available, waiting (2/15)...
mgr is available
Enabling cephadm module...
Waiting for the mgr to restart...
Waiting for mgr epoch 5...
mgr epoch 5 is available
Setting orchestrator backend to cephadm...
Generating ssh key...
Wrote public SSH key to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub
Adding key to ljishen@localhost authorized_keys...
Adding host node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us...
Deploying unmanaged mon service...
Deploying unmanaged mgr service...
You can access the Ceph CLI with:

	sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --fsid dc25683e-36e7-11ec-b561-09a495c6360a -c /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf -k /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring

Please consider enabling telemetry to help improve Ceph:

	ceph telemetry on

For more information see:

	https://docs.ceph.com/docs/pacific/mgr/telemetry/

Bootstrap complete.

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T01:37:43,509756683-04:00] INFO: > Set the default number of monitor daemons to 1[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:330 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T01:37:43,518361398-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T01:37:43,521026089-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mon', '1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1
Inferring fsid dc25683e-36e7-11ec-b561-09a495c6360a
Inferring config /var/lib/ceph/dc25683e-36e7-11ec-b561-09a495c6360a/mon.node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Scheduled mon update...

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T01:37:52,843913396-04:00] INFO: > Set the default number of manager daemons to 1[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:335 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T01:37:52,853435439-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T01:37:52,856803676-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mgr', '1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1
Inferring fsid dc25683e-36e7-11ec-b561-09a495c6360a
Inferring config /var/lib/ceph/dc25683e-36e7-11ec-b561-09a495c6360a/mon.node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Scheduled mgr update...


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T01:38:02,052196866-04:00] STAGE: Add hosts to the cluster...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T01:38:02,057454524-04:00] INFO: > Adding host 'sm1'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:362 - add_hosts() > ssh-copy-id -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -f -i /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub ljishen@sm1
/usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: "/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub"

Number of key(s) added: 1

Now try logging into the machine, with:   "ssh -o 'GlobalKnownHostsFile=/dev/null' -o 'LogLevel=ERROR' -o 'PasswordAuthentication=no' -o 'StrictHostKeyChecking=no' -o 'UserKnownHostsFile=/dev/null' 'ljishen@sm1'"
and check to make sure that only the key(s) you wanted were added.

# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:368 - add_hosts() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add sm1 10.10.2.5
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T01:38:03,196070216-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T01:38:03,199098943-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'host', 'add', 'sm1', '10.10.2.5'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add sm1 10.10.2.5
Inferring fsid dc25683e-36e7-11ec-b561-09a495c6360a
Inferring config /var/lib/ceph/dc25683e-36e7-11ec-b561-09a495c6360a/mon.node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Added host 'sm1' with addr '10.10.2.5'

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T01:38:13,111148624-04:00] INFO: Sleep for 20 seconds[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T01:38:33,114770076-04:00] STAGE: Add OSDs to the cluster...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T01:38:33,120156447-04:00] INFO: > Adding devices ['/dev/nvme0n1'] on host 'sm1' as OSDs[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:387 - add_host_devices_as_osds() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd sm1:/dev/nvme0n1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T01:38:33,128985093-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T01:38:33,131697314-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'daemon', 'add', 'osd', 'sm1:/dev/nvme0n1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd sm1:/dev/nvme0n1
Inferring fsid dc25683e-36e7-11ec-b561-09a495c6360a
Inferring config /var/lib/ceph/dc25683e-36e7-11ec-b561-09a495c6360a/mon.node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Created osd(s) 0 on host 'sm1'

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T01:38:57,009365447-04:00] INFO: > Sleep for 20 seconds[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T01:39:17,014301440-04:00] STAGE: Show Cluster Status[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:343 - show_status() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T01:39:17,023124727-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T01:39:17,026220180-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', '--status'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status
Inferring fsid dc25683e-36e7-11ec-b561-09a495c6360a
Inferring config /var/lib/ceph/dc25683e-36e7-11ec-b561-09a495c6360a/mon.node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
  cluster:
    id:     dc25683e-36e7-11ec-b561-09a495c6360a
    health: HEALTH_WARN
            1 pool(s) have no replicas configured
 
  services:
    mon: 1 daemons, quorum node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us (age 2m)
    mgr: node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us.wqithz(active, since 2m)
    osd: 1 osds: 1 up (since 20s), 1 in (since 40s)
 
  data:
    pools:   1 pools, 1 pgs
    objects: 0 objects, 0 B
    usage:   4.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     1 active+clean
 
  progress:
 

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:39:26,052846403-07:00] INFO: > Copy configuration files to local[0m
# ./benchmarks/bench-rados:208 - launch_ceph_cluster() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.3:/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf
# ./benchmarks/bench-rados:212 - launch_ceph_cluster() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.3 sudo chmod 644 /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:39:26,211224327-07:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.3 sudo chmod 644 /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring
[1] 22:39:26 [SUCCESS] ljishen@10.10.2.3
# ./benchmarks/bench-rados:214 - launch_ceph_cluster() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.3:/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:39:26,542772060-07:00] STAGE: Update local ceph.conf as a client...[0m


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:39:26,546702888-07:00] STAGE: Configure Ceph pool...[0m
# ./benchmarks/bench-rados:245 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_max_backfills 32
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:39:26,570211327-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:39:26,573242545-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'dc25683e-36e7-11ec-b561-09a495c6360a', '--', 'ceph', 'config', 'set', 'osd', 'osd_max_backfills', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid dc25683e-36e7-11ec-b561-09a495c6360a -- ceph config set osd osd_max_backfills 32
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:246 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_active 32
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:39:29,739773192-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:39:29,742703931-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'dc25683e-36e7-11ec-b561-09a495c6360a', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_active', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid dc25683e-36e7-11ec-b561-09a495c6360a -- ceph config set osd osd_recovery_max_active 32
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:247 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_single_start 8
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:39:33,007058249-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:39:33,010744828-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'dc25683e-36e7-11ec-b561-09a495c6360a', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_single_start', '8'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid dc25683e-36e7-11ec-b561-09a495c6360a -- ceph config set osd osd_recovery_max_single_start 8
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:248 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_op_priority 63
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:39:36,173811484-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:39:36,177237995-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'dc25683e-36e7-11ec-b561-09a495c6360a', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_op_priority', '63'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid dc25683e-36e7-11ec-b561-09a495c6360a -- ceph config set osd osd_recovery_op_priority 63
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:250 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config show-with-defaults osd.0
# ./benchmarks/bench-rados:251 - configure_ceph_pool() > grep 'osd_max_backfills\|osd_recovery'
# ./benchmarks/bench-rados:251 - configure_ceph_pool() > column -t -s ' '
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
osd_max_backfills                        32         mon
osd_recovery_cost                        20971520   default
osd_recovery_delay_start                 0.000000   default
osd_recovery_max_active                  32         mon
osd_recovery_max_active_hdd              3          default
osd_recovery_max_active_ssd              10         default
osd_recovery_max_chunk                   8388608    default
osd_recovery_max_omap_entries_per_chunk  8096       default
osd_recovery_max_single_start            8          mon
osd_recovery_op_priority                 63         mon
osd_recovery_op_warn_multiple            16         default
osd_recovery_priority                    5          default
osd_recovery_retry_interval              30.000000  default
osd_recovery_sleep                       0.000000   default
osd_recovery_sleep_hdd                   0.100000   default
osd_recovery_sleep_hybrid                0.025000   default
osd_recovery_sleep_ssd                   0.000000   default
# ./benchmarks/bench-rados:253 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool create bench_rados 128 128 replicated --size 1 --pg-num-min 128
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:39:42,630628244-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:39:42,634010992-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'dc25683e-36e7-11ec-b561-09a495c6360a', '--', 'ceph', 'osd', 'pool', 'create', 'bench_rados', '128', '128', 'replicated', '--size', '1', '--pg-num-min', '128'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid dc25683e-36e7-11ec-b561-09a495c6360a -- ceph osd pool create bench_rados 128 128 replicated --size 1 --pg-num-min 128
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
pool 'bench_rados' created
# ./benchmarks/bench-rados:255 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados min_size 1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:39:46,163574746-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:39:46,166867926-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'dc25683e-36e7-11ec-b561-09a495c6360a', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'min_size', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid dc25683e-36e7-11ec-b561-09a495c6360a -- ceph osd pool set bench_rados min_size 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 min_size to 1
# ./benchmarks/bench-rados:256 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados pg_autoscale_mode off
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:39:50,378816743-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:39:50,382057966-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'dc25683e-36e7-11ec-b561-09a495c6360a', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'pg_autoscale_mode', 'off'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid dc25683e-36e7-11ec-b561-09a495c6360a -- ceph osd pool set bench_rados pg_autoscale_mode off
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 pg_autoscale_mode to off
# ./benchmarks/bench-rados:257 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool application enable bench_rados benchmark
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:39:53,821719800-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:39:53,825584925-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'dc25683e-36e7-11ec-b561-09a495c6360a', '--', 'ceph', 'osd', 'pool', 'application', 'enable', 'bench_rados', 'benchmark'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid dc25683e-36e7-11ec-b561-09a495c6360a -- ceph osd pool application enable bench_rados benchmark
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
enabled application 'benchmark' on pool 'bench_rados'
# ./benchmarks/bench-rados:258 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados noscrub 1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:39:57,227809288-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:39:57,230982713-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'dc25683e-36e7-11ec-b561-09a495c6360a', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'noscrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid dc25683e-36e7-11ec-b561-09a495c6360a -- ceph osd pool set bench_rados noscrub 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 noscrub to 1
# ./benchmarks/bench-rados:259 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados nodeep-scrub 1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:40:01,477143729-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:40:01,480443181-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'dc25683e-36e7-11ec-b561-09a495c6360a', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'nodeep-scrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid dc25683e-36e7-11ec-b561-09a495c6360a -- ceph osd pool set bench_rados nodeep-scrub 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 nodeep-scrub to 1
# ./benchmarks/bench-rados:260 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool ls detail
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:40:06,094538025-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:40:06,098379606-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'dc25683e-36e7-11ec-b561-09a495c6360a', '--', 'ceph', 'osd', 'pool', 'ls', 'detail'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid dc25683e-36e7-11ec-b561-09a495c6360a -- ceph osd pool ls detail
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
pool 1 'device_health_metrics' replicated size 1 min_size 1 crush_rule 0 object_hash rjenkins pg_num 128 pgp_num 128 autoscale_mode on last_change 13 lfor 0/0/11 flags hashpspool stripe_width 0 pg_num_min 1 application mgr_devicehealth
pool 2 'bench_rados' replicated size 1 min_size 1 crush_rule 0 object_hash rjenkins pg_num 128 pgp_num 128 autoscale_mode off last_change 21 flags hashpspool,noscrub,nodeep-scrub stripe_width 0 pg_num_min 128 application benchmark

# ./benchmarks/bench-rados:261 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd df tree
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:40:09,261695742-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:40:09,265434249-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'dc25683e-36e7-11ec-b561-09a495c6360a', '--', 'ceph', 'osd', 'df', 'tree'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid dc25683e-36e7-11ec-b561-09a495c6360a -- ceph osd df tree
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
ID  CLASS  WEIGHT   REWEIGHT  SIZE     RAW USE  DATA     OMAP  META     AVAIL    %USE  VAR   PGS  STATUS  TYPE NAME    
-1         0.09769         -  100 GiB  6.0 MiB  176 KiB   0 B  5.8 MiB  100 GiB  0.01  1.00    -          root default 
-3         0.09769         -  100 GiB  6.0 MiB  176 KiB   0 B  5.8 MiB  100 GiB  0.01  1.00    -              host sm1 
 0    ssd  0.09769   1.00000  100 GiB  6.0 MiB  176 KiB   0 B  5.8 MiB  100 GiB  0.01  1.00  256      up          osd.0
                       TOTAL  100 GiB  6.0 MiB  176 KiB   0 B  5.8 MiB  100 GiB  0.01                                  
MIN/MAX VAR: 1.00/1.00  STDDEV: 0


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:40:12,525635942-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:40:35,765050913-07:00] INFO: >>   data:
    pools:   2 pools, 256 pgs
    objects: 0 objects, 0 B
    usage:   6.0 MiB used, 100 GiB / 100 GiB avail
    pgs:     256 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:40:35,772581368-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:40:43,923168792-07:00] INFO: >>   data:
    pools:   2 pools, 256 pgs
    objects: 0 objects, 0 B
    usage:   6.0 MiB used, 100 GiB / 100 GiB avail
    pgs:     256 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:40:43,932032894-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:40:52,049643711-07:00] INFO: >>   data:
    pools:   2 pools, 256 pgs
    objects: 0 objects, 0 B
    usage:   6.0 MiB used, 100 GiB / 100 GiB avail
    pgs:     256 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:40:52,058568998-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:41:00,332198276-07:00] INFO: >>   data:
    pools:   2 pools, 256 pgs
    objects: 0 objects, 0 B
    usage:   6.0 MiB used, 100 GiB / 100 GiB avail
    pgs:     256 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:41:00,340052349-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:41:08,604270025-07:00] INFO: >>   data:
    pools:   2 pools, 256 pgs
    objects: 0 objects, 0 B
    usage:   6.0 MiB used, 100 GiB / 100 GiB avail
    pgs:     256 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:41:08,612725148-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:41:08,618701752-07:00] INFO: > The cluster is idle now.[0m


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:41:08,622548002-07:00] STAGE: Start benchmarking of mode write ...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:41:08,629826223-07:00] INFO: > Start system activity collection services[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:41:08,635204303-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:329 - rados_bench() > rm --force /mnt/sda8/ceph-research/scripts/output/sys_activity_256KB_write.dat.rados_bench_host.2
# ./benchmarks/bench-rados:330 - rados_bench() > sar_pid_rados_bench_host=828600
# ./benchmarks/bench-rados:330 - rados_bench() > S_TIME_FORMAT=ISO
# ./benchmarks/bench-rados:330 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/output/sys_activity_256KB_write.dat.rados_bench_host.2 2

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:41:08,642341008-07:00] INFO: >> for OSD host[0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:41:08,650814205-07:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.5 bash
10.10.2.5: b'86669\n'
[1] 22:41:09 [SUCCESS] ljishen@10.10.2.5
86669

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:41:09,762899439-07:00] INFO: > Run rados bench[0m
# ./benchmarks/bench-rados:358 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/output/rados_bench_256KB_write.log.2
## ./benchmarks/bench-rados:349 - rados_bench() > count_cpus 32-63,96-127
# ./benchmarks/bench-rados:349 - rados_bench() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 30 write --pool bench_rados -b 262144 -O 262144 --concurrent-ios 128 --show-time --write-object --write-omap --write-xattr --no-cleanup
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:41:09,784475896-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:41:09,787438415-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'dc25683e-36e7-11ec-b561-09a495c6360a', '--', 'rados', 'bench', '30', 'write', '--pool', 'bench_rados', '-b', '262144', '-O', '262144', '--concurrent-ios', '128', '--show-time', '--write-object', '--write-omap', '--write-xattr', '--no-cleanup'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid dc25683e-36e7-11ec-b561-09a495c6360a -- rados bench 30 write --pool bench_rados -b 262144 -O 262144 --concurrent-ios 128 --show-time --write-object --write-omap --write-xattr --no-cleanup
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
hints = 1
2021-10-27T05:41:12.229513+0000 Maintaining 128 concurrent writes of 262144 bytes to objects of size 262144 for up to 30 seconds or 0 objects
2021-10-27T05:41:12.229524+0000 Object prefix: benchmark_data_node-0.bluefield2.ucsc-cmps10_7
2021-10-27T05:41:12.237376+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-27T05:41:12.237376+0000     0       0         0         0         0         0           -           0
2021-10-27T05:41:13.237525+0000     1     128       341       213   53.2457     53.25    0.427331    0.470037
2021-10-27T05:41:14.237608+0000     2     128       600       472   58.9952     64.75    0.349228    0.480924
2021-10-27T05:41:15.237690+0000     3     128       864       736   61.3283        66    0.516239    0.478604
2021-10-27T05:41:16.237759+0000     4     128      1136      1008    62.995        68    0.378834    0.479501
2021-10-27T05:41:17.237834+0000     5     128      1407      1279    63.945     67.75    0.542944    0.484076
2021-10-27T05:41:18.238059+0000     6     128      1619      1491   62.1186        53    0.512317    0.484753
2021-10-27T05:41:19.238129+0000     7     128      1877      1749   62.4582      64.5    0.652528    0.484802
2021-10-27T05:41:20.238237+0000     8     128      2137      2009    62.775        65    0.448541    0.482262
2021-10-27T05:41:21.238452+0000     9     128      2308      2180   60.5488     42.75     0.81967     0.50618
2021-10-27T05:41:22.238683+0000    10     128      2434      2306   57.6429      31.5     0.76402    0.534528
2021-10-27T05:41:23.238751+0000    11     128      2536      2408   54.7208      25.5    0.953295    0.550136
2021-10-27T05:41:24.238923+0000    12     128      2699      2571   53.5559     40.75    0.778791    0.576327
2021-10-27T05:41:25.239018+0000    13     128      2832      2704   51.9937     33.25    0.958673    0.595172
2021-10-27T05:41:26.239113+0000    14     128      2933      2805   50.0833     25.25     1.09058    0.606353
2021-10-27T05:41:27.239191+0000    15     128      3103      2975   49.5776      42.5    0.816601    0.628375
2021-10-27T05:41:28.239291+0000    16     128      3210      3082   48.1507     26.75    0.944749    0.642282
2021-10-27T05:41:29.239504+0000    17     128      3488      3360   49.4058      69.5    0.338802    0.635869
2021-10-27T05:41:30.239615+0000    18     128      3755      3627   50.3689     66.75    0.472206    0.625008
2021-10-27T05:41:31.239689+0000    19     128      4017      3889    51.165      65.5    0.496761    0.615259
2021-10-27T05:41:32.239765+0000 min lat: 0.134954 max lat: 1.59995 avg lat: 0.609478
2021-10-27T05:41:32.239765+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-27T05:41:32.239765+0000    20     128      4242      4114    51.419     56.25    0.639503    0.609478
2021-10-27T05:41:33.239842+0000    21     128      4395      4267   50.7918     38.25    0.760807    0.620321
2021-10-27T05:41:34.239920+0000    22     128      4460      4332   49.2217     16.25     1.20859    0.630841
2021-10-27T05:41:35.239988+0000    23     128      4519      4391    47.723     14.75     1.43241    0.642138
2021-10-27T05:41:36.240068+0000    24     128      4606      4478   46.6407     21.75     1.20658    0.657176
2021-10-27T05:41:37.240217+0000    25     128      4714      4586   45.8549        27     1.18291    0.679414
2021-10-27T05:41:38.240294+0000    26     128      4816      4688    45.072      25.5     1.20388       0.695
2021-10-27T05:41:39.240374+0000    27     128      4925      4797   44.4118     27.25     1.22947    0.711619
2021-10-27T05:41:40.240483+0000    28     128      4940      4812   42.9596      3.75    0.447542    0.713284
2021-10-27T05:41:41.240569+0000    29     128      5045      4917   42.3834     26.25     1.25851    0.731337
2021-10-27T05:41:42.240725+0000    30     128      5162      5034   41.9454     29.25     1.39847    0.748019
2021-10-27T05:41:43.240964+0000    31      33      5162      5129   41.3582     23.75     1.18201    0.758974
2021-10-27T05:41:44.241109+0000 Total time run:         31.0212
Total writes made:      5162
Write size:             262144
Object size:            262144
Bandwidth (MB/sec):     41.6006
Stddev Bandwidth:       19.6238
Max bandwidth (MB/sec): 69.5
Min bandwidth (MB/sec): 3.75
Average IOPS:           166
Stddev IOPS:            78.4954
Max IOPS:               278
Min IOPS:               15
Average Latency(s):     0.763233
Stddev Latency(s):      0.420674
Max latency(s):         2.57429
Min latency(s):         0.134954

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:41:44,828521723-07:00] INFO: > Stop system activity collection services[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:41:44,835086042-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:379 - rados_bench() > kill -INT 828600

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:41:44,841553149-07:00] INFO: >> for OSD host[0m
# ./benchmarks/bench-rados:384 - rados_bench() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.5 kill -INT 86669
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:41:44,850039350-07:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.5 kill -INT 86669
[1] 22:41:46 [SUCCESS] ljishen@10.10.2.5

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:41:46,147492332-07:00] INFO: >>> Copy system activity file from OSD host[0m
## ./benchmarks/bench-rados:391 - rados_bench() > basename -- /tmp/bench-rados/sys_activity_256KB_write.dat.osd_host.2
# ./benchmarks/bench-rados:391 - rados_bench() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.5:/tmp/bench-rados/sys_activity_256KB_write.dat.osd_host.2 /mnt/sda8/ceph-research/scripts/output/sys_activity_256KB_write.dat.osd_host.2


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:41:47,416551531-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:42:10,440613245-07:00] INFO: >>   data:
    pools:   2 pools, 256 pgs
    objects: 5.16k objects, 1.3 GiB
    usage:   3.8 GiB used, 96 GiB / 100 GiB avail
    pgs:     256 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:42:10,448896856-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:42:18,673841383-07:00] INFO: >>   data:
    pools:   2 pools, 256 pgs
    objects: 5.16k objects, 1.3 GiB
    usage:   3.8 GiB used, 96 GiB / 100 GiB avail
    pgs:     256 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:42:18,682437953-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:42:26,964611735-07:00] INFO: >>   data:
    pools:   2 pools, 256 pgs
    objects: 5.16k objects, 1.3 GiB
    usage:   3.8 GiB used, 96 GiB / 100 GiB avail
    pgs:     256 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:42:26,972708394-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:42:35,213982308-07:00] INFO: >>   data:
    pools:   2 pools, 256 pgs
    objects: 5.16k objects, 1.3 GiB
    usage:   3.8 GiB used, 96 GiB / 100 GiB avail
    pgs:     256 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:42:35,222539833-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:42:43,488080872-07:00] INFO: >>   data:
    pools:   2 pools, 256 pgs
    objects: 5.16k objects, 1.3 GiB
    usage:   3.8 GiB used, 96 GiB / 100 GiB avail
    pgs:     256 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:42:43,496074066-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:42:43,502575387-07:00] INFO: > The cluster is idle now.[0m


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:42:43,506285932-07:00] STAGE: Start benchmarking of mode seq ...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:42:43,513774168-07:00] INFO: > Start system activity collection services[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:42:43,519204205-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:329 - rados_bench() > rm --force /mnt/sda8/ceph-research/scripts/output/sys_activity_256KB_seq.dat.rados_bench_host.2
# ./benchmarks/bench-rados:330 - rados_bench() > sar_pid_rados_bench_host=830460
# ./benchmarks/bench-rados:330 - rados_bench() > S_TIME_FORMAT=ISO
# ./benchmarks/bench-rados:330 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/output/sys_activity_256KB_seq.dat.rados_bench_host.2 2

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:42:43,525993366-07:00] INFO: >> for OSD host[0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:42:43,534492612-07:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.5 bash
10.10.2.5: b'87068\n'
[1] 22:42:44 [SUCCESS] ljishen@10.10.2.5
87068

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:42:44,649000150-07:00] INFO: > Run rados bench[0m
# ./benchmarks/bench-rados:367 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/output/rados_bench_256KB_seq.log.2
## ./benchmarks/bench-rados:364 - rados_bench() > count_cpus 32-63,96-127
# ./benchmarks/bench-rados:364 - rados_bench() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 128 --show-time
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:42:44,669793716-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:42:44,673353969-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'dc25683e-36e7-11ec-b561-09a495c6360a', '--', 'rados', 'bench', '99999999', 'seq', '--pool', 'bench_rados', '--concurrent-ios', '128', '--show-time'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid dc25683e-36e7-11ec-b561-09a495c6360a -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 128 --show-time
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
hints = 1
2021-10-27T05:42:47.222454+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-27T05:42:47.222454+0000     0       1         1         0         0         0           -           0
2021-10-27T05:42:48.222584+0000     1     128      3488      3360   839.833       840   0.0428133   0.0374655
2021-10-27T05:42:49.222722+0000 Total time run:       1.51907
Total reads made:     5162
Read size:            262144
Object size:          262144
Bandwidth (MB/sec):   849.535
Average IOPS:         3398
Stddev IOPS:          0
Max IOPS:             3360
Min IOPS:             3360
Average Latency(s):   0.0372236
Max latency(s):       0.0663239
Min latency(s):       0.00450993

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:42:49,805030199-07:00] INFO: > Stop system activity collection services[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:42:49,810662536-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:379 - rados_bench() > kill -INT 830460

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:42:49,816915319-07:00] INFO: >> for OSD host[0m
# ./benchmarks/bench-rados:384 - rados_bench() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.5 kill -INT 87068
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:42:49,825651491-07:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.5 kill -INT 87068
[1] 22:42:50 [SUCCESS] ljishen@10.10.2.5

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:42:50,899157288-07:00] INFO: >>> Copy system activity file from OSD host[0m
## ./benchmarks/bench-rados:391 - rados_bench() > basename -- /tmp/bench-rados/sys_activity_256KB_seq.dat.osd_host.2
# ./benchmarks/bench-rados:391 - rados_bench() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.5:/tmp/bench-rados/sys_activity_256KB_seq.dat.osd_host.2 /mnt/sda8/ceph-research/scripts/output/sys_activity_256KB_seq.dat.osd_host.2


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:42:51,953193527-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:43:15,057071852-07:00] INFO: >>   data:
    pools:   2 pools, 256 pgs
    objects: 5.16k objects, 1.3 GiB
    usage:   3.8 GiB used, 96 GiB / 100 GiB avail
    pgs:     256 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:43:15,064920054-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:43:23,308254984-07:00] INFO: >>   data:
    pools:   2 pools, 256 pgs
    objects: 5.16k objects, 1.3 GiB
    usage:   3.8 GiB used, 96 GiB / 100 GiB avail
    pgs:     256 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:43:23,317537653-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:43:31,413842299-07:00] INFO: >>   data:
    pools:   2 pools, 256 pgs
    objects: 5.16k objects, 1.3 GiB
    usage:   3.8 GiB used, 96 GiB / 100 GiB avail
    pgs:     256 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:43:31,422670473-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:43:39,630454799-07:00] INFO: >>   data:
    pools:   2 pools, 256 pgs
    objects: 5.16k objects, 1.3 GiB
    usage:   3.8 GiB used, 96 GiB / 100 GiB avail
    pgs:     256 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:43:39,638355550-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:43:47,906376984-07:00] INFO: >>   data:
    pools:   2 pools, 256 pgs
    objects: 5.16k objects, 1.3 GiB
    usage:   3.8 GiB used, 96 GiB / 100 GiB avail
    pgs:     256 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:43:47,915007406-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:43:47,920975735-07:00] INFO: > The cluster is idle now.[0m


[1;7;39;49m[2021-10-26T22:43:47,923234250-07:00][RUNNING][ROUND 3/4/21] object_size=256KB[0m


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:43:47,926767241-07:00] STAGE: Launch a Ceph cluster (CEPH_HOST: 10.10.2.3, OSD_HOST: 10.10.2.5) ...[0m
# ./benchmarks/bench-rados:172 - launch_ceph_cluster() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.3 bash -euo pipefail
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:43:47,935912432-07:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.3 bash -euo pipefail
10.10.2.3: b'\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T01:43:48,362125485-04:00] INFO: > Remove the existing cluster\x1b[0m\n'
10.10.2.3: b'## bash:13 -  > ls /var/lib/ceph\n## bash:13 -  > tail -n1\n'
10.10.2.3: b'# bash:13 -  > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm --host 10.10.2.3 --host 10.10.2.5 rm-cluster --force --fsid dc25683e-36e7-11ec-b561-09a495c6360a\n'
10.10.2.3: b'\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T01:43:57,414793171-04:00] INFO: > Deploy a new cluster\x1b[0m\n'
10.10.2.3: b'## bash:22 -  > /tmp/bench-rados/ceph-research/scripts/parallel-ssh --host 10.10.2.5 uname --nodename\n## bash:22 -  > tail -n1\n'
10.10.2.3: b'# bash:22 -  > osd_hostname=sm1\n# bash:23 -  > /tmp/bench-rados/ceph-research/scripts/ceph-deploy -m 10.10.2.3 -o sm1/10.10.2.5:/dev/nvme0n1\n'
10.10.2.3: b'\n\n\x1b[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T01:43:58,545714268-04:00] STAGE: Verify passwordless SSH to hosts, passwordless sudo via ljishen, and the existence of devices on hosts...\x1b[0m\n'
10.10.2.3: b'\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T01:43:58,549166833-04:00] INFO: > Check the host for the monitor\x1b[0m\n# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:181 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@10.10.2.3 sudo --non-interactive --validate\n'
10.10.2.3: b'\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T01:43:58,695275126-04:00] INFO: > Check hosts for OSDs\x1b[0m\n'
10.10.2.3: b"\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T01:43:58,699226992-04:00] INFO: >> Check host 'sm1'\x1b[0m\n# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:200 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 sudo --non-interactive --validate\n"
10.10.2.3: b"\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T01:43:59,763954425-04:00] INFO: >>> Check device '/dev/nvme0n1'\x1b[0m\n"
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:215 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 test -b /dev/nvme0n1\n'
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:226 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 lsblk --nodeps --noheadings --output TYPE /dev/nvme0n1\n'
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:227 - verify_parameters() > grep --word-regexp disk\n'
10.10.2.3: b'\n\n\x1b[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T01:44:01,888911246-04:00] STAGE: Reset devices on hosts...\x1b[0m\n'
10.10.2.3: b"\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T01:44:01,892777721-04:00] INFO: > Resetting device '/dev/nvme0n1' on host 'sm1'\x1b[0m\n"
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:288 - reset_host_devices() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 sudo sh\n'
10.10.2.3: b'  Removing ceph--96467807--d21e--4abd--b561--9f6d0f91534e-osd--block--aa0a35b7--ead4--401b--94e5--82a008ff307d (252:0)\n'
10.10.2.3: b'  Archiving volume group "ceph-96467807-d21e-4abd-b561-9f6d0f91534e" metadata (seqno 5).\n'
10.10.2.3: b'  Releasing logical volume "osd-block-aa0a35b7-ead4-401b-94e5-82a008ff307d"\n'
10.10.2.3: b'  Creating volume group backup "/etc/lvm/backup/ceph-96467807-d21e-4abd-b561-9f6d0f91534e" (seqno 6).\n'
10.10.2.3: b'  Logical volume "osd-block-aa0a35b7-ead4-401b-94e5-82a008ff307d" successfully removed\n'
10.10.2.3: b'  Removing physical volume "/dev/nvme0n1" from volume group "ceph-96467807-d21e-4abd-b561-9f6d0f91534e"\n'
10.10.2.3: b'  Volume group "ceph-96467807-d21e-4abd-b561-9f6d0f91534e" successfully removed\n'
10.10.2.3: b'\n\n\x1b[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T01:44:04,222304788-04:00] STAGE: Bootstrap a new cluster...\x1b[0m\n# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:312 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.3 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults\n'
10.10.2.3: b'\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T01:44:04,230619977-04:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.3: b"\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T01:44:04,233537705-04:00] DEBUG: CEPHADM_PARAMS: ['bootstrap', '--config', '/tmp/bench-rados/ceph-research/scripts/ceph.conf', '--mon-ip', '10.10.2.3', '--output-dir', '/tmp/bench-rados/deployment_data_root/etc/ceph', '--ssh-user', 'ljishen', '--skip-dashboard', '--no-minimize-config', '--allow-overwrite', '--allow-fqdn-hostname', '--orphan-initial-daemons', '--skip-monitoring-stack', '--single-host-defaults']\x1b[0m\n"
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.3 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults\n'
10.10.2.3: b'Verifying podman|docker is present...\nVerifying lvm2 is present...\nVerifying time synchronization is in place...\n'
10.10.2.3: b'Unit ntp.service is enabled and running\nRepeating the final host check...\npodman|docker (/usr/bin/docker) is present\nsystemctl is present\nlvcreate is present\n'
10.10.2.3: b'Unit ntp.service is enabled and running\nHost looks OK\n'
10.10.2.3: b'Cluster fsid: e50a5ad0-36e8-11ec-b561-09a495c6360a\n'
10.10.2.3: b'Verifying IP 10.10.2.3 port 3300 ...\n'
10.10.2.3: b'Verifying IP 10.10.2.3 port 6789 ...\n'
10.10.2.3: b'Mon IP `10.10.2.3` is in CIDR network `10.10.2.0/24`\n- internal network (--cluster-network) has not been provided, OSD replication will default to the public_network\n'
10.10.2.3: b'Adjusting default settings to suit single-host cluster...\n'
10.10.2.3: b'Pulling container image quay.io/ceph/ceph:v16...\n'
10.10.2.3: b'Ceph version: ceph version 16.2.6 (ee28fb57e47e9f88813e24bbf4c14496ca299d31) pacific (stable)\nExtracting ceph user uid/gid from container image...\n'
10.10.2.3: b'Creating initial keys...\n'
10.10.2.3: b'Creating initial monmap...\n'
10.10.2.3: b'Creating mon...\n'
10.10.2.3: b'Waiting for mon to start...\nWaiting for mon...\n'
10.10.2.3: b'mon is available\nSetting mon public_network to 10.10.2.0/24\n'
10.10.2.3: b'Wrote config to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf\nWrote keyring to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring\n'
10.10.2.3: b'Creating mgr...\n'
10.10.2.3: b'Verifying port 9283 ...\n'
10.10.2.3: b'Waiting for mgr to start...\nWaiting for mgr...\n'
10.10.2.3: b'mgr not available, waiting (1/15)...\n'
10.10.2.3: b'mgr not available, waiting (2/15)...\n'
10.10.2.3: b'mgr is available\n'
10.10.2.3: b'Enabling cephadm module...\n'
10.10.2.3: b'Waiting for the mgr to restart...\nWaiting for mgr epoch 5...\n'
10.10.2.3: b'mgr epoch 5 is available\nSetting orchestrator backend to cephadm...\n'
10.10.2.3: b'Generating ssh key...\n'
10.10.2.3: b'Wrote public SSH key to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub\nAdding key to ljishen@localhost authorized_keys...\n'
10.10.2.3: b'Adding host node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us...\n'
10.10.2.3: b'Deploying unmanaged mon service...\n'
10.10.2.3: b'Deploying unmanaged mgr service...\n'
10.10.2.3: b'You can access the Ceph CLI with:\n\n\tsudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --fsid e50a5ad0-36e8-11ec-b561-09a495c6360a -c /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf -k /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring\n\nPlease consider enabling telemetry to help improve Ceph:\n\n\tceph telemetry on\n\nFor more information see:\n\n\thttps://docs.ceph.com/docs/pacific/mgr/telemetry/\n\nBootstrap complete.\n'
10.10.2.3: b'\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T01:45:07,780103347-04:00] INFO: > Set the default number of monitor daemons to 1\x1b[0m\n# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:330 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1\n'
10.10.2.3: b'\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T01:45:07,788557158-04:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.3: b"\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T01:45:07,791381861-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mon', '1']\x1b[0m\n"
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1\n'
10.10.2.3: b'Inferring fsid e50a5ad0-36e8-11ec-b561-09a495c6360a\n'
10.10.2.3: b'Inferring config /var/lib/ceph/e50a5ad0-36e8-11ec-b561-09a495c6360a/mon.node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.3: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.3: b'Scheduled mon update...\n'
10.10.2.3: b'\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T01:45:17,318175150-04:00] INFO: > Set the default number of manager daemons to 1\x1b[0m\n# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:335 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1\n'
10.10.2.3: b'\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T01:45:17,327085540-04:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.3: b"\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T01:45:17,329818650-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mgr', '1']\x1b[0m\n"
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1\n'
10.10.2.3: b'Inferring fsid e50a5ad0-36e8-11ec-b561-09a495c6360a\n'
10.10.2.3: b'Inferring config /var/lib/ceph/e50a5ad0-36e8-11ec-b561-09a495c6360a/mon.node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.3: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.3: b'Scheduled mgr update...\n'
10.10.2.3: b'\n\n\x1b[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T01:45:27,508807710-04:00] STAGE: Add hosts to the cluster...\x1b[0m\n'
10.10.2.3: b"\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T01:45:27,513885558-04:00] INFO: > Adding host 'sm1'\x1b[0m\n"
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:362 - add_hosts() > ssh-copy-id -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -f -i /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub ljishen@sm1\n'
10.10.2.3: b'/usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: "/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub"\n'
10.10.2.3: b'\nNumber of key(s) added: 1\n\nNow try logging into the machine, with:   "ssh -o \'GlobalKnownHostsFile=/dev/null\' -o \'LogLevel=ERROR\' -o \'PasswordAuthentication=no\' -o \'StrictHostKeyChecking=no\' -o \'UserKnownHostsFile=/dev/null\' \'ljishen@sm1\'"\nand check to make sure that only the key(s) you wanted were added.\n\n'
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:368 - add_hosts() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add sm1 10.10.2.5\n'
10.10.2.3: b'\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T01:45:28,647998850-04:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.3: b"\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T01:45:28,651022769-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'host', 'add', 'sm1', '10.10.2.5']\x1b[0m\n# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add sm1 10.10.2.5\n"
10.10.2.3: b'Inferring fsid e50a5ad0-36e8-11ec-b561-09a495c6360a\n'
10.10.2.3: b'Inferring config /var/lib/ceph/e50a5ad0-36e8-11ec-b561-09a495c6360a/mon.node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.3: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.3: b"Added host 'sm1' with addr '10.10.2.5'\n"
10.10.2.3: b'\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T01:45:39,790528831-04:00] INFO: Sleep for 20 seconds\x1b[0m\n'
10.10.2.3: b'\n\n\x1b[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T01:45:59,794868687-04:00] STAGE: Add OSDs to the cluster...\x1b[0m\n'
10.10.2.3: b"\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T01:45:59,800249146-04:00] INFO: > Adding devices ['/dev/nvme0n1'] on host 'sm1' as OSDs\x1b[0m\n"
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:387 - add_host_devices_as_osds() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd sm1:/dev/nvme0n1\n'
10.10.2.3: b'\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T01:45:59,808575856-04:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.3: b"\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T01:45:59,811360463-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'daemon', 'add', 'osd', 'sm1:/dev/nvme0n1']\x1b[0m\n"
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd sm1:/dev/nvme0n1\n'
10.10.2.3: b'Inferring fsid e50a5ad0-36e8-11ec-b561-09a495c6360a\n'
10.10.2.3: b'Inferring config /var/lib/ceph/e50a5ad0-36e8-11ec-b561-09a495c6360a/mon.node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.3: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.3: b"Created osd(s) 0 on host 'sm1'\n"
10.10.2.3: b'\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T01:46:23,430939070-04:00] INFO: > Sleep for 20 seconds\x1b[0m\n'
10.10.2.3: b'\n\n\x1b[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T01:46:43,435519873-04:00] STAGE: Show Cluster Status\x1b[0m\n# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:343 - show_status() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status\n'
10.10.2.3: b'\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T01:46:43,443986638-04:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.3: b"\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T01:46:43,446701073-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', '--status']\x1b[0m\n# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status\n"
10.10.2.3: b'Inferring fsid e50a5ad0-36e8-11ec-b561-09a495c6360a\n'
10.10.2.3: b'Inferring config /var/lib/ceph/e50a5ad0-36e8-11ec-b561-09a495c6360a/mon.node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.3: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.3: b'  cluster:\n    id:     e50a5ad0-36e8-11ec-b561-09a495c6360a\n    health: HEALTH_WARN\n            1 pool(s) have no replicas configured\n \n  services:\n    mon: 1 daemons, quorum node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us (age 2m)\n    mgr: node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us.olihop(active, since 2m)\n    osd: 1 osds: 1 up (since 21s), 1 in (since 40s)\n \n  data:\n    pools:   1 pools, 128 pgs\n    objects: 0 objects, 0 B\n    usage:   4.7 MiB used, 100 GiB / 100 GiB avail\n    pgs:     99.219% pgs unknown\n             127 unknown\n             1   active+clean\n \n  progress:\n \n'
[1] 22:46:52 [SUCCESS] ljishen@10.10.2.3

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T01:43:48,362125485-04:00] INFO: > Remove the existing cluster[0m
## bash:13 -  > ls /var/lib/ceph
## bash:13 -  > tail -n1
# bash:13 -  > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm --host 10.10.2.3 --host 10.10.2.5 rm-cluster --force --fsid dc25683e-36e7-11ec-b561-09a495c6360a

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T01:43:57,414793171-04:00] INFO: > Deploy a new cluster[0m
## bash:22 -  > /tmp/bench-rados/ceph-research/scripts/parallel-ssh --host 10.10.2.5 uname --nodename
## bash:22 -  > tail -n1
# bash:22 -  > osd_hostname=sm1
# bash:23 -  > /tmp/bench-rados/ceph-research/scripts/ceph-deploy -m 10.10.2.3 -o sm1/10.10.2.5:/dev/nvme0n1


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T01:43:58,545714268-04:00] STAGE: Verify passwordless SSH to hosts, passwordless sudo via ljishen, and the existence of devices on hosts...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T01:43:58,549166833-04:00] INFO: > Check the host for the monitor[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:181 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@10.10.2.3 sudo --non-interactive --validate

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T01:43:58,695275126-04:00] INFO: > Check hosts for OSDs[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T01:43:58,699226992-04:00] INFO: >> Check host 'sm1'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:200 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 sudo --non-interactive --validate

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T01:43:59,763954425-04:00] INFO: >>> Check device '/dev/nvme0n1'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:215 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 test -b /dev/nvme0n1
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:226 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 lsblk --nodeps --noheadings --output TYPE /dev/nvme0n1
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:227 - verify_parameters() > grep --word-regexp disk


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T01:44:01,888911246-04:00] STAGE: Reset devices on hosts...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T01:44:01,892777721-04:00] INFO: > Resetting device '/dev/nvme0n1' on host 'sm1'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:288 - reset_host_devices() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 sudo sh
  Removing ceph--96467807--d21e--4abd--b561--9f6d0f91534e-osd--block--aa0a35b7--ead4--401b--94e5--82a008ff307d (252:0)
  Archiving volume group "ceph-96467807-d21e-4abd-b561-9f6d0f91534e" metadata (seqno 5).
  Releasing logical volume "osd-block-aa0a35b7-ead4-401b-94e5-82a008ff307d"
  Creating volume group backup "/etc/lvm/backup/ceph-96467807-d21e-4abd-b561-9f6d0f91534e" (seqno 6).
  Logical volume "osd-block-aa0a35b7-ead4-401b-94e5-82a008ff307d" successfully removed
  Removing physical volume "/dev/nvme0n1" from volume group "ceph-96467807-d21e-4abd-b561-9f6d0f91534e"
  Volume group "ceph-96467807-d21e-4abd-b561-9f6d0f91534e" successfully removed


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T01:44:04,222304788-04:00] STAGE: Bootstrap a new cluster...[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:312 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.3 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T01:44:04,230619977-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T01:44:04,233537705-04:00] DEBUG: CEPHADM_PARAMS: ['bootstrap', '--config', '/tmp/bench-rados/ceph-research/scripts/ceph.conf', '--mon-ip', '10.10.2.3', '--output-dir', '/tmp/bench-rados/deployment_data_root/etc/ceph', '--ssh-user', 'ljishen', '--skip-dashboard', '--no-minimize-config', '--allow-overwrite', '--allow-fqdn-hostname', '--orphan-initial-daemons', '--skip-monitoring-stack', '--single-host-defaults'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.3 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults
Verifying podman|docker is present...
Verifying lvm2 is present...
Verifying time synchronization is in place...
Unit ntp.service is enabled and running
Repeating the final host check...
podman|docker (/usr/bin/docker) is present
systemctl is present
lvcreate is present
Unit ntp.service is enabled and running
Host looks OK
Cluster fsid: e50a5ad0-36e8-11ec-b561-09a495c6360a
Verifying IP 10.10.2.3 port 3300 ...
Verifying IP 10.10.2.3 port 6789 ...
Mon IP `10.10.2.3` is in CIDR network `10.10.2.0/24`
- internal network (--cluster-network) has not been provided, OSD replication will default to the public_network
Adjusting default settings to suit single-host cluster...
Pulling container image quay.io/ceph/ceph:v16...
Ceph version: ceph version 16.2.6 (ee28fb57e47e9f88813e24bbf4c14496ca299d31) pacific (stable)
Extracting ceph user uid/gid from container image...
Creating initial keys...
Creating initial monmap...
Creating mon...
Waiting for mon to start...
Waiting for mon...
mon is available
Setting mon public_network to 10.10.2.0/24
Wrote config to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf
Wrote keyring to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring
Creating mgr...
Verifying port 9283 ...
Waiting for mgr to start...
Waiting for mgr...
mgr not available, waiting (1/15)...
mgr not available, waiting (2/15)...
mgr is available
Enabling cephadm module...
Waiting for the mgr to restart...
Waiting for mgr epoch 5...
mgr epoch 5 is available
Setting orchestrator backend to cephadm...
Generating ssh key...
Wrote public SSH key to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub
Adding key to ljishen@localhost authorized_keys...
Adding host node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us...
Deploying unmanaged mon service...
Deploying unmanaged mgr service...
You can access the Ceph CLI with:

	sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --fsid e50a5ad0-36e8-11ec-b561-09a495c6360a -c /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf -k /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring

Please consider enabling telemetry to help improve Ceph:

	ceph telemetry on

For more information see:

	https://docs.ceph.com/docs/pacific/mgr/telemetry/

Bootstrap complete.

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T01:45:07,780103347-04:00] INFO: > Set the default number of monitor daemons to 1[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:330 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T01:45:07,788557158-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T01:45:07,791381861-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mon', '1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1
Inferring fsid e50a5ad0-36e8-11ec-b561-09a495c6360a
Inferring config /var/lib/ceph/e50a5ad0-36e8-11ec-b561-09a495c6360a/mon.node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Scheduled mon update...

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T01:45:17,318175150-04:00] INFO: > Set the default number of manager daemons to 1[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:335 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T01:45:17,327085540-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T01:45:17,329818650-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mgr', '1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1
Inferring fsid e50a5ad0-36e8-11ec-b561-09a495c6360a
Inferring config /var/lib/ceph/e50a5ad0-36e8-11ec-b561-09a495c6360a/mon.node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Scheduled mgr update...


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T01:45:27,508807710-04:00] STAGE: Add hosts to the cluster...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T01:45:27,513885558-04:00] INFO: > Adding host 'sm1'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:362 - add_hosts() > ssh-copy-id -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -f -i /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub ljishen@sm1
/usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: "/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub"

Number of key(s) added: 1

Now try logging into the machine, with:   "ssh -o 'GlobalKnownHostsFile=/dev/null' -o 'LogLevel=ERROR' -o 'PasswordAuthentication=no' -o 'StrictHostKeyChecking=no' -o 'UserKnownHostsFile=/dev/null' 'ljishen@sm1'"
and check to make sure that only the key(s) you wanted were added.

# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:368 - add_hosts() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add sm1 10.10.2.5
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T01:45:28,647998850-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T01:45:28,651022769-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'host', 'add', 'sm1', '10.10.2.5'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add sm1 10.10.2.5
Inferring fsid e50a5ad0-36e8-11ec-b561-09a495c6360a
Inferring config /var/lib/ceph/e50a5ad0-36e8-11ec-b561-09a495c6360a/mon.node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Added host 'sm1' with addr '10.10.2.5'

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T01:45:39,790528831-04:00] INFO: Sleep for 20 seconds[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T01:45:59,794868687-04:00] STAGE: Add OSDs to the cluster...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T01:45:59,800249146-04:00] INFO: > Adding devices ['/dev/nvme0n1'] on host 'sm1' as OSDs[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:387 - add_host_devices_as_osds() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd sm1:/dev/nvme0n1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T01:45:59,808575856-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T01:45:59,811360463-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'daemon', 'add', 'osd', 'sm1:/dev/nvme0n1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd sm1:/dev/nvme0n1
Inferring fsid e50a5ad0-36e8-11ec-b561-09a495c6360a
Inferring config /var/lib/ceph/e50a5ad0-36e8-11ec-b561-09a495c6360a/mon.node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Created osd(s) 0 on host 'sm1'

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T01:46:23,430939070-04:00] INFO: > Sleep for 20 seconds[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T01:46:43,435519873-04:00] STAGE: Show Cluster Status[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:343 - show_status() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T01:46:43,443986638-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T01:46:43,446701073-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', '--status'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status
Inferring fsid e50a5ad0-36e8-11ec-b561-09a495c6360a
Inferring config /var/lib/ceph/e50a5ad0-36e8-11ec-b561-09a495c6360a/mon.node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
  cluster:
    id:     e50a5ad0-36e8-11ec-b561-09a495c6360a
    health: HEALTH_WARN
            1 pool(s) have no replicas configured
 
  services:
    mon: 1 daemons, quorum node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us (age 2m)
    mgr: node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us.olihop(active, since 2m)
    osd: 1 osds: 1 up (since 21s), 1 in (since 40s)
 
  data:
    pools:   1 pools, 128 pgs
    objects: 0 objects, 0 B
    usage:   4.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     99.219% pgs unknown
             127 unknown
             1   active+clean
 
  progress:
 

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:46:52,866592602-07:00] INFO: > Copy configuration files to local[0m
# ./benchmarks/bench-rados:208 - launch_ceph_cluster() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.3:/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf
# ./benchmarks/bench-rados:212 - launch_ceph_cluster() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.3 sudo chmod 644 /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:46:53,023640918-07:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.3 sudo chmod 644 /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring
[1] 22:46:53 [SUCCESS] ljishen@10.10.2.3
# ./benchmarks/bench-rados:214 - launch_ceph_cluster() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.3:/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:46:53,358838582-07:00] STAGE: Update local ceph.conf as a client...[0m


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:46:53,362977862-07:00] STAGE: Configure Ceph pool...[0m
# ./benchmarks/bench-rados:245 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_max_backfills 32
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:46:53,389042467-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:46:53,392376423-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'e50a5ad0-36e8-11ec-b561-09a495c6360a', '--', 'ceph', 'config', 'set', 'osd', 'osd_max_backfills', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid e50a5ad0-36e8-11ec-b561-09a495c6360a -- ceph config set osd osd_max_backfills 32
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:246 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_active 32
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:46:56,663637051-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:46:56,666953014-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'e50a5ad0-36e8-11ec-b561-09a495c6360a', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_active', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid e50a5ad0-36e8-11ec-b561-09a495c6360a -- ceph config set osd osd_recovery_max_active 32
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:247 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_single_start 8
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:47:00,031615069-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:47:00,034981898-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'e50a5ad0-36e8-11ec-b561-09a495c6360a', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_single_start', '8'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid e50a5ad0-36e8-11ec-b561-09a495c6360a -- ceph config set osd osd_recovery_max_single_start 8
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:248 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_op_priority 63
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:47:03,173404128-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:47:03,177406150-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'e50a5ad0-36e8-11ec-b561-09a495c6360a', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_op_priority', '63'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid e50a5ad0-36e8-11ec-b561-09a495c6360a -- ceph config set osd osd_recovery_op_priority 63
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:250 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config show-with-defaults osd.0
# ./benchmarks/bench-rados:251 - configure_ceph_pool() > grep 'osd_max_backfills\|osd_recovery'
# ./benchmarks/bench-rados:251 - configure_ceph_pool() > column -t -s ' '
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
osd_max_backfills                        32         mon
osd_recovery_cost                        20971520   default
osd_recovery_delay_start                 0.000000   default
osd_recovery_max_active                  32         mon
osd_recovery_max_active_hdd              3          default
osd_recovery_max_active_ssd              10         default
osd_recovery_max_chunk                   8388608    default
osd_recovery_max_omap_entries_per_chunk  8096       default
osd_recovery_max_single_start            8          mon
osd_recovery_op_priority                 63         mon
osd_recovery_op_warn_multiple            16         default
osd_recovery_priority                    5          default
osd_recovery_retry_interval              30.000000  default
osd_recovery_sleep                       0.000000   default
osd_recovery_sleep_hdd                   0.100000   default
osd_recovery_sleep_hybrid                0.025000   default
osd_recovery_sleep_ssd                   0.000000   default
# ./benchmarks/bench-rados:253 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool create bench_rados 128 128 replicated --size 1 --pg-num-min 128
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:47:09,766003586-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:47:09,769729559-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'e50a5ad0-36e8-11ec-b561-09a495c6360a', '--', 'ceph', 'osd', 'pool', 'create', 'bench_rados', '128', '128', 'replicated', '--size', '1', '--pg-num-min', '128'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid e50a5ad0-36e8-11ec-b561-09a495c6360a -- ceph osd pool create bench_rados 128 128 replicated --size 1 --pg-num-min 128
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
pool 'bench_rados' created
# ./benchmarks/bench-rados:255 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados min_size 1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:47:13,582847682-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:47:13,586593112-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'e50a5ad0-36e8-11ec-b561-09a495c6360a', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'min_size', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid e50a5ad0-36e8-11ec-b561-09a495c6360a -- ceph osd pool set bench_rados min_size 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 min_size to 1
# ./benchmarks/bench-rados:256 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados pg_autoscale_mode off
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:47:17,782546553-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:47:17,786020223-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'e50a5ad0-36e8-11ec-b561-09a495c6360a', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'pg_autoscale_mode', 'off'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid e50a5ad0-36e8-11ec-b561-09a495c6360a -- ceph osd pool set bench_rados pg_autoscale_mode off
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 pg_autoscale_mode to off
# ./benchmarks/bench-rados:257 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool application enable bench_rados benchmark
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:47:22,125824542-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:47:22,129735493-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'e50a5ad0-36e8-11ec-b561-09a495c6360a', '--', 'ceph', 'osd', 'pool', 'application', 'enable', 'bench_rados', 'benchmark'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid e50a5ad0-36e8-11ec-b561-09a495c6360a -- ceph osd pool application enable bench_rados benchmark
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
enabled application 'benchmark' on pool 'bench_rados'
# ./benchmarks/bench-rados:258 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados noscrub 1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:47:26,541299941-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:47:26,544265566-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'e50a5ad0-36e8-11ec-b561-09a495c6360a', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'noscrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid e50a5ad0-36e8-11ec-b561-09a495c6360a -- ceph osd pool set bench_rados noscrub 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 noscrub to 1
# ./benchmarks/bench-rados:259 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados nodeep-scrub 1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:47:31,052297825-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:47:31,055780502-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'e50a5ad0-36e8-11ec-b561-09a495c6360a', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'nodeep-scrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid e50a5ad0-36e8-11ec-b561-09a495c6360a -- ceph osd pool set bench_rados nodeep-scrub 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 nodeep-scrub to 1
# ./benchmarks/bench-rados:260 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool ls detail
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:47:35,259408219-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:47:35,262620497-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'e50a5ad0-36e8-11ec-b561-09a495c6360a', '--', 'ceph', 'osd', 'pool', 'ls', 'detail'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid e50a5ad0-36e8-11ec-b561-09a495c6360a -- ceph osd pool ls detail
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
pool 1 'device_health_metrics' replicated size 1 min_size 1 crush_rule 0 object_hash rjenkins pg_num 128 pgp_num 128 autoscale_mode on last_change 13 lfor 0/0/11 flags hashpspool stripe_width 0 pg_num_min 1 application mgr_devicehealth
pool 2 'bench_rados' replicated size 1 min_size 1 crush_rule 0 object_hash rjenkins pg_num 128 pgp_num 128 autoscale_mode off last_change 21 flags hashpspool,noscrub,nodeep-scrub stripe_width 0 pg_num_min 128 application benchmark

# ./benchmarks/bench-rados:261 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd df tree
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:47:38,667980172-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:47:38,671296676-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'e50a5ad0-36e8-11ec-b561-09a495c6360a', '--', 'ceph', 'osd', 'df', 'tree'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid e50a5ad0-36e8-11ec-b561-09a495c6360a -- ceph osd df tree
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
ID  CLASS  WEIGHT   REWEIGHT  SIZE     RAW USE  DATA     OMAP  META     AVAIL    %USE  VAR   PGS  STATUS  TYPE NAME    
-1         0.09769         -  100 GiB  6.0 MiB  176 KiB   0 B  5.8 MiB  100 GiB  0.01  1.00    -          root default 
-3         0.09769         -  100 GiB  6.0 MiB  176 KiB   0 B  5.8 MiB  100 GiB  0.01  1.00    -              host sm1 
 0    ssd  0.09769   1.00000  100 GiB  6.0 MiB  176 KiB   0 B  5.8 MiB  100 GiB  0.01  1.00  256      up          osd.0
                       TOTAL  100 GiB  6.0 MiB  176 KiB   0 B  5.8 MiB  100 GiB  0.01                                  
MIN/MAX VAR: 1.00/1.00  STDDEV: 0


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:47:42,018434081-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:48:05,168288014-07:00] INFO: >>   data:
    pools:   2 pools, 256 pgs
    objects: 0 objects, 0 B
    usage:   6.0 MiB used, 100 GiB / 100 GiB avail
    pgs:     256 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:48:05,176838707-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:48:13,458652429-07:00] INFO: >>   data:
    pools:   2 pools, 256 pgs
    objects: 0 objects, 0 B
    usage:   6.0 MiB used, 100 GiB / 100 GiB avail
    pgs:     256 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:48:13,466698594-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:48:21,751323971-07:00] INFO: >>   data:
    pools:   2 pools, 256 pgs
    objects: 0 objects, 0 B
    usage:   6.0 MiB used, 100 GiB / 100 GiB avail
    pgs:     256 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:48:21,758877450-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:48:29,997165195-07:00] INFO: >>   data:
    pools:   2 pools, 256 pgs
    objects: 0 objects, 0 B
    usage:   6.0 MiB used, 100 GiB / 100 GiB avail
    pgs:     256 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:48:30,005307971-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:48:38,246510325-07:00] INFO: >>   data:
    pools:   2 pools, 256 pgs
    objects: 0 objects, 0 B
    usage:   6.0 MiB used, 100 GiB / 100 GiB avail
    pgs:     256 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:48:38,254250504-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:48:38,260418378-07:00] INFO: > The cluster is idle now.[0m


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:48:38,264833026-07:00] STAGE: Start benchmarking of mode write ...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:48:38,272296786-07:00] INFO: > Start system activity collection services[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:48:38,278728316-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:329 - rados_bench() > rm --force /mnt/sda8/ceph-research/scripts/output/sys_activity_256KB_write.dat.rados_bench_host.3
# ./benchmarks/bench-rados:330 - rados_bench() > sar_pid_rados_bench_host=837126
# ./benchmarks/bench-rados:330 - rados_bench() > S_TIME_FORMAT=ISO
# ./benchmarks/bench-rados:330 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/output/sys_activity_256KB_write.dat.rados_bench_host.3 2

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:48:38,286251637-07:00] INFO: >> for OSD host[0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:48:38,294297631-07:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.5 bash
10.10.2.5: b'91953\n'
[1] 22:48:39 [SUCCESS] ljishen@10.10.2.5
91953

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:48:39,400870914-07:00] INFO: > Run rados bench[0m
# ./benchmarks/bench-rados:358 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/output/rados_bench_256KB_write.log.3
## ./benchmarks/bench-rados:349 - rados_bench() > count_cpus 32-63,96-127
# ./benchmarks/bench-rados:349 - rados_bench() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 30 write --pool bench_rados -b 262144 -O 262144 --concurrent-ios 128 --show-time --write-object --write-omap --write-xattr --no-cleanup
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:48:39,422452271-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:48:39,426009147-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'e50a5ad0-36e8-11ec-b561-09a495c6360a', '--', 'rados', 'bench', '30', 'write', '--pool', 'bench_rados', '-b', '262144', '-O', '262144', '--concurrent-ios', '128', '--show-time', '--write-object', '--write-omap', '--write-xattr', '--no-cleanup'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid e50a5ad0-36e8-11ec-b561-09a495c6360a -- rados bench 30 write --pool bench_rados -b 262144 -O 262144 --concurrent-ios 128 --show-time --write-object --write-omap --write-xattr --no-cleanup
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
hints = 1
2021-10-27T05:48:41.918375+0000 Maintaining 128 concurrent writes of 262144 bytes to objects of size 262144 for up to 30 seconds or 0 objects
2021-10-27T05:48:41.918385+0000 Object prefix: benchmark_data_node-0.bluefield2.ucsc-cmps10_7
2021-10-27T05:48:41.926254+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-27T05:48:41.926254+0000     0       0         0         0         0         0           -           0
2021-10-27T05:48:42.926371+0000     1     128       316       188   46.9964        47    0.658509    0.446055
2021-10-27T05:48:43.926445+0000     2     128       634       506   63.2452      79.5     0.29715    0.456064
2021-10-27T05:48:44.926617+0000     3     128       902       774    64.493        67     0.33317    0.457925
2021-10-27T05:48:45.926727+0000     4     128      1157      1029   64.3055     63.75    0.470359    0.461347
2021-10-27T05:48:46.926985+0000     5     128      1428      1300    64.991     67.75    0.470561    0.466513
2021-10-27T05:48:47.927053+0000     6     128      1679      1551   64.6168     62.75    0.381494     0.46349
2021-10-27T05:48:48.927160+0000     7     128      1934      1806    64.492     63.75    0.422067    0.466734
2021-10-27T05:48:49.927287+0000     8     128      2208      2080   64.9919      68.5    0.616482    0.471792
2021-10-27T05:48:50.927546+0000     9     128      2304      2176    60.436        24    0.870151    0.488893
2021-10-27T05:48:51.927690+0000    10     128      2493      2365   59.1167     47.25    0.849661    0.520306
2021-10-27T05:48:52.927798+0000    11     128      2592      2464   55.9923     24.75    0.813931    0.538394
2021-10-27T05:48:53.927891+0000    12     128      2790      2662   55.4509      49.5    0.862379    0.558353
2021-10-27T05:48:54.927961+0000    13     128      2953      2825     54.32     40.75    0.682706    0.574284
2021-10-27T05:48:55.928029+0000    14     128      3053      2925   52.2257        25    0.786281    0.586237
2021-10-27T05:48:56.928137+0000    15     128      3213      3085   51.4103        40    0.738156    0.603623
2021-10-27T05:48:57.928230+0000    16     128      3483      3355   52.4155      67.5    0.466209    0.600737
2021-10-27T05:48:58.928299+0000    17     128      3757      3629   53.3614      68.5    0.448328    0.591103
2021-10-27T05:48:59.928378+0000    18     128      4028      3900   54.1604     67.75    0.479971    0.582724
2021-10-27T05:49:00.928487+0000    19     128      4225      4097   53.9017     49.25    0.540977    0.577452
2021-10-27T05:49:01.928690+0000 min lat: 0.0263477 max lat: 1.39543 avg lat: 0.583369
2021-10-27T05:49:01.928690+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-27T05:49:01.928690+0000    20     128      4368      4240   52.9936     35.75    0.535472    0.583369
2021-10-27T05:49:02.928920+0000    21     128      4459      4331   51.5531     22.75    0.948152    0.591432
2021-10-27T05:49:03.928989+0000    22     128      4569      4441   50.4597      27.5     1.27295    0.613705
2021-10-27T05:49:04.929056+0000    23     128      4675      4547    49.418      26.5     1.19505    0.633164
2021-10-27T05:49:05.929298+0000    24     128      4773      4645   48.3794      24.5     1.16997    0.649446
2021-10-27T05:49:06.929531+0000    25     128      4784      4656    46.554      2.75     1.26809     0.65083
2021-10-27T05:49:07.929618+0000    26     128      4927      4799   46.1383     35.75    0.491618    0.673687
2021-10-27T05:49:08.929690+0000    27     128      5055      4927   45.6146        32     1.28584     0.68921
2021-10-27T05:49:09.929782+0000    28     128      5149      5021   44.8248      23.5     1.11147    0.699521
2021-10-27T05:49:10.929889+0000    29     128      5202      5074    43.736     13.25     1.15425    0.707264
2021-10-27T05:49:11.929958+0000    30     128      5356      5228   43.5613      38.5     0.68207    0.723802
2021-10-27T05:49:12.930061+0000 Total time run:         30.6048
Total writes made:      5356
Write size:             262144
Object size:            262144
Bandwidth (MB/sec):     43.7513
Stddev Bandwidth:       20.1885
Max bandwidth (MB/sec): 79.5
Min bandwidth (MB/sec): 2.75
Average IOPS:           175
Stddev IOPS:            80.7542
Max IOPS:               318
Min IOPS:               11
Average Latency(s):     0.726335
Stddev Latency(s):      0.390607
Max latency(s):         2.60017
Min latency(s):         0.0263477

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:49:13,492034305-07:00] INFO: > Stop system activity collection services[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:49:13,498197991-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:379 - rados_bench() > kill -INT 837126

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:49:13,504039321-07:00] INFO: >> for OSD host[0m
# ./benchmarks/bench-rados:384 - rados_bench() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.5 kill -INT 91953
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:49:13,512676396-07:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.5 kill -INT 91953
[1] 22:49:14 [SUCCESS] ljishen@10.10.2.5

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:49:14,859510533-07:00] INFO: >>> Copy system activity file from OSD host[0m
## ./benchmarks/bench-rados:391 - rados_bench() > basename -- /tmp/bench-rados/sys_activity_256KB_write.dat.osd_host.3
# ./benchmarks/bench-rados:391 - rados_bench() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.5:/tmp/bench-rados/sys_activity_256KB_write.dat.osd_host.3 /mnt/sda8/ceph-research/scripts/output/sys_activity_256KB_write.dat.osd_host.3


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:49:16,082848343-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:49:39,386242349-07:00] INFO: >>   data:
    pools:   2 pools, 256 pgs
    objects: 5.36k objects, 1.3 GiB
    usage:   3.9 GiB used, 96 GiB / 100 GiB avail
    pgs:     256 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:49:39,395064112-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:49:47,701500220-07:00] INFO: >>   data:
    pools:   2 pools, 256 pgs
    objects: 5.36k objects, 1.3 GiB
    usage:   3.9 GiB used, 96 GiB / 100 GiB avail
    pgs:     256 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:49:47,709636795-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:49:56,174594229-07:00] INFO: >>   data:
    pools:   2 pools, 256 pgs
    objects: 5.36k objects, 1.3 GiB
    usage:   3.9 GiB used, 96 GiB / 100 GiB avail
    pgs:     256 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:49:56,182790495-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:50:04,764148068-07:00] INFO: >>   data:
    pools:   2 pools, 256 pgs
    objects: 5.36k objects, 1.3 GiB
    usage:   3.9 GiB used, 96 GiB / 100 GiB avail
    pgs:     256 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:50:04,772071882-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:50:12,976674976-07:00] INFO: >>   data:
    pools:   2 pools, 256 pgs
    objects: 5.36k objects, 1.3 GiB
    usage:   3.9 GiB used, 96 GiB / 100 GiB avail
    pgs:     256 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:50:12,984675604-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:50:12,991491456-07:00] INFO: > The cluster is idle now.[0m


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:50:12,995206008-07:00] STAGE: Start benchmarking of mode seq ...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:50:13,002738247-07:00] INFO: > Start system activity collection services[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:50:13,009776787-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:329 - rados_bench() > rm --force /mnt/sda8/ceph-research/scripts/output/sys_activity_256KB_seq.dat.rados_bench_host.3
# ./benchmarks/bench-rados:330 - rados_bench() > sar_pid_rados_bench_host=838973
# ./benchmarks/bench-rados:330 - rados_bench() > S_TIME_FORMAT=ISO
# ./benchmarks/bench-rados:330 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/output/sys_activity_256KB_seq.dat.rados_bench_host.3 2

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:50:13,017563424-07:00] INFO: >> for OSD host[0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:50:13,026002898-07:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.5 bash
10.10.2.5: b'92481\n'
[1] 22:50:14 [SUCCESS] ljishen@10.10.2.5
92481

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:50:14,129612244-07:00] INFO: > Run rados bench[0m
# ./benchmarks/bench-rados:367 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/output/rados_bench_256KB_seq.log.3
## ./benchmarks/bench-rados:364 - rados_bench() > count_cpus 32-63,96-127
# ./benchmarks/bench-rados:364 - rados_bench() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 128 --show-time
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:50:14,151356517-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:50:14,154365723-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'e50a5ad0-36e8-11ec-b561-09a495c6360a', '--', 'rados', 'bench', '99999999', 'seq', '--pool', 'bench_rados', '--concurrent-ios', '128', '--show-time'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid e50a5ad0-36e8-11ec-b561-09a495c6360a -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 128 --show-time
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
hints = 1
2021-10-27T05:50:16.772762+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-27T05:50:16.772762+0000     0       2         2         0         0         0           -           0
2021-10-27T05:50:17.772885+0000     1     128      3508      3380   844.838       845   0.0334241   0.0370692
2021-10-27T05:50:18.773071+0000 Total time run:       1.59333
Total reads made:     5356
Read size:            262144
Object size:          262144
Bandwidth (MB/sec):   840.379
Average IOPS:         3361
Stddev IOPS:          0
Max IOPS:             3380
Min IOPS:             3380
Average Latency(s):   0.0376527
Max latency(s):       0.0645113
Min latency(s):       0.0040638

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:50:19,381913627-07:00] INFO: > Stop system activity collection services[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:50:19,388355626-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:379 - rados_bench() > kill -INT 838973

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:50:19,394454601-07:00] INFO: >> for OSD host[0m
# ./benchmarks/bench-rados:384 - rados_bench() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.5 kill -INT 92481
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:50:19,402512647-07:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.5 kill -INT 92481
[1] 22:50:20 [SUCCESS] ljishen@10.10.2.5

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:50:20,502905257-07:00] INFO: >>> Copy system activity file from OSD host[0m
## ./benchmarks/bench-rados:391 - rados_bench() > basename -- /tmp/bench-rados/sys_activity_256KB_seq.dat.osd_host.3
# ./benchmarks/bench-rados:391 - rados_bench() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.5:/tmp/bench-rados/sys_activity_256KB_seq.dat.osd_host.3 /mnt/sda8/ceph-research/scripts/output/sys_activity_256KB_seq.dat.osd_host.3


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:50:21,569154055-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:50:44,761134634-07:00] INFO: >>   data:
    pools:   2 pools, 256 pgs
    objects: 5.36k objects, 1.3 GiB
    usage:   3.9 GiB used, 96 GiB / 100 GiB avail
    pgs:     256 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:50:44,769038381-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:50:53,049652706-07:00] INFO: >>   data:
    pools:   2 pools, 256 pgs
    objects: 5.36k objects, 1.3 GiB
    usage:   3.9 GiB used, 96 GiB / 100 GiB avail
    pgs:     256 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:50:53,057473617-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:51:01,344895197-07:00] INFO: >>   data:
    pools:   2 pools, 256 pgs
    objects: 5.36k objects, 1.3 GiB
    usage:   3.9 GiB used, 96 GiB / 100 GiB avail
    pgs:     256 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:51:01,353556959-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:51:09,653151732-07:00] INFO: >>   data:
    pools:   2 pools, 256 pgs
    objects: 5.36k objects, 1.3 GiB
    usage:   3.9 GiB used, 96 GiB / 100 GiB avail
    pgs:     256 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:51:09,661540280-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:51:17,940449130-07:00] INFO: >>   data:
    pools:   2 pools, 256 pgs
    objects: 5.36k objects, 1.3 GiB
    usage:   3.9 GiB used, 96 GiB / 100 GiB avail
    pgs:     256 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:51:17,949227551-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:51:17,956178407-07:00] INFO: > The cluster is idle now.[0m


[1;7;39;49m[2021-10-26T22:51:17,959830512-07:00][RUNNING][ROUND 1/5/21] object_size=1MB[0m


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:51:17,963115236-07:00] STAGE: Launch a Ceph cluster (CEPH_HOST: 10.10.2.3, OSD_HOST: 10.10.2.5) ...[0m
# ./benchmarks/bench-rados:172 - launch_ceph_cluster() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.3 bash -euo pipefail
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:51:17,970945324-07:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.3 bash -euo pipefail
10.10.2.3: b'\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T01:51:18,407691480-04:00] INFO: > Remove the existing cluster\x1b[0m\n'
10.10.2.3: b'## bash:13 -  > ls /var/lib/ceph\n## bash:13 -  > tail -n1\n'
10.10.2.3: b'# bash:13 -  > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm --host 10.10.2.3 --host 10.10.2.5 rm-cluster --force --fsid e50a5ad0-36e8-11ec-b561-09a495c6360a\n'
10.10.2.3: b'\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T01:51:27,445802407-04:00] INFO: > Deploy a new cluster\x1b[0m\n'
10.10.2.3: b'## bash:22 -  > /tmp/bench-rados/ceph-research/scripts/parallel-ssh --host 10.10.2.5 uname --nodename\n## bash:22 -  > tail -n1\n'
10.10.2.3: b'# bash:22 -  > osd_hostname=sm1\n# bash:23 -  > /tmp/bench-rados/ceph-research/scripts/ceph-deploy -m 10.10.2.3 -o sm1/10.10.2.5:/dev/nvme0n1\n'
10.10.2.3: b'\n\n\x1b[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T01:51:28,572952910-04:00] STAGE: Verify passwordless SSH to hosts, passwordless sudo via ljishen, and the existence of devices on hosts...\x1b[0m\n'
10.10.2.3: b'\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T01:51:28,576935865-04:00] INFO: > Check the host for the monitor\x1b[0m\n'
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:181 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@10.10.2.3 sudo --non-interactive --validate\n'
10.10.2.3: b'\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T01:51:28,728558159-04:00] INFO: > Check hosts for OSDs\x1b[0m\n'
10.10.2.3: b"\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T01:51:28,732927392-04:00] INFO: >> Check host 'sm1'\x1b[0m\n# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:200 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 sudo --non-interactive --validate\n"
10.10.2.3: b"\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T01:51:29,775916704-04:00] INFO: >>> Check device '/dev/nvme0n1'\x1b[0m\n"
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:215 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 test -b /dev/nvme0n1\n'
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:226 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 lsblk --nodeps --noheadings --output TYPE /dev/nvme0n1\n# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:227 - verify_parameters() > grep --word-regexp disk\n'
10.10.2.3: b'\n\n\x1b[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T01:51:31,909375109-04:00] STAGE: Reset devices on hosts...\x1b[0m\n'
10.10.2.3: b"\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T01:51:31,913450609-04:00] INFO: > Resetting device '/dev/nvme0n1' on host 'sm1'\x1b[0m\n# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:288 - reset_host_devices() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 sudo sh\n"
10.10.2.3: b'  Removing ceph--72a1e5f4--db66--404f--bc1f--c8161b7b3c42-osd--block--bb4bebb8--ee60--4069--9d40--9cb4d66a851f (252:0)\n'
10.10.2.3: b'  Archiving volume group "ceph-72a1e5f4-db66-404f-bc1f-c8161b7b3c42" metadata (seqno 5).\n'
10.10.2.3: b'  Releasing logical volume "osd-block-bb4bebb8-ee60-4069-9d40-9cb4d66a851f"\n'
10.10.2.3: b'  Creating volume group backup "/etc/lvm/backup/ceph-72a1e5f4-db66-404f-bc1f-c8161b7b3c42" (seqno 6).\n'
10.10.2.3: b'  Logical volume "osd-block-bb4bebb8-ee60-4069-9d40-9cb4d66a851f" successfully removed\n'
10.10.2.3: b'  Removing physical volume "/dev/nvme0n1" from volume group "ceph-72a1e5f4-db66-404f-bc1f-c8161b7b3c42"\n'
10.10.2.3: b'  Volume group "ceph-72a1e5f4-db66-404f-bc1f-c8161b7b3c42" successfully removed\n'
10.10.2.3: b'\n\n\x1b[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T01:51:34,232825610-04:00] STAGE: Bootstrap a new cluster...\x1b[0m\n# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:312 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.3 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults\n'
10.10.2.3: b'\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T01:51:34,241609562-04:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.3: b"\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T01:51:34,244770558-04:00] DEBUG: CEPHADM_PARAMS: ['bootstrap', '--config', '/tmp/bench-rados/ceph-research/scripts/ceph.conf', '--mon-ip', '10.10.2.3', '--output-dir', '/tmp/bench-rados/deployment_data_root/etc/ceph', '--ssh-user', 'ljishen', '--skip-dashboard', '--no-minimize-config', '--allow-overwrite', '--allow-fqdn-hostname', '--orphan-initial-daemons', '--skip-monitoring-stack', '--single-host-defaults']\x1b[0m\n"
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.3 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults\n'
10.10.2.3: b'Verifying podman|docker is present...\nVerifying lvm2 is present...\nVerifying time synchronization is in place...\n'
10.10.2.3: b'Unit ntp.service is enabled and running\nRepeating the final host check...\npodman|docker (/usr/bin/docker) is present\n'
10.10.2.3: b'systemctl is present\nlvcreate is present\n'
10.10.2.3: b'Unit ntp.service is enabled and running\nHost looks OK\n'
10.10.2.3: b'Cluster fsid: f1454c32-36e9-11ec-b561-09a495c6360a\n'
10.10.2.3: b'Verifying IP 10.10.2.3 port 3300 ...\n'
10.10.2.3: b'Verifying IP 10.10.2.3 port 6789 ...\n'
10.10.2.3: b'Mon IP `10.10.2.3` is in CIDR network `10.10.2.0/24`\n- internal network (--cluster-network) has not been provided, OSD replication will default to the public_network\n'
10.10.2.3: b'Adjusting default settings to suit single-host cluster...\n'
10.10.2.3: b'Pulling container image quay.io/ceph/ceph:v16...\n'
10.10.2.3: b'Ceph version: ceph version 16.2.6 (ee28fb57e47e9f88813e24bbf4c14496ca299d31) pacific (stable)\nExtracting ceph user uid/gid from container image...\n'
10.10.2.3: b'Creating initial keys...\n'
10.10.2.3: b'Creating initial monmap...\n'
10.10.2.3: b'Creating mon...\n'
10.10.2.3: b'Waiting for mon to start...\nWaiting for mon...\n'
10.10.2.3: b'mon is available\nSetting mon public_network to 10.10.2.0/24\n'
10.10.2.3: b'Wrote config to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf\n'
10.10.2.3: b'Wrote keyring to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring\n'
10.10.2.3: b'Creating mgr...\n'
10.10.2.3: b'Verifying port 9283 ...\n'
10.10.2.3: b'Waiting for mgr to start...\nWaiting for mgr...\n'
10.10.2.3: b'mgr not available, waiting (1/15)...\n'
10.10.2.3: b'mgr not available, waiting (2/15)...\n'
10.10.2.3: b'mgr is available\n'
10.10.2.3: b'Enabling cephadm module...\n'
10.10.2.3: b'Waiting for the mgr to restart...\nWaiting for mgr epoch 5...\n'
10.10.2.3: b'mgr epoch 5 is available\nSetting orchestrator backend to cephadm...\n'
10.10.2.3: b'Generating ssh key...\n'
10.10.2.3: b'Wrote public SSH key to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub\nAdding key to ljishen@localhost authorized_keys...\n'
10.10.2.3: b'Adding host node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us...\n'
10.10.2.3: b'Deploying unmanaged mon service...\n'
10.10.2.3: b'Deploying unmanaged mgr service...\n'
10.10.2.3: b'You can access the Ceph CLI with:\n\n\tsudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --fsid f1454c32-36e9-11ec-b561-09a495c6360a -c /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf -k /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring\n\nPlease consider enabling telemetry to help improve Ceph:\n\n\tceph telemetry on\n\nFor more information see:\n\n\thttps://docs.ceph.com/docs/pacific/mgr/telemetry/\n\n'
10.10.2.3: b'Bootstrap complete.\n'
10.10.2.3: b'\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T01:52:39,874508597-04:00] INFO: > Set the default number of monitor daemons to 1\x1b[0m\n'
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:330 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1\n'
10.10.2.3: b'\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T01:52:39,883407516-04:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.3: b"\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T01:52:39,886534398-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mon', '1']\x1b[0m\n# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1\n"
10.10.2.3: b'Inferring fsid f1454c32-36e9-11ec-b561-09a495c6360a\n'
10.10.2.3: b'Inferring config /var/lib/ceph/f1454c32-36e9-11ec-b561-09a495c6360a/mon.node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.3: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.3: b'Scheduled mon update...\n'
10.10.2.3: b'\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T01:52:49,321617521-04:00] INFO: > Set the default number of manager daemons to 1\x1b[0m\n# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:335 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1\n'
10.10.2.3: b'\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T01:52:49,330147775-04:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.3: b"\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T01:52:49,333006162-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mgr', '1']\x1b[0m\n"
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1\n'
10.10.2.3: b'Inferring fsid f1454c32-36e9-11ec-b561-09a495c6360a\n'
10.10.2.3: b'Inferring config /var/lib/ceph/f1454c32-36e9-11ec-b561-09a495c6360a/mon.node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.3: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.3: b'Scheduled mgr update...\n'
10.10.2.3: b'\n\n\x1b[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T01:52:58,821024195-04:00] STAGE: Add hosts to the cluster...\x1b[0m\n'
10.10.2.3: b"\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T01:52:58,825692211-04:00] INFO: > Adding host 'sm1'\x1b[0m\n# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:362 - add_hosts() > ssh-copy-id -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -f -i /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub ljishen@sm1\n"
10.10.2.3: b'/usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: "/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub"\n'
10.10.2.3: b'\nNumber of key(s) added: 1\n\nNow try logging into the machine, with:   "ssh -o \'GlobalKnownHostsFile=/dev/null\' -o \'LogLevel=ERROR\' -o \'PasswordAuthentication=no\' -o \'StrictHostKeyChecking=no\' -o \'UserKnownHostsFile=/dev/null\' \'ljishen@sm1\'"\nand check to make sure that only the key(s) you wanted were added.\n\n'
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:368 - add_hosts() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add sm1 10.10.2.5\n'
10.10.2.3: b'\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T01:52:59,968174797-04:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.3: b"\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T01:52:59,971050175-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'host', 'add', 'sm1', '10.10.2.5']\x1b[0m\n"
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add sm1 10.10.2.5\n'
10.10.2.3: b'Inferring fsid f1454c32-36e9-11ec-b561-09a495c6360a\n'
10.10.2.3: b'Inferring config /var/lib/ceph/f1454c32-36e9-11ec-b561-09a495c6360a/mon.node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.3: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.3: b"Added host 'sm1' with addr '10.10.2.5'\n"
10.10.2.3: b'\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T01:53:11,219552771-04:00] INFO: Sleep for 20 seconds\x1b[0m\n'
10.10.2.3: b'\n\n\x1b[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T01:53:31,223740812-04:00] STAGE: Add OSDs to the cluster...\x1b[0m\n'
10.10.2.3: b"\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T01:53:31,229258289-04:00] INFO: > Adding devices ['/dev/nvme0n1'] on host 'sm1' as OSDs\x1b[0m\n# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:387 - add_host_devices_as_osds() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd sm1:/dev/nvme0n1\n"
10.10.2.3: b'\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T01:53:31,237965727-04:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.3: b"\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T01:53:31,240708576-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'daemon', 'add', 'osd', 'sm1:/dev/nvme0n1']\x1b[0m\n"
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd sm1:/dev/nvme0n1\n'
10.10.2.3: b'Inferring fsid f1454c32-36e9-11ec-b561-09a495c6360a\n'
10.10.2.3: b'Inferring config /var/lib/ceph/f1454c32-36e9-11ec-b561-09a495c6360a/mon.node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.3: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.3: b"Created osd(s) 0 on host 'sm1'\n"
10.10.2.3: b'\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T01:53:55,184335681-04:00] INFO: > Sleep for 20 seconds\x1b[0m\n'
10.10.2.3: b'\n\n\x1b[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T01:54:15,188969307-04:00] STAGE: Show Cluster Status\x1b[0m\n# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:343 - show_status() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status\n'
10.10.2.3: b'\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T01:54:15,197503308-04:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.3: b"\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T01:54:15,200545641-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', '--status']\x1b[0m\n"
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status\n'
10.10.2.3: b'Inferring fsid f1454c32-36e9-11ec-b561-09a495c6360a\n'
10.10.2.3: b'Inferring config /var/lib/ceph/f1454c32-36e9-11ec-b561-09a495c6360a/mon.node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.3: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.3: b'  cluster:\n    id:     f1454c32-36e9-11ec-b561-09a495c6360a\n    health: HEALTH_WARN\n            1 pool(s) have no replicas configured\n \n  services:\n    mon: 1 daemons, quorum node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us (age 2m)\n    mgr: node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us.hjnamb(active, since 2m)\n    osd: 1 osds: 1 up (since 20s), 1 in (since 40s)\n \n  data:\n    pools:   1 pools, 128 pgs\n    objects: 0 objects, 0 B\n    usage:   4.7 MiB used, 100 GiB / 100 GiB avail\n    pgs:     99.219% pgs unknown\n             127 unknown\n             1   active+clean\n \n  progress:\n \n'
[1] 22:54:24 [SUCCESS] ljishen@10.10.2.3

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T01:51:18,407691480-04:00] INFO: > Remove the existing cluster[0m
## bash:13 -  > ls /var/lib/ceph
## bash:13 -  > tail -n1
# bash:13 -  > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm --host 10.10.2.3 --host 10.10.2.5 rm-cluster --force --fsid e50a5ad0-36e8-11ec-b561-09a495c6360a

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T01:51:27,445802407-04:00] INFO: > Deploy a new cluster[0m
## bash:22 -  > /tmp/bench-rados/ceph-research/scripts/parallel-ssh --host 10.10.2.5 uname --nodename
## bash:22 -  > tail -n1
# bash:22 -  > osd_hostname=sm1
# bash:23 -  > /tmp/bench-rados/ceph-research/scripts/ceph-deploy -m 10.10.2.3 -o sm1/10.10.2.5:/dev/nvme0n1


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T01:51:28,572952910-04:00] STAGE: Verify passwordless SSH to hosts, passwordless sudo via ljishen, and the existence of devices on hosts...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T01:51:28,576935865-04:00] INFO: > Check the host for the monitor[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:181 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@10.10.2.3 sudo --non-interactive --validate

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T01:51:28,728558159-04:00] INFO: > Check hosts for OSDs[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T01:51:28,732927392-04:00] INFO: >> Check host 'sm1'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:200 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 sudo --non-interactive --validate

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T01:51:29,775916704-04:00] INFO: >>> Check device '/dev/nvme0n1'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:215 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 test -b /dev/nvme0n1
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:226 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 lsblk --nodeps --noheadings --output TYPE /dev/nvme0n1
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:227 - verify_parameters() > grep --word-regexp disk


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T01:51:31,909375109-04:00] STAGE: Reset devices on hosts...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T01:51:31,913450609-04:00] INFO: > Resetting device '/dev/nvme0n1' on host 'sm1'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:288 - reset_host_devices() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 sudo sh
  Removing ceph--72a1e5f4--db66--404f--bc1f--c8161b7b3c42-osd--block--bb4bebb8--ee60--4069--9d40--9cb4d66a851f (252:0)
  Archiving volume group "ceph-72a1e5f4-db66-404f-bc1f-c8161b7b3c42" metadata (seqno 5).
  Releasing logical volume "osd-block-bb4bebb8-ee60-4069-9d40-9cb4d66a851f"
  Creating volume group backup "/etc/lvm/backup/ceph-72a1e5f4-db66-404f-bc1f-c8161b7b3c42" (seqno 6).
  Logical volume "osd-block-bb4bebb8-ee60-4069-9d40-9cb4d66a851f" successfully removed
  Removing physical volume "/dev/nvme0n1" from volume group "ceph-72a1e5f4-db66-404f-bc1f-c8161b7b3c42"
  Volume group "ceph-72a1e5f4-db66-404f-bc1f-c8161b7b3c42" successfully removed


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T01:51:34,232825610-04:00] STAGE: Bootstrap a new cluster...[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:312 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.3 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T01:51:34,241609562-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T01:51:34,244770558-04:00] DEBUG: CEPHADM_PARAMS: ['bootstrap', '--config', '/tmp/bench-rados/ceph-research/scripts/ceph.conf', '--mon-ip', '10.10.2.3', '--output-dir', '/tmp/bench-rados/deployment_data_root/etc/ceph', '--ssh-user', 'ljishen', '--skip-dashboard', '--no-minimize-config', '--allow-overwrite', '--allow-fqdn-hostname', '--orphan-initial-daemons', '--skip-monitoring-stack', '--single-host-defaults'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.3 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults
Verifying podman|docker is present...
Verifying lvm2 is present...
Verifying time synchronization is in place...
Unit ntp.service is enabled and running
Repeating the final host check...
podman|docker (/usr/bin/docker) is present
systemctl is present
lvcreate is present
Unit ntp.service is enabled and running
Host looks OK
Cluster fsid: f1454c32-36e9-11ec-b561-09a495c6360a
Verifying IP 10.10.2.3 port 3300 ...
Verifying IP 10.10.2.3 port 6789 ...
Mon IP `10.10.2.3` is in CIDR network `10.10.2.0/24`
- internal network (--cluster-network) has not been provided, OSD replication will default to the public_network
Adjusting default settings to suit single-host cluster...
Pulling container image quay.io/ceph/ceph:v16...
Ceph version: ceph version 16.2.6 (ee28fb57e47e9f88813e24bbf4c14496ca299d31) pacific (stable)
Extracting ceph user uid/gid from container image...
Creating initial keys...
Creating initial monmap...
Creating mon...
Waiting for mon to start...
Waiting for mon...
mon is available
Setting mon public_network to 10.10.2.0/24
Wrote config to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf
Wrote keyring to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring
Creating mgr...
Verifying port 9283 ...
Waiting for mgr to start...
Waiting for mgr...
mgr not available, waiting (1/15)...
mgr not available, waiting (2/15)...
mgr is available
Enabling cephadm module...
Waiting for the mgr to restart...
Waiting for mgr epoch 5...
mgr epoch 5 is available
Setting orchestrator backend to cephadm...
Generating ssh key...
Wrote public SSH key to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub
Adding key to ljishen@localhost authorized_keys...
Adding host node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us...
Deploying unmanaged mon service...
Deploying unmanaged mgr service...
You can access the Ceph CLI with:

	sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --fsid f1454c32-36e9-11ec-b561-09a495c6360a -c /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf -k /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring

Please consider enabling telemetry to help improve Ceph:

	ceph telemetry on

For more information see:

	https://docs.ceph.com/docs/pacific/mgr/telemetry/

Bootstrap complete.

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T01:52:39,874508597-04:00] INFO: > Set the default number of monitor daemons to 1[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:330 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T01:52:39,883407516-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T01:52:39,886534398-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mon', '1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1
Inferring fsid f1454c32-36e9-11ec-b561-09a495c6360a
Inferring config /var/lib/ceph/f1454c32-36e9-11ec-b561-09a495c6360a/mon.node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Scheduled mon update...

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T01:52:49,321617521-04:00] INFO: > Set the default number of manager daemons to 1[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:335 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T01:52:49,330147775-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T01:52:49,333006162-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mgr', '1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1
Inferring fsid f1454c32-36e9-11ec-b561-09a495c6360a
Inferring config /var/lib/ceph/f1454c32-36e9-11ec-b561-09a495c6360a/mon.node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Scheduled mgr update...


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T01:52:58,821024195-04:00] STAGE: Add hosts to the cluster...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T01:52:58,825692211-04:00] INFO: > Adding host 'sm1'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:362 - add_hosts() > ssh-copy-id -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -f -i /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub ljishen@sm1
/usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: "/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub"

Number of key(s) added: 1

Now try logging into the machine, with:   "ssh -o 'GlobalKnownHostsFile=/dev/null' -o 'LogLevel=ERROR' -o 'PasswordAuthentication=no' -o 'StrictHostKeyChecking=no' -o 'UserKnownHostsFile=/dev/null' 'ljishen@sm1'"
and check to make sure that only the key(s) you wanted were added.

# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:368 - add_hosts() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add sm1 10.10.2.5
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T01:52:59,968174797-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T01:52:59,971050175-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'host', 'add', 'sm1', '10.10.2.5'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add sm1 10.10.2.5
Inferring fsid f1454c32-36e9-11ec-b561-09a495c6360a
Inferring config /var/lib/ceph/f1454c32-36e9-11ec-b561-09a495c6360a/mon.node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Added host 'sm1' with addr '10.10.2.5'

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T01:53:11,219552771-04:00] INFO: Sleep for 20 seconds[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T01:53:31,223740812-04:00] STAGE: Add OSDs to the cluster...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T01:53:31,229258289-04:00] INFO: > Adding devices ['/dev/nvme0n1'] on host 'sm1' as OSDs[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:387 - add_host_devices_as_osds() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd sm1:/dev/nvme0n1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T01:53:31,237965727-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T01:53:31,240708576-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'daemon', 'add', 'osd', 'sm1:/dev/nvme0n1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd sm1:/dev/nvme0n1
Inferring fsid f1454c32-36e9-11ec-b561-09a495c6360a
Inferring config /var/lib/ceph/f1454c32-36e9-11ec-b561-09a495c6360a/mon.node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Created osd(s) 0 on host 'sm1'

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T01:53:55,184335681-04:00] INFO: > Sleep for 20 seconds[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T01:54:15,188969307-04:00] STAGE: Show Cluster Status[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:343 - show_status() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T01:54:15,197503308-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T01:54:15,200545641-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', '--status'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status
Inferring fsid f1454c32-36e9-11ec-b561-09a495c6360a
Inferring config /var/lib/ceph/f1454c32-36e9-11ec-b561-09a495c6360a/mon.node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
  cluster:
    id:     f1454c32-36e9-11ec-b561-09a495c6360a
    health: HEALTH_WARN
            1 pool(s) have no replicas configured
 
  services:
    mon: 1 daemons, quorum node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us (age 2m)
    mgr: node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us.hjnamb(active, since 2m)
    osd: 1 osds: 1 up (since 20s), 1 in (since 40s)
 
  data:
    pools:   1 pools, 128 pgs
    objects: 0 objects, 0 B
    usage:   4.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     99.219% pgs unknown
             127 unknown
             1   active+clean
 
  progress:
 

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:54:24,280353986-07:00] INFO: > Copy configuration files to local[0m
# ./benchmarks/bench-rados:208 - launch_ceph_cluster() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.3:/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf
# ./benchmarks/bench-rados:212 - launch_ceph_cluster() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.3 sudo chmod 644 /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:54:24,434419715-07:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.3 sudo chmod 644 /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring
[1] 22:54:24 [SUCCESS] ljishen@10.10.2.3
# ./benchmarks/bench-rados:214 - launch_ceph_cluster() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.3:/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:54:24,757935862-07:00] STAGE: Update local ceph.conf as a client...[0m


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:54:24,761986145-07:00] STAGE: Configure Ceph pool...[0m
# ./benchmarks/bench-rados:245 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_max_backfills 32
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:54:24,787227813-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:54:24,790864629-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'f1454c32-36e9-11ec-b561-09a495c6360a', '--', 'ceph', 'config', 'set', 'osd', 'osd_max_backfills', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid f1454c32-36e9-11ec-b561-09a495c6360a -- ceph config set osd osd_max_backfills 32
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:246 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_active 32
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:54:27,923280276-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:54:27,926726413-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'f1454c32-36e9-11ec-b561-09a495c6360a', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_active', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid f1454c32-36e9-11ec-b561-09a495c6360a -- ceph config set osd osd_recovery_max_active 32
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:247 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_single_start 8
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:54:31,312968912-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:54:31,316649310-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'f1454c32-36e9-11ec-b561-09a495c6360a', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_single_start', '8'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid f1454c32-36e9-11ec-b561-09a495c6360a -- ceph config set osd osd_recovery_max_single_start 8
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:248 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_op_priority 63
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:54:34,713529674-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:54:34,716801825-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'f1454c32-36e9-11ec-b561-09a495c6360a', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_op_priority', '63'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid f1454c32-36e9-11ec-b561-09a495c6360a -- ceph config set osd osd_recovery_op_priority 63
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:250 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config show-with-defaults osd.0
# ./benchmarks/bench-rados:251 - configure_ceph_pool() > grep 'osd_max_backfills\|osd_recovery'
# ./benchmarks/bench-rados:251 - configure_ceph_pool() > column -t -s ' '
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
osd_max_backfills                        32         mon
osd_recovery_cost                        20971520   default
osd_recovery_delay_start                 0.000000   default
osd_recovery_max_active                  32         mon
osd_recovery_max_active_hdd              3          default
osd_recovery_max_active_ssd              10         default
osd_recovery_max_chunk                   8388608    default
osd_recovery_max_omap_entries_per_chunk  8096       default
osd_recovery_max_single_start            8          mon
osd_recovery_op_priority                 63         mon
osd_recovery_op_warn_multiple            16         default
osd_recovery_priority                    5          default
osd_recovery_retry_interval              30.000000  default
osd_recovery_sleep                       0.000000   default
osd_recovery_sleep_hdd                   0.100000   default
osd_recovery_sleep_hybrid                0.025000   default
osd_recovery_sleep_ssd                   0.000000   default
# ./benchmarks/bench-rados:253 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool create bench_rados 128 128 replicated --size 1 --pg-num-min 128
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:54:41,176914982-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:54:41,180483650-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'f1454c32-36e9-11ec-b561-09a495c6360a', '--', 'ceph', 'osd', 'pool', 'create', 'bench_rados', '128', '128', 'replicated', '--size', '1', '--pg-num-min', '128'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid f1454c32-36e9-11ec-b561-09a495c6360a -- ceph osd pool create bench_rados 128 128 replicated --size 1 --pg-num-min 128
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
pool 'bench_rados' created
# ./benchmarks/bench-rados:255 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados min_size 1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:54:45,674168160-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:54:45,677926325-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'f1454c32-36e9-11ec-b561-09a495c6360a', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'min_size', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid f1454c32-36e9-11ec-b561-09a495c6360a -- ceph osd pool set bench_rados min_size 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 min_size to 1
# ./benchmarks/bench-rados:256 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados pg_autoscale_mode off
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:54:49,850028402-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:54:49,853825520-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'f1454c32-36e9-11ec-b561-09a495c6360a', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'pg_autoscale_mode', 'off'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid f1454c32-36e9-11ec-b561-09a495c6360a -- ceph osd pool set bench_rados pg_autoscale_mode off
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 pg_autoscale_mode to off
# ./benchmarks/bench-rados:257 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool application enable bench_rados benchmark
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:54:54,364841708-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:54:54,368277316-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'f1454c32-36e9-11ec-b561-09a495c6360a', '--', 'ceph', 'osd', 'pool', 'application', 'enable', 'bench_rados', 'benchmark'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid f1454c32-36e9-11ec-b561-09a495c6360a -- ceph osd pool application enable bench_rados benchmark
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
enabled application 'benchmark' on pool 'bench_rados'
# ./benchmarks/bench-rados:258 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados noscrub 1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:54:58,370132028-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:54:58,373757994-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'f1454c32-36e9-11ec-b561-09a495c6360a', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'noscrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid f1454c32-36e9-11ec-b561-09a495c6360a -- ceph osd pool set bench_rados noscrub 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 noscrub to 1
# ./benchmarks/bench-rados:259 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados nodeep-scrub 1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:55:01,762550446-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:55:01,766231225-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'f1454c32-36e9-11ec-b561-09a495c6360a', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'nodeep-scrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid f1454c32-36e9-11ec-b561-09a495c6360a -- ceph osd pool set bench_rados nodeep-scrub 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 nodeep-scrub to 1
# ./benchmarks/bench-rados:260 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool ls detail
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:55:06,044078062-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:55:06,047299227-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'f1454c32-36e9-11ec-b561-09a495c6360a', '--', 'ceph', 'osd', 'pool', 'ls', 'detail'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid f1454c32-36e9-11ec-b561-09a495c6360a -- ceph osd pool ls detail
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
pool 1 'device_health_metrics' replicated size 1 min_size 1 crush_rule 0 object_hash rjenkins pg_num 128 pgp_num 128 autoscale_mode on last_change 13 lfor 0/0/11 flags hashpspool stripe_width 0 pg_num_min 1 application mgr_devicehealth
pool 2 'bench_rados' replicated size 1 min_size 1 crush_rule 0 object_hash rjenkins pg_num 128 pgp_num 128 autoscale_mode off last_change 21 flags hashpspool,noscrub,nodeep-scrub stripe_width 0 pg_num_min 128 application benchmark

# ./benchmarks/bench-rados:261 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd df tree
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:55:09,208564614-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:55:09,211711309-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'f1454c32-36e9-11ec-b561-09a495c6360a', '--', 'ceph', 'osd', 'df', 'tree'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid f1454c32-36e9-11ec-b561-09a495c6360a -- ceph osd df tree
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
ID  CLASS  WEIGHT   REWEIGHT  SIZE     RAW USE  DATA     OMAP  META     AVAIL    %USE  VAR   PGS  STATUS  TYPE NAME    
-1         0.09769         -  100 GiB  6.0 MiB  176 KiB   0 B  5.8 MiB  100 GiB  0.01  1.00    -          root default 
-3         0.09769         -  100 GiB  6.0 MiB  176 KiB   0 B  5.8 MiB  100 GiB  0.01  1.00    -              host sm1 
 0    ssd  0.09769   1.00000  100 GiB  6.0 MiB  176 KiB   0 B  5.8 MiB  100 GiB  0.01  1.00  256      up          osd.0
                       TOTAL  100 GiB  6.0 MiB  176 KiB   0 B  5.8 MiB  100 GiB  0.01                                  
MIN/MAX VAR: 1.00/1.00  STDDEV: 0


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:55:12,416172762-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:55:35,564240632-07:00] INFO: >>   data:
    pools:   2 pools, 256 pgs
    objects: 0 objects, 0 B
    usage:   6.0 MiB used, 100 GiB / 100 GiB avail
    pgs:     256 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:55:35,573144118-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:55:43,894910388-07:00] INFO: >>   data:
    pools:   2 pools, 256 pgs
    objects: 0 objects, 0 B
    usage:   6.0 MiB used, 100 GiB / 100 GiB avail
    pgs:     256 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:55:43,903610452-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:55:52,176799502-07:00] INFO: >>   data:
    pools:   2 pools, 256 pgs
    objects: 0 objects, 0 B
    usage:   6.0 MiB used, 100 GiB / 100 GiB avail
    pgs:     256 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:55:52,184515496-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:56:00,516732010-07:00] INFO: >>   data:
    pools:   2 pools, 256 pgs
    objects: 0 objects, 0 B
    usage:   6.0 MiB used, 100 GiB / 100 GiB avail
    pgs:     256 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:56:00,524942534-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:56:08,815413780-07:00] INFO: >>   data:
    pools:   2 pools, 256 pgs
    objects: 0 objects, 0 B
    usage:   6.0 MiB used, 100 GiB / 100 GiB avail
    pgs:     256 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:56:08,823583917-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:56:08,830545804-07:00] INFO: > The cluster is idle now.[0m


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:56:08,834267089-07:00] STAGE: Start benchmarking of mode write ...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:56:08,841080927-07:00] INFO: > Start system activity collection services[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:56:08,846686624-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:329 - rados_bench() > rm --force /mnt/sda8/ceph-research/scripts/output/sys_activity_1MB_write.dat.rados_bench_host.1
# ./benchmarks/bench-rados:330 - rados_bench() > sar_pid_rados_bench_host=845739
# ./benchmarks/bench-rados:330 - rados_bench() > S_TIME_FORMAT=ISO
# ./benchmarks/bench-rados:330 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/output/sys_activity_1MB_write.dat.rados_bench_host.1 2

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:56:08,853329290-07:00] INFO: >> for OSD host[0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:56:08,861546095-07:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.5 bash
10.10.2.5: b'97236\n'
[1] 22:56:09 [SUCCESS] ljishen@10.10.2.5
97236

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:56:09,978983466-07:00] INFO: > Run rados bench[0m
# ./benchmarks/bench-rados:358 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/output/rados_bench_1MB_write.log.1
## ./benchmarks/bench-rados:349 - rados_bench() > count_cpus 32-63,96-127
# ./benchmarks/bench-rados:349 - rados_bench() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 30 write --pool bench_rados -b 1048576 -O 1048576 --concurrent-ios 128 --show-time --write-object --write-omap --write-xattr --no-cleanup
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:56:10,000011352-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:56:10,003154580-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'f1454c32-36e9-11ec-b561-09a495c6360a', '--', 'rados', 'bench', '30', 'write', '--pool', 'bench_rados', '-b', '1048576', '-O', '1048576', '--concurrent-ios', '128', '--show-time', '--write-object', '--write-omap', '--write-xattr', '--no-cleanup'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid f1454c32-36e9-11ec-b561-09a495c6360a -- rados bench 30 write --pool bench_rados -b 1048576 -O 1048576 --concurrent-ios 128 --show-time --write-object --write-omap --write-xattr --no-cleanup
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
hints = 1
2021-10-27T05:56:12.540597+0000 Maintaining 128 concurrent writes of 1048576 bytes to objects of size 1048576 for up to 30 seconds or 0 objects
2021-10-27T05:56:12.540607+0000 Object prefix: benchmark_data_node-0.bluefield2.ucsc-cmps10_7
2021-10-27T05:56:12.568774+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-27T05:56:12.568774+0000     0       0         0         0         0         0           -           0
2021-10-27T05:56:13.569013+0000     1      83        83         0         0         0           -           0
2021-10-27T05:56:14.569088+0000     2     127       156        29   14.4982      14.5     1.99357     1.82778
2021-10-27T05:56:15.569157+0000     3     127       227       100   33.3299        71     1.83244     1.92124
2021-10-27T05:56:16.569254+0000     4     127       298       171   42.7456        71     1.71538     1.88603
2021-10-27T05:56:17.569360+0000     5     127       361       234   46.7952        63     1.63905      1.8833
2021-10-27T05:56:18.569438+0000     6     127       426       299   49.8284        65     1.96452      1.8991
2021-10-27T05:56:19.569640+0000     7     127       485       358    51.137        59     1.90865     1.90334
2021-10-27T05:56:20.569718+0000     8     127       536       409   51.1194        51     1.87468     1.90599
2021-10-27T05:56:21.569826+0000     9     127       565       438   48.6614        29     2.26875     1.92776
2021-10-27T05:56:22.569904+0000    10     127       613       486   48.5949        48     3.16202     2.02618
2021-10-27T05:56:23.569973+0000    11     127       644       517   46.9952        31     3.65527     2.11756
2021-10-27T05:56:24.570051+0000    12     127       672       545   45.4121        28     3.77516       2.211
2021-10-27T05:56:25.570157+0000    13     127       699       572   43.9956        27     3.76752     2.30157
2021-10-27T05:56:26.570238+0000    14     127       730       603   43.0671        31     3.73032     2.39298
2021-10-27T05:56:27.570307+0000    15     127       765       638   42.5292        35     3.72289     2.48041
2021-10-27T05:56:28.570391+0000    16     127       839       712   44.4957        74      2.7233     2.57093
2021-10-27T05:56:29.570497+0000    17     127       900       773   45.4662        61     1.90895     2.54646
2021-10-27T05:56:30.570575+0000    18     127       965       838   46.5511        65     1.97843     2.49883
2021-10-27T05:56:31.570644+0000    19     127      1037       910   47.8902        72     1.83813     2.45462
2021-10-27T05:56:32.570803+0000 min lat: 1.5826 max lat: 4.33618 avg lat: 2.44427
2021-10-27T05:56:32.570803+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-27T05:56:32.570803+0000    20     127      1061       934   46.6954        24     2.54477     2.44427
2021-10-27T05:56:33.570996+0000    21     127      1091       964   45.9001        30     2.54722     2.44387
2021-10-27T05:56:34.571224+0000    22     127      1113       986   44.8133        22     3.24896     2.46433
2021-10-27T05:56:35.571456+0000    23     127      1126       999   43.4299        13     3.68661     2.48093
2021-10-27T05:56:36.571534+0000    24     127      1159      1032   42.9952        33     4.97595     2.55236
2021-10-27T05:56:37.571640+0000    25     127      1176      1049   41.9553        17     5.44366     2.60093
2021-10-27T05:56:38.571720+0000    26     127      1192      1065    40.957        16     5.56835     2.65258
2021-10-27T05:56:39.571787+0000    27     127      1207      1080   39.9956        15     6.29838     2.70326
2021-10-27T05:56:40.571986+0000    28     127      1225      1098   39.2099        18     5.79525     2.76599
2021-10-27T05:56:41.572096+0000    29     127      1240      1113    38.375        15     6.51534     2.81654
2021-10-27T05:56:42.572175+0000    30     127      1280      1153   38.4291        40     6.11098     2.94377
2021-10-27T05:56:43.572297+0000 Total time run:         30.7232
Total writes made:      1281
Write size:             1048576
Object size:            1048576
Bandwidth (MB/sec):     41.6948
Stddev Bandwidth:       21.9012
Max bandwidth (MB/sec): 74
Min bandwidth (MB/sec): 0
Average IOPS:           41
Stddev IOPS:            21.9198
Max IOPS:               74
Min IOPS:               0
Average Latency(s):     2.94322
Stddev Latency(s):      1.45949
Max latency(s):         7.51137
Min latency(s):         0.617592

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:56:44,160987644-07:00] INFO: > Stop system activity collection services[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:56:44,167638596-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:379 - rados_bench() > kill -INT 845739

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:56:44,173735747-07:00] INFO: >> for OSD host[0m
# ./benchmarks/bench-rados:384 - rados_bench() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.5 kill -INT 97236
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:56:44,181849508-07:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.5 kill -INT 97236
[1] 22:56:45 [SUCCESS] ljishen@10.10.2.5

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:56:45,520173973-07:00] INFO: >>> Copy system activity file from OSD host[0m
## ./benchmarks/bench-rados:391 - rados_bench() > basename -- /tmp/bench-rados/sys_activity_1MB_write.dat.osd_host.1
# ./benchmarks/bench-rados:391 - rados_bench() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.5:/tmp/bench-rados/sys_activity_1MB_write.dat.osd_host.1 /mnt/sda8/ceph-research/scripts/output/sys_activity_1MB_write.dat.osd_host.1


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:56:46,786830532-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:57:09,930962175-07:00] INFO: >>   data:
    pools:   2 pools, 256 pgs
    objects: 1.28k objects, 1.3 GiB
    usage:   3.8 GiB used, 96 GiB / 100 GiB avail
    pgs:     256 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:57:09,939885608-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:57:18,343902400-07:00] INFO: >>   data:
    pools:   2 pools, 256 pgs
    objects: 1.28k objects, 1.3 GiB
    usage:   3.8 GiB used, 96 GiB / 100 GiB avail
    pgs:     256 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:57:18,352549864-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:57:26,617498018-07:00] INFO: >>   data:
    pools:   2 pools, 256 pgs
    objects: 1.28k objects, 1.3 GiB
    usage:   3.8 GiB used, 96 GiB / 100 GiB avail
    pgs:     256 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:57:26,625274405-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:57:34,897104394-07:00] INFO: >>   data:
    pools:   2 pools, 256 pgs
    objects: 1.28k objects, 1.3 GiB
    usage:   3.8 GiB used, 96 GiB / 100 GiB avail
    pgs:     256 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:57:34,905715921-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:57:43,095485020-07:00] INFO: >>   data:
    pools:   2 pools, 256 pgs
    objects: 1.28k objects, 1.3 GiB
    usage:   3.8 GiB used, 96 GiB / 100 GiB avail
    pgs:     256 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:57:43,103712275-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:57:43,110107146-07:00] INFO: > The cluster is idle now.[0m


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:57:43,114015142-07:00] STAGE: Start benchmarking of mode seq ...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:57:43,120794755-07:00] INFO: > Start system activity collection services[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:57:43,126772432-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:329 - rados_bench() > rm --force /mnt/sda8/ceph-research/scripts/output/sys_activity_1MB_seq.dat.rados_bench_host.1
# ./benchmarks/bench-rados:330 - rados_bench() > sar_pid_rados_bench_host=847603
# ./benchmarks/bench-rados:330 - rados_bench() > S_TIME_FORMAT=ISO
# ./benchmarks/bench-rados:330 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/output/sys_activity_1MB_seq.dat.rados_bench_host.1 2

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:57:43,133820340-07:00] INFO: >> for OSD host[0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:57:43,142310579-07:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.5 bash
10.10.2.5: b'97656\n'
[1] 22:57:44 [SUCCESS] ljishen@10.10.2.5
97656

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:57:44,245288129-07:00] INFO: > Run rados bench[0m
# ./benchmarks/bench-rados:367 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/output/rados_bench_1MB_seq.log.1
## ./benchmarks/bench-rados:364 - rados_bench() > count_cpus 32-63,96-127
# ./benchmarks/bench-rados:364 - rados_bench() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 128 --show-time
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:57:44,266479964-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:57:44,269938094-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'f1454c32-36e9-11ec-b561-09a495c6360a', '--', 'rados', 'bench', '99999999', 'seq', '--pool', 'bench_rados', '--concurrent-ios', '128', '--show-time'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid f1454c32-36e9-11ec-b561-09a495c6360a -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 128 --show-time
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
hints = 1
2021-10-27T05:57:46.675245+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-27T05:57:46.675245+0000     0       1         1         0         0         0           -           0
2021-10-27T05:57:47.675346+0000     1     127       932       805   804.864       805    0.147253    0.145559
2021-10-27T05:57:48.675622+0000 Total time run:       1.51456
Total reads made:     1281
Read size:            1048576
Object size:          1048576
Bandwidth (MB/sec):   845.788
Average IOPS:         845
Stddev IOPS:          0
Max IOPS:             805
Min IOPS:             805
Average Latency(s):   0.143643
Max latency(s):       0.256808
Min latency(s):       0.0724679

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:57:49,359834584-07:00] INFO: > Stop system activity collection services[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:57:49,366710098-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:379 - rados_bench() > kill -INT 847603

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:57:49,373444247-07:00] INFO: >> for OSD host[0m
# ./benchmarks/bench-rados:384 - rados_bench() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.5 kill -INT 97656
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:57:49,381740982-07:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.5 kill -INT 97656
[1] 22:57:50 [SUCCESS] ljishen@10.10.2.5

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:57:50,467348282-07:00] INFO: >>> Copy system activity file from OSD host[0m
## ./benchmarks/bench-rados:391 - rados_bench() > basename -- /tmp/bench-rados/sys_activity_1MB_seq.dat.osd_host.1
# ./benchmarks/bench-rados:391 - rados_bench() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.5:/tmp/bench-rados/sys_activity_1MB_seq.dat.osd_host.1 /mnt/sda8/ceph-research/scripts/output/sys_activity_1MB_seq.dat.osd_host.1


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:57:51,537535225-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:58:14,593911109-07:00] INFO: >>   data:
    pools:   2 pools, 256 pgs
    objects: 1.28k objects, 1.3 GiB
    usage:   3.8 GiB used, 96 GiB / 100 GiB avail
    pgs:     256 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:58:14,602335414-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:58:22,875785661-07:00] INFO: >>   data:
    pools:   2 pools, 256 pgs
    objects: 1.28k objects, 1.3 GiB
    usage:   3.8 GiB used, 96 GiB / 100 GiB avail
    pgs:     256 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:58:22,884246976-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:58:31,115490406-07:00] INFO: >>   data:
    pools:   2 pools, 256 pgs
    objects: 1.28k objects, 1.3 GiB
    usage:   3.8 GiB used, 96 GiB / 100 GiB avail
    pgs:     256 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:58:31,124672245-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:58:39,495831618-07:00] INFO: >>   data:
    pools:   2 pools, 256 pgs
    objects: 1.28k objects, 1.3 GiB
    usage:   3.8 GiB used, 96 GiB / 100 GiB avail
    pgs:     256 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:58:39,504533815-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:58:47,745413620-07:00] INFO: >>   data:
    pools:   2 pools, 256 pgs
    objects: 1.28k objects, 1.3 GiB
    usage:   3.8 GiB used, 96 GiB / 100 GiB avail
    pgs:     256 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:58:47,753470445-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:58:47,759717818-07:00] INFO: > The cluster is idle now.[0m


[1;7;39;49m[2021-10-26T22:58:47,762300242-07:00][RUNNING][ROUND 2/5/21] object_size=1MB[0m


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:58:47,766053808-07:00] STAGE: Launch a Ceph cluster (CEPH_HOST: 10.10.2.3, OSD_HOST: 10.10.2.5) ...[0m
# ./benchmarks/bench-rados:172 - launch_ceph_cluster() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.3 bash -euo pipefail
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T22:58:47,774434431-07:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.3 bash -euo pipefail
10.10.2.3: b'\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T01:58:48,189095570-04:00] INFO: > Remove the existing cluster\x1b[0m\n'
10.10.2.3: b'## bash:13 -  > ls /var/lib/ceph\n'
10.10.2.3: b'## bash:13 -  > tail -n1\n'
10.10.2.3: b'# bash:13 -  > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm --host 10.10.2.3 --host 10.10.2.5 rm-cluster --force --fsid f1454c32-36e9-11ec-b561-09a495c6360a\n'
10.10.2.3: b'\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T01:58:57,124042803-04:00] INFO: > Deploy a new cluster\x1b[0m\n'
10.10.2.3: b'## bash:22 -  > tail -n1\n## bash:22 -  > /tmp/bench-rados/ceph-research/scripts/parallel-ssh --host 10.10.2.5 uname --nodename\n'
10.10.2.3: b'# bash:22 -  > osd_hostname=sm1\n# bash:23 -  > /tmp/bench-rados/ceph-research/scripts/ceph-deploy -m 10.10.2.3 -o sm1/10.10.2.5:/dev/nvme0n1\n'
10.10.2.3: b'\n\n\x1b[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T01:58:58,257042371-04:00] STAGE: Verify passwordless SSH to hosts, passwordless sudo via ljishen, and the existence of devices on hosts...\x1b[0m\n'
10.10.2.3: b'\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T01:58:58,261008324-04:00] INFO: > Check the host for the monitor\x1b[0m\n# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:181 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@10.10.2.3 sudo --non-interactive --validate\n'
10.10.2.3: b'\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T01:58:58,407342353-04:00] INFO: > Check hosts for OSDs\x1b[0m\n'
10.10.2.3: b"\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T01:58:58,411124680-04:00] INFO: >> Check host 'sm1'\x1b[0m\n# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:200 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 sudo --non-interactive --validate\n"
10.10.2.3: b"\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T01:58:59,468313311-04:00] INFO: >>> Check device '/dev/nvme0n1'\x1b[0m\n# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:215 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 test -b /dev/nvme0n1\n"
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:226 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 lsblk --nodeps --noheadings --output TYPE /dev/nvme0n1\n'
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:227 - verify_parameters() > grep --word-regexp disk\n'
10.10.2.3: b'\n\n\x1b[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T01:59:01,604615188-04:00] STAGE: Reset devices on hosts...\x1b[0m\n'
10.10.2.3: b"\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T01:59:01,608400090-04:00] INFO: > Resetting device '/dev/nvme0n1' on host 'sm1'\x1b[0m\n"
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:288 - reset_host_devices() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 sudo sh\n'
10.10.2.3: b'  Removing ceph--0b438252--4104--42b4--821d--147894cebd3e-osd--block--199772d9--0b6e--402f--85fb--c8be1303f4e2 (252:0)\n'
10.10.2.3: b'  Archiving volume group "ceph-0b438252-4104-42b4-821d-147894cebd3e" metadata (seqno 5).\n'
10.10.2.3: b'  Releasing logical volume "osd-block-199772d9-0b6e-402f-85fb-c8be1303f4e2"\n'
10.10.2.3: b'  Creating volume group backup "/etc/lvm/backup/ceph-0b438252-4104-42b4-821d-147894cebd3e" (seqno 6).\n'
10.10.2.3: b'  Logical volume "osd-block-199772d9-0b6e-402f-85fb-c8be1303f4e2" successfully removed\n'
10.10.2.3: b'  Removing physical volume "/dev/nvme0n1" from volume group "ceph-0b438252-4104-42b4-821d-147894cebd3e"\n'
10.10.2.3: b'  Volume group "ceph-0b438252-4104-42b4-821d-147894cebd3e" successfully removed\n'
10.10.2.3: b'\n\n\x1b[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T01:59:03,849562544-04:00] STAGE: Bootstrap a new cluster...\x1b[0m\n'
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:312 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.3 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults\n'
10.10.2.3: b'\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T01:59:03,858034809-04:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.3: b"\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T01:59:03,861007321-04:00] DEBUG: CEPHADM_PARAMS: ['bootstrap', '--config', '/tmp/bench-rados/ceph-research/scripts/ceph.conf', '--mon-ip', '10.10.2.3', '--output-dir', '/tmp/bench-rados/deployment_data_root/etc/ceph', '--ssh-user', 'ljishen', '--skip-dashboard', '--no-minimize-config', '--allow-overwrite', '--allow-fqdn-hostname', '--orphan-initial-daemons', '--skip-monitoring-stack', '--single-host-defaults']\x1b[0m\n"
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.3 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults\n'
10.10.2.3: b'Verifying podman|docker is present...\nVerifying lvm2 is present...\nVerifying time synchronization is in place...\n'
10.10.2.3: b'Unit ntp.service is enabled and running\nRepeating the final host check...\npodman|docker (/usr/bin/docker) is present\n'
10.10.2.3: b'systemctl is present\nlvcreate is present\n'
10.10.2.3: b'Unit ntp.service is enabled and running\nHost looks OK\n'
10.10.2.3: b'Cluster fsid: fd42eef8-36ea-11ec-b561-09a495c6360a\n'
10.10.2.3: b'Verifying IP 10.10.2.3 port 3300 ...\nVerifying IP 10.10.2.3 port 6789 ...\n'
10.10.2.3: b'Mon IP `10.10.2.3` is in CIDR network `10.10.2.0/24`\n- internal network (--cluster-network) has not been provided, OSD replication will default to the public_network\n'
10.10.2.3: b'Adjusting default settings to suit single-host cluster...\nPulling container image quay.io/ceph/ceph:v16...\n'
10.10.2.3: b'Ceph version: ceph version 16.2.6 (ee28fb57e47e9f88813e24bbf4c14496ca299d31) pacific (stable)\nExtracting ceph user uid/gid from container image...\n'
10.10.2.3: b'Creating initial keys...\n'
10.10.2.3: b'Creating initial monmap...\n'
10.10.2.3: b'Creating mon...\n'
10.10.2.3: b'Waiting for mon to start...\nWaiting for mon...\n'
10.10.2.3: b'mon is available\nSetting mon public_network to 10.10.2.0/24\n'
10.10.2.3: b'Wrote config to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf\nWrote keyring to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring\n'
10.10.2.3: b'Creating mgr...\n'
10.10.2.3: b'Verifying port 9283 ...\n'
10.10.2.3: b'Waiting for mgr to start...\nWaiting for mgr...\n'
10.10.2.3: b'mgr not available, waiting (1/15)...\n'
10.10.2.3: b'mgr not available, waiting (2/15)...\n'
10.10.2.3: b'mgr is available\n'
10.10.2.3: b'Enabling cephadm module...\n'
10.10.2.3: b'Waiting for the mgr to restart...\nWaiting for mgr epoch 5...\n'
10.10.2.3: b'mgr epoch 5 is available\nSetting orchestrator backend to cephadm...\n'
10.10.2.3: b'Generating ssh key...\n'
10.10.2.3: b'Wrote public SSH key to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub\nAdding key to ljishen@localhost authorized_keys...\n'
10.10.2.3: b'Adding host node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us...\n'
10.10.2.3: b'Deploying unmanaged mon service...\n'
10.10.2.3: b'Deploying unmanaged mgr service...\n'
10.10.2.3: b'You can access the Ceph CLI with:\n\n\tsudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --fsid fd42eef8-36ea-11ec-b561-09a495c6360a -c /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf -k /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring\n\nPlease consider enabling telemetry to help improve Ceph:\n\n\tceph telemetry on\n\nFor more information see:\n\n\thttps://docs.ceph.com/docs/pacific/mgr/telemetry/\n\n'
10.10.2.3: b'Bootstrap complete.\n'
10.10.2.3: b'\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T02:00:07,695804741-04:00] INFO: > Set the default number of monitor daemons to 1\x1b[0m\n# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:330 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1\n'
10.10.2.3: b'\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T02:00:07,704700905-04:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.3: b"\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T02:00:07,707698613-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mon', '1']\x1b[0m\n"
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1\n'
10.10.2.3: b'Inferring fsid fd42eef8-36ea-11ec-b561-09a495c6360a\n'
10.10.2.3: b'Inferring config /var/lib/ceph/fd42eef8-36ea-11ec-b561-09a495c6360a/mon.node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.3: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.3: b'Scheduled mon update...\n'
10.10.2.3: b'\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T02:00:16,895956123-04:00] INFO: > Set the default number of manager daemons to 1\x1b[0m\n'
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:335 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1\n'
10.10.2.3: b'\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T02:00:16,904898403-04:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.3: b"\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T02:00:16,908333105-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mgr', '1']\x1b[0m\n"
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1\n'
10.10.2.3: b'Inferring fsid fd42eef8-36ea-11ec-b561-09a495c6360a\n'
10.10.2.3: b'Inferring config /var/lib/ceph/fd42eef8-36ea-11ec-b561-09a495c6360a/mon.node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.3: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.3: b'Scheduled mgr update...\n'
10.10.2.3: b'\n\n\x1b[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T02:00:26,694070751-04:00] STAGE: Add hosts to the cluster...\x1b[0m\n'
10.10.2.3: b"\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T02:00:26,698847612-04:00] INFO: > Adding host 'sm1'\x1b[0m\n"
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:362 - add_hosts() > ssh-copy-id -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -f -i /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub ljishen@sm1\n'
10.10.2.3: b'/usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: "/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub"\n'
10.10.2.3: b'\nNumber of key(s) added: 1\n\nNow try logging into the machine, with:   "ssh -o \'GlobalKnownHostsFile=/dev/null\' -o \'LogLevel=ERROR\' -o \'PasswordAuthentication=no\' -o \'StrictHostKeyChecking=no\' -o \'UserKnownHostsFile=/dev/null\' \'ljishen@sm1\'"\nand check to make sure that only the key(s) you wanted were added.\n\n'
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:368 - add_hosts() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add sm1 10.10.2.5\n'
10.10.2.3: b'\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T02:00:27,816020708-04:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.3: b"\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T02:00:27,818963102-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'host', 'add', 'sm1', '10.10.2.5']\x1b[0m\n"
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add sm1 10.10.2.5\n'
10.10.2.3: b'Inferring fsid fd42eef8-36ea-11ec-b561-09a495c6360a\n'
10.10.2.3: b'Inferring config /var/lib/ceph/fd42eef8-36ea-11ec-b561-09a495c6360a/mon.node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.3: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.3: b"Added host 'sm1' with addr '10.10.2.5'\n"
10.10.2.3: b'\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T02:00:38,669968997-04:00] INFO: Sleep for 20 seconds\x1b[0m\n'
10.10.2.3: b'\n\n\x1b[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T02:00:58,674147380-04:00] STAGE: Add OSDs to the cluster...\x1b[0m\n'
10.10.2.3: b"\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T02:00:58,679929876-04:00] INFO: > Adding devices ['/dev/nvme0n1'] on host 'sm1' as OSDs\x1b[0m\n# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:387 - add_host_devices_as_osds() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd sm1:/dev/nvme0n1\n"
10.10.2.3: b'\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T02:00:58,688279249-04:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.3: b"\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T02:00:58,691054178-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'daemon', 'add', 'osd', 'sm1:/dev/nvme0n1']\x1b[0m\n"
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd sm1:/dev/nvme0n1\n'
10.10.2.3: b'Inferring fsid fd42eef8-36ea-11ec-b561-09a495c6360a\n'
10.10.2.3: b'Inferring config /var/lib/ceph/fd42eef8-36ea-11ec-b561-09a495c6360a/mon.node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.3: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.3: b"Created osd(s) 0 on host 'sm1'\n"
10.10.2.3: b'\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T02:01:22,353473435-04:00] INFO: > Sleep for 20 seconds\x1b[0m\n'
10.10.2.3: b'\n\n\x1b[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T02:01:42,358163446-04:00] STAGE: Show Cluster Status\x1b[0m\n# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:343 - show_status() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status\n'
10.10.2.3: b'\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T02:01:42,367201767-04:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.3: b"\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T02:01:42,370372993-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', '--status']\x1b[0m\n# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status\n"
10.10.2.3: b'Inferring fsid fd42eef8-36ea-11ec-b561-09a495c6360a\n'
10.10.2.3: b'Inferring config /var/lib/ceph/fd42eef8-36ea-11ec-b561-09a495c6360a/mon.node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.3: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.3: b'  cluster:\n    id:     fd42eef8-36ea-11ec-b561-09a495c6360a\n    health: HEALTH_WARN\n            1 pool(s) have no replicas configured\n \n  services:\n    mon: 1 daemons, quorum node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us (age 2m)\n    mgr: node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us.khtooo(active, since 2m)\n    osd: 1 osds: 1 up (since 20s), 1 in (since 40s)\n \n  data:\n    pools:   1 pools, 128 pgs\n    objects: 0 objects, 0 B\n    usage:   4.7 MiB used, 100 GiB / 100 GiB avail\n    pgs:     99.219% pgs unknown\n             127 unknown\n             1   active+clean\n \n  progress:\n \n'
[1] 23:01:51 [SUCCESS] ljishen@10.10.2.3

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T01:58:48,189095570-04:00] INFO: > Remove the existing cluster[0m
## bash:13 -  > ls /var/lib/ceph
## bash:13 -  > tail -n1
# bash:13 -  > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm --host 10.10.2.3 --host 10.10.2.5 rm-cluster --force --fsid f1454c32-36e9-11ec-b561-09a495c6360a

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T01:58:57,124042803-04:00] INFO: > Deploy a new cluster[0m
## bash:22 -  > tail -n1
## bash:22 -  > /tmp/bench-rados/ceph-research/scripts/parallel-ssh --host 10.10.2.5 uname --nodename
# bash:22 -  > osd_hostname=sm1
# bash:23 -  > /tmp/bench-rados/ceph-research/scripts/ceph-deploy -m 10.10.2.3 -o sm1/10.10.2.5:/dev/nvme0n1


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T01:58:58,257042371-04:00] STAGE: Verify passwordless SSH to hosts, passwordless sudo via ljishen, and the existence of devices on hosts...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T01:58:58,261008324-04:00] INFO: > Check the host for the monitor[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:181 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@10.10.2.3 sudo --non-interactive --validate

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T01:58:58,407342353-04:00] INFO: > Check hosts for OSDs[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T01:58:58,411124680-04:00] INFO: >> Check host 'sm1'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:200 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 sudo --non-interactive --validate

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T01:58:59,468313311-04:00] INFO: >>> Check device '/dev/nvme0n1'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:215 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 test -b /dev/nvme0n1
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:226 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 lsblk --nodeps --noheadings --output TYPE /dev/nvme0n1
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:227 - verify_parameters() > grep --word-regexp disk


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T01:59:01,604615188-04:00] STAGE: Reset devices on hosts...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T01:59:01,608400090-04:00] INFO: > Resetting device '/dev/nvme0n1' on host 'sm1'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:288 - reset_host_devices() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 sudo sh
  Removing ceph--0b438252--4104--42b4--821d--147894cebd3e-osd--block--199772d9--0b6e--402f--85fb--c8be1303f4e2 (252:0)
  Archiving volume group "ceph-0b438252-4104-42b4-821d-147894cebd3e" metadata (seqno 5).
  Releasing logical volume "osd-block-199772d9-0b6e-402f-85fb-c8be1303f4e2"
  Creating volume group backup "/etc/lvm/backup/ceph-0b438252-4104-42b4-821d-147894cebd3e" (seqno 6).
  Logical volume "osd-block-199772d9-0b6e-402f-85fb-c8be1303f4e2" successfully removed
  Removing physical volume "/dev/nvme0n1" from volume group "ceph-0b438252-4104-42b4-821d-147894cebd3e"
  Volume group "ceph-0b438252-4104-42b4-821d-147894cebd3e" successfully removed


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T01:59:03,849562544-04:00] STAGE: Bootstrap a new cluster...[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:312 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.3 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T01:59:03,858034809-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T01:59:03,861007321-04:00] DEBUG: CEPHADM_PARAMS: ['bootstrap', '--config', '/tmp/bench-rados/ceph-research/scripts/ceph.conf', '--mon-ip', '10.10.2.3', '--output-dir', '/tmp/bench-rados/deployment_data_root/etc/ceph', '--ssh-user', 'ljishen', '--skip-dashboard', '--no-minimize-config', '--allow-overwrite', '--allow-fqdn-hostname', '--orphan-initial-daemons', '--skip-monitoring-stack', '--single-host-defaults'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.3 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults
Verifying podman|docker is present...
Verifying lvm2 is present...
Verifying time synchronization is in place...
Unit ntp.service is enabled and running
Repeating the final host check...
podman|docker (/usr/bin/docker) is present
systemctl is present
lvcreate is present
Unit ntp.service is enabled and running
Host looks OK
Cluster fsid: fd42eef8-36ea-11ec-b561-09a495c6360a
Verifying IP 10.10.2.3 port 3300 ...
Verifying IP 10.10.2.3 port 6789 ...
Mon IP `10.10.2.3` is in CIDR network `10.10.2.0/24`
- internal network (--cluster-network) has not been provided, OSD replication will default to the public_network
Adjusting default settings to suit single-host cluster...
Pulling container image quay.io/ceph/ceph:v16...
Ceph version: ceph version 16.2.6 (ee28fb57e47e9f88813e24bbf4c14496ca299d31) pacific (stable)
Extracting ceph user uid/gid from container image...
Creating initial keys...
Creating initial monmap...
Creating mon...
Waiting for mon to start...
Waiting for mon...
mon is available
Setting mon public_network to 10.10.2.0/24
Wrote config to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf
Wrote keyring to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring
Creating mgr...
Verifying port 9283 ...
Waiting for mgr to start...
Waiting for mgr...
mgr not available, waiting (1/15)...
mgr not available, waiting (2/15)...
mgr is available
Enabling cephadm module...
Waiting for the mgr to restart...
Waiting for mgr epoch 5...
mgr epoch 5 is available
Setting orchestrator backend to cephadm...
Generating ssh key...
Wrote public SSH key to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub
Adding key to ljishen@localhost authorized_keys...
Adding host node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us...
Deploying unmanaged mon service...
Deploying unmanaged mgr service...
You can access the Ceph CLI with:

	sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --fsid fd42eef8-36ea-11ec-b561-09a495c6360a -c /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf -k /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring

Please consider enabling telemetry to help improve Ceph:

	ceph telemetry on

For more information see:

	https://docs.ceph.com/docs/pacific/mgr/telemetry/

Bootstrap complete.

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T02:00:07,695804741-04:00] INFO: > Set the default number of monitor daemons to 1[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:330 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T02:00:07,704700905-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T02:00:07,707698613-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mon', '1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1
Inferring fsid fd42eef8-36ea-11ec-b561-09a495c6360a
Inferring config /var/lib/ceph/fd42eef8-36ea-11ec-b561-09a495c6360a/mon.node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Scheduled mon update...

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T02:00:16,895956123-04:00] INFO: > Set the default number of manager daemons to 1[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:335 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T02:00:16,904898403-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T02:00:16,908333105-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mgr', '1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1
Inferring fsid fd42eef8-36ea-11ec-b561-09a495c6360a
Inferring config /var/lib/ceph/fd42eef8-36ea-11ec-b561-09a495c6360a/mon.node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Scheduled mgr update...


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T02:00:26,694070751-04:00] STAGE: Add hosts to the cluster...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T02:00:26,698847612-04:00] INFO: > Adding host 'sm1'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:362 - add_hosts() > ssh-copy-id -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -f -i /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub ljishen@sm1
/usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: "/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub"

Number of key(s) added: 1

Now try logging into the machine, with:   "ssh -o 'GlobalKnownHostsFile=/dev/null' -o 'LogLevel=ERROR' -o 'PasswordAuthentication=no' -o 'StrictHostKeyChecking=no' -o 'UserKnownHostsFile=/dev/null' 'ljishen@sm1'"
and check to make sure that only the key(s) you wanted were added.

# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:368 - add_hosts() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add sm1 10.10.2.5
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T02:00:27,816020708-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T02:00:27,818963102-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'host', 'add', 'sm1', '10.10.2.5'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add sm1 10.10.2.5
Inferring fsid fd42eef8-36ea-11ec-b561-09a495c6360a
Inferring config /var/lib/ceph/fd42eef8-36ea-11ec-b561-09a495c6360a/mon.node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Added host 'sm1' with addr '10.10.2.5'

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T02:00:38,669968997-04:00] INFO: Sleep for 20 seconds[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T02:00:58,674147380-04:00] STAGE: Add OSDs to the cluster...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T02:00:58,679929876-04:00] INFO: > Adding devices ['/dev/nvme0n1'] on host 'sm1' as OSDs[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:387 - add_host_devices_as_osds() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd sm1:/dev/nvme0n1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T02:00:58,688279249-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T02:00:58,691054178-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'daemon', 'add', 'osd', 'sm1:/dev/nvme0n1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd sm1:/dev/nvme0n1
Inferring fsid fd42eef8-36ea-11ec-b561-09a495c6360a
Inferring config /var/lib/ceph/fd42eef8-36ea-11ec-b561-09a495c6360a/mon.node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Created osd(s) 0 on host 'sm1'

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T02:01:22,353473435-04:00] INFO: > Sleep for 20 seconds[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T02:01:42,358163446-04:00] STAGE: Show Cluster Status[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:343 - show_status() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T02:01:42,367201767-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T02:01:42,370372993-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', '--status'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status
Inferring fsid fd42eef8-36ea-11ec-b561-09a495c6360a
Inferring config /var/lib/ceph/fd42eef8-36ea-11ec-b561-09a495c6360a/mon.node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
  cluster:
    id:     fd42eef8-36ea-11ec-b561-09a495c6360a
    health: HEALTH_WARN
            1 pool(s) have no replicas configured
 
  services:
    mon: 1 daemons, quorum node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us (age 2m)
    mgr: node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us.khtooo(active, since 2m)
    osd: 1 osds: 1 up (since 20s), 1 in (since 40s)
 
  data:
    pools:   1 pools, 128 pgs
    objects: 0 objects, 0 B
    usage:   4.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     99.219% pgs unknown
             127 unknown
             1   active+clean
 
  progress:
 

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:01:51,946349733-07:00] INFO: > Copy configuration files to local[0m
# ./benchmarks/bench-rados:208 - launch_ceph_cluster() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.3:/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf
# ./benchmarks/bench-rados:212 - launch_ceph_cluster() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.3 sudo chmod 644 /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:01:52,103647779-07:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.3 sudo chmod 644 /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring
[1] 23:01:52 [SUCCESS] ljishen@10.10.2.3
# ./benchmarks/bench-rados:214 - launch_ceph_cluster() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.3:/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:01:52,430230560-07:00] STAGE: Update local ceph.conf as a client...[0m


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:01:52,434101316-07:00] STAGE: Configure Ceph pool...[0m
# ./benchmarks/bench-rados:245 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_max_backfills 32
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:01:52,458409108-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:01:52,461739879-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'fd42eef8-36ea-11ec-b561-09a495c6360a', '--', 'ceph', 'config', 'set', 'osd', 'osd_max_backfills', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid fd42eef8-36ea-11ec-b561-09a495c6360a -- ceph config set osd osd_max_backfills 32
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:246 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_active 32
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:01:55,641193250-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:01:55,644605404-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'fd42eef8-36ea-11ec-b561-09a495c6360a', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_active', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid fd42eef8-36ea-11ec-b561-09a495c6360a -- ceph config set osd osd_recovery_max_active 32
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:247 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_single_start 8
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:01:58,982943482-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:01:58,986384681-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'fd42eef8-36ea-11ec-b561-09a495c6360a', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_single_start', '8'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid fd42eef8-36ea-11ec-b561-09a495c6360a -- ceph config set osd osd_recovery_max_single_start 8
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:248 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_op_priority 63
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:02:02,289943014-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:02:02,293274486-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'fd42eef8-36ea-11ec-b561-09a495c6360a', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_op_priority', '63'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid fd42eef8-36ea-11ec-b561-09a495c6360a -- ceph config set osd osd_recovery_op_priority 63
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:250 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config show-with-defaults osd.0
# ./benchmarks/bench-rados:251 - configure_ceph_pool() > grep 'osd_max_backfills\|osd_recovery'
# ./benchmarks/bench-rados:251 - configure_ceph_pool() > column -t -s ' '
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
osd_max_backfills                        32         mon
osd_recovery_cost                        20971520   default
osd_recovery_delay_start                 0.000000   default
osd_recovery_max_active                  32         mon
osd_recovery_max_active_hdd              3          default
osd_recovery_max_active_ssd              10         default
osd_recovery_max_chunk                   8388608    default
osd_recovery_max_omap_entries_per_chunk  8096       default
osd_recovery_max_single_start            8          mon
osd_recovery_op_priority                 63         mon
osd_recovery_op_warn_multiple            16         default
osd_recovery_priority                    5          default
osd_recovery_retry_interval              30.000000  default
osd_recovery_sleep                       0.000000   default
osd_recovery_sleep_hdd                   0.100000   default
osd_recovery_sleep_hybrid                0.025000   default
osd_recovery_sleep_ssd                   0.000000   default
# ./benchmarks/bench-rados:253 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool create bench_rados 128 128 replicated --size 1 --pg-num-min 128
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:02:08,947453405-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:02:08,951053603-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'fd42eef8-36ea-11ec-b561-09a495c6360a', '--', 'ceph', 'osd', 'pool', 'create', 'bench_rados', '128', '128', 'replicated', '--size', '1', '--pg-num-min', '128'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid fd42eef8-36ea-11ec-b561-09a495c6360a -- ceph osd pool create bench_rados 128 128 replicated --size 1 --pg-num-min 128
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
pool 'bench_rados' created
# ./benchmarks/bench-rados:255 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados min_size 1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:02:13,204080194-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:02:13,207945390-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'fd42eef8-36ea-11ec-b561-09a495c6360a', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'min_size', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid fd42eef8-36ea-11ec-b561-09a495c6360a -- ceph osd pool set bench_rados min_size 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 min_size to 1
# ./benchmarks/bench-rados:256 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados pg_autoscale_mode off
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:02:17,401712119-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:02:17,405356650-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'fd42eef8-36ea-11ec-b561-09a495c6360a', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'pg_autoscale_mode', 'off'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid fd42eef8-36ea-11ec-b561-09a495c6360a -- ceph osd pool set bench_rados pg_autoscale_mode off
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 pg_autoscale_mode to off
# ./benchmarks/bench-rados:257 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool application enable bench_rados benchmark
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:02:21,750147743-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:02:21,753873366-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'fd42eef8-36ea-11ec-b561-09a495c6360a', '--', 'ceph', 'osd', 'pool', 'application', 'enable', 'bench_rados', 'benchmark'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid fd42eef8-36ea-11ec-b561-09a495c6360a -- ceph osd pool application enable bench_rados benchmark
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
enabled application 'benchmark' on pool 'bench_rados'
# ./benchmarks/bench-rados:258 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados noscrub 1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:02:25,105772135-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:02:25,109257105-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'fd42eef8-36ea-11ec-b561-09a495c6360a', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'noscrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid fd42eef8-36ea-11ec-b561-09a495c6360a -- ceph osd pool set bench_rados noscrub 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 noscrub to 1
# ./benchmarks/bench-rados:259 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados nodeep-scrub 1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:02:28,697961572-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:02:28,701227932-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'fd42eef8-36ea-11ec-b561-09a495c6360a', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'nodeep-scrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid fd42eef8-36ea-11ec-b561-09a495c6360a -- ceph osd pool set bench_rados nodeep-scrub 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 nodeep-scrub to 1
# ./benchmarks/bench-rados:260 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool ls detail
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:02:32,214042056-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:02:32,217897954-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'fd42eef8-36ea-11ec-b561-09a495c6360a', '--', 'ceph', 'osd', 'pool', 'ls', 'detail'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid fd42eef8-36ea-11ec-b561-09a495c6360a -- ceph osd pool ls detail
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
pool 1 'device_health_metrics' replicated size 1 min_size 1 crush_rule 0 object_hash rjenkins pg_num 128 pgp_num 128 autoscale_mode on last_change 13 lfor 0/0/11 flags hashpspool stripe_width 0 pg_num_min 1 application mgr_devicehealth
pool 2 'bench_rados' replicated size 1 min_size 1 crush_rule 0 object_hash rjenkins pg_num 128 pgp_num 128 autoscale_mode off last_change 21 flags hashpspool,noscrub,nodeep-scrub stripe_width 0 pg_num_min 128 application benchmark

# ./benchmarks/bench-rados:261 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd df tree
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:02:35,454915220-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:02:35,458850116-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'fd42eef8-36ea-11ec-b561-09a495c6360a', '--', 'ceph', 'osd', 'df', 'tree'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid fd42eef8-36ea-11ec-b561-09a495c6360a -- ceph osd df tree
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
ID  CLASS  WEIGHT   REWEIGHT  SIZE     RAW USE  DATA     OMAP  META     AVAIL    %USE  VAR   PGS  STATUS  TYPE NAME    
-1         0.09769         -  100 GiB  6.0 MiB  176 KiB   0 B  5.8 MiB  100 GiB  0.01  1.00    -          root default 
-3         0.09769         -  100 GiB  6.0 MiB  176 KiB   0 B  5.8 MiB  100 GiB  0.01  1.00    -              host sm1 
 0    ssd  0.09769   1.00000  100 GiB  6.0 MiB  176 KiB   0 B  5.8 MiB  100 GiB  0.01  1.00  256      up          osd.0
                       TOTAL  100 GiB  6.0 MiB  176 KiB   0 B  5.8 MiB  100 GiB  0.01                                  
MIN/MAX VAR: 1.00/1.00  STDDEV: 0


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:02:38,609726811-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:03:01,774705507-07:00] INFO: >>   data:
    pools:   2 pools, 256 pgs
    objects: 0 objects, 0 B
    usage:   6.0 MiB used, 100 GiB / 100 GiB avail
    pgs:     256 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:03:01,783146574-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:03:10,088129544-07:00] INFO: >>   data:
    pools:   2 pools, 256 pgs
    objects: 0 objects, 0 B
    usage:   6.0 MiB used, 100 GiB / 100 GiB avail
    pgs:     256 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:03:10,095986944-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:03:18,238040564-07:00] INFO: >>   data:
    pools:   2 pools, 256 pgs
    objects: 0 objects, 0 B
    usage:   6.0 MiB used, 100 GiB / 100 GiB avail
    pgs:     256 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:03:18,246507620-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:03:26,504802289-07:00] INFO: >>   data:
    pools:   2 pools, 256 pgs
    objects: 0 objects, 0 B
    usage:   6.0 MiB used, 100 GiB / 100 GiB avail
    pgs:     256 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:03:26,512912313-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:03:34,814866010-07:00] INFO: >>   data:
    pools:   2 pools, 256 pgs
    objects: 0 objects, 0 B
    usage:   6.0 MiB used, 100 GiB / 100 GiB avail
    pgs:     256 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:03:34,823323127-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:03:34,829829036-07:00] INFO: > The cluster is idle now.[0m


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:03:34,834070959-07:00] STAGE: Start benchmarking of mode write ...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:03:34,842029148-07:00] INFO: > Start system activity collection services[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:03:34,847768166-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:329 - rados_bench() > rm --force /mnt/sda8/ceph-research/scripts/output/sys_activity_1MB_write.dat.rados_bench_host.2
# ./benchmarks/bench-rados:330 - rados_bench() > sar_pid_rados_bench_host=854405
# ./benchmarks/bench-rados:330 - rados_bench() > S_TIME_FORMAT=ISO
# ./benchmarks/bench-rados:330 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/output/sys_activity_1MB_write.dat.rados_bench_host.2 2

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:03:34,855228499-07:00] INFO: >> for OSD host[0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:03:34,864229388-07:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.5 bash
10.10.2.5: b'102422\n'
[1] 23:03:35 [SUCCESS] ljishen@10.10.2.5
102422

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:03:35,953878697-07:00] INFO: > Run rados bench[0m
# ./benchmarks/bench-rados:358 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/output/rados_bench_1MB_write.log.2
## ./benchmarks/bench-rados:349 - rados_bench() > count_cpus 32-63,96-127
# ./benchmarks/bench-rados:349 - rados_bench() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 30 write --pool bench_rados -b 1048576 -O 1048576 --concurrent-ios 128 --show-time --write-object --write-omap --write-xattr --no-cleanup
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:03:35,974795575-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:03:35,978252002-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'fd42eef8-36ea-11ec-b561-09a495c6360a', '--', 'rados', 'bench', '30', 'write', '--pool', 'bench_rados', '-b', '1048576', '-O', '1048576', '--concurrent-ios', '128', '--show-time', '--write-object', '--write-omap', '--write-xattr', '--no-cleanup'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid fd42eef8-36ea-11ec-b561-09a495c6360a -- rados bench 30 write --pool bench_rados -b 1048576 -O 1048576 --concurrent-ios 128 --show-time --write-object --write-omap --write-xattr --no-cleanup
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
hints = 1
2021-10-27T06:03:38.402247+0000 Maintaining 128 concurrent writes of 1048576 bytes to objects of size 1048576 for up to 30 seconds or 0 objects
2021-10-27T06:03:38.402267+0000 Object prefix: benchmark_data_node-0.bluefield2.ucsc-cmps10_7
2021-10-27T06:03:38.430445+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-27T06:03:38.430445+0000     0       0         0         0         0         0           -           0
2021-10-27T06:03:39.430706+0000     1      86        86         0         0         0           -           0
2021-10-27T06:03:40.430797+0000     2     127       162        35   17.4975      17.5     1.89805     1.77386
2021-10-27T06:03:41.430872+0000     3     127       224        97   32.3295        62     1.81368     1.81868
2021-10-27T06:03:42.430943+0000     4     127       300       173   43.2454        76      1.7461     1.81933
2021-10-27T06:03:43.431010+0000     5     127       364       237   47.3953        64     1.89152     1.82551
2021-10-27T06:03:44.431080+0000     6     127       431       304   50.6619        67      1.7763     1.83845
2021-10-27T06:03:45.431259+0000     7     127       505       378   53.9943        74     1.82703     1.84566
2021-10-27T06:03:46.431473+0000     8     127       549       422   52.7437        44      2.0403     1.84893
2021-10-27T06:03:47.431617+0000     9     127       576       449   49.8828        27     2.41055      1.8751
2021-10-27T06:03:48.431685+0000    10     127       607       480   47.9944        31      2.6982     1.93003
2021-10-27T06:03:49.431752+0000    11     127       656       529   48.0855        49     3.60231     2.06301
2021-10-27T06:03:50.431824+0000    12     127       685       558   46.4949        29     3.56347     2.15874
2021-10-27T06:03:51.431892+0000    13     127       715       588    45.226        30     3.63191     2.24656
2021-10-27T06:03:52.431962+0000    14     127       745       618   44.1383        30     3.57732     2.32606
2021-10-27T06:03:53.432177+0000    15     127       807       680   45.3283        62     3.01272     2.43697
2021-10-27T06:03:54.432249+0000    16     127       881       754   47.1199        74     1.77162     2.44835
2021-10-27T06:03:55.432317+0000    17     127       948       821    48.289        67     1.65672     2.40137
2021-10-27T06:03:56.432386+0000    18     127      1017       890   49.4393        69     1.81623     2.35912
2021-10-27T06:03:57.432456+0000    19     127      1067       940   49.4686        50     1.94335     2.33678
2021-10-27T06:03:58.432697+0000 min lat: 1.45765 max lat: 4.33392 avg lat: 2.33692
2021-10-27T06:03:58.432697+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-27T06:03:58.432697+0000    20     127      1083       956   47.7948        16     2.41818     2.33692
2021-10-27T06:03:59.432774+0000    21     127      1115       988   47.0426        32     3.48946     2.36329
2021-10-27T06:04:00.432846+0000    22     127      1137      1010   45.9042        22     4.04332     2.40196
2021-10-27T06:04:01.433002+0000    23     127      1147      1020    44.343        10     4.47091     2.42374
2021-10-27T06:04:02.433074+0000    24     127      1169      1042    43.412        22     5.24093     2.48501
2021-10-27T06:04:03.433288+0000    25     127      1179      1052   42.0753        10     5.35842      2.5154
2021-10-27T06:04:04.433429+0000    26     127      1211      1084   41.6876        32     6.38897     2.62927
2021-10-27T06:04:05.433495+0000    27     127      1232      1105   40.9214        21     6.57308     2.70413
2021-10-27T06:04:06.433613+0000    28     127      1243      1116   39.8527        11     6.61241     2.74422
2021-10-27T06:04:07.433681+0000    29     127      1273      1146   39.5129        30       6.347     2.84496
2021-10-27T06:04:08.433794+0000    30     127      1303      1176   39.1957        30     5.59753     2.92401
2021-10-27T06:04:09.434032+0000 Total time run:         30.5002
Total writes made:      1304
Write size:             1048576
Object size:            1048576
Bandwidth (MB/sec):     42.7538
Stddev Bandwidth:       22.6075
Max bandwidth (MB/sec): 76
Min bandwidth (MB/sec): 0
Average IOPS:           42
Stddev IOPS:            22.6238
Max IOPS:               76
Min IOPS:               0
Average Latency(s):     2.90117
Stddev Latency(s):      1.51743
Max latency(s):         7.19719
Min latency(s):         0.639649

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:04:09,984753816-07:00] INFO: > Stop system activity collection services[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:04:09,991621195-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:379 - rados_bench() > kill -INT 854405

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:04:09,998019843-07:00] INFO: >> for OSD host[0m
# ./benchmarks/bench-rados:384 - rados_bench() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.5 kill -INT 102422
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:04:10,006513809-07:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.5 kill -INT 102422
[1] 23:04:11 [SUCCESS] ljishen@10.10.2.5

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:04:11,477679209-07:00] INFO: >>> Copy system activity file from OSD host[0m
## ./benchmarks/bench-rados:391 - rados_bench() > basename -- /tmp/bench-rados/sys_activity_1MB_write.dat.osd_host.2
# ./benchmarks/bench-rados:391 - rados_bench() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.5:/tmp/bench-rados/sys_activity_1MB_write.dat.osd_host.2 /mnt/sda8/ceph-research/scripts/output/sys_activity_1MB_write.dat.osd_host.2


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:04:12,738687614-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:04:35,937805381-07:00] INFO: >>   data:
    pools:   2 pools, 256 pgs
    objects: 1.30k objects, 1.3 GiB
    usage:   3.8 GiB used, 96 GiB / 100 GiB avail
    pgs:     256 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:04:35,946254994-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:04:44,288437832-07:00] INFO: >>   data:
    pools:   2 pools, 256 pgs
    objects: 1.30k objects, 1.3 GiB
    usage:   3.8 GiB used, 96 GiB / 100 GiB avail
    pgs:     256 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:04:44,297375302-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:04:52,620062485-07:00] INFO: >>   data:
    pools:   2 pools, 256 pgs
    objects: 1.30k objects, 1.3 GiB
    usage:   3.8 GiB used, 96 GiB / 100 GiB avail
    pgs:     256 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:04:52,628727222-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:05:00,722108246-07:00] INFO: >>   data:
    pools:   2 pools, 256 pgs
    objects: 1.30k objects, 1.3 GiB
    usage:   3.8 GiB used, 96 GiB / 100 GiB avail
    pgs:     256 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:05:00,730410492-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:05:09,002306051-07:00] INFO: >>   data:
    pools:   2 pools, 256 pgs
    objects: 1.30k objects, 1.3 GiB
    usage:   3.8 GiB used, 96 GiB / 100 GiB avail
    pgs:     256 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:05:09,010494664-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:05:09,017626099-07:00] INFO: > The cluster is idle now.[0m


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:05:09,021745592-07:00] STAGE: Start benchmarking of mode seq ...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:05:09,029425198-07:00] INFO: > Start system activity collection services[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:05:09,035626394-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:329 - rados_bench() > rm --force /mnt/sda8/ceph-research/scripts/output/sys_activity_1MB_seq.dat.rados_bench_host.2
# ./benchmarks/bench-rados:330 - rados_bench() > sar_pid_rados_bench_host=856272
# ./benchmarks/bench-rados:330 - rados_bench() > S_TIME_FORMAT=ISO
# ./benchmarks/bench-rados:330 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/output/sys_activity_1MB_seq.dat.rados_bench_host.2 2

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:05:09,043502368-07:00] INFO: >> for OSD host[0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:05:09,052450418-07:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.5 bash
10.10.2.5: b'102820\n'
[1] 23:05:10 [SUCCESS] ljishen@10.10.2.5
102820

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:05:10,153658174-07:00] INFO: > Run rados bench[0m
# ./benchmarks/bench-rados:367 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/output/rados_bench_1MB_seq.log.2
## ./benchmarks/bench-rados:364 - rados_bench() > count_cpus 32-63,96-127
# ./benchmarks/bench-rados:364 - rados_bench() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 128 --show-time
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:05:10,174472419-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:05:10,177813770-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'fd42eef8-36ea-11ec-b561-09a495c6360a', '--', 'rados', 'bench', '99999999', 'seq', '--pool', 'bench_rados', '--concurrent-ios', '128', '--show-time'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid fd42eef8-36ea-11ec-b561-09a495c6360a -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 128 --show-time
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
hints = 1
2021-10-27T06:05:12.758967+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-27T06:05:12.758967+0000     0       1         1         0         0         0           -           0
2021-10-27T06:05:13.759113+0000     1     127       946       819   818.818       819    0.155468    0.141612
2021-10-27T06:05:14.759243+0000 Total time run:       1.55972
Total reads made:     1304
Read size:            1048576
Object size:          1048576
Bandwidth (MB/sec):   836.05
Average IOPS:         836
Stddev IOPS:          0
Max IOPS:             819
Min IOPS:             819
Average Latency(s):   0.145751
Max latency(s):       0.258562
Min latency(s):       0.0649384

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:05:15,385077427-07:00] INFO: > Stop system activity collection services[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:05:15,391993578-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:379 - rados_bench() > kill -INT 856272

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:05:15,398496371-07:00] INFO: >> for OSD host[0m
# ./benchmarks/bench-rados:384 - rados_bench() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.5 kill -INT 102820
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:05:15,407498753-07:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.5 kill -INT 102820
[1] 23:05:16 [SUCCESS] ljishen@10.10.2.5

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:05:16,503670292-07:00] INFO: >>> Copy system activity file from OSD host[0m
## ./benchmarks/bench-rados:391 - rados_bench() > basename -- /tmp/bench-rados/sys_activity_1MB_seq.dat.osd_host.2
# ./benchmarks/bench-rados:391 - rados_bench() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.5:/tmp/bench-rados/sys_activity_1MB_seq.dat.osd_host.2 /mnt/sda8/ceph-research/scripts/output/sys_activity_1MB_seq.dat.osd_host.2


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:05:17,549277735-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:05:40,691072808-07:00] INFO: >>   data:
    pools:   2 pools, 256 pgs
    objects: 1.30k objects, 1.3 GiB
    usage:   3.8 GiB used, 96 GiB / 100 GiB avail
    pgs:     256 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:05:40,699722698-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:05:49,113115363-07:00] INFO: >>   data:
    pools:   2 pools, 256 pgs
    objects: 1.30k objects, 1.3 GiB
    usage:   3.8 GiB used, 96 GiB / 100 GiB avail
    pgs:     256 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:05:49,121233042-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:05:57,553675546-07:00] INFO: >>   data:
    pools:   2 pools, 256 pgs
    objects: 1.30k objects, 1.3 GiB
    usage:   3.8 GiB used, 96 GiB / 100 GiB avail
    pgs:     256 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:05:57,562358738-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:06:05,828511114-07:00] INFO: >>   data:
    pools:   2 pools, 256 pgs
    objects: 1.30k objects, 1.3 GiB
    usage:   3.8 GiB used, 96 GiB / 100 GiB avail
    pgs:     256 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:06:05,837288753-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:06:13,944979267-07:00] INFO: >>   data:
    pools:   2 pools, 256 pgs
    objects: 1.30k objects, 1.3 GiB
    usage:   3.8 GiB used, 96 GiB / 100 GiB avail
    pgs:     256 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:06:13,953473052-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:06:13,960542701-07:00] INFO: > The cluster is idle now.[0m


[1;7;39;49m[2021-10-26T23:06:13,962891977-07:00][RUNNING][ROUND 3/5/21] object_size=1MB[0m


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:06:13,966310032-07:00] STAGE: Launch a Ceph cluster (CEPH_HOST: 10.10.2.3, OSD_HOST: 10.10.2.5) ...[0m
# ./benchmarks/bench-rados:172 - launch_ceph_cluster() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.3 bash -euo pipefail
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:06:13,975056523-07:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.3 bash -euo pipefail
10.10.2.3: b'\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T02:06:14,671006882-04:00] INFO: > Remove the existing cluster\x1b[0m\n'
10.10.2.3: b'## bash:13 -  > ls /var/lib/ceph\n## bash:13 -  > tail -n1\n'
10.10.2.3: b'# bash:13 -  > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm --host 10.10.2.3 --host 10.10.2.5 rm-cluster --force --fsid fd42eef8-36ea-11ec-b561-09a495c6360a\n'
10.10.2.3: b'\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T02:06:23,679267963-04:00] INFO: > Deploy a new cluster\x1b[0m\n'
10.10.2.3: b'## bash:22 -  > /tmp/bench-rados/ceph-research/scripts/parallel-ssh --host 10.10.2.5 uname --nodename\n## bash:22 -  > tail -n1\n'
10.10.2.3: b'# bash:22 -  > osd_hostname=sm1\n# bash:23 -  > /tmp/bench-rados/ceph-research/scripts/ceph-deploy -m 10.10.2.3 -o sm1/10.10.2.5:/dev/nvme0n1\n'
10.10.2.3: b'\n\n\x1b[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T02:06:24,801595242-04:00] STAGE: Verify passwordless SSH to hosts, passwordless sudo via ljishen, and the existence of devices on hosts...\x1b[0m\n'
10.10.2.3: b'\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T02:06:24,805380285-04:00] INFO: > Check the host for the monitor\x1b[0m\n# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:181 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@10.10.2.3 sudo --non-interactive --validate\n'
10.10.2.3: b'\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T02:06:24,951588726-04:00] INFO: > Check hosts for OSDs\x1b[0m\n'
10.10.2.3: b"\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T02:06:24,955343411-04:00] INFO: >> Check host 'sm1'\x1b[0m\n# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:200 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 sudo --non-interactive --validate\n"
10.10.2.3: b"\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T02:06:26,088170646-04:00] INFO: >>> Check device '/dev/nvme0n1'\x1b[0m\n# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:215 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 test -b /dev/nvme0n1\n"
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:226 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 lsblk --nodeps --noheadings --output TYPE /dev/nvme0n1\n'
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:227 - verify_parameters() > grep --word-regexp disk\n'
10.10.2.3: b'\n\n\x1b[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T02:06:28,208950408-04:00] STAGE: Reset devices on hosts...\x1b[0m\n'
10.10.2.3: b"\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T02:06:28,212852791-04:00] INFO: > Resetting device '/dev/nvme0n1' on host 'sm1'\x1b[0m\n"
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:288 - reset_host_devices() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 sudo sh\n'
10.10.2.3: b'  Removing ceph--5fc41a64--9f33--4915--993e--d6f3f8a2da25-osd--block--865c2c20--7a90--4623--b444--0af133c67471 (252:0)\n'
10.10.2.3: b'  Archiving volume group "ceph-5fc41a64-9f33-4915-993e-d6f3f8a2da25" metadata (seqno 5).\n'
10.10.2.3: b'  Releasing logical volume "osd-block-865c2c20-7a90-4623-b444-0af133c67471"\n'
10.10.2.3: b'  Creating volume group backup "/etc/lvm/backup/ceph-5fc41a64-9f33-4915-993e-d6f3f8a2da25" (seqno 6).\n'
10.10.2.3: b'  Logical volume "osd-block-865c2c20-7a90-4623-b444-0af133c67471" successfully removed\n'
10.10.2.3: b'  Removing physical volume "/dev/nvme0n1" from volume group "ceph-5fc41a64-9f33-4915-993e-d6f3f8a2da25"\n'
10.10.2.3: b'  Volume group "ceph-5fc41a64-9f33-4915-993e-d6f3f8a2da25" successfully removed\n'
10.10.2.3: b'\n\n\x1b[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T02:06:30,467176660-04:00] STAGE: Bootstrap a new cluster...\x1b[0m\n# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:312 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.3 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults\n'
10.10.2.3: b'\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T02:06:30,475546582-04:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.3: b"\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T02:06:30,478597421-04:00] DEBUG: CEPHADM_PARAMS: ['bootstrap', '--config', '/tmp/bench-rados/ceph-research/scripts/ceph.conf', '--mon-ip', '10.10.2.3', '--output-dir', '/tmp/bench-rados/deployment_data_root/etc/ceph', '--ssh-user', 'ljishen', '--skip-dashboard', '--no-minimize-config', '--allow-overwrite', '--allow-fqdn-hostname', '--orphan-initial-daemons', '--skip-monitoring-stack', '--single-host-defaults']\x1b[0m\n# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.3 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults\n"
10.10.2.3: b'Verifying podman|docker is present...\nVerifying lvm2 is present...\n'
10.10.2.3: b'Verifying time synchronization is in place...\n'
10.10.2.3: b'Unit ntp.service is enabled and running\nRepeating the final host check...\npodman|docker (/usr/bin/docker) is present\n'
10.10.2.3: b'systemctl is present\nlvcreate is present\n'
10.10.2.3: b'Unit ntp.service is enabled and running\nHost looks OK\n'
10.10.2.3: b'Cluster fsid: 07779c24-36ec-11ec-b561-09a495c6360a\n'
10.10.2.3: b'Verifying IP 10.10.2.3 port 3300 ...\nVerifying IP 10.10.2.3 port 6789 ...\n'
10.10.2.3: b'Mon IP `10.10.2.3` is in CIDR network `10.10.2.0/24`\n- internal network (--cluster-network) has not been provided, OSD replication will default to the public_network\n'
10.10.2.3: b'Adjusting default settings to suit single-host cluster...\nPulling container image quay.io/ceph/ceph:v16...\n'
10.10.2.3: b'Ceph version: ceph version 16.2.6 (ee28fb57e47e9f88813e24bbf4c14496ca299d31) pacific (stable)\nExtracting ceph user uid/gid from container image...\n'
10.10.2.3: b'Creating initial keys...\n'
10.10.2.3: b'Creating initial monmap...\n'
10.10.2.3: b'Creating mon...\n'
10.10.2.3: b'Waiting for mon to start...\nWaiting for mon...\n'
10.10.2.3: b'mon is available\nSetting mon public_network to 10.10.2.0/24\n'
10.10.2.3: b'Wrote config to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf\nWrote keyring to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring\nCreating mgr...\n'
10.10.2.3: b'Verifying port 9283 ...\n'
10.10.2.3: b'Waiting for mgr to start...\nWaiting for mgr...\n'
10.10.2.3: b'mgr not available, waiting (1/15)...\n'
10.10.2.3: b'mgr not available, waiting (2/15)...\n'
10.10.2.3: b'mgr is available\n'
10.10.2.3: b'Enabling cephadm module...\n'
10.10.2.3: b'Waiting for the mgr to restart...\nWaiting for mgr epoch 5...\n'
10.10.2.3: b'mgr epoch 5 is available\nSetting orchestrator backend to cephadm...\n'
10.10.2.3: b'Generating ssh key...\n'
10.10.2.3: b'Wrote public SSH key to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub\nAdding key to ljishen@localhost authorized_keys...\n'
10.10.2.3: b'Adding host node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us...\n'
10.10.2.3: b'Deploying unmanaged mon service...\n'
10.10.2.3: b'Deploying unmanaged mgr service...\n'
10.10.2.3: b'You can access the Ceph CLI with:\n\n\tsudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --fsid 07779c24-36ec-11ec-b561-09a495c6360a -c /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf -k /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring\n\nPlease consider enabling telemetry to help improve Ceph:\n\n\tceph telemetry on\n\nFor more information see:\n\n\thttps://docs.ceph.com/docs/pacific/mgr/telemetry/\n\nBootstrap complete.\n'
10.10.2.3: b'\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T02:07:35,775528975-04:00] INFO: > Set the default number of monitor daemons to 1\x1b[0m\n# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:330 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1\n'
10.10.2.3: b'\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T02:07:35,784116618-04:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.3: b"\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T02:07:35,786991956-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mon', '1']\x1b[0m\n"
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1\n'
10.10.2.3: b'Inferring fsid 07779c24-36ec-11ec-b561-09a495c6360a\n'
10.10.2.3: b'Inferring config /var/lib/ceph/07779c24-36ec-11ec-b561-09a495c6360a/mon.node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.3: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.3: b'Scheduled mon update...\n'
10.10.2.3: b'\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T02:07:45,507091232-04:00] INFO: > Set the default number of manager daemons to 1\x1b[0m\n# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:335 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1\n'
10.10.2.3: b'\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T02:07:45,515646994-04:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.3: b"\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T02:07:45,518780008-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mgr', '1']\x1b[0m\n# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1\n"
10.10.2.3: b'Inferring fsid 07779c24-36ec-11ec-b561-09a495c6360a\n'
10.10.2.3: b'Inferring config /var/lib/ceph/07779c24-36ec-11ec-b561-09a495c6360a/mon.node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.3: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.3: b'Scheduled mgr update...\n'
10.10.2.3: b'\n\n\x1b[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T02:07:54,997214286-04:00] STAGE: Add hosts to the cluster...\x1b[0m\n'
10.10.2.3: b"\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T02:07:55,002008600-04:00] INFO: > Adding host 'sm1'\x1b[0m\n# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:362 - add_hosts() > ssh-copy-id -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -f -i /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub ljishen@sm1\n"
10.10.2.3: b'/usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: "/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub"\n'
10.10.2.3: b'\nNumber of key(s) added: 1\n\nNow try logging into the machine, with:   "ssh -o \'GlobalKnownHostsFile=/dev/null\' -o \'LogLevel=ERROR\' -o \'PasswordAuthentication=no\' -o \'StrictHostKeyChecking=no\' -o \'UserKnownHostsFile=/dev/null\' \'ljishen@sm1\'"\nand check to make sure that only the key(s) you wanted were added.\n\n'
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:368 - add_hosts() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add sm1 10.10.2.5\n'
10.10.2.3: b'\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T02:07:56,144226732-04:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.3: b"\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T02:07:56,147201047-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'host', 'add', 'sm1', '10.10.2.5']\x1b[0m\n"
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add sm1 10.10.2.5\n'
10.10.2.3: b'Inferring fsid 07779c24-36ec-11ec-b561-09a495c6360a\n'
10.10.2.3: b'Inferring config /var/lib/ceph/07779c24-36ec-11ec-b561-09a495c6360a/mon.node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.3: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.3: b"Added host 'sm1' with addr '10.10.2.5'\n"
10.10.2.3: b'\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T02:08:06,972164003-04:00] INFO: Sleep for 20 seconds\x1b[0m\n'
10.10.2.3: b'\n\n\x1b[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T02:08:26,976668272-04:00] STAGE: Add OSDs to the cluster...\x1b[0m\n'
10.10.2.3: b"\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T02:08:26,981932132-04:00] INFO: > Adding devices ['/dev/nvme0n1'] on host 'sm1' as OSDs\x1b[0m\n# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:387 - add_host_devices_as_osds() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd sm1:/dev/nvme0n1\n"
10.10.2.3: b'\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T02:08:26,990605586-04:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.3: b"\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T02:08:26,993570272-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'daemon', 'add', 'osd', 'sm1:/dev/nvme0n1']\x1b[0m\n"
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd sm1:/dev/nvme0n1\n'
10.10.2.3: b'Inferring fsid 07779c24-36ec-11ec-b561-09a495c6360a\n'
10.10.2.3: b'Inferring config /var/lib/ceph/07779c24-36ec-11ec-b561-09a495c6360a/mon.node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.3: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.3: b"Created osd(s) 0 on host 'sm1'\n"
10.10.2.3: b'\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T02:08:51,080725736-04:00] INFO: > Sleep for 20 seconds\x1b[0m\n'
10.10.2.3: b'\n\n\x1b[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T02:09:11,085313448-04:00] STAGE: Show Cluster Status\x1b[0m\n# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:343 - show_status() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status\n'
10.10.2.3: b'\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T02:09:11,094379091-04:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.3: b"\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T02:09:11,097539166-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', '--status']\x1b[0m\n"
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status\n'
10.10.2.3: b'Inferring fsid 07779c24-36ec-11ec-b561-09a495c6360a\n'
10.10.2.3: b'Inferring config /var/lib/ceph/07779c24-36ec-11ec-b561-09a495c6360a/mon.node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.3: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.3: b'  cluster:\n    id:     07779c24-36ec-11ec-b561-09a495c6360a\n    health: HEALTH_WARN\n            1 pool(s) have no replicas configured\n \n  services:\n    mon: 1 daemons, quorum node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us (age 2m)\n    mgr: node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us.gcpzvr(active, since 2m)\n    osd: 1 osds: 1 up (since 21s), 1 in (since 40s)\n \n  data:\n    pools:   1 pools, 128 pgs\n    objects: 0 objects, 0 B\n    usage:   4.7 MiB used, 100 GiB / 100 GiB avail\n    pgs:     99.219% pgs unknown\n             127 unknown\n             1   active+clean\n \n  progress:\n \n'
[1] 23:09:20 [SUCCESS] ljishen@10.10.2.3

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T02:06:14,671006882-04:00] INFO: > Remove the existing cluster[0m
## bash:13 -  > ls /var/lib/ceph
## bash:13 -  > tail -n1
# bash:13 -  > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm --host 10.10.2.3 --host 10.10.2.5 rm-cluster --force --fsid fd42eef8-36ea-11ec-b561-09a495c6360a

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T02:06:23,679267963-04:00] INFO: > Deploy a new cluster[0m
## bash:22 -  > /tmp/bench-rados/ceph-research/scripts/parallel-ssh --host 10.10.2.5 uname --nodename
## bash:22 -  > tail -n1
# bash:22 -  > osd_hostname=sm1
# bash:23 -  > /tmp/bench-rados/ceph-research/scripts/ceph-deploy -m 10.10.2.3 -o sm1/10.10.2.5:/dev/nvme0n1


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T02:06:24,801595242-04:00] STAGE: Verify passwordless SSH to hosts, passwordless sudo via ljishen, and the existence of devices on hosts...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T02:06:24,805380285-04:00] INFO: > Check the host for the monitor[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:181 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@10.10.2.3 sudo --non-interactive --validate

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T02:06:24,951588726-04:00] INFO: > Check hosts for OSDs[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T02:06:24,955343411-04:00] INFO: >> Check host 'sm1'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:200 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 sudo --non-interactive --validate

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T02:06:26,088170646-04:00] INFO: >>> Check device '/dev/nvme0n1'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:215 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 test -b /dev/nvme0n1
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:226 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 lsblk --nodeps --noheadings --output TYPE /dev/nvme0n1
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:227 - verify_parameters() > grep --word-regexp disk


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T02:06:28,208950408-04:00] STAGE: Reset devices on hosts...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T02:06:28,212852791-04:00] INFO: > Resetting device '/dev/nvme0n1' on host 'sm1'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:288 - reset_host_devices() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 sudo sh
  Removing ceph--5fc41a64--9f33--4915--993e--d6f3f8a2da25-osd--block--865c2c20--7a90--4623--b444--0af133c67471 (252:0)
  Archiving volume group "ceph-5fc41a64-9f33-4915-993e-d6f3f8a2da25" metadata (seqno 5).
  Releasing logical volume "osd-block-865c2c20-7a90-4623-b444-0af133c67471"
  Creating volume group backup "/etc/lvm/backup/ceph-5fc41a64-9f33-4915-993e-d6f3f8a2da25" (seqno 6).
  Logical volume "osd-block-865c2c20-7a90-4623-b444-0af133c67471" successfully removed
  Removing physical volume "/dev/nvme0n1" from volume group "ceph-5fc41a64-9f33-4915-993e-d6f3f8a2da25"
  Volume group "ceph-5fc41a64-9f33-4915-993e-d6f3f8a2da25" successfully removed


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T02:06:30,467176660-04:00] STAGE: Bootstrap a new cluster...[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:312 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.3 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T02:06:30,475546582-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T02:06:30,478597421-04:00] DEBUG: CEPHADM_PARAMS: ['bootstrap', '--config', '/tmp/bench-rados/ceph-research/scripts/ceph.conf', '--mon-ip', '10.10.2.3', '--output-dir', '/tmp/bench-rados/deployment_data_root/etc/ceph', '--ssh-user', 'ljishen', '--skip-dashboard', '--no-minimize-config', '--allow-overwrite', '--allow-fqdn-hostname', '--orphan-initial-daemons', '--skip-monitoring-stack', '--single-host-defaults'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.3 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults
Verifying podman|docker is present...
Verifying lvm2 is present...
Verifying time synchronization is in place...
Unit ntp.service is enabled and running
Repeating the final host check...
podman|docker (/usr/bin/docker) is present
systemctl is present
lvcreate is present
Unit ntp.service is enabled and running
Host looks OK
Cluster fsid: 07779c24-36ec-11ec-b561-09a495c6360a
Verifying IP 10.10.2.3 port 3300 ...
Verifying IP 10.10.2.3 port 6789 ...
Mon IP `10.10.2.3` is in CIDR network `10.10.2.0/24`
- internal network (--cluster-network) has not been provided, OSD replication will default to the public_network
Adjusting default settings to suit single-host cluster...
Pulling container image quay.io/ceph/ceph:v16...
Ceph version: ceph version 16.2.6 (ee28fb57e47e9f88813e24bbf4c14496ca299d31) pacific (stable)
Extracting ceph user uid/gid from container image...
Creating initial keys...
Creating initial monmap...
Creating mon...
Waiting for mon to start...
Waiting for mon...
mon is available
Setting mon public_network to 10.10.2.0/24
Wrote config to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf
Wrote keyring to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring
Creating mgr...
Verifying port 9283 ...
Waiting for mgr to start...
Waiting for mgr...
mgr not available, waiting (1/15)...
mgr not available, waiting (2/15)...
mgr is available
Enabling cephadm module...
Waiting for the mgr to restart...
Waiting for mgr epoch 5...
mgr epoch 5 is available
Setting orchestrator backend to cephadm...
Generating ssh key...
Wrote public SSH key to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub
Adding key to ljishen@localhost authorized_keys...
Adding host node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us...
Deploying unmanaged mon service...
Deploying unmanaged mgr service...
You can access the Ceph CLI with:

	sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --fsid 07779c24-36ec-11ec-b561-09a495c6360a -c /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf -k /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring

Please consider enabling telemetry to help improve Ceph:

	ceph telemetry on

For more information see:

	https://docs.ceph.com/docs/pacific/mgr/telemetry/

Bootstrap complete.

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T02:07:35,775528975-04:00] INFO: > Set the default number of monitor daemons to 1[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:330 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T02:07:35,784116618-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T02:07:35,786991956-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mon', '1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1
Inferring fsid 07779c24-36ec-11ec-b561-09a495c6360a
Inferring config /var/lib/ceph/07779c24-36ec-11ec-b561-09a495c6360a/mon.node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Scheduled mon update...

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T02:07:45,507091232-04:00] INFO: > Set the default number of manager daemons to 1[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:335 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T02:07:45,515646994-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T02:07:45,518780008-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mgr', '1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1
Inferring fsid 07779c24-36ec-11ec-b561-09a495c6360a
Inferring config /var/lib/ceph/07779c24-36ec-11ec-b561-09a495c6360a/mon.node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Scheduled mgr update...


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T02:07:54,997214286-04:00] STAGE: Add hosts to the cluster...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T02:07:55,002008600-04:00] INFO: > Adding host 'sm1'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:362 - add_hosts() > ssh-copy-id -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -f -i /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub ljishen@sm1
/usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: "/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub"

Number of key(s) added: 1

Now try logging into the machine, with:   "ssh -o 'GlobalKnownHostsFile=/dev/null' -o 'LogLevel=ERROR' -o 'PasswordAuthentication=no' -o 'StrictHostKeyChecking=no' -o 'UserKnownHostsFile=/dev/null' 'ljishen@sm1'"
and check to make sure that only the key(s) you wanted were added.

# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:368 - add_hosts() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add sm1 10.10.2.5
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T02:07:56,144226732-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T02:07:56,147201047-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'host', 'add', 'sm1', '10.10.2.5'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add sm1 10.10.2.5
Inferring fsid 07779c24-36ec-11ec-b561-09a495c6360a
Inferring config /var/lib/ceph/07779c24-36ec-11ec-b561-09a495c6360a/mon.node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Added host 'sm1' with addr '10.10.2.5'

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T02:08:06,972164003-04:00] INFO: Sleep for 20 seconds[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T02:08:26,976668272-04:00] STAGE: Add OSDs to the cluster...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T02:08:26,981932132-04:00] INFO: > Adding devices ['/dev/nvme0n1'] on host 'sm1' as OSDs[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:387 - add_host_devices_as_osds() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd sm1:/dev/nvme0n1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T02:08:26,990605586-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T02:08:26,993570272-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'daemon', 'add', 'osd', 'sm1:/dev/nvme0n1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd sm1:/dev/nvme0n1
Inferring fsid 07779c24-36ec-11ec-b561-09a495c6360a
Inferring config /var/lib/ceph/07779c24-36ec-11ec-b561-09a495c6360a/mon.node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Created osd(s) 0 on host 'sm1'

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T02:08:51,080725736-04:00] INFO: > Sleep for 20 seconds[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T02:09:11,085313448-04:00] STAGE: Show Cluster Status[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:343 - show_status() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T02:09:11,094379091-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T02:09:11,097539166-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', '--status'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status
Inferring fsid 07779c24-36ec-11ec-b561-09a495c6360a
Inferring config /var/lib/ceph/07779c24-36ec-11ec-b561-09a495c6360a/mon.node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
  cluster:
    id:     07779c24-36ec-11ec-b561-09a495c6360a
    health: HEALTH_WARN
            1 pool(s) have no replicas configured
 
  services:
    mon: 1 daemons, quorum node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us (age 2m)
    mgr: node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us.gcpzvr(active, since 2m)
    osd: 1 osds: 1 up (since 21s), 1 in (since 40s)
 
  data:
    pools:   1 pools, 128 pgs
    objects: 0 objects, 0 B
    usage:   4.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     99.219% pgs unknown
             127 unknown
             1   active+clean
 
  progress:
 

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:09:20,306862743-07:00] INFO: > Copy configuration files to local[0m
# ./benchmarks/bench-rados:208 - launch_ceph_cluster() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.3:/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf
# ./benchmarks/bench-rados:212 - launch_ceph_cluster() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.3 sudo chmod 644 /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:09:20,458544230-07:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.3 sudo chmod 644 /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring
[1] 23:09:20 [SUCCESS] ljishen@10.10.2.3
# ./benchmarks/bench-rados:214 - launch_ceph_cluster() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.3:/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:09:20,789983211-07:00] STAGE: Update local ceph.conf as a client...[0m


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:09:20,793672246-07:00] STAGE: Configure Ceph pool...[0m
# ./benchmarks/bench-rados:245 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_max_backfills 32
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:09:20,818749384-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:09:20,822137102-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '07779c24-36ec-11ec-b561-09a495c6360a', '--', 'ceph', 'config', 'set', 'osd', 'osd_max_backfills', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 07779c24-36ec-11ec-b561-09a495c6360a -- ceph config set osd osd_max_backfills 32
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:246 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_active 32
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:09:23,975489546-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:09:23,979035762-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '07779c24-36ec-11ec-b561-09a495c6360a', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_active', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 07779c24-36ec-11ec-b561-09a495c6360a -- ceph config set osd osd_recovery_max_active 32
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:247 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_single_start 8
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:09:27,259662727-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:09:27,263128982-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '07779c24-36ec-11ec-b561-09a495c6360a', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_single_start', '8'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 07779c24-36ec-11ec-b561-09a495c6360a -- ceph config set osd osd_recovery_max_single_start 8
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:248 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_op_priority 63
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:09:30,393872609-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:09:30,397935025-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '07779c24-36ec-11ec-b561-09a495c6360a', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_op_priority', '63'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 07779c24-36ec-11ec-b561-09a495c6360a -- ceph config set osd osd_recovery_op_priority 63
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:250 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config show-with-defaults osd.0
# ./benchmarks/bench-rados:251 - configure_ceph_pool() > grep 'osd_max_backfills\|osd_recovery'
# ./benchmarks/bench-rados:251 - configure_ceph_pool() > column -t -s ' '
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
osd_max_backfills                        32         mon
osd_recovery_cost                        20971520   default
osd_recovery_delay_start                 0.000000   default
osd_recovery_max_active                  32         mon
osd_recovery_max_active_hdd              3          default
osd_recovery_max_active_ssd              10         default
osd_recovery_max_chunk                   8388608    default
osd_recovery_max_omap_entries_per_chunk  8096       default
osd_recovery_max_single_start            8          mon
osd_recovery_op_priority                 63         mon
osd_recovery_op_warn_multiple            16         default
osd_recovery_priority                    5          default
osd_recovery_retry_interval              30.000000  default
osd_recovery_sleep                       0.000000   default
osd_recovery_sleep_hdd                   0.100000   default
osd_recovery_sleep_hybrid                0.025000   default
osd_recovery_sleep_ssd                   0.000000   default
# ./benchmarks/bench-rados:253 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool create bench_rados 128 128 replicated --size 1 --pg-num-min 128
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:09:37,086223713-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:09:37,089732669-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '07779c24-36ec-11ec-b561-09a495c6360a', '--', 'ceph', 'osd', 'pool', 'create', 'bench_rados', '128', '128', 'replicated', '--size', '1', '--pg-num-min', '128'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 07779c24-36ec-11ec-b561-09a495c6360a -- ceph osd pool create bench_rados 128 128 replicated --size 1 --pg-num-min 128
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
pool 'bench_rados' created
# ./benchmarks/bench-rados:255 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados min_size 1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:09:41,387729551-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:09:41,391412534-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '07779c24-36ec-11ec-b561-09a495c6360a', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'min_size', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 07779c24-36ec-11ec-b561-09a495c6360a -- ceph osd pool set bench_rados min_size 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 min_size to 1
# ./benchmarks/bench-rados:256 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados pg_autoscale_mode off
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:09:45,544714399-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:09:45,547850594-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '07779c24-36ec-11ec-b561-09a495c6360a', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'pg_autoscale_mode', 'off'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 07779c24-36ec-11ec-b561-09a495c6360a -- ceph osd pool set bench_rados pg_autoscale_mode off
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 pg_autoscale_mode to off
# ./benchmarks/bench-rados:257 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool application enable bench_rados benchmark
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:09:49,947464916-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:09:49,950979523-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '07779c24-36ec-11ec-b561-09a495c6360a', '--', 'ceph', 'osd', 'pool', 'application', 'enable', 'bench_rados', 'benchmark'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 07779c24-36ec-11ec-b561-09a495c6360a -- ceph osd pool application enable bench_rados benchmark
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
enabled application 'benchmark' on pool 'bench_rados'
# ./benchmarks/bench-rados:258 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados noscrub 1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:09:54,129920082-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:09:54,134207551-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '07779c24-36ec-11ec-b561-09a495c6360a', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'noscrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 07779c24-36ec-11ec-b561-09a495c6360a -- ceph osd pool set bench_rados noscrub 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 noscrub to 1
# ./benchmarks/bench-rados:259 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados nodeep-scrub 1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:09:58,446724065-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:09:58,450116201-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '07779c24-36ec-11ec-b561-09a495c6360a', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'nodeep-scrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 07779c24-36ec-11ec-b561-09a495c6360a -- ceph osd pool set bench_rados nodeep-scrub 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 nodeep-scrub to 1
# ./benchmarks/bench-rados:260 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool ls detail
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:10:02,905355503-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:10:02,908877985-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '07779c24-36ec-11ec-b561-09a495c6360a', '--', 'ceph', 'osd', 'pool', 'ls', 'detail'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 07779c24-36ec-11ec-b561-09a495c6360a -- ceph osd pool ls detail
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
pool 1 'device_health_metrics' replicated size 1 min_size 1 crush_rule 0 object_hash rjenkins pg_num 128 pgp_num 128 autoscale_mode on last_change 13 lfor 0/0/11 flags hashpspool stripe_width 0 pg_num_min 1 application mgr_devicehealth
pool 2 'bench_rados' replicated size 1 min_size 1 crush_rule 0 object_hash rjenkins pg_num 128 pgp_num 128 autoscale_mode off last_change 21 flags hashpspool,noscrub,nodeep-scrub stripe_width 0 pg_num_min 128 application benchmark

# ./benchmarks/bench-rados:261 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd df tree
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:10:06,097220741-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:10:06,100522017-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '07779c24-36ec-11ec-b561-09a495c6360a', '--', 'ceph', 'osd', 'df', 'tree'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 07779c24-36ec-11ec-b561-09a495c6360a -- ceph osd df tree
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
ID  CLASS  WEIGHT   REWEIGHT  SIZE     RAW USE  DATA     OMAP  META     AVAIL    %USE  VAR   PGS  STATUS  TYPE NAME    
-1         0.09769         -  100 GiB  6.0 MiB  176 KiB   0 B  5.8 MiB  100 GiB  0.01  1.00    -          root default 
-3         0.09769         -  100 GiB  6.0 MiB  176 KiB   0 B  5.8 MiB  100 GiB  0.01  1.00    -              host sm1 
 0    ssd  0.09769   1.00000  100 GiB  6.0 MiB  176 KiB   0 B  5.8 MiB  100 GiB  0.01  1.00  256      up          osd.0
                       TOTAL  100 GiB  6.0 MiB  176 KiB   0 B  5.8 MiB  100 GiB  0.01                                  
MIN/MAX VAR: 1.00/1.00  STDDEV: 0


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:10:09,399440445-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:10:32,398006807-07:00] INFO: >>   data:
    pools:   2 pools, 256 pgs
    objects: 0 objects, 0 B
    usage:   6.0 MiB used, 100 GiB / 100 GiB avail
    pgs:     256 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:10:32,406458193-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:10:40,630906526-07:00] INFO: >>   data:
    pools:   2 pools, 256 pgs
    objects: 0 objects, 0 B
    usage:   6.0 MiB used, 100 GiB / 100 GiB avail
    pgs:     256 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:10:40,639388570-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:10:48,879004655-07:00] INFO: >>   data:
    pools:   2 pools, 256 pgs
    objects: 0 objects, 0 B
    usage:   6.0 MiB used, 100 GiB / 100 GiB avail
    pgs:     256 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:10:48,886918501-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:10:57,229550517-07:00] INFO: >>   data:
    pools:   2 pools, 256 pgs
    objects: 0 objects, 0 B
    usage:   6.0 MiB used, 100 GiB / 100 GiB avail
    pgs:     256 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:10:57,238269336-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:11:05,544423734-07:00] INFO: >>   data:
    pools:   2 pools, 256 pgs
    objects: 0 objects, 0 B
    usage:   6.0 MiB used, 100 GiB / 100 GiB avail
    pgs:     256 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:11:05,552504764-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:11:05,559763017-07:00] INFO: > The cluster is idle now.[0m


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:11:05,563340923-07:00] STAGE: Start benchmarking of mode write ...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:11:05,571516450-07:00] INFO: > Start system activity collection services[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:11:05,578292507-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:329 - rados_bench() > rm --force /mnt/sda8/ceph-research/scripts/output/sys_activity_1MB_write.dat.rados_bench_host.3
# ./benchmarks/bench-rados:330 - rados_bench() > sar_pid_rados_bench_host=863426
# ./benchmarks/bench-rados:330 - rados_bench() > S_TIME_FORMAT=ISO
# ./benchmarks/bench-rados:330 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/output/sys_activity_1MB_write.dat.rados_bench_host.3 2

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:11:05,585970921-07:00] INFO: >> for OSD host[0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:11:05,594874958-07:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.5 bash
10.10.2.5: b'107593\n'
[1] 23:11:06 [SUCCESS] ljishen@10.10.2.5
107593

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:11:06,760652899-07:00] INFO: > Run rados bench[0m
# ./benchmarks/bench-rados:358 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/output/rados_bench_1MB_write.log.3
## ./benchmarks/bench-rados:349 - rados_bench() > count_cpus 32-63,96-127
# ./benchmarks/bench-rados:349 - rados_bench() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 30 write --pool bench_rados -b 1048576 -O 1048576 --concurrent-ios 128 --show-time --write-object --write-omap --write-xattr --no-cleanup
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:11:06,782126303-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:11:06,785846867-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '07779c24-36ec-11ec-b561-09a495c6360a', '--', 'rados', 'bench', '30', 'write', '--pool', 'bench_rados', '-b', '1048576', '-O', '1048576', '--concurrent-ios', '128', '--show-time', '--write-object', '--write-omap', '--write-xattr', '--no-cleanup'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 07779c24-36ec-11ec-b561-09a495c6360a -- rados bench 30 write --pool bench_rados -b 1048576 -O 1048576 --concurrent-ios 128 --show-time --write-object --write-omap --write-xattr --no-cleanup
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
hints = 1
2021-10-27T06:11:09.237804+0000 Maintaining 128 concurrent writes of 1048576 bytes to objects of size 1048576 for up to 30 seconds or 0 objects
2021-10-27T06:11:09.237815+0000 Object prefix: benchmark_data_node-0.bluefield2.ucsc-cmps10_7
2021-10-27T06:11:09.265985+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-27T06:11:09.265985+0000     0       0         0         0         0         0           -           0
2021-10-27T06:11:10.266084+0000     1      81        81         0         0         0           -           0
2021-10-27T06:11:11.266154+0000     2     127       158        31   15.4991      15.5     1.97717     1.82206
2021-10-27T06:11:12.266223+0000     3     127       220        93   30.9981        62     1.74719     1.88544
2021-10-27T06:11:13.266291+0000     4     127       283       156   38.9975        63     2.01643     1.90637
2021-10-27T06:11:14.266398+0000     5     127       355       228   45.5967        72     1.78565     1.90664
2021-10-27T06:11:15.266465+0000     6     127       424       297   49.4965        69      1.8369     1.91185
2021-10-27T06:11:16.266699+0000     7     127       490       363   51.8523        66     1.74578     1.91207
2021-10-27T06:11:17.266766+0000     8     127       544       417   52.1203        54     1.92281     1.91372
2021-10-27T06:11:18.266990+0000     9     127       575       448   49.7725        31     2.47871     1.94721
2021-10-27T06:11:19.267206+0000    10     127       610       483   48.2944        35     3.04281      2.0197
2021-10-27T06:11:20.267299+0000    11     127       642       515   46.8128        32     3.60076     2.11138
2021-10-27T06:11:21.267367+0000    12     127       666       539   44.9117        24     4.00524     2.19416
2021-10-27T06:11:22.267593+0000    13     127       700       573   44.0717        34     4.07992     2.30916
2021-10-27T06:11:23.267662+0000    14     127       729       602    42.995        29     3.99474     2.39238
2021-10-27T06:11:24.267729+0000    15     127       780       653   43.5284        51     3.35928     2.50579
2021-10-27T06:11:25.267796+0000    16     127       828       701   43.8077        48     2.94932     2.55326
2021-10-27T06:11:26.267873+0000    17     127       911       784   46.1127        83     1.90072     2.52192
2021-10-27T06:11:27.267945+0000    18     127       978       851   47.2728        67     1.96181     2.47431
2021-10-27T06:11:28.268013+0000    19     127      1038       911   47.9424        60     1.95167     2.44103
2021-10-27T06:11:29.268082+0000 min lat: 1.54318 max lat: 4.66719 avg lat: 2.42866
2021-10-27T06:11:29.268082+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-27T06:11:29.268082+0000    20     127      1068       941   47.0452        30     2.04572     2.42866
2021-10-27T06:11:30.268323+0000    21     127      1099       972   46.2807        31     2.97066     2.43744
2021-10-27T06:11:31.268392+0000    22     127      1121       994    45.177        22     3.63365     2.46593
2021-10-27T06:11:32.268459+0000    23     127      1131      1004   43.6476        10     4.04197     2.48333
2021-10-27T06:11:33.268685+0000    24     127      1152      1025   42.7036        21     4.79485     2.53223
2021-10-27T06:11:34.268761+0000    25     127      1162      1035   41.3955        10     5.25203     2.55955
2021-10-27T06:11:35.268964+0000    26     127      1196      1069   41.1108        34     6.28515     2.67405
2021-10-27T06:11:36.269032+0000    27     127      1212      1085   40.1807        16     6.74555     2.73411
2021-10-27T06:11:37.269102+0000    28     127      1229      1102   39.3528        17      5.8262     2.79649
2021-10-27T06:11:38.269286+0000    29     127      1248      1121   38.6508        19     6.80576     2.86439
2021-10-27T06:11:39.269353+0000    30     127      1282      1155   38.4957        34     5.81765     2.97139
2021-10-27T06:11:40.269581+0000 Total time run:         30.9016
Total writes made:      1283
Write size:             1048576
Object size:            1048576
Bandwidth (MB/sec):     41.5189
Stddev Bandwidth:       21.8602
Max bandwidth (MB/sec): 83
Min bandwidth (MB/sec): 0
Average IOPS:           41
Stddev IOPS:            21.8782
Max IOPS:               83
Min IOPS:               0
Average Latency(s):     2.98556
Stddev Latency(s):      1.52429
Max latency(s):         7.52525
Min latency(s):         0.604029

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:11:40,835259070-07:00] INFO: > Stop system activity collection services[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:11:40,842040617-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:379 - rados_bench() > kill -INT 863426

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:11:40,849028863-07:00] INFO: >> for OSD host[0m
# ./benchmarks/bench-rados:384 - rados_bench() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.5 kill -INT 107593
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:11:40,857513441-07:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.5 kill -INT 107593
[1] 23:11:42 [SUCCESS] ljishen@10.10.2.5

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:11:42,144489380-07:00] INFO: >>> Copy system activity file from OSD host[0m
## ./benchmarks/bench-rados:391 - rados_bench() > basename -- /tmp/bench-rados/sys_activity_1MB_write.dat.osd_host.3
# ./benchmarks/bench-rados:391 - rados_bench() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.5:/tmp/bench-rados/sys_activity_1MB_write.dat.osd_host.3 /mnt/sda8/ceph-research/scripts/output/sys_activity_1MB_write.dat.osd_host.3


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:11:43,410805426-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:12:06,546377925-07:00] INFO: >>   data:
    pools:   2 pools, 256 pgs
    objects: 1.28k objects, 1.3 GiB
    usage:   3.8 GiB used, 96 GiB / 100 GiB avail
    pgs:     256 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:12:06,555012284-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:12:14,711616369-07:00] INFO: >>   data:
    pools:   2 pools, 256 pgs
    objects: 1.28k objects, 1.3 GiB
    usage:   3.8 GiB used, 96 GiB / 100 GiB avail
    pgs:     256 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:12:14,720109042-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:12:22,876258132-07:00] INFO: >>   data:
    pools:   2 pools, 256 pgs
    objects: 1.28k objects, 1.3 GiB
    usage:   3.8 GiB used, 96 GiB / 100 GiB avail
    pgs:     256 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:12:22,885642261-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:12:31,132815777-07:00] INFO: >>   data:
    pools:   2 pools, 256 pgs
    objects: 1.28k objects, 1.3 GiB
    usage:   3.8 GiB used, 96 GiB / 100 GiB avail
    pgs:     256 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:12:31,141302328-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:12:39,449121728-07:00] INFO: >>   data:
    pools:   2 pools, 256 pgs
    objects: 1.28k objects, 1.3 GiB
    usage:   3.8 GiB used, 96 GiB / 100 GiB avail
    pgs:     256 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:12:39,457316521-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:12:39,463692867-07:00] INFO: > The cluster is idle now.[0m


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:12:39,467112845-07:00] STAGE: Start benchmarking of mode seq ...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:12:39,474933545-07:00] INFO: > Start system activity collection services[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:12:39,480933383-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:329 - rados_bench() > rm --force /mnt/sda8/ceph-research/scripts/output/sys_activity_1MB_seq.dat.rados_bench_host.3
# ./benchmarks/bench-rados:330 - rados_bench() > sar_pid_rados_bench_host=865233
# ./benchmarks/bench-rados:330 - rados_bench() > S_TIME_FORMAT=ISO
# ./benchmarks/bench-rados:330 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/output/sys_activity_1MB_seq.dat.rados_bench_host.3 2

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:12:39,488900137-07:00] INFO: >> for OSD host[0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:12:39,497127051-07:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.5 bash
10.10.2.5: b'108124\n'
[1] 23:12:40 [SUCCESS] ljishen@10.10.2.5
108124

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:12:40,613058927-07:00] INFO: > Run rados bench[0m
# ./benchmarks/bench-rados:367 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/output/rados_bench_1MB_seq.log.3
## ./benchmarks/bench-rados:364 - rados_bench() > count_cpus 32-63,96-127
# ./benchmarks/bench-rados:364 - rados_bench() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 128 --show-time
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:12:40,635087193-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:12:40,638308408-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '07779c24-36ec-11ec-b561-09a495c6360a', '--', 'rados', 'bench', '99999999', 'seq', '--pool', 'bench_rados', '--concurrent-ios', '128', '--show-time'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 07779c24-36ec-11ec-b561-09a495c6360a -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 128 --show-time
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
hints = 1
2021-10-27T06:12:43.231466+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-27T06:12:43.231466+0000     0       1         1         0         0         0           -           0
2021-10-27T06:12:44.231560+0000     1     127       956       829   828.862       829    0.137338     0.14172
2021-10-27T06:12:45.231755+0000 Total time run:       1.54522
Total reads made:     1283
Read size:            1048576
Object size:          1048576
Bandwidth (MB/sec):   830.301
Average IOPS:         830
Stddev IOPS:          0
Max IOPS:             829
Min IOPS:             829
Average Latency(s):   0.146382
Max latency(s):       0.304214
Min latency(s):       0.0707694

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:12:45,795239186-07:00] INFO: > Stop system activity collection services[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:12:45,802024911-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:379 - rados_bench() > kill -INT 865233

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:12:45,808733511-07:00] INFO: >> for OSD host[0m
# ./benchmarks/bench-rados:384 - rados_bench() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.5 kill -INT 108124
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:12:45,816786678-07:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.5 kill -INT 108124
[1] 23:12:46 [SUCCESS] ljishen@10.10.2.5

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:12:46,925969829-07:00] INFO: >>> Copy system activity file from OSD host[0m
## ./benchmarks/bench-rados:391 - rados_bench() > basename -- /tmp/bench-rados/sys_activity_1MB_seq.dat.osd_host.3
# ./benchmarks/bench-rados:391 - rados_bench() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.5:/tmp/bench-rados/sys_activity_1MB_seq.dat.osd_host.3 /mnt/sda8/ceph-research/scripts/output/sys_activity_1MB_seq.dat.osd_host.3


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:12:48,005397530-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:13:11,166172771-07:00] INFO: >>   data:
    pools:   2 pools, 256 pgs
    objects: 1.28k objects, 1.3 GiB
    usage:   3.8 GiB used, 96 GiB / 100 GiB avail
    pgs:     256 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:13:11,174665615-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:13:19,413913528-07:00] INFO: >>   data:
    pools:   2 pools, 256 pgs
    objects: 1.28k objects, 1.3 GiB
    usage:   3.8 GiB used, 96 GiB / 100 GiB avail
    pgs:     256 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:13:19,422347141-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:13:27,597462978-07:00] INFO: >>   data:
    pools:   2 pools, 256 pgs
    objects: 1.28k objects, 1.3 GiB
    usage:   3.8 GiB used, 96 GiB / 100 GiB avail
    pgs:     256 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:13:27,606578282-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:13:36,051989011-07:00] INFO: >>   data:
    pools:   2 pools, 256 pgs
    objects: 1.28k objects, 1.3 GiB
    usage:   3.8 GiB used, 96 GiB / 100 GiB avail
    pgs:     256 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:13:36,060646755-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:13:44,426704659-07:00] INFO: >>   data:
    pools:   2 pools, 256 pgs
    objects: 1.28k objects, 1.3 GiB
    usage:   3.8 GiB used, 96 GiB / 100 GiB avail
    pgs:     256 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:13:44,435009750-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:13:44,441424488-07:00] INFO: > The cluster is idle now.[0m


[1;7;39;49m[2021-10-26T23:13:44,445470022-07:00][RUNNING][ROUND 1/6/21] object_size=4MB[0m


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:13:44,449721493-07:00] STAGE: Launch a Ceph cluster (CEPH_HOST: 10.10.2.3, OSD_HOST: 10.10.2.5) ...[0m
# ./benchmarks/bench-rados:172 - launch_ceph_cluster() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.3 bash -euo pipefail
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:13:44,458052633-07:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.3 bash -euo pipefail
10.10.2.3: b'\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T02:13:44,874829917-04:00] INFO: > Remove the existing cluster\x1b[0m\n'
10.10.2.3: b'## bash:13 -  > ls /var/lib/ceph\n## bash:13 -  > tail -n1\n'
10.10.2.3: b'# bash:13 -  > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm --host 10.10.2.3 --host 10.10.2.5 rm-cluster --force --fsid 07779c24-36ec-11ec-b561-09a495c6360a\n'
10.10.2.3: b'\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T02:13:53,832249798-04:00] INFO: > Deploy a new cluster\x1b[0m\n'
10.10.2.3: b'## bash:22 -  > /tmp/bench-rados/ceph-research/scripts/parallel-ssh --host 10.10.2.5 uname --nodename\n## bash:22 -  > tail -n1\n'
10.10.2.3: b'# bash:22 -  > osd_hostname=sm1\n# bash:23 -  > /tmp/bench-rados/ceph-research/scripts/ceph-deploy -m 10.10.2.3 -o sm1/10.10.2.5:/dev/nvme0n1\n'
10.10.2.3: b'\n\n\x1b[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T02:13:54,957722296-04:00] STAGE: Verify passwordless SSH to hosts, passwordless sudo via ljishen, and the existence of devices on hosts...\x1b[0m\n'
10.10.2.3: b'\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T02:13:54,961371452-04:00] INFO: > Check the host for the monitor\x1b[0m\n'
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:181 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@10.10.2.3 sudo --non-interactive --validate\n'
10.10.2.3: b'\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T02:13:55,107801522-04:00] INFO: > Check hosts for OSDs\x1b[0m\n'
10.10.2.3: b"\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T02:13:55,111697193-04:00] INFO: >> Check host 'sm1'\x1b[0m\n# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:200 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 sudo --non-interactive --validate\n"
10.10.2.3: b"\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T02:13:56,188793311-04:00] INFO: >>> Check device '/dev/nvme0n1'\x1b[0m\n"
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:215 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 test -b /dev/nvme0n1\n'
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:226 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 lsblk --nodeps --noheadings --output TYPE /dev/nvme0n1\n'
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:227 - verify_parameters() > grep --word-regexp disk\n'
10.10.2.3: b'\n\n\x1b[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T02:13:58,333057109-04:00] STAGE: Reset devices on hosts...\x1b[0m\n'
10.10.2.3: b"\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T02:13:58,336758864-04:00] INFO: > Resetting device '/dev/nvme0n1' on host 'sm1'\x1b[0m\n# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:288 - reset_host_devices() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 sudo sh\n"
10.10.2.3: b'  Removing ceph--a1a58707--ab39--4eb7--ac61--5a0408d59087-osd--block--1c713cb6--0c75--4cb6--b132--790759aac5b0 (252:0)\n'
10.10.2.3: b'  Archiving volume group "ceph-a1a58707-ab39-4eb7-ac61-5a0408d59087" metadata (seqno 5).\n'
10.10.2.3: b'  Releasing logical volume "osd-block-1c713cb6-0c75-4cb6-b132-790759aac5b0"\n'
10.10.2.3: b'  Creating volume group backup "/etc/lvm/backup/ceph-a1a58707-ab39-4eb7-ac61-5a0408d59087" (seqno 6).\n'
10.10.2.3: b'  Logical volume "osd-block-1c713cb6-0c75-4cb6-b132-790759aac5b0" successfully removed\n  Removing physical volume "/dev/nvme0n1" from volume group "ceph-a1a58707-ab39-4eb7-ac61-5a0408d59087"\n'
10.10.2.3: b'  Volume group "ceph-a1a58707-ab39-4eb7-ac61-5a0408d59087" successfully removed\n'
10.10.2.3: b'\n\n\x1b[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T02:14:00,613068406-04:00] STAGE: Bootstrap a new cluster...\x1b[0m\n'
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:312 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.3 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults\n'
10.10.2.3: b'\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T02:14:00,621669363-04:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.3: b"\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T02:14:00,624486291-04:00] DEBUG: CEPHADM_PARAMS: ['bootstrap', '--config', '/tmp/bench-rados/ceph-research/scripts/ceph.conf', '--mon-ip', '10.10.2.3', '--output-dir', '/tmp/bench-rados/deployment_data_root/etc/ceph', '--ssh-user', 'ljishen', '--skip-dashboard', '--no-minimize-config', '--allow-overwrite', '--allow-fqdn-hostname', '--orphan-initial-daemons', '--skip-monitoring-stack', '--single-host-defaults']\x1b[0m\n"
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.3 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults\n'
10.10.2.3: b'Verifying podman|docker is present...\nVerifying lvm2 is present...\n'
10.10.2.3: b'Verifying time synchronization is in place...\n'
10.10.2.3: b'Unit ntp.service is enabled and running\n'
10.10.2.3: b'Repeating the final host check...\npodman|docker (/usr/bin/docker) is present\n'
10.10.2.3: b'systemctl is present\n'
10.10.2.3: b'lvcreate is present\n'
10.10.2.3: b'Unit ntp.service is enabled and running\nHost looks OK\n'
10.10.2.3: b'Cluster fsid: 13c63ffc-36ed-11ec-b561-09a495c6360a\n'
10.10.2.3: b'Verifying IP 10.10.2.3 port 3300 ...\n'
10.10.2.3: b'Verifying IP 10.10.2.3 port 6789 ...\n'
10.10.2.3: b'Mon IP `10.10.2.3` is in CIDR network `10.10.2.0/24`\n- internal network (--cluster-network) has not been provided, OSD replication will default to the public_network\n'
10.10.2.3: b'Adjusting default settings to suit single-host cluster...\nPulling container image quay.io/ceph/ceph:v16...\n'
10.10.2.3: b'Ceph version: ceph version 16.2.6 (ee28fb57e47e9f88813e24bbf4c14496ca299d31) pacific (stable)\nExtracting ceph user uid/gid from container image...\n'
10.10.2.3: b'Creating initial keys...\n'
10.10.2.3: b'Creating initial monmap...\n'
10.10.2.3: b'Creating mon...\n'
10.10.2.3: b'Waiting for mon to start...\nWaiting for mon...\n'
10.10.2.3: b'mon is available\nSetting mon public_network to 10.10.2.0/24\n'
10.10.2.3: b'Wrote config to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf\nWrote keyring to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring\nCreating mgr...\n'
10.10.2.3: b'Verifying port 9283 ...\n'
10.10.2.3: b'Waiting for mgr to start...\nWaiting for mgr...\n'
10.10.2.3: b'mgr not available, waiting (1/15)...\n'
10.10.2.3: b'mgr not available, waiting (2/15)...\n'
10.10.2.3: b'mgr is available\n'
10.10.2.3: b'Enabling cephadm module...\n'
10.10.2.3: b'Waiting for the mgr to restart...\nWaiting for mgr epoch 5...\n'
10.10.2.3: b'mgr epoch 5 is available\nSetting orchestrator backend to cephadm...\n'
10.10.2.3: b'Generating ssh key...\n'
10.10.2.3: b'Wrote public SSH key to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub\n'
10.10.2.3: b'Adding key to ljishen@localhost authorized_keys...\n'
10.10.2.3: b'Adding host node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us...\n'
10.10.2.3: b'Deploying unmanaged mon service...\n'
10.10.2.3: b'Deploying unmanaged mgr service...\n'
10.10.2.3: b'You can access the Ceph CLI with:\n\n\tsudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --fsid 13c63ffc-36ed-11ec-b561-09a495c6360a -c /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf -k /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring\n\nPlease consider enabling telemetry to help improve Ceph:\n\n\tceph telemetry on\n\nFor more information see:\n\n\thttps://docs.ceph.com/docs/pacific/mgr/telemetry/\n\nBootstrap complete.\n'
10.10.2.3: b'\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T02:15:05,299658324-04:00] INFO: > Set the default number of monitor daemons to 1\x1b[0m\n# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:330 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1\n'
10.10.2.3: b'\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T02:15:05,308065225-04:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.3: b"\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T02:15:05,311050390-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mon', '1']\x1b[0m\n"
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1\n'
10.10.2.3: b'Inferring fsid 13c63ffc-36ed-11ec-b561-09a495c6360a\n'
10.10.2.3: b'Inferring config /var/lib/ceph/13c63ffc-36ed-11ec-b561-09a495c6360a/mon.node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.3: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.3: b'Scheduled mon update...\n'
10.10.2.3: b'\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T02:15:15,174650101-04:00] INFO: > Set the default number of manager daemons to 1\x1b[0m\n# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:335 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1\n'
10.10.2.3: b'\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T02:15:15,183182139-04:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.3: b"\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T02:15:15,186460987-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mgr', '1']\x1b[0m\n# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1\n"
10.10.2.3: b'Inferring fsid 13c63ffc-36ed-11ec-b561-09a495c6360a\n'
10.10.2.3: b'Inferring config /var/lib/ceph/13c63ffc-36ed-11ec-b561-09a495c6360a/mon.node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.3: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.3: b'Scheduled mgr update...\n'
10.10.2.3: b'\n\n\x1b[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T02:15:25,052818945-04:00] STAGE: Add hosts to the cluster...\x1b[0m\n'
10.10.2.3: b"\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T02:15:25,058081102-04:00] INFO: > Adding host 'sm1'\x1b[0m\n# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:362 - add_hosts() > ssh-copy-id -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -f -i /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub ljishen@sm1\n"
10.10.2.3: b'/usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: "/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub"\n'
10.10.2.3: b'\nNumber of key(s) added: 1\n\nNow try logging into the machine, with:   "ssh -o \'GlobalKnownHostsFile=/dev/null\' -o \'LogLevel=ERROR\' -o \'PasswordAuthentication=no\' -o \'StrictHostKeyChecking=no\' -o \'UserKnownHostsFile=/dev/null\' \'ljishen@sm1\'"\nand check to make sure that only the key(s) you wanted were added.\n\n'
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:368 - add_hosts() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add sm1 10.10.2.5\n'
10.10.2.3: b'\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T02:15:26,245611720-04:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.3: b"\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T02:15:26,248405385-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'host', 'add', 'sm1', '10.10.2.5']\x1b[0m\n"
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add sm1 10.10.2.5\n'
10.10.2.3: b'Inferring fsid 13c63ffc-36ed-11ec-b561-09a495c6360a\n'
10.10.2.3: b'Inferring config /var/lib/ceph/13c63ffc-36ed-11ec-b561-09a495c6360a/mon.node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.3: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.3: b"Added host 'sm1' with addr '10.10.2.5'\n"
10.10.2.3: b'\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T02:15:37,386542979-04:00] INFO: Sleep for 20 seconds\x1b[0m\n'
10.10.2.3: b'\n\n\x1b[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T02:15:57,390718580-04:00] STAGE: Add OSDs to the cluster...\x1b[0m\n'
10.10.2.3: b"\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T02:15:57,396244853-04:00] INFO: > Adding devices ['/dev/nvme0n1'] on host 'sm1' as OSDs\x1b[0m\n# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:387 - add_host_devices_as_osds() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd sm1:/dev/nvme0n1\n"
10.10.2.3: b'\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T02:15:57,404449604-04:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.3: b"\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T02:15:57,407586485-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'daemon', 'add', 'osd', 'sm1:/dev/nvme0n1']\x1b[0m\n# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd sm1:/dev/nvme0n1\n"
10.10.2.3: b'Inferring fsid 13c63ffc-36ed-11ec-b561-09a495c6360a\n'
10.10.2.3: b'Inferring config /var/lib/ceph/13c63ffc-36ed-11ec-b561-09a495c6360a/mon.node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.3: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.3: b"Created osd(s) 0 on host 'sm1'\n"
10.10.2.3: b'\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T02:16:21,535048145-04:00] INFO: > Sleep for 20 seconds\x1b[0m\n'
10.10.2.3: b'\n\n\x1b[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T02:16:41,539633625-04:00] STAGE: Show Cluster Status\x1b[0m\n# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:343 - show_status() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status\n'
10.10.2.3: b'\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T02:16:41,547969103-04:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.3: b"\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T02:16:41,550900807-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', '--status']\x1b[0m\n"
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status\n'
10.10.2.3: b'Inferring fsid 13c63ffc-36ed-11ec-b561-09a495c6360a\n'
10.10.2.3: b'Inferring config /var/lib/ceph/13c63ffc-36ed-11ec-b561-09a495c6360a/mon.node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.3: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.3: b'  cluster:\n    id:     13c63ffc-36ed-11ec-b561-09a495c6360a\n    health: HEALTH_WARN\n            1 pool(s) have no replicas configured\n \n  services:\n    mon: 1 daemons, quorum node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us (age 2m)\n    mgr: node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us.xfvpck(active, since 2m)\n    osd: 1 osds: 1 up (since 21s), 1 in (since 40s)\n \n  data:\n    pools:   1 pools, 128 pgs\n    objects: 0 objects, 0 B\n    usage:   4.7 MiB used, 100 GiB / 100 GiB avail\n    pgs:     99.219% pgs unknown\n             0.781% pgs not active\n             127 unknown\n             1   peering\n \n  progress:\n    Global Recovery Event (0s)\n      [............................] \n \n'
[1] 23:16:51 [SUCCESS] ljishen@10.10.2.3

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T02:13:44,874829917-04:00] INFO: > Remove the existing cluster[0m
## bash:13 -  > ls /var/lib/ceph
## bash:13 -  > tail -n1
# bash:13 -  > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm --host 10.10.2.3 --host 10.10.2.5 rm-cluster --force --fsid 07779c24-36ec-11ec-b561-09a495c6360a

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T02:13:53,832249798-04:00] INFO: > Deploy a new cluster[0m
## bash:22 -  > /tmp/bench-rados/ceph-research/scripts/parallel-ssh --host 10.10.2.5 uname --nodename
## bash:22 -  > tail -n1
# bash:22 -  > osd_hostname=sm1
# bash:23 -  > /tmp/bench-rados/ceph-research/scripts/ceph-deploy -m 10.10.2.3 -o sm1/10.10.2.5:/dev/nvme0n1


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T02:13:54,957722296-04:00] STAGE: Verify passwordless SSH to hosts, passwordless sudo via ljishen, and the existence of devices on hosts...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T02:13:54,961371452-04:00] INFO: > Check the host for the monitor[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:181 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@10.10.2.3 sudo --non-interactive --validate

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T02:13:55,107801522-04:00] INFO: > Check hosts for OSDs[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T02:13:55,111697193-04:00] INFO: >> Check host 'sm1'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:200 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 sudo --non-interactive --validate

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T02:13:56,188793311-04:00] INFO: >>> Check device '/dev/nvme0n1'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:215 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 test -b /dev/nvme0n1
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:226 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 lsblk --nodeps --noheadings --output TYPE /dev/nvme0n1
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:227 - verify_parameters() > grep --word-regexp disk


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T02:13:58,333057109-04:00] STAGE: Reset devices on hosts...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T02:13:58,336758864-04:00] INFO: > Resetting device '/dev/nvme0n1' on host 'sm1'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:288 - reset_host_devices() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 sudo sh
  Removing ceph--a1a58707--ab39--4eb7--ac61--5a0408d59087-osd--block--1c713cb6--0c75--4cb6--b132--790759aac5b0 (252:0)
  Archiving volume group "ceph-a1a58707-ab39-4eb7-ac61-5a0408d59087" metadata (seqno 5).
  Releasing logical volume "osd-block-1c713cb6-0c75-4cb6-b132-790759aac5b0"
  Creating volume group backup "/etc/lvm/backup/ceph-a1a58707-ab39-4eb7-ac61-5a0408d59087" (seqno 6).
  Logical volume "osd-block-1c713cb6-0c75-4cb6-b132-790759aac5b0" successfully removed
  Removing physical volume "/dev/nvme0n1" from volume group "ceph-a1a58707-ab39-4eb7-ac61-5a0408d59087"
  Volume group "ceph-a1a58707-ab39-4eb7-ac61-5a0408d59087" successfully removed


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T02:14:00,613068406-04:00] STAGE: Bootstrap a new cluster...[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:312 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.3 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T02:14:00,621669363-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T02:14:00,624486291-04:00] DEBUG: CEPHADM_PARAMS: ['bootstrap', '--config', '/tmp/bench-rados/ceph-research/scripts/ceph.conf', '--mon-ip', '10.10.2.3', '--output-dir', '/tmp/bench-rados/deployment_data_root/etc/ceph', '--ssh-user', 'ljishen', '--skip-dashboard', '--no-minimize-config', '--allow-overwrite', '--allow-fqdn-hostname', '--orphan-initial-daemons', '--skip-monitoring-stack', '--single-host-defaults'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.3 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults
Verifying podman|docker is present...
Verifying lvm2 is present...
Verifying time synchronization is in place...
Unit ntp.service is enabled and running
Repeating the final host check...
podman|docker (/usr/bin/docker) is present
systemctl is present
lvcreate is present
Unit ntp.service is enabled and running
Host looks OK
Cluster fsid: 13c63ffc-36ed-11ec-b561-09a495c6360a
Verifying IP 10.10.2.3 port 3300 ...
Verifying IP 10.10.2.3 port 6789 ...
Mon IP `10.10.2.3` is in CIDR network `10.10.2.0/24`
- internal network (--cluster-network) has not been provided, OSD replication will default to the public_network
Adjusting default settings to suit single-host cluster...
Pulling container image quay.io/ceph/ceph:v16...
Ceph version: ceph version 16.2.6 (ee28fb57e47e9f88813e24bbf4c14496ca299d31) pacific (stable)
Extracting ceph user uid/gid from container image...
Creating initial keys...
Creating initial monmap...
Creating mon...
Waiting for mon to start...
Waiting for mon...
mon is available
Setting mon public_network to 10.10.2.0/24
Wrote config to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf
Wrote keyring to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring
Creating mgr...
Verifying port 9283 ...
Waiting for mgr to start...
Waiting for mgr...
mgr not available, waiting (1/15)...
mgr not available, waiting (2/15)...
mgr is available
Enabling cephadm module...
Waiting for the mgr to restart...
Waiting for mgr epoch 5...
mgr epoch 5 is available
Setting orchestrator backend to cephadm...
Generating ssh key...
Wrote public SSH key to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub
Adding key to ljishen@localhost authorized_keys...
Adding host node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us...
Deploying unmanaged mon service...
Deploying unmanaged mgr service...
You can access the Ceph CLI with:

	sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --fsid 13c63ffc-36ed-11ec-b561-09a495c6360a -c /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf -k /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring

Please consider enabling telemetry to help improve Ceph:

	ceph telemetry on

For more information see:

	https://docs.ceph.com/docs/pacific/mgr/telemetry/

Bootstrap complete.

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T02:15:05,299658324-04:00] INFO: > Set the default number of monitor daemons to 1[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:330 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T02:15:05,308065225-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T02:15:05,311050390-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mon', '1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1
Inferring fsid 13c63ffc-36ed-11ec-b561-09a495c6360a
Inferring config /var/lib/ceph/13c63ffc-36ed-11ec-b561-09a495c6360a/mon.node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Scheduled mon update...

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T02:15:15,174650101-04:00] INFO: > Set the default number of manager daemons to 1[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:335 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T02:15:15,183182139-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T02:15:15,186460987-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mgr', '1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1
Inferring fsid 13c63ffc-36ed-11ec-b561-09a495c6360a
Inferring config /var/lib/ceph/13c63ffc-36ed-11ec-b561-09a495c6360a/mon.node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Scheduled mgr update...


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T02:15:25,052818945-04:00] STAGE: Add hosts to the cluster...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T02:15:25,058081102-04:00] INFO: > Adding host 'sm1'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:362 - add_hosts() > ssh-copy-id -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -f -i /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub ljishen@sm1
/usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: "/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub"

Number of key(s) added: 1

Now try logging into the machine, with:   "ssh -o 'GlobalKnownHostsFile=/dev/null' -o 'LogLevel=ERROR' -o 'PasswordAuthentication=no' -o 'StrictHostKeyChecking=no' -o 'UserKnownHostsFile=/dev/null' 'ljishen@sm1'"
and check to make sure that only the key(s) you wanted were added.

# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:368 - add_hosts() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add sm1 10.10.2.5
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T02:15:26,245611720-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T02:15:26,248405385-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'host', 'add', 'sm1', '10.10.2.5'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add sm1 10.10.2.5
Inferring fsid 13c63ffc-36ed-11ec-b561-09a495c6360a
Inferring config /var/lib/ceph/13c63ffc-36ed-11ec-b561-09a495c6360a/mon.node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Added host 'sm1' with addr '10.10.2.5'

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T02:15:37,386542979-04:00] INFO: Sleep for 20 seconds[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T02:15:57,390718580-04:00] STAGE: Add OSDs to the cluster...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T02:15:57,396244853-04:00] INFO: > Adding devices ['/dev/nvme0n1'] on host 'sm1' as OSDs[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:387 - add_host_devices_as_osds() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd sm1:/dev/nvme0n1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T02:15:57,404449604-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T02:15:57,407586485-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'daemon', 'add', 'osd', 'sm1:/dev/nvme0n1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd sm1:/dev/nvme0n1
Inferring fsid 13c63ffc-36ed-11ec-b561-09a495c6360a
Inferring config /var/lib/ceph/13c63ffc-36ed-11ec-b561-09a495c6360a/mon.node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Created osd(s) 0 on host 'sm1'

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T02:16:21,535048145-04:00] INFO: > Sleep for 20 seconds[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T02:16:41,539633625-04:00] STAGE: Show Cluster Status[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:343 - show_status() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T02:16:41,547969103-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T02:16:41,550900807-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', '--status'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status
Inferring fsid 13c63ffc-36ed-11ec-b561-09a495c6360a
Inferring config /var/lib/ceph/13c63ffc-36ed-11ec-b561-09a495c6360a/mon.node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
  cluster:
    id:     13c63ffc-36ed-11ec-b561-09a495c6360a
    health: HEALTH_WARN
            1 pool(s) have no replicas configured
 
  services:
    mon: 1 daemons, quorum node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us (age 2m)
    mgr: node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us.xfvpck(active, since 2m)
    osd: 1 osds: 1 up (since 21s), 1 in (since 40s)
 
  data:
    pools:   1 pools, 128 pgs
    objects: 0 objects, 0 B
    usage:   4.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     99.219% pgs unknown
             0.781% pgs not active
             127 unknown
             1   peering
 
  progress:
    Global Recovery Event (0s)
      [............................] 
 

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:16:51,184478849-07:00] INFO: > Copy configuration files to local[0m
# ./benchmarks/bench-rados:208 - launch_ceph_cluster() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.3:/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf
# ./benchmarks/bench-rados:212 - launch_ceph_cluster() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.3 sudo chmod 644 /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:16:51,338575809-07:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.3 sudo chmod 644 /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring
[1] 23:16:51 [SUCCESS] ljishen@10.10.2.3
# ./benchmarks/bench-rados:214 - launch_ceph_cluster() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.3:/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:16:51,666402941-07:00] STAGE: Update local ceph.conf as a client...[0m


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:16:51,670446081-07:00] STAGE: Configure Ceph pool...[0m
# ./benchmarks/bench-rados:245 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_max_backfills 32
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:16:51,696185072-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:16:51,699969255-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '13c63ffc-36ed-11ec-b561-09a495c6360a', '--', 'ceph', 'config', 'set', 'osd', 'osd_max_backfills', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 13c63ffc-36ed-11ec-b561-09a495c6360a -- ceph config set osd osd_max_backfills 32
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:246 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_active 32
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:16:54,998706180-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:16:55,002241796-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '13c63ffc-36ed-11ec-b561-09a495c6360a', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_active', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 13c63ffc-36ed-11ec-b561-09a495c6360a -- ceph config set osd osd_recovery_max_active 32
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:247 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_single_start 8
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:16:58,265809295-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:16:58,269240404-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '13c63ffc-36ed-11ec-b561-09a495c6360a', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_single_start', '8'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 13c63ffc-36ed-11ec-b561-09a495c6360a -- ceph config set osd osd_recovery_max_single_start 8
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:248 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_op_priority 63
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:17:01,598495564-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:17:01,601900855-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '13c63ffc-36ed-11ec-b561-09a495c6360a', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_op_priority', '63'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 13c63ffc-36ed-11ec-b561-09a495c6360a -- ceph config set osd osd_recovery_op_priority 63
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:250 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config show-with-defaults osd.0
# ./benchmarks/bench-rados:251 - configure_ceph_pool() > grep 'osd_max_backfills\|osd_recovery'
# ./benchmarks/bench-rados:251 - configure_ceph_pool() > column -t -s ' '
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
osd_max_backfills                        32         mon
osd_recovery_cost                        20971520   default
osd_recovery_delay_start                 0.000000   default
osd_recovery_max_active                  32         mon
osd_recovery_max_active_hdd              3          default
osd_recovery_max_active_ssd              10         default
osd_recovery_max_chunk                   8388608    default
osd_recovery_max_omap_entries_per_chunk  8096       default
osd_recovery_max_single_start            8          mon
osd_recovery_op_priority                 63         mon
osd_recovery_op_warn_multiple            16         default
osd_recovery_priority                    5          default
osd_recovery_retry_interval              30.000000  default
osd_recovery_sleep                       0.000000   default
osd_recovery_sleep_hdd                   0.100000   default
osd_recovery_sleep_hybrid                0.025000   default
osd_recovery_sleep_ssd                   0.000000   default
# ./benchmarks/bench-rados:253 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool create bench_rados 128 128 replicated --size 1 --pg-num-min 128
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:17:08,241110546-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:17:08,244678833-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '13c63ffc-36ed-11ec-b561-09a495c6360a', '--', 'ceph', 'osd', 'pool', 'create', 'bench_rados', '128', '128', 'replicated', '--size', '1', '--pg-num-min', '128'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 13c63ffc-36ed-11ec-b561-09a495c6360a -- ceph osd pool create bench_rados 128 128 replicated --size 1 --pg-num-min 128
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
pool 'bench_rados' created
# ./benchmarks/bench-rados:255 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados min_size 1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:17:11,740901986-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:17:11,744204343-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '13c63ffc-36ed-11ec-b561-09a495c6360a', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'min_size', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 13c63ffc-36ed-11ec-b561-09a495c6360a -- ceph osd pool set bench_rados min_size 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 min_size to 1
# ./benchmarks/bench-rados:256 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados pg_autoscale_mode off
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:17:15,130914489-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:17:15,134305203-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '13c63ffc-36ed-11ec-b561-09a495c6360a', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'pg_autoscale_mode', 'off'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 13c63ffc-36ed-11ec-b561-09a495c6360a -- ceph osd pool set bench_rados pg_autoscale_mode off
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 pg_autoscale_mode to off
# ./benchmarks/bench-rados:257 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool application enable bench_rados benchmark
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:17:18,598605963-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:17:18,602089410-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '13c63ffc-36ed-11ec-b561-09a495c6360a', '--', 'ceph', 'osd', 'pool', 'application', 'enable', 'bench_rados', 'benchmark'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 13c63ffc-36ed-11ec-b561-09a495c6360a -- ceph osd pool application enable bench_rados benchmark
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
enabled application 'benchmark' on pool 'bench_rados'
# ./benchmarks/bench-rados:258 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados noscrub 1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:17:22,864966611-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:17:22,868277955-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '13c63ffc-36ed-11ec-b561-09a495c6360a', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'noscrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 13c63ffc-36ed-11ec-b561-09a495c6360a -- ceph osd pool set bench_rados noscrub 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 noscrub to 1
# ./benchmarks/bench-rados:259 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados nodeep-scrub 1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:17:26,313267145-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:17:26,316230235-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '13c63ffc-36ed-11ec-b561-09a495c6360a', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'nodeep-scrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 13c63ffc-36ed-11ec-b561-09a495c6360a -- ceph osd pool set bench_rados nodeep-scrub 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 nodeep-scrub to 1
# ./benchmarks/bench-rados:260 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool ls detail
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:17:30,588883702-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:17:30,592117070-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '13c63ffc-36ed-11ec-b561-09a495c6360a', '--', 'ceph', 'osd', 'pool', 'ls', 'detail'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 13c63ffc-36ed-11ec-b561-09a495c6360a -- ceph osd pool ls detail
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
pool 1 'device_health_metrics' replicated size 1 min_size 1 crush_rule 0 object_hash rjenkins pg_num 128 pgp_num 128 autoscale_mode on last_change 13 lfor 0/0/11 flags hashpspool stripe_width 0 pg_num_min 1 application mgr_devicehealth
pool 2 'bench_rados' replicated size 1 min_size 1 crush_rule 0 object_hash rjenkins pg_num 128 pgp_num 128 autoscale_mode off last_change 21 flags hashpspool,noscrub,nodeep-scrub stripe_width 0 pg_num_min 128 application benchmark

# ./benchmarks/bench-rados:261 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd df tree
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:17:33,819752640-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:17:33,823572410-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '13c63ffc-36ed-11ec-b561-09a495c6360a', '--', 'ceph', 'osd', 'df', 'tree'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 13c63ffc-36ed-11ec-b561-09a495c6360a -- ceph osd df tree
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
ID  CLASS  WEIGHT   REWEIGHT  SIZE     RAW USE  DATA     OMAP  META     AVAIL    %USE  VAR   PGS  STATUS  TYPE NAME    
-1         0.09769         -  100 GiB  6.0 MiB  168 KiB   0 B  5.8 MiB  100 GiB  0.01  1.00    -          root default 
-3         0.09769         -  100 GiB  6.0 MiB  168 KiB   0 B  5.8 MiB  100 GiB  0.01  1.00    -              host sm1 
 0    ssd  0.09769   1.00000  100 GiB  6.0 MiB  168 KiB   0 B  5.8 MiB  100 GiB  0.01  1.00  256      up          osd.0
                       TOTAL  100 GiB  6.0 MiB  168 KiB   0 B  5.8 MiB  100 GiB  0.01                                  
MIN/MAX VAR: 1.00/1.00  STDDEV: 0


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:17:37,021725865-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:18:00,352801751-07:00] INFO: >>   data:
    pools:   2 pools, 256 pgs
    objects: 0 objects, 0 B
    usage:   6.0 MiB used, 100 GiB / 100 GiB avail
    pgs:     256 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:18:00,361645294-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:18:08,750363989-07:00] INFO: >>   data:
    pools:   2 pools, 256 pgs
    objects: 0 objects, 0 B
    usage:   6.0 MiB used, 100 GiB / 100 GiB avail
    pgs:     256 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:18:08,758737548-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:18:17,097919011-07:00] INFO: >>   data:
    pools:   2 pools, 256 pgs
    objects: 0 objects, 0 B
    usage:   6.0 MiB used, 100 GiB / 100 GiB avail
    pgs:     256 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:18:17,106984831-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:18:25,417241446-07:00] INFO: >>   data:
    pools:   2 pools, 256 pgs
    objects: 0 objects, 0 B
    usage:   6.0 MiB used, 100 GiB / 100 GiB avail
    pgs:     256 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:18:25,425550524-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:18:33,756471012-07:00] INFO: >>   data:
    pools:   2 pools, 256 pgs
    objects: 0 objects, 0 B
    usage:   6.0 MiB used, 100 GiB / 100 GiB avail
    pgs:     256 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:18:33,765501206-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:18:33,771644373-07:00] INFO: > The cluster is idle now.[0m


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:18:33,775206639-07:00] STAGE: Start benchmarking of mode write ...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:18:33,783440026-07:00] INFO: > Start system activity collection services[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:18:33,789758061-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:329 - rados_bench() > rm --force /mnt/sda8/ceph-research/scripts/output/sys_activity_4MB_write.dat.rados_bench_host.1
# ./benchmarks/bench-rados:330 - rados_bench() > sar_pid_rados_bench_host=871989
# ./benchmarks/bench-rados:330 - rados_bench() > S_TIME_FORMAT=ISO
# ./benchmarks/bench-rados:330 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/output/sys_activity_4MB_write.dat.rados_bench_host.1 2

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:18:33,797584462-07:00] INFO: >> for OSD host[0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:18:33,806354046-07:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.5 bash
10.10.2.5: b'112895\n'
[1] 23:18:34 [SUCCESS] ljishen@10.10.2.5
112895

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:18:34,920677775-07:00] INFO: > Run rados bench[0m
# ./benchmarks/bench-rados:358 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/output/rados_bench_4MB_write.log.1
## ./benchmarks/bench-rados:349 - rados_bench() > count_cpus 32-63,96-127
# ./benchmarks/bench-rados:349 - rados_bench() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 30 write --pool bench_rados -b 4194304 -O 4194304 --concurrent-ios 128 --show-time --write-object --write-omap --write-xattr --no-cleanup
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:18:34,942871712-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:18:34,946386830-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '13c63ffc-36ed-11ec-b561-09a495c6360a', '--', 'rados', 'bench', '30', 'write', '--pool', 'bench_rados', '-b', '4194304', '-O', '4194304', '--concurrent-ios', '128', '--show-time', '--write-object', '--write-omap', '--write-xattr', '--no-cleanup'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 13c63ffc-36ed-11ec-b561-09a495c6360a -- rados bench 30 write --pool bench_rados -b 4194304 -O 4194304 --concurrent-ios 128 --show-time --write-object --write-omap --write-xattr --no-cleanup
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
hints = 1
2021-10-27T06:18:37.479163+0000 Maintaining 128 concurrent writes of 4194304 bytes to objects of size 4194304 for up to 30 seconds or 0 objects
2021-10-27T06:18:37.479176+0000 Object prefix: benchmark_data_node-0.bluefield2.ucsc-cmps10_7
2021-10-27T06:18:37.587630+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-27T06:18:37.587630+0000     0       0         0         0         0         0           -           0
2021-10-27T06:18:38.587731+0000     1      23        23         0         0         0           -           0
2021-10-27T06:18:39.587798+0000     2      39        39         0         0         0           -           0
2021-10-27T06:18:40.587863+0000     3      56        56         0         0         0           -           0
2021-10-27T06:18:41.587959+0000     4      73        73         0         0         0           -           0
2021-10-27T06:18:42.588027+0000     5      91        91         0         0         0           -           0
2021-10-27T06:18:43.588092+0000     6     107       107         0         0         0           -           0
2021-10-27T06:18:44.588301+0000     7     123       123         0         0         0           -           0
2021-10-27T06:18:45.588410+0000     8     127       139        12   5.99946         6     7.66181     7.49258
2021-10-27T06:18:46.588477+0000     9     127       147        20   8.88812        32     8.15649     7.70742
2021-10-27T06:18:47.588682+0000    10     127       155        28   11.1989        32     8.67654      7.9531
2021-10-27T06:18:48.588750+0000    11     127       163        36   13.0897        32     9.02882     8.20078
2021-10-27T06:18:49.588982+0000    12     127       171        44   14.6651        32     9.68695      8.4562
2021-10-27T06:18:50.589055+0000    13     127       178        51   15.6907        28     10.1386      8.6867
2021-10-27T06:18:51.589123+0000    14     127       193        66   18.8552        60     10.5223     9.09247
2021-10-27T06:18:52.589311+0000    15     127       207        80    21.331        56     10.4159      9.3376
2021-10-27T06:18:53.589407+0000    16     127       227       100   24.9973        80     10.4907     9.57858
2021-10-27T06:18:54.589483+0000    17     127       241       114   26.8207        56     10.4475     9.69474
2021-10-27T06:18:55.589552+0000    18     127       261       134   29.7747        80     10.4795     9.82292
2021-10-27T06:18:56.589621+0000    19     127       269       142   29.8917        32     10.1938     9.86555
2021-10-27T06:18:57.589718+0000 min lat: 7.23499 max lat: 10.8016 avg lat: 9.88035
2021-10-27T06:18:57.589718+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-27T06:18:57.589718+0000    20     127       272       145   28.9971        12     10.5807     9.88035
2021-10-27T06:18:58.589796+0000    21     127       280       153   29.1399        32     11.3867     9.94895
2021-10-27T06:18:59.589864+0000    22     127       285       158   28.7244        20     11.4544      10.008
2021-10-27T06:19:00.589932+0000    23     127       286       159   27.6495         4     11.9763     10.0204
2021-10-27T06:19:01.590029+0000    24     127       292       165   27.4973        24     12.1001     10.1189
2021-10-27T06:19:02.590098+0000    25     127       294       167   26.7174         8       12.66     10.1494
2021-10-27T06:19:03.590166+0000    26     127       302       175   26.9205        32     13.3732     10.3045
2021-10-27T06:19:04.590236+0000    27     127       310       183   27.1086        32     13.4536     10.4456
2021-10-27T06:19:05.590333+0000    28     127       323       196   27.9974        52      13.735     10.6638
2021-10-27T06:19:06.590403+0000    29     127       333       206   28.4111        40      14.217     10.8315
2021-10-27T06:19:07.590475+0000    30     127       343       216   28.7973        40     14.4812      10.999
2021-10-27T06:19:08.590739+0000 Total time run:         30.6423
Total writes made:      344
Write size:             4194304
Object size:            4194304
Bandwidth (MB/sec):     44.9053
Stddev Bandwidth:       23.6827
Max bandwidth (MB/sec): 80
Min bandwidth (MB/sec): 0
Average IOPS:           11
Stddev IOPS:            5.93693
Max IOPS:               20
Min IOPS:               0
Average Latency(s):     9.96567
Stddev Latency(s):      3.62656
Max latency(s):         14.8795
Min latency(s):         0.753539

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:19:09,145661331-07:00] INFO: > Stop system activity collection services[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:19:09,152208758-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:379 - rados_bench() > kill -INT 871989

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:19:09,158984343-07:00] INFO: >> for OSD host[0m
# ./benchmarks/bench-rados:384 - rados_bench() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.5 kill -INT 112895
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:19:09,167647407-07:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.5 kill -INT 112895
[1] 23:19:10 [SUCCESS] ljishen@10.10.2.5

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:19:10,517193564-07:00] INFO: >>> Copy system activity file from OSD host[0m
## ./benchmarks/bench-rados:391 - rados_bench() > basename -- /tmp/bench-rados/sys_activity_4MB_write.dat.osd_host.1
# ./benchmarks/bench-rados:391 - rados_bench() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.5:/tmp/bench-rados/sys_activity_4MB_write.dat.osd_host.1 /mnt/sda8/ceph-research/scripts/output/sys_activity_4MB_write.dat.osd_host.1


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:19:11,940530682-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:19:35,098877190-07:00] INFO: >>   data:
    pools:   2 pools, 256 pgs
    objects: 345 objects, 1.3 GiB
    usage:   4.0 GiB used, 96 GiB / 100 GiB avail
    pgs:     256 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:19:35,107138900-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:19:43,381000491-07:00] INFO: >>   data:
    pools:   2 pools, 256 pgs
    objects: 345 objects, 1.3 GiB
    usage:   4.0 GiB used, 96 GiB / 100 GiB avail
    pgs:     256 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:19:43,388846079-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:19:51,712790158-07:00] INFO: >>   data:
    pools:   2 pools, 256 pgs
    objects: 345 objects, 1.3 GiB
    usage:   4.0 GiB used, 96 GiB / 100 GiB avail
    pgs:     256 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:19:51,720943062-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:20:00,069116958-07:00] INFO: >>   data:
    pools:   2 pools, 256 pgs
    objects: 345 objects, 1.3 GiB
    usage:   4.0 GiB used, 96 GiB / 100 GiB avail
    pgs:     256 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:20:00,078019452-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:20:08,425842738-07:00] INFO: >>   data:
    pools:   2 pools, 256 pgs
    objects: 345 objects, 1.3 GiB
    usage:   4.0 GiB used, 96 GiB / 100 GiB avail
    pgs:     256 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:20:08,434886618-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:20:08,442299281-07:00] INFO: > The cluster is idle now.[0m


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:20:08,445958689-07:00] STAGE: Start benchmarking of mode seq ...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:20:08,454133485-07:00] INFO: > Start system activity collection services[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:20:08,460267575-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:329 - rados_bench() > rm --force /mnt/sda8/ceph-research/scripts/output/sys_activity_4MB_seq.dat.rados_bench_host.1
# ./benchmarks/bench-rados:330 - rados_bench() > sar_pid_rados_bench_host=873799
# ./benchmarks/bench-rados:330 - rados_bench() > S_TIME_FORMAT=ISO
# ./benchmarks/bench-rados:330 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/output/sys_activity_4MB_seq.dat.rados_bench_host.1 2

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:20:08,467818829-07:00] INFO: >> for OSD host[0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:20:08,476876966-07:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.5 bash
10.10.2.5: b'113288\n'
[1] 23:20:09 [SUCCESS] ljishen@10.10.2.5
113288

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:20:09,568365983-07:00] INFO: > Run rados bench[0m
# ./benchmarks/bench-rados:367 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/output/rados_bench_4MB_seq.log.1
## ./benchmarks/bench-rados:364 - rados_bench() > count_cpus 32-63,96-127
# ./benchmarks/bench-rados:364 - rados_bench() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 128 --show-time
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:20:09,590196667-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:20:09,594037146-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '13c63ffc-36ed-11ec-b561-09a495c6360a', '--', 'rados', 'bench', '99999999', 'seq', '--pool', 'bench_rados', '--concurrent-ios', '128', '--show-time'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 13c63ffc-36ed-11ec-b561-09a495c6360a -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 128 --show-time
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
hints = 1
2021-10-27T06:20:12.130425+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-27T06:20:12.130425+0000     0       1         1         0         0         0           -           0
2021-10-27T06:20:13.130521+0000     1     127       228       101   403.933       404    0.599961    0.598114
2021-10-27T06:20:14.130730+0000 Total time run:       1.65877
Total reads made:     344
Read size:            4194304
Object size:          4194304
Bandwidth (MB/sec):   829.533
Average IOPS:         207
Stddev IOPS:          0
Max IOPS:             101
Min IOPS:             101
Average Latency(s):   0.504417
Max latency(s):       0.642524
Min latency(s):       0.102694

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:20:14,723887218-07:00] INFO: > Stop system activity collection services[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:20:14,731099996-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:379 - rados_bench() > kill -INT 873799

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:20:14,737937087-07:00] INFO: >> for OSD host[0m
# ./benchmarks/bench-rados:384 - rados_bench() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.5 kill -INT 113288
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:20:14,745992388-07:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.5 kill -INT 113288
[1] 23:20:15 [SUCCESS] ljishen@10.10.2.5

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:20:15,851303828-07:00] INFO: >>> Copy system activity file from OSD host[0m
## ./benchmarks/bench-rados:391 - rados_bench() > basename -- /tmp/bench-rados/sys_activity_4MB_seq.dat.osd_host.1
# ./benchmarks/bench-rados:391 - rados_bench() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.5:/tmp/bench-rados/sys_activity_4MB_seq.dat.osd_host.1 /mnt/sda8/ceph-research/scripts/output/sys_activity_4MB_seq.dat.osd_host.1


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:20:16,905296055-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:20:40,068665422-07:00] INFO: >>   data:
    pools:   2 pools, 256 pgs
    objects: 345 objects, 1.3 GiB
    usage:   4.0 GiB used, 96 GiB / 100 GiB avail
    pgs:     256 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:20:40,077276177-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:20:48,242973172-07:00] INFO: >>   data:
    pools:   2 pools, 256 pgs
    objects: 345 objects, 1.3 GiB
    usage:   4.0 GiB used, 96 GiB / 100 GiB avail
    pgs:     256 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:20:48,251705285-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:20:56,633167376-07:00] INFO: >>   data:
    pools:   2 pools, 256 pgs
    objects: 345 objects, 1.3 GiB
    usage:   4.0 GiB used, 96 GiB / 100 GiB avail
    pgs:     256 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:20:56,641736303-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:21:04,990746315-07:00] INFO: >>   data:
    pools:   2 pools, 256 pgs
    objects: 345 objects, 1.3 GiB
    usage:   4.0 GiB used, 96 GiB / 100 GiB avail
    pgs:     256 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:21:05,000062246-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:21:13,379639284-07:00] INFO: >>   data:
    pools:   2 pools, 256 pgs
    objects: 345 objects, 1.3 GiB
    usage:   4.0 GiB used, 96 GiB / 100 GiB avail
    pgs:     256 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:21:13,388036889-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:21:13,394532208-07:00] INFO: > The cluster is idle now.[0m


[1;7;39;49m[2021-10-26T23:21:13,397178221-07:00][RUNNING][ROUND 2/6/21] object_size=4MB[0m


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:21:13,401088872-07:00] STAGE: Launch a Ceph cluster (CEPH_HOST: 10.10.2.3, OSD_HOST: 10.10.2.5) ...[0m
# ./benchmarks/bench-rados:172 - launch_ceph_cluster() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.3 bash -euo pipefail
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:21:13,409361953-07:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.3 bash -euo pipefail
10.10.2.3: b'\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T02:21:13,812962418-04:00] INFO: > Remove the existing cluster\x1b[0m\n'
10.10.2.3: b'## bash:13 -  > ls /var/lib/ceph\n'
10.10.2.3: b'## bash:13 -  > tail -n1\n'
10.10.2.3: b'# bash:13 -  > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm --host 10.10.2.3 --host 10.10.2.5 rm-cluster --force --fsid 13c63ffc-36ed-11ec-b561-09a495c6360a\n'
10.10.2.3: b'\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T02:21:22,998520055-04:00] INFO: > Deploy a new cluster\x1b[0m\n'
10.10.2.3: b'## bash:22 -  > /tmp/bench-rados/ceph-research/scripts/parallel-ssh --host 10.10.2.5 uname --nodename\n## bash:22 -  > tail -n1\n'
10.10.2.3: b'# bash:22 -  > osd_hostname=sm1\n# bash:23 -  > /tmp/bench-rados/ceph-research/scripts/ceph-deploy -m 10.10.2.3 -o sm1/10.10.2.5:/dev/nvme0n1\n'
10.10.2.3: b'\n\n\x1b[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T02:21:24,133769500-04:00] STAGE: Verify passwordless SSH to hosts, passwordless sudo via ljishen, and the existence of devices on hosts...\x1b[0m\n'
10.10.2.3: b'\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T02:21:24,137525207-04:00] INFO: > Check the host for the monitor\x1b[0m\n# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:181 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@10.10.2.3 sudo --non-interactive --validate\n'
10.10.2.3: b'\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T02:21:24,288928771-04:00] INFO: > Check hosts for OSDs\x1b[0m\n'
10.10.2.3: b"\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T02:21:24,292858937-04:00] INFO: >> Check host 'sm1'\x1b[0m\n# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:200 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 sudo --non-interactive --validate\n"
10.10.2.3: b"\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T02:21:25,372204752-04:00] INFO: >>> Check device '/dev/nvme0n1'\x1b[0m\n"
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:215 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 test -b /dev/nvme0n1\n'
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:226 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 lsblk --nodeps --noheadings --output TYPE /dev/nvme0n1\n# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:227 - verify_parameters() > grep --word-regexp disk\n'
10.10.2.3: b'\n\n\x1b[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T02:21:27,529211606-04:00] STAGE: Reset devices on hosts...\x1b[0m\n'
10.10.2.3: b"\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T02:21:27,533059516-04:00] INFO: > Resetting device '/dev/nvme0n1' on host 'sm1'\x1b[0m\n# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:288 - reset_host_devices() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 sudo sh\n"
10.10.2.3: b'  Removing ceph--f2ceff2f--1eac--4725--bb87--f5cdfff6b86b-osd--block--e018ffe4--bc21--452a--97e8--f5ebddb790c0 (252:0)\n'
10.10.2.3: b'  Archiving volume group "ceph-f2ceff2f-1eac-4725-bb87-f5cdfff6b86b" metadata (seqno 5).\n'
10.10.2.3: b'  Releasing logical volume "osd-block-e018ffe4-bc21-452a-97e8-f5ebddb790c0"\n'
10.10.2.3: b'  Creating volume group backup "/etc/lvm/backup/ceph-f2ceff2f-1eac-4725-bb87-f5cdfff6b86b" (seqno 6).\n'
10.10.2.3: b'  Logical volume "osd-block-e018ffe4-bc21-452a-97e8-f5ebddb790c0" successfully removed\n'
10.10.2.3: b'  Removing physical volume "/dev/nvme0n1" from volume group "ceph-f2ceff2f-1eac-4725-bb87-f5cdfff6b86b"\n'
10.10.2.3: b'  Volume group "ceph-f2ceff2f-1eac-4725-bb87-f5cdfff6b86b" successfully removed\n'
10.10.2.3: b'\n\n\x1b[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T02:21:29,836227988-04:00] STAGE: Bootstrap a new cluster...\x1b[0m\n# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:312 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.3 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults\n'
10.10.2.3: b'\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T02:21:29,845026136-04:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.3: b"\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T02:21:29,848101521-04:00] DEBUG: CEPHADM_PARAMS: ['bootstrap', '--config', '/tmp/bench-rados/ceph-research/scripts/ceph.conf', '--mon-ip', '10.10.2.3', '--output-dir', '/tmp/bench-rados/deployment_data_root/etc/ceph', '--ssh-user', 'ljishen', '--skip-dashboard', '--no-minimize-config', '--allow-overwrite', '--allow-fqdn-hostname', '--orphan-initial-daemons', '--skip-monitoring-stack', '--single-host-defaults']\x1b[0m\n# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.3 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults\n"
10.10.2.3: b'Verifying podman|docker is present...\nVerifying lvm2 is present...\nVerifying time synchronization is in place...\n'
10.10.2.3: b'Unit ntp.service is enabled and running\nRepeating the final host check...\npodman|docker (/usr/bin/docker) is present\nsystemctl is present\n'
10.10.2.3: b'lvcreate is present\n'
10.10.2.3: b'Unit ntp.service is enabled and running\nHost looks OK\n'
10.10.2.3: b'Cluster fsid: 1f88a1d0-36ee-11ec-b561-09a495c6360a\n'
10.10.2.3: b'Verifying IP 10.10.2.3 port 3300 ...\nVerifying IP 10.10.2.3 port 6789 ...\n'
10.10.2.3: b'Mon IP `10.10.2.3` is in CIDR network `10.10.2.0/24`\n- internal network (--cluster-network) has not been provided, OSD replication will default to the public_network\n'
10.10.2.3: b'Adjusting default settings to suit single-host cluster...\n'
10.10.2.3: b'Pulling container image quay.io/ceph/ceph:v16...\n'
10.10.2.3: b'Ceph version: ceph version 16.2.6 (ee28fb57e47e9f88813e24bbf4c14496ca299d31) pacific (stable)\nExtracting ceph user uid/gid from container image...\n'
10.10.2.3: b'Creating initial keys...\n'
10.10.2.3: b'Creating initial monmap...\n'
10.10.2.3: b'Creating mon...\n'
10.10.2.3: b'Waiting for mon to start...\nWaiting for mon...\n'
10.10.2.3: b'mon is available\nSetting mon public_network to 10.10.2.0/24\n'
10.10.2.3: b'Wrote config to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf\nWrote keyring to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring\n'
10.10.2.3: b'Creating mgr...\n'
10.10.2.3: b'Verifying port 9283 ...\n'
10.10.2.3: b'Waiting for mgr to start...\nWaiting for mgr...\n'
10.10.2.3: b'mgr not available, waiting (1/15)...\n'
10.10.2.3: b'mgr not available, waiting (2/15)...\n'
10.10.2.3: b'mgr is available\n'
10.10.2.3: b'Enabling cephadm module...\n'
10.10.2.3: b'Waiting for the mgr to restart...\nWaiting for mgr epoch 5...\n'
10.10.2.3: b'mgr epoch 5 is available\nSetting orchestrator backend to cephadm...\n'
10.10.2.3: b'Generating ssh key...\n'
10.10.2.3: b'Wrote public SSH key to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub\nAdding key to ljishen@localhost authorized_keys...\n'
10.10.2.3: b'Adding host node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us...\n'
10.10.2.3: b'Deploying unmanaged mon service...\n'
10.10.2.3: b'Deploying unmanaged mgr service...\n'
10.10.2.3: b'You can access the Ceph CLI with:\n\n\tsudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --fsid 1f88a1d0-36ee-11ec-b561-09a495c6360a -c /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf -k /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring\n\nPlease consider enabling telemetry to help improve Ceph:\n\n\tceph telemetry on\n\nFor more information see:\n\n\thttps://docs.ceph.com/docs/pacific/mgr/telemetry/\n\nBootstrap complete.\n'
10.10.2.3: b'\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T02:22:35,442617942-04:00] INFO: > Set the default number of monitor daemons to 1\x1b[0m\n# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:330 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1\n'
10.10.2.3: b'\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T02:22:35,451396725-04:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.3: b"\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T02:22:35,454837368-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mon', '1']\x1b[0m\n"
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1\n'
10.10.2.3: b'Inferring fsid 1f88a1d0-36ee-11ec-b561-09a495c6360a\n'
10.10.2.3: b'Inferring config /var/lib/ceph/1f88a1d0-36ee-11ec-b561-09a495c6360a/mon.node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.3: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.3: b'Scheduled mon update...\n'
10.10.2.3: b'\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T02:22:47,106189225-04:00] INFO: > Set the default number of manager daemons to 1\x1b[0m\n# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:335 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1\n'
10.10.2.3: b'\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T02:22:47,115089206-04:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.3: b"\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T02:22:47,117945969-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mgr', '1']\x1b[0m\n"
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1\n'
10.10.2.3: b'Inferring fsid 1f88a1d0-36ee-11ec-b561-09a495c6360a\n'
10.10.2.3: b'Inferring config /var/lib/ceph/1f88a1d0-36ee-11ec-b561-09a495c6360a/mon.node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.3: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.3: b'Scheduled mgr update...\n'
10.10.2.3: b'\n\n\x1b[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T02:22:56,543216624-04:00] STAGE: Add hosts to the cluster...\x1b[0m\n'
10.10.2.3: b"\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T02:22:56,548275626-04:00] INFO: > Adding host 'sm1'\x1b[0m\n# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:362 - add_hosts() > ssh-copy-id -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -f -i /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub ljishen@sm1\n"
10.10.2.3: b'/usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: "/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub"\n'
10.10.2.3: b'\nNumber of key(s) added: 1\n\nNow try logging into the machine, with:   "ssh -o \'GlobalKnownHostsFile=/dev/null\' -o \'LogLevel=ERROR\' -o \'PasswordAuthentication=no\' -o \'StrictHostKeyChecking=no\' -o \'UserKnownHostsFile=/dev/null\' \'ljishen@sm1\'"\nand check to make sure that only the key(s) you wanted were added.\n\n'
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:368 - add_hosts() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add sm1 10.10.2.5\n'
10.10.2.3: b'\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T02:22:57,692732643-04:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.3: b"\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T02:22:57,695700556-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'host', 'add', 'sm1', '10.10.2.5']\x1b[0m\n# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add sm1 10.10.2.5\n"
10.10.2.3: b'Inferring fsid 1f88a1d0-36ee-11ec-b561-09a495c6360a\n'
10.10.2.3: b'Inferring config /var/lib/ceph/1f88a1d0-36ee-11ec-b561-09a495c6360a/mon.node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.3: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.3: b"Added host 'sm1' with addr '10.10.2.5'\n"
10.10.2.3: b'\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T02:23:08,480564857-04:00] INFO: Sleep for 20 seconds\x1b[0m\n'
10.10.2.3: b'\n\n\x1b[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T02:23:28,485037962-04:00] STAGE: Add OSDs to the cluster...\x1b[0m\n'
10.10.2.3: b"\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T02:23:28,490421556-04:00] INFO: > Adding devices ['/dev/nvme0n1'] on host 'sm1' as OSDs\x1b[0m\n# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:387 - add_host_devices_as_osds() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd sm1:/dev/nvme0n1\n"
10.10.2.3: b'\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T02:23:28,499215837-04:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.3: b"\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T02:23:28,502209989-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'daemon', 'add', 'osd', 'sm1:/dev/nvme0n1']\x1b[0m\n"
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd sm1:/dev/nvme0n1\n'
10.10.2.3: b'Inferring fsid 1f88a1d0-36ee-11ec-b561-09a495c6360a\n'
10.10.2.3: b'Inferring config /var/lib/ceph/1f88a1d0-36ee-11ec-b561-09a495c6360a/mon.node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.3: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.3: b"Created osd(s) 0 on host 'sm1'\n"
10.10.2.3: b'\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T02:23:52,035972830-04:00] INFO: > Sleep for 20 seconds\x1b[0m\n'
10.10.2.3: b'\n\n\x1b[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T02:24:12,040999009-04:00] STAGE: Show Cluster Status\x1b[0m\n# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:343 - show_status() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status\n'
10.10.2.3: b'\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T02:24:12,049979572-04:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.3: b"\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T02:24:12,052873986-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', '--status']\x1b[0m\n"
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status\n'
10.10.2.3: b'Inferring fsid 1f88a1d0-36ee-11ec-b561-09a495c6360a\n'
10.10.2.3: b'Inferring config /var/lib/ceph/1f88a1d0-36ee-11ec-b561-09a495c6360a/mon.node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.3: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.3: b'  cluster:\n    id:     1f88a1d0-36ee-11ec-b561-09a495c6360a\n    health: HEALTH_WARN\n            1 pool(s) have no replicas configured\n \n  services:\n    mon: 1 daemons, quorum node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us (age 2m)\n    mgr: node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us.tdoctk(active, since 2m)\n    osd: 1 osds: 1 up (since 20s), 1 in (since 40s)\n \n  data:\n    pools:   1 pools, 128 pgs\n    objects: 0 objects, 0 B\n    usage:   4.9 MiB used, 100 GiB / 100 GiB avail\n    pgs:     99.219% pgs unknown\n             0.781% pgs not active\n             127 unknown\n             1   peering\n \n  progress:\n    Global Recovery Event (0s)\n      [............................] \n \n'
[1] 23:24:21 [SUCCESS] ljishen@10.10.2.3

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T02:21:13,812962418-04:00] INFO: > Remove the existing cluster[0m
## bash:13 -  > ls /var/lib/ceph
## bash:13 -  > tail -n1
# bash:13 -  > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm --host 10.10.2.3 --host 10.10.2.5 rm-cluster --force --fsid 13c63ffc-36ed-11ec-b561-09a495c6360a

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T02:21:22,998520055-04:00] INFO: > Deploy a new cluster[0m
## bash:22 -  > /tmp/bench-rados/ceph-research/scripts/parallel-ssh --host 10.10.2.5 uname --nodename
## bash:22 -  > tail -n1
# bash:22 -  > osd_hostname=sm1
# bash:23 -  > /tmp/bench-rados/ceph-research/scripts/ceph-deploy -m 10.10.2.3 -o sm1/10.10.2.5:/dev/nvme0n1


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T02:21:24,133769500-04:00] STAGE: Verify passwordless SSH to hosts, passwordless sudo via ljishen, and the existence of devices on hosts...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T02:21:24,137525207-04:00] INFO: > Check the host for the monitor[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:181 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@10.10.2.3 sudo --non-interactive --validate

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T02:21:24,288928771-04:00] INFO: > Check hosts for OSDs[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T02:21:24,292858937-04:00] INFO: >> Check host 'sm1'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:200 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 sudo --non-interactive --validate

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T02:21:25,372204752-04:00] INFO: >>> Check device '/dev/nvme0n1'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:215 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 test -b /dev/nvme0n1
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:226 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 lsblk --nodeps --noheadings --output TYPE /dev/nvme0n1
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:227 - verify_parameters() > grep --word-regexp disk


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T02:21:27,529211606-04:00] STAGE: Reset devices on hosts...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T02:21:27,533059516-04:00] INFO: > Resetting device '/dev/nvme0n1' on host 'sm1'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:288 - reset_host_devices() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 sudo sh
  Removing ceph--f2ceff2f--1eac--4725--bb87--f5cdfff6b86b-osd--block--e018ffe4--bc21--452a--97e8--f5ebddb790c0 (252:0)
  Archiving volume group "ceph-f2ceff2f-1eac-4725-bb87-f5cdfff6b86b" metadata (seqno 5).
  Releasing logical volume "osd-block-e018ffe4-bc21-452a-97e8-f5ebddb790c0"
  Creating volume group backup "/etc/lvm/backup/ceph-f2ceff2f-1eac-4725-bb87-f5cdfff6b86b" (seqno 6).
  Logical volume "osd-block-e018ffe4-bc21-452a-97e8-f5ebddb790c0" successfully removed
  Removing physical volume "/dev/nvme0n1" from volume group "ceph-f2ceff2f-1eac-4725-bb87-f5cdfff6b86b"
  Volume group "ceph-f2ceff2f-1eac-4725-bb87-f5cdfff6b86b" successfully removed


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T02:21:29,836227988-04:00] STAGE: Bootstrap a new cluster...[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:312 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.3 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T02:21:29,845026136-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T02:21:29,848101521-04:00] DEBUG: CEPHADM_PARAMS: ['bootstrap', '--config', '/tmp/bench-rados/ceph-research/scripts/ceph.conf', '--mon-ip', '10.10.2.3', '--output-dir', '/tmp/bench-rados/deployment_data_root/etc/ceph', '--ssh-user', 'ljishen', '--skip-dashboard', '--no-minimize-config', '--allow-overwrite', '--allow-fqdn-hostname', '--orphan-initial-daemons', '--skip-monitoring-stack', '--single-host-defaults'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.3 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults
Verifying podman|docker is present...
Verifying lvm2 is present...
Verifying time synchronization is in place...
Unit ntp.service is enabled and running
Repeating the final host check...
podman|docker (/usr/bin/docker) is present
systemctl is present
lvcreate is present
Unit ntp.service is enabled and running
Host looks OK
Cluster fsid: 1f88a1d0-36ee-11ec-b561-09a495c6360a
Verifying IP 10.10.2.3 port 3300 ...
Verifying IP 10.10.2.3 port 6789 ...
Mon IP `10.10.2.3` is in CIDR network `10.10.2.0/24`
- internal network (--cluster-network) has not been provided, OSD replication will default to the public_network
Adjusting default settings to suit single-host cluster...
Pulling container image quay.io/ceph/ceph:v16...
Ceph version: ceph version 16.2.6 (ee28fb57e47e9f88813e24bbf4c14496ca299d31) pacific (stable)
Extracting ceph user uid/gid from container image...
Creating initial keys...
Creating initial monmap...
Creating mon...
Waiting for mon to start...
Waiting for mon...
mon is available
Setting mon public_network to 10.10.2.0/24
Wrote config to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf
Wrote keyring to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring
Creating mgr...
Verifying port 9283 ...
Waiting for mgr to start...
Waiting for mgr...
mgr not available, waiting (1/15)...
mgr not available, waiting (2/15)...
mgr is available
Enabling cephadm module...
Waiting for the mgr to restart...
Waiting for mgr epoch 5...
mgr epoch 5 is available
Setting orchestrator backend to cephadm...
Generating ssh key...
Wrote public SSH key to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub
Adding key to ljishen@localhost authorized_keys...
Adding host node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us...
Deploying unmanaged mon service...
Deploying unmanaged mgr service...
You can access the Ceph CLI with:

	sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --fsid 1f88a1d0-36ee-11ec-b561-09a495c6360a -c /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf -k /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring

Please consider enabling telemetry to help improve Ceph:

	ceph telemetry on

For more information see:

	https://docs.ceph.com/docs/pacific/mgr/telemetry/

Bootstrap complete.

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T02:22:35,442617942-04:00] INFO: > Set the default number of monitor daemons to 1[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:330 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T02:22:35,451396725-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T02:22:35,454837368-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mon', '1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1
Inferring fsid 1f88a1d0-36ee-11ec-b561-09a495c6360a
Inferring config /var/lib/ceph/1f88a1d0-36ee-11ec-b561-09a495c6360a/mon.node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Scheduled mon update...

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T02:22:47,106189225-04:00] INFO: > Set the default number of manager daemons to 1[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:335 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T02:22:47,115089206-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T02:22:47,117945969-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mgr', '1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1
Inferring fsid 1f88a1d0-36ee-11ec-b561-09a495c6360a
Inferring config /var/lib/ceph/1f88a1d0-36ee-11ec-b561-09a495c6360a/mon.node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Scheduled mgr update...


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T02:22:56,543216624-04:00] STAGE: Add hosts to the cluster...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T02:22:56,548275626-04:00] INFO: > Adding host 'sm1'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:362 - add_hosts() > ssh-copy-id -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -f -i /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub ljishen@sm1
/usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: "/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub"

Number of key(s) added: 1

Now try logging into the machine, with:   "ssh -o 'GlobalKnownHostsFile=/dev/null' -o 'LogLevel=ERROR' -o 'PasswordAuthentication=no' -o 'StrictHostKeyChecking=no' -o 'UserKnownHostsFile=/dev/null' 'ljishen@sm1'"
and check to make sure that only the key(s) you wanted were added.

# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:368 - add_hosts() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add sm1 10.10.2.5
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T02:22:57,692732643-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T02:22:57,695700556-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'host', 'add', 'sm1', '10.10.2.5'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add sm1 10.10.2.5
Inferring fsid 1f88a1d0-36ee-11ec-b561-09a495c6360a
Inferring config /var/lib/ceph/1f88a1d0-36ee-11ec-b561-09a495c6360a/mon.node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Added host 'sm1' with addr '10.10.2.5'

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T02:23:08,480564857-04:00] INFO: Sleep for 20 seconds[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T02:23:28,485037962-04:00] STAGE: Add OSDs to the cluster...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T02:23:28,490421556-04:00] INFO: > Adding devices ['/dev/nvme0n1'] on host 'sm1' as OSDs[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:387 - add_host_devices_as_osds() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd sm1:/dev/nvme0n1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T02:23:28,499215837-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T02:23:28,502209989-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'daemon', 'add', 'osd', 'sm1:/dev/nvme0n1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd sm1:/dev/nvme0n1
Inferring fsid 1f88a1d0-36ee-11ec-b561-09a495c6360a
Inferring config /var/lib/ceph/1f88a1d0-36ee-11ec-b561-09a495c6360a/mon.node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Created osd(s) 0 on host 'sm1'

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T02:23:52,035972830-04:00] INFO: > Sleep for 20 seconds[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T02:24:12,040999009-04:00] STAGE: Show Cluster Status[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:343 - show_status() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T02:24:12,049979572-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T02:24:12,052873986-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', '--status'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status
Inferring fsid 1f88a1d0-36ee-11ec-b561-09a495c6360a
Inferring config /var/lib/ceph/1f88a1d0-36ee-11ec-b561-09a495c6360a/mon.node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
  cluster:
    id:     1f88a1d0-36ee-11ec-b561-09a495c6360a
    health: HEALTH_WARN
            1 pool(s) have no replicas configured
 
  services:
    mon: 1 daemons, quorum node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us (age 2m)
    mgr: node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us.tdoctk(active, since 2m)
    osd: 1 osds: 1 up (since 20s), 1 in (since 40s)
 
  data:
    pools:   1 pools, 128 pgs
    objects: 0 objects, 0 B
    usage:   4.9 MiB used, 100 GiB / 100 GiB avail
    pgs:     99.219% pgs unknown
             0.781% pgs not active
             127 unknown
             1   peering
 
  progress:
    Global Recovery Event (0s)
      [............................] 
 

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:24:21,123997749-07:00] INFO: > Copy configuration files to local[0m
# ./benchmarks/bench-rados:208 - launch_ceph_cluster() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.3:/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf
# ./benchmarks/bench-rados:212 - launch_ceph_cluster() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.3 sudo chmod 644 /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:24:21,279019047-07:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.3 sudo chmod 644 /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring
[1] 23:24:21 [SUCCESS] ljishen@10.10.2.3
# ./benchmarks/bench-rados:214 - launch_ceph_cluster() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.3:/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:24:21,606732046-07:00] STAGE: Update local ceph.conf as a client...[0m


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:24:21,610729450-07:00] STAGE: Configure Ceph pool...[0m
# ./benchmarks/bench-rados:245 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_max_backfills 32
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:24:21,636550615-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:24:21,639774846-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '1f88a1d0-36ee-11ec-b561-09a495c6360a', '--', 'ceph', 'config', 'set', 'osd', 'osd_max_backfills', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 1f88a1d0-36ee-11ec-b561-09a495c6360a -- ceph config set osd osd_max_backfills 32
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:246 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_active 32
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:24:24,869266490-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:24:24,872918344-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '1f88a1d0-36ee-11ec-b561-09a495c6360a', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_active', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 1f88a1d0-36ee-11ec-b561-09a495c6360a -- ceph config set osd osd_recovery_max_active 32
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:247 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_single_start 8
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:24:28,284456594-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:24:28,287882314-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '1f88a1d0-36ee-11ec-b561-09a495c6360a', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_single_start', '8'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 1f88a1d0-36ee-11ec-b561-09a495c6360a -- ceph config set osd osd_recovery_max_single_start 8
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:248 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_op_priority 63
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:24:31,584885257-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:24:31,588550366-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '1f88a1d0-36ee-11ec-b561-09a495c6360a', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_op_priority', '63'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 1f88a1d0-36ee-11ec-b561-09a495c6360a -- ceph config set osd osd_recovery_op_priority 63
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:250 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config show-with-defaults osd.0
# ./benchmarks/bench-rados:251 - configure_ceph_pool() > grep 'osd_max_backfills\|osd_recovery'
# ./benchmarks/bench-rados:251 - configure_ceph_pool() > column -t -s ' '
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
osd_max_backfills                        32         mon
osd_recovery_cost                        20971520   default
osd_recovery_delay_start                 0.000000   default
osd_recovery_max_active                  32         mon
osd_recovery_max_active_hdd              3          default
osd_recovery_max_active_ssd              10         default
osd_recovery_max_chunk                   8388608    default
osd_recovery_max_omap_entries_per_chunk  8096       default
osd_recovery_max_single_start            8          mon
osd_recovery_op_priority                 63         mon
osd_recovery_op_warn_multiple            16         default
osd_recovery_priority                    5          default
osd_recovery_retry_interval              30.000000  default
osd_recovery_sleep                       0.000000   default
osd_recovery_sleep_hdd                   0.100000   default
osd_recovery_sleep_hybrid                0.025000   default
osd_recovery_sleep_ssd                   0.000000   default
# ./benchmarks/bench-rados:253 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool create bench_rados 128 128 replicated --size 1 --pg-num-min 128
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:24:38,142327415-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:24:38,145543230-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '1f88a1d0-36ee-11ec-b561-09a495c6360a', '--', 'ceph', 'osd', 'pool', 'create', 'bench_rados', '128', '128', 'replicated', '--size', '1', '--pg-num-min', '128'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 1f88a1d0-36ee-11ec-b561-09a495c6360a -- ceph osd pool create bench_rados 128 128 replicated --size 1 --pg-num-min 128
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
pool 'bench_rados' created
# ./benchmarks/bench-rados:255 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados min_size 1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:24:42,433317939-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:24:42,436645043-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '1f88a1d0-36ee-11ec-b561-09a495c6360a', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'min_size', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 1f88a1d0-36ee-11ec-b561-09a495c6360a -- ceph osd pool set bench_rados min_size 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 min_size to 1
# ./benchmarks/bench-rados:256 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados pg_autoscale_mode off
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:24:46,765682984-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:24:46,769288230-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '1f88a1d0-36ee-11ec-b561-09a495c6360a', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'pg_autoscale_mode', 'off'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 1f88a1d0-36ee-11ec-b561-09a495c6360a -- ceph osd pool set bench_rados pg_autoscale_mode off
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 pg_autoscale_mode to off
# ./benchmarks/bench-rados:257 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool application enable bench_rados benchmark
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:24:50,829999959-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:24:50,833529974-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '1f88a1d0-36ee-11ec-b561-09a495c6360a', '--', 'ceph', 'osd', 'pool', 'application', 'enable', 'bench_rados', 'benchmark'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 1f88a1d0-36ee-11ec-b561-09a495c6360a -- ceph osd pool application enable bench_rados benchmark
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
enabled application 'benchmark' on pool 'bench_rados'
# ./benchmarks/bench-rados:258 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados noscrub 1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:24:54,589432153-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:24:54,592764658-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '1f88a1d0-36ee-11ec-b561-09a495c6360a', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'noscrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 1f88a1d0-36ee-11ec-b561-09a495c6360a -- ceph osd pool set bench_rados noscrub 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 noscrub to 1
# ./benchmarks/bench-rados:259 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados nodeep-scrub 1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:24:58,080950280-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:24:58,084524228-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '1f88a1d0-36ee-11ec-b561-09a495c6360a', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'nodeep-scrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 1f88a1d0-36ee-11ec-b561-09a495c6360a -- ceph osd pool set bench_rados nodeep-scrub 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 nodeep-scrub to 1
# ./benchmarks/bench-rados:260 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool ls detail
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:25:02,688059792-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:25:02,692323116-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '1f88a1d0-36ee-11ec-b561-09a495c6360a', '--', 'ceph', 'osd', 'pool', 'ls', 'detail'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 1f88a1d0-36ee-11ec-b561-09a495c6360a -- ceph osd pool ls detail
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
pool 1 'device_health_metrics' replicated size 1 min_size 1 crush_rule 0 object_hash rjenkins pg_num 128 pgp_num 128 autoscale_mode on last_change 13 lfor 0/0/11 flags hashpspool stripe_width 0 pg_num_min 1 application mgr_devicehealth
pool 2 'bench_rados' replicated size 1 min_size 1 crush_rule 0 object_hash rjenkins pg_num 128 pgp_num 128 autoscale_mode off last_change 21 flags hashpspool,noscrub,nodeep-scrub stripe_width 0 pg_num_min 128 application benchmark

# ./benchmarks/bench-rados:261 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd df tree
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:25:05,829984882-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:25:05,833411443-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '1f88a1d0-36ee-11ec-b561-09a495c6360a', '--', 'ceph', 'osd', 'df', 'tree'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 1f88a1d0-36ee-11ec-b561-09a495c6360a -- ceph osd df tree
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
ID  CLASS  WEIGHT   REWEIGHT  SIZE     RAW USE  DATA     OMAP  META     AVAIL    %USE  VAR   PGS  STATUS  TYPE NAME    
-1         0.09769         -  100 GiB  6.0 MiB  176 KiB   0 B  5.8 MiB  100 GiB  0.01  1.00    -          root default 
-3         0.09769         -  100 GiB  6.0 MiB  176 KiB   0 B  5.8 MiB  100 GiB  0.01  1.00    -              host sm1 
 0    ssd  0.09769   1.00000  100 GiB  6.0 MiB  176 KiB   0 B  5.8 MiB  100 GiB  0.01  1.00  256      up          osd.0
                       TOTAL  100 GiB  6.0 MiB  176 KiB   0 B  5.8 MiB  100 GiB  0.01                                  
MIN/MAX VAR: 1.00/1.00  STDDEV: 0


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:25:09,155722658-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:25:32,394874873-07:00] INFO: >>   data:
    pools:   2 pools, 256 pgs
    objects: 0 objects, 0 B
    usage:   6.0 MiB used, 100 GiB / 100 GiB avail
    pgs:     256 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:25:32,403046633-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:25:40,750804776-07:00] INFO: >>   data:
    pools:   2 pools, 256 pgs
    objects: 0 objects, 0 B
    usage:   6.0 MiB used, 100 GiB / 100 GiB avail
    pgs:     256 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:25:40,759809062-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:25:49,024207218-07:00] INFO: >>   data:
    pools:   2 pools, 256 pgs
    objects: 0 objects, 0 B
    usage:   6.0 MiB used, 100 GiB / 100 GiB avail
    pgs:     256 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:25:49,032405748-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:25:57,356434269-07:00] INFO: >>   data:
    pools:   2 pools, 256 pgs
    objects: 0 objects, 0 B
    usage:   6.0 MiB used, 100 GiB / 100 GiB avail
    pgs:     256 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:25:57,364467038-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:26:05,664461764-07:00] INFO: >>   data:
    pools:   2 pools, 256 pgs
    objects: 0 objects, 0 B
    usage:   6.0 MiB used, 100 GiB / 100 GiB avail
    pgs:     256 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:26:05,673412679-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:26:05,680499720-07:00] INFO: > The cluster is idle now.[0m


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:26:05,684754848-07:00] STAGE: Start benchmarking of mode write ...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:26:05,692779211-07:00] INFO: > Start system activity collection services[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:26:05,699553846-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:329 - rados_bench() > rm --force /mnt/sda8/ceph-research/scripts/output/sys_activity_4MB_write.dat.rados_bench_host.2
# ./benchmarks/bench-rados:330 - rados_bench() > sar_pid_rados_bench_host=880521
# ./benchmarks/bench-rados:330 - rados_bench() > S_TIME_FORMAT=ISO
# ./benchmarks/bench-rados:330 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/output/sys_activity_4MB_write.dat.rados_bench_host.2 2

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:26:05,707015561-07:00] INFO: >> for OSD host[0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:26:05,716531488-07:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.5 bash
10.10.2.5: b'118131\n'
[1] 23:26:06 [SUCCESS] ljishen@10.10.2.5
118131

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:26:06,837406896-07:00] INFO: > Run rados bench[0m
# ./benchmarks/bench-rados:358 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/output/rados_bench_4MB_write.log.2
## ./benchmarks/bench-rados:349 - rados_bench() > count_cpus 32-63,96-127
# ./benchmarks/bench-rados:349 - rados_bench() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 30 write --pool bench_rados -b 4194304 -O 4194304 --concurrent-ios 128 --show-time --write-object --write-omap --write-xattr --no-cleanup
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:26:06,859940011-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:26:06,863485736-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '1f88a1d0-36ee-11ec-b561-09a495c6360a', '--', 'rados', 'bench', '30', 'write', '--pool', 'bench_rados', '-b', '4194304', '-O', '4194304', '--concurrent-ios', '128', '--show-time', '--write-object', '--write-omap', '--write-xattr', '--no-cleanup'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 1f88a1d0-36ee-11ec-b561-09a495c6360a -- rados bench 30 write --pool bench_rados -b 4194304 -O 4194304 --concurrent-ios 128 --show-time --write-object --write-omap --write-xattr --no-cleanup
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
hints = 1
2021-10-27T06:26:09.245627+0000 Maintaining 128 concurrent writes of 4194304 bytes to objects of size 4194304 for up to 30 seconds or 0 objects
2021-10-27T06:26:09.245638+0000 Object prefix: benchmark_data_node-0.bluefield2.ucsc-cmps10_7
2021-10-27T06:26:09.355505+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-27T06:26:09.355505+0000     0       0         0         0         0         0           -           0
2021-10-27T06:26:10.355635+0000     1      21        21         0         0         0           -           0
2021-10-27T06:26:11.355742+0000     2      39        39         0         0         0           -           0
2021-10-27T06:26:12.355863+0000     3      55        55         0         0         0           -           0
2021-10-27T06:26:13.355971+0000     4      74        74         0         0         0           -           0
2021-10-27T06:26:14.356085+0000     5      88        88         0         0         0           -           0
2021-10-27T06:26:15.356211+0000     6     108       108         0         0         0           -           0
2021-10-27T06:26:16.356338+0000     7     125       125         0         0         0           -           0
2021-10-27T06:26:17.356446+0000     8     127       139        12   5.99934         6     7.38522     7.48451
2021-10-27T06:26:18.356711+0000     9     127       147        20   8.88775        32     8.05297     7.69238
2021-10-27T06:26:19.356828+0000    10     127       155        28   11.1986        32     8.60997     7.93159
2021-10-27T06:26:20.356959+0000    11     127       163        36   13.0892        32     9.10195     8.18041
2021-10-27T06:26:21.357093+0000    12     127       171        44   14.6648        32     9.50667     8.43356
2021-10-27T06:26:22.357272+0000    13     127       179        52   15.9979        32     10.1754      8.6886
2021-10-27T06:26:23.357396+0000    14     127       193        66   18.8547        56     10.2323     9.02002
2021-10-27T06:26:24.357495+0000    15     127       212        85   22.6637        76     10.3836     9.31785
2021-10-27T06:26:25.357591+0000    16     127       230       103   25.7467        72     10.3185     9.49883
2021-10-27T06:26:26.357706+0000    17     127       246       119   27.9965        64     10.4008      9.6191
2021-10-27T06:26:27.357815+0000    18     127       264       137   30.4406        72     10.0235     9.71483
2021-10-27T06:26:28.357909+0000    19     127       272       145   30.5225        32      10.036     9.75246
2021-10-27T06:26:29.358009+0000 min lat: 7.14055 max lat: 10.6377 avg lat: 9.76941
2021-10-27T06:26:29.358009+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-27T06:26:29.358009+0000    20     127       275       148   29.5964        12     10.5893     9.76941
2021-10-27T06:26:30.358129+0000    21     127       280       153   29.1393        20     10.7774     9.81426
2021-10-27T06:26:31.358236+0000    22     127       283       156   28.3602        12     11.5622     9.84762
2021-10-27T06:26:32.358329+0000    23     127       287       160   27.8227        16     11.4493     9.89853
2021-10-27T06:26:33.358441+0000    24     127       291       164   27.3301        16     12.4186     9.95997
2021-10-27T06:26:34.358541+0000    25     127       295       168   26.8768        16     12.4771     10.0306
2021-10-27T06:26:35.358636+0000    26     127       307       180    27.689        48     13.0935     10.2429
2021-10-27T06:26:36.358760+0000    27     127       316       189   27.9967        36     13.5194     10.3927
2021-10-27T06:26:37.358981+0000    28     127       324       197   28.1394        32     13.5879     10.5263
2021-10-27T06:26:38.359252+0000    29     127       337       210   28.9618        52     14.1195     10.7424
2021-10-27T06:26:39.359541+0000    30     127       348       221   29.4628        44     14.5328     10.9251
2021-10-27T06:26:40.359744+0000 Total time run:         30.6911
Total writes made:      349
Write size:             4194304
Object size:            4194304
Bandwidth (MB/sec):     45.4855
Stddev Bandwidth:       23.9568
Max bandwidth (MB/sec): 76
Min bandwidth (MB/sec): 0
Average IOPS:           11
Stddev IOPS:            6.00574
Max IOPS:               19
Min IOPS:               0
Average Latency(s):     9.87896
Stddev Latency(s):      3.63029
Max latency(s):         14.8406
Min latency(s):         0.7315

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:26:40,887128319-07:00] INFO: > Stop system activity collection services[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:26:40,894017058-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:379 - rados_bench() > kill -INT 880521

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:26:40,900790359-07:00] INFO: >> for OSD host[0m
# ./benchmarks/bench-rados:384 - rados_bench() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.5 kill -INT 118131
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:26:40,908906555-07:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.5 kill -INT 118131
[1] 23:26:42 [SUCCESS] ljishen@10.10.2.5

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:26:42,360569964-07:00] INFO: >>> Copy system activity file from OSD host[0m
## ./benchmarks/bench-rados:391 - rados_bench() > basename -- /tmp/bench-rados/sys_activity_4MB_write.dat.osd_host.2
# ./benchmarks/bench-rados:391 - rados_bench() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.5:/tmp/bench-rados/sys_activity_4MB_write.dat.osd_host.2 /mnt/sda8/ceph-research/scripts/output/sys_activity_4MB_write.dat.osd_host.2


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:26:43,681175328-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:27:06,908409253-07:00] INFO: >>   data:
    pools:   2 pools, 256 pgs
    objects: 350 objects, 1.4 GiB
    usage:   4.1 GiB used, 96 GiB / 100 GiB avail
    pgs:     256 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:27:06,917035888-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:27:15,333517997-07:00] INFO: >>   data:
    pools:   2 pools, 256 pgs
    objects: 350 objects, 1.4 GiB
    usage:   4.1 GiB used, 96 GiB / 100 GiB avail
    pgs:     256 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:27:15,342108965-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:27:23,708360839-07:00] INFO: >>   data:
    pools:   2 pools, 256 pgs
    objects: 350 objects, 1.4 GiB
    usage:   4.1 GiB used, 96 GiB / 100 GiB avail
    pgs:     256 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:27:23,717430417-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:27:32,132955809-07:00] INFO: >>   data:
    pools:   2 pools, 256 pgs
    objects: 350 objects, 1.4 GiB
    usage:   4.1 GiB used, 96 GiB / 100 GiB avail
    pgs:     256 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:27:32,140788753-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:27:40,523450363-07:00] INFO: >>   data:
    pools:   2 pools, 256 pgs
    objects: 350 objects, 1.4 GiB
    usage:   4.1 GiB used, 96 GiB / 100 GiB avail
    pgs:     256 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:27:40,532934280-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:27:40,540130737-07:00] INFO: > The cluster is idle now.[0m


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:27:40,544353744-07:00] STAGE: Start benchmarking of mode seq ...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:27:40,552293679-07:00] INFO: > Start system activity collection services[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:27:40,558441625-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:329 - rados_bench() > rm --force /mnt/sda8/ceph-research/scripts/output/sys_activity_4MB_seq.dat.rados_bench_host.2
# ./benchmarks/bench-rados:330 - rados_bench() > sar_pid_rados_bench_host=882724
# ./benchmarks/bench-rados:330 - rados_bench() > S_TIME_FORMAT=ISO
# ./benchmarks/bench-rados:330 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/output/sys_activity_4MB_seq.dat.rados_bench_host.2 2

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:27:40,566008278-07:00] INFO: >> for OSD host[0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:27:40,574899140-07:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.5 bash
10.10.2.5: b'118664\n'
[1] 23:27:41 [SUCCESS] ljishen@10.10.2.5
118664

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:27:41,685022910-07:00] INFO: > Run rados bench[0m
# ./benchmarks/bench-rados:367 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/output/rados_bench_4MB_seq.log.2
## ./benchmarks/bench-rados:364 - rados_bench() > count_cpus 32-63,96-127
# ./benchmarks/bench-rados:364 - rados_bench() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 128 --show-time
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:27:41,706778673-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:27:41,710143809-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '1f88a1d0-36ee-11ec-b561-09a495c6360a', '--', 'rados', 'bench', '99999999', 'seq', '--pool', 'bench_rados', '--concurrent-ios', '128', '--show-time'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 1f88a1d0-36ee-11ec-b561-09a495c6360a -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 128 --show-time
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
hints = 1
2021-10-27T06:27:44.135828+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-27T06:27:44.135828+0000     0       1         1         0         0         0           -           0
2021-10-27T06:27:45.135934+0000     1     127       232       105   419.926       420    0.590461    0.589195
2021-10-27T06:27:46.136133+0000 Total time run:       1.65489
Total reads made:     349
Read size:            4194304
Object size:          4194304
Bandwidth (MB/sec):   843.562
Average IOPS:         210
Stddev IOPS:          0
Max IOPS:             105
Min IOPS:             105
Average Latency(s):   0.493385
Max latency(s):       0.63102
Min latency(s):       0.0612694

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:27:46,772374093-07:00] INFO: > Stop system activity collection services[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:27:46,779522580-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:379 - rados_bench() > kill -INT 882724

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:27:46,786287566-07:00] INFO: >> for OSD host[0m
# ./benchmarks/bench-rados:384 - rados_bench() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.5 kill -INT 118664
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:27:46,794453725-07:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.5 kill -INT 118664
[1] 23:27:47 [SUCCESS] ljishen@10.10.2.5

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:27:47,899553111-07:00] INFO: >>> Copy system activity file from OSD host[0m
## ./benchmarks/bench-rados:391 - rados_bench() > basename -- /tmp/bench-rados/sys_activity_4MB_seq.dat.osd_host.2
# ./benchmarks/bench-rados:391 - rados_bench() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.5:/tmp/bench-rados/sys_activity_4MB_seq.dat.osd_host.2 /mnt/sda8/ceph-research/scripts/output/sys_activity_4MB_seq.dat.osd_host.2


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:27:48,973121409-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:28:12,192571217-07:00] INFO: >>   data:
    pools:   2 pools, 256 pgs
    objects: 350 objects, 1.4 GiB
    usage:   4.1 GiB used, 96 GiB / 100 GiB avail
    pgs:     256 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:28:12,201518065-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:28:20,608396718-07:00] INFO: >>   data:
    pools:   2 pools, 256 pgs
    objects: 350 objects, 1.4 GiB
    usage:   4.1 GiB used, 96 GiB / 100 GiB avail
    pgs:     256 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:28:20,617449654-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:28:28,973784032-07:00] INFO: >>   data:
    pools:   2 pools, 256 pgs
    objects: 350 objects, 1.4 GiB
    usage:   4.1 GiB used, 96 GiB / 100 GiB avail
    pgs:     256 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:28:28,982308325-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:28:37,223400938-07:00] INFO: >>   data:
    pools:   2 pools, 256 pgs
    objects: 350 objects, 1.4 GiB
    usage:   4.1 GiB used, 96 GiB / 100 GiB avail
    pgs:     256 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:28:37,232752946-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:28:45,388828025-07:00] INFO: >>   data:
    pools:   2 pools, 256 pgs
    objects: 350 objects, 1.4 GiB
    usage:   4.1 GiB used, 96 GiB / 100 GiB avail
    pgs:     256 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:28:45,397900479-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:28:45,405094030-07:00] INFO: > The cluster is idle now.[0m


[1;7;39;49m[2021-10-26T23:28:45,407706050-07:00][RUNNING][ROUND 3/6/21] object_size=4MB[0m


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:28:45,412122501-07:00] STAGE: Launch a Ceph cluster (CEPH_HOST: 10.10.2.3, OSD_HOST: 10.10.2.5) ...[0m
# ./benchmarks/bench-rados:172 - launch_ceph_cluster() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.3 bash -euo pipefail
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:28:45,420582393-07:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.3 bash -euo pipefail
10.10.2.3: b'\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T02:28:45,844205778-04:00] INFO: > Remove the existing cluster\x1b[0m\n'
10.10.2.3: b'## bash:13 -  > ls /var/lib/ceph\n'
10.10.2.3: b'## bash:13 -  > tail -n1\n'
10.10.2.3: b'# bash:13 -  > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm --host 10.10.2.3 --host 10.10.2.5 rm-cluster --force --fsid 1f88a1d0-36ee-11ec-b561-09a495c6360a\n'
10.10.2.3: b'\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T02:28:54,990129611-04:00] INFO: > Deploy a new cluster\x1b[0m\n'
10.10.2.3: b'## bash:22 -  > /tmp/bench-rados/ceph-research/scripts/parallel-ssh --host 10.10.2.5 uname --nodename\n'
10.10.2.3: b'## bash:22 -  > tail -n1\n'
10.10.2.3: b'# bash:22 -  > osd_hostname=sm1\n# bash:23 -  > /tmp/bench-rados/ceph-research/scripts/ceph-deploy -m 10.10.2.3 -o sm1/10.10.2.5:/dev/nvme0n1\n'
10.10.2.3: b'\n\n\x1b[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T02:28:56,117129466-04:00] STAGE: Verify passwordless SSH to hosts, passwordless sudo via ljishen, and the existence of devices on hosts...\x1b[0m\n'
10.10.2.3: b'\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T02:28:56,121209023-04:00] INFO: > Check the host for the monitor\x1b[0m\n# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:181 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@10.10.2.3 sudo --non-interactive --validate\n'
10.10.2.3: b'\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T02:28:56,272585998-04:00] INFO: > Check hosts for OSDs\x1b[0m\n'
10.10.2.3: b"\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T02:28:56,276653002-04:00] INFO: >> Check host 'sm1'\x1b[0m\n# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:200 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 sudo --non-interactive --validate\n"
10.10.2.3: b"\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T02:28:57,348438965-04:00] INFO: >>> Check device '/dev/nvme0n1'\x1b[0m\n"
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:215 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 test -b /dev/nvme0n1\n'
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:226 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 lsblk --nodeps --noheadings --output TYPE /dev/nvme0n1\n'
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:227 - verify_parameters() > grep --word-regexp disk\n'
10.10.2.3: b'\n\n\x1b[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T02:28:59,497026954-04:00] STAGE: Reset devices on hosts...\x1b[0m\n'
10.10.2.3: b"\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T02:28:59,500960526-04:00] INFO: > Resetting device '/dev/nvme0n1' on host 'sm1'\x1b[0m\n"
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:288 - reset_host_devices() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 sudo sh\n'
10.10.2.3: b'  Removing ceph--977031cb--95f6--4831--adad--f208088bc096-osd--block--1acf9e02--0136--4d8a--a375--5d05dbcd06b2 (252:0)\n'
10.10.2.3: b'  Archiving volume group "ceph-977031cb-95f6-4831-adad-f208088bc096" metadata (seqno 5).\n'
10.10.2.3: b'  Releasing logical volume "osd-block-1acf9e02-0136-4d8a-a375-5d05dbcd06b2"\n'
10.10.2.3: b'  Creating volume group backup "/etc/lvm/backup/ceph-977031cb-95f6-4831-adad-f208088bc096" (seqno 6).\n'
10.10.2.3: b'  Logical volume "osd-block-1acf9e02-0136-4d8a-a375-5d05dbcd06b2" successfully removed\n'
10.10.2.3: b'  Removing physical volume "/dev/nvme0n1" from volume group "ceph-977031cb-95f6-4831-adad-f208088bc096"\n'
10.10.2.3: b'  Volume group "ceph-977031cb-95f6-4831-adad-f208088bc096" successfully removed\n'
10.10.2.3: b'\n\n\x1b[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T02:29:01,776555579-04:00] STAGE: Bootstrap a new cluster...\x1b[0m\n# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:312 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.3 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults\n'
10.10.2.3: b'\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T02:29:01,785111862-04:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.3: b"\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T02:29:01,787997319-04:00] DEBUG: CEPHADM_PARAMS: ['bootstrap', '--config', '/tmp/bench-rados/ceph-research/scripts/ceph.conf', '--mon-ip', '10.10.2.3', '--output-dir', '/tmp/bench-rados/deployment_data_root/etc/ceph', '--ssh-user', 'ljishen', '--skip-dashboard', '--no-minimize-config', '--allow-overwrite', '--allow-fqdn-hostname', '--orphan-initial-daemons', '--skip-monitoring-stack', '--single-host-defaults']\x1b[0m\n"
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.3 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults\n'
10.10.2.3: b'Verifying podman|docker is present...\n'
10.10.2.3: b'Verifying lvm2 is present...\nVerifying time synchronization is in place...\n'
10.10.2.3: b'Unit ntp.service is enabled and running\n'
10.10.2.3: b'Repeating the final host check...\npodman|docker (/usr/bin/docker) is present\nsystemctl is present\n'
10.10.2.3: b'lvcreate is present\n'
10.10.2.3: b'Unit ntp.service is enabled and running\nHost looks OK\n'
10.10.2.3: b'Cluster fsid: 2ce98762-36ef-11ec-b561-09a495c6360a\n'
10.10.2.3: b'Verifying IP 10.10.2.3 port 3300 ...\n'
10.10.2.3: b'Verifying IP 10.10.2.3 port 6789 ...\n'
10.10.2.3: b'Mon IP `10.10.2.3` is in CIDR network `10.10.2.0/24`\n'
10.10.2.3: b'- internal network (--cluster-network) has not been provided, OSD replication will default to the public_network\n'
10.10.2.3: b'Adjusting default settings to suit single-host cluster...\n'
10.10.2.3: b'Pulling container image quay.io/ceph/ceph:v16...\n'
10.10.2.3: b'Ceph version: ceph version 16.2.6 (ee28fb57e47e9f88813e24bbf4c14496ca299d31) pacific (stable)\nExtracting ceph user uid/gid from container image...\n'
10.10.2.3: b'Creating initial keys...\n'
10.10.2.3: b'Creating initial monmap...\n'
10.10.2.3: b'Creating mon...\n'
10.10.2.3: b'Waiting for mon to start...\nWaiting for mon...\n'
10.10.2.3: b'mon is available\nSetting mon public_network to 10.10.2.0/24\n'
10.10.2.3: b'Wrote config to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf\n'
10.10.2.3: b'Wrote keyring to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring\n'
10.10.2.3: b'Creating mgr...\nVerifying port 9283 ...\n'
10.10.2.3: b'Waiting for mgr to start...\n'
10.10.2.3: b'Waiting for mgr...\n'
10.10.2.3: b'mgr not available, waiting (1/15)...\n'
10.10.2.3: b'mgr not available, waiting (2/15)...\n'
10.10.2.3: b'mgr is available\n'
10.10.2.3: b'Enabling cephadm module...\n'
10.10.2.3: b'Waiting for the mgr to restart...\nWaiting for mgr epoch 5...\n'
10.10.2.3: b'mgr epoch 5 is available\nSetting orchestrator backend to cephadm...\n'
10.10.2.3: b'Generating ssh key...\n'
10.10.2.3: b'Wrote public SSH key to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub\n'
10.10.2.3: b'Adding key to ljishen@localhost authorized_keys...\n'
10.10.2.3: b'Adding host node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us...\n'
10.10.2.3: b'Deploying unmanaged mon service...\n'
10.10.2.3: b'Deploying unmanaged mgr service...\n'
10.10.2.3: b'You can access the Ceph CLI with:\n\n\tsudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --fsid 2ce98762-36ef-11ec-b561-09a495c6360a -c /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf -k /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring\n\n'
10.10.2.3: b'Please consider enabling telemetry to help improve Ceph:\n\n\tceph telemetry on\n\nFor more information see:\n\n\thttps://docs.ceph.com/docs/pacific/mgr/telemetry/\n\nBootstrap complete.\n'
10.10.2.3: b'\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T02:30:07,368866775-04:00] INFO: > Set the default number of monitor daemons to 1\x1b[0m\n# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:330 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1\n'
10.10.2.3: b'\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T02:30:07,378023811-04:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.3: b"\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T02:30:07,381065041-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mon', '1']\x1b[0m\n"
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1\n'
10.10.2.3: b'Inferring fsid 2ce98762-36ef-11ec-b561-09a495c6360a\n'
10.10.2.3: b'Inferring config /var/lib/ceph/2ce98762-36ef-11ec-b561-09a495c6360a/mon.node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.3: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.3: b'Scheduled mon update...\n'
10.10.2.3: b'\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T02:30:16,793899970-04:00] INFO: > Set the default number of manager daemons to 1\x1b[0m\n# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:335 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1\n'
10.10.2.3: b'\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T02:30:16,802721724-04:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.3: b"\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T02:30:16,805550925-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mgr', '1']\x1b[0m\n"
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1\n'
10.10.2.3: b'Inferring fsid 2ce98762-36ef-11ec-b561-09a495c6360a\n'
10.10.2.3: b'Inferring config /var/lib/ceph/2ce98762-36ef-11ec-b561-09a495c6360a/mon.node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.3: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.3: b'Scheduled mgr update...\n'
10.10.2.3: b'\n\n\x1b[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T02:30:26,841803334-04:00] STAGE: Add hosts to the cluster...\x1b[0m\n'
10.10.2.3: b"\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T02:30:26,846894036-04:00] INFO: > Adding host 'sm1'\x1b[0m\n"
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:362 - add_hosts() > ssh-copy-id -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -f -i /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub ljishen@sm1\n'
10.10.2.3: b'/usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: "/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub"\n'
10.10.2.3: b'\nNumber of key(s) added: 1\n\nNow try logging into the machine, with:   "ssh -o \'GlobalKnownHostsFile=/dev/null\' -o \'LogLevel=ERROR\' -o \'PasswordAuthentication=no\' -o \'StrictHostKeyChecking=no\' -o \'UserKnownHostsFile=/dev/null\' \'ljishen@sm1\'"\nand check to make sure that only the key(s) you wanted were added.\n\n'
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:368 - add_hosts() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add sm1 10.10.2.5\n'
10.10.2.3: b'\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T02:30:28,452780455-04:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.3: b"\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T02:30:28,455663758-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'host', 'add', 'sm1', '10.10.2.5']\x1b[0m\n"
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add sm1 10.10.2.5\n'
10.10.2.3: b'Inferring fsid 2ce98762-36ef-11ec-b561-09a495c6360a\n'
10.10.2.3: b'Inferring config /var/lib/ceph/2ce98762-36ef-11ec-b561-09a495c6360a/mon.node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.3: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.3: b"Added host 'sm1' with addr '10.10.2.5'\n"
10.10.2.3: b'\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T02:30:39,695253511-04:00] INFO: Sleep for 20 seconds\x1b[0m\n'
10.10.2.3: b'\n\n\x1b[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T02:30:59,699660362-04:00] STAGE: Add OSDs to the cluster...\x1b[0m\n'
10.10.2.3: b"\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T02:30:59,705065587-04:00] INFO: > Adding devices ['/dev/nvme0n1'] on host 'sm1' as OSDs\x1b[0m\n# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:387 - add_host_devices_as_osds() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd sm1:/dev/nvme0n1\n"
10.10.2.3: b'\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T02:30:59,713490913-04:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.3: b"\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T02:30:59,716409433-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'daemon', 'add', 'osd', 'sm1:/dev/nvme0n1']\x1b[0m\n"
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd sm1:/dev/nvme0n1\n'
10.10.2.3: b'Inferring fsid 2ce98762-36ef-11ec-b561-09a495c6360a\n'
10.10.2.3: b'Inferring config /var/lib/ceph/2ce98762-36ef-11ec-b561-09a495c6360a/mon.node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.3: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.3: b"Created osd(s) 0 on host 'sm1'\n"
10.10.2.3: b'\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T02:31:23,072546137-04:00] INFO: > Sleep for 20 seconds\x1b[0m\n'
10.10.2.3: b'\n\n\x1b[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T02:31:43,077022731-04:00] STAGE: Show Cluster Status\x1b[0m\n# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:343 - show_status() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status\n'
10.10.2.3: b'\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T02:31:43,085925948-04:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.3: b"\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T02:31:43,088935530-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', '--status']\x1b[0m\n"
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status\n'
10.10.2.3: b'Inferring fsid 2ce98762-36ef-11ec-b561-09a495c6360a\n'
10.10.2.3: b'Inferring config /var/lib/ceph/2ce98762-36ef-11ec-b561-09a495c6360a/mon.node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.3: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.3: b'  cluster:\n    id:     2ce98762-36ef-11ec-b561-09a495c6360a\n    health: HEALTH_WARN\n            1 pool(s) have no replicas configured\n \n  services:\n    mon: 1 daemons, quorum node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us (age 2m)\n    mgr: node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us.denrrz(active, since 2m)\n    osd: 1 osds: 1 up (since 20s), 1 in (since 39s)\n \n  data:\n    pools:   1 pools, 128 pgs\n    objects: 0 objects, 0 B\n    usage:   4.7 MiB used, 100 GiB / 100 GiB avail\n    pgs:     99.219% pgs unknown\n             127 unknown\n             1   active+clean\n \n  progress:\n \n'
[1] 23:31:52 [SUCCESS] ljishen@10.10.2.3

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T02:28:45,844205778-04:00] INFO: > Remove the existing cluster[0m
## bash:13 -  > ls /var/lib/ceph
## bash:13 -  > tail -n1
# bash:13 -  > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm --host 10.10.2.3 --host 10.10.2.5 rm-cluster --force --fsid 1f88a1d0-36ee-11ec-b561-09a495c6360a

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T02:28:54,990129611-04:00] INFO: > Deploy a new cluster[0m
## bash:22 -  > /tmp/bench-rados/ceph-research/scripts/parallel-ssh --host 10.10.2.5 uname --nodename
## bash:22 -  > tail -n1
# bash:22 -  > osd_hostname=sm1
# bash:23 -  > /tmp/bench-rados/ceph-research/scripts/ceph-deploy -m 10.10.2.3 -o sm1/10.10.2.5:/dev/nvme0n1


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T02:28:56,117129466-04:00] STAGE: Verify passwordless SSH to hosts, passwordless sudo via ljishen, and the existence of devices on hosts...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T02:28:56,121209023-04:00] INFO: > Check the host for the monitor[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:181 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@10.10.2.3 sudo --non-interactive --validate

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T02:28:56,272585998-04:00] INFO: > Check hosts for OSDs[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T02:28:56,276653002-04:00] INFO: >> Check host 'sm1'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:200 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 sudo --non-interactive --validate

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T02:28:57,348438965-04:00] INFO: >>> Check device '/dev/nvme0n1'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:215 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 test -b /dev/nvme0n1
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:226 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 lsblk --nodeps --noheadings --output TYPE /dev/nvme0n1
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:227 - verify_parameters() > grep --word-regexp disk


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T02:28:59,497026954-04:00] STAGE: Reset devices on hosts...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T02:28:59,500960526-04:00] INFO: > Resetting device '/dev/nvme0n1' on host 'sm1'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:288 - reset_host_devices() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 sudo sh
  Removing ceph--977031cb--95f6--4831--adad--f208088bc096-osd--block--1acf9e02--0136--4d8a--a375--5d05dbcd06b2 (252:0)
  Archiving volume group "ceph-977031cb-95f6-4831-adad-f208088bc096" metadata (seqno 5).
  Releasing logical volume "osd-block-1acf9e02-0136-4d8a-a375-5d05dbcd06b2"
  Creating volume group backup "/etc/lvm/backup/ceph-977031cb-95f6-4831-adad-f208088bc096" (seqno 6).
  Logical volume "osd-block-1acf9e02-0136-4d8a-a375-5d05dbcd06b2" successfully removed
  Removing physical volume "/dev/nvme0n1" from volume group "ceph-977031cb-95f6-4831-adad-f208088bc096"
  Volume group "ceph-977031cb-95f6-4831-adad-f208088bc096" successfully removed


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T02:29:01,776555579-04:00] STAGE: Bootstrap a new cluster...[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:312 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.3 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T02:29:01,785111862-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T02:29:01,787997319-04:00] DEBUG: CEPHADM_PARAMS: ['bootstrap', '--config', '/tmp/bench-rados/ceph-research/scripts/ceph.conf', '--mon-ip', '10.10.2.3', '--output-dir', '/tmp/bench-rados/deployment_data_root/etc/ceph', '--ssh-user', 'ljishen', '--skip-dashboard', '--no-minimize-config', '--allow-overwrite', '--allow-fqdn-hostname', '--orphan-initial-daemons', '--skip-monitoring-stack', '--single-host-defaults'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.3 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults
Verifying podman|docker is present...
Verifying lvm2 is present...
Verifying time synchronization is in place...
Unit ntp.service is enabled and running
Repeating the final host check...
podman|docker (/usr/bin/docker) is present
systemctl is present
lvcreate is present
Unit ntp.service is enabled and running
Host looks OK
Cluster fsid: 2ce98762-36ef-11ec-b561-09a495c6360a
Verifying IP 10.10.2.3 port 3300 ...
Verifying IP 10.10.2.3 port 6789 ...
Mon IP `10.10.2.3` is in CIDR network `10.10.2.0/24`
- internal network (--cluster-network) has not been provided, OSD replication will default to the public_network
Adjusting default settings to suit single-host cluster...
Pulling container image quay.io/ceph/ceph:v16...
Ceph version: ceph version 16.2.6 (ee28fb57e47e9f88813e24bbf4c14496ca299d31) pacific (stable)
Extracting ceph user uid/gid from container image...
Creating initial keys...
Creating initial monmap...
Creating mon...
Waiting for mon to start...
Waiting for mon...
mon is available
Setting mon public_network to 10.10.2.0/24
Wrote config to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf
Wrote keyring to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring
Creating mgr...
Verifying port 9283 ...
Waiting for mgr to start...
Waiting for mgr...
mgr not available, waiting (1/15)...
mgr not available, waiting (2/15)...
mgr is available
Enabling cephadm module...
Waiting for the mgr to restart...
Waiting for mgr epoch 5...
mgr epoch 5 is available
Setting orchestrator backend to cephadm...
Generating ssh key...
Wrote public SSH key to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub
Adding key to ljishen@localhost authorized_keys...
Adding host node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us...
Deploying unmanaged mon service...
Deploying unmanaged mgr service...
You can access the Ceph CLI with:

	sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --fsid 2ce98762-36ef-11ec-b561-09a495c6360a -c /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf -k /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring

Please consider enabling telemetry to help improve Ceph:

	ceph telemetry on

For more information see:

	https://docs.ceph.com/docs/pacific/mgr/telemetry/

Bootstrap complete.

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T02:30:07,368866775-04:00] INFO: > Set the default number of monitor daemons to 1[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:330 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T02:30:07,378023811-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T02:30:07,381065041-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mon', '1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1
Inferring fsid 2ce98762-36ef-11ec-b561-09a495c6360a
Inferring config /var/lib/ceph/2ce98762-36ef-11ec-b561-09a495c6360a/mon.node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Scheduled mon update...

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T02:30:16,793899970-04:00] INFO: > Set the default number of manager daemons to 1[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:335 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T02:30:16,802721724-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T02:30:16,805550925-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mgr', '1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1
Inferring fsid 2ce98762-36ef-11ec-b561-09a495c6360a
Inferring config /var/lib/ceph/2ce98762-36ef-11ec-b561-09a495c6360a/mon.node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Scheduled mgr update...


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T02:30:26,841803334-04:00] STAGE: Add hosts to the cluster...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T02:30:26,846894036-04:00] INFO: > Adding host 'sm1'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:362 - add_hosts() > ssh-copy-id -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -f -i /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub ljishen@sm1
/usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: "/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub"

Number of key(s) added: 1

Now try logging into the machine, with:   "ssh -o 'GlobalKnownHostsFile=/dev/null' -o 'LogLevel=ERROR' -o 'PasswordAuthentication=no' -o 'StrictHostKeyChecking=no' -o 'UserKnownHostsFile=/dev/null' 'ljishen@sm1'"
and check to make sure that only the key(s) you wanted were added.

# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:368 - add_hosts() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add sm1 10.10.2.5
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T02:30:28,452780455-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T02:30:28,455663758-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'host', 'add', 'sm1', '10.10.2.5'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add sm1 10.10.2.5
Inferring fsid 2ce98762-36ef-11ec-b561-09a495c6360a
Inferring config /var/lib/ceph/2ce98762-36ef-11ec-b561-09a495c6360a/mon.node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Added host 'sm1' with addr '10.10.2.5'

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T02:30:39,695253511-04:00] INFO: Sleep for 20 seconds[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T02:30:59,699660362-04:00] STAGE: Add OSDs to the cluster...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T02:30:59,705065587-04:00] INFO: > Adding devices ['/dev/nvme0n1'] on host 'sm1' as OSDs[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:387 - add_host_devices_as_osds() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd sm1:/dev/nvme0n1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T02:30:59,713490913-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T02:30:59,716409433-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'daemon', 'add', 'osd', 'sm1:/dev/nvme0n1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd sm1:/dev/nvme0n1
Inferring fsid 2ce98762-36ef-11ec-b561-09a495c6360a
Inferring config /var/lib/ceph/2ce98762-36ef-11ec-b561-09a495c6360a/mon.node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Created osd(s) 0 on host 'sm1'

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T02:31:23,072546137-04:00] INFO: > Sleep for 20 seconds[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T02:31:43,077022731-04:00] STAGE: Show Cluster Status[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:343 - show_status() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T02:31:43,085925948-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T02:31:43,088935530-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', '--status'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status
Inferring fsid 2ce98762-36ef-11ec-b561-09a495c6360a
Inferring config /var/lib/ceph/2ce98762-36ef-11ec-b561-09a495c6360a/mon.node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
  cluster:
    id:     2ce98762-36ef-11ec-b561-09a495c6360a
    health: HEALTH_WARN
            1 pool(s) have no replicas configured
 
  services:
    mon: 1 daemons, quorum node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us (age 2m)
    mgr: node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us.denrrz(active, since 2m)
    osd: 1 osds: 1 up (since 20s), 1 in (since 39s)
 
  data:
    pools:   1 pools, 128 pgs
    objects: 0 objects, 0 B
    usage:   4.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     99.219% pgs unknown
             127 unknown
             1   active+clean
 
  progress:
 

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:31:52,374382153-07:00] INFO: > Copy configuration files to local[0m
# ./benchmarks/bench-rados:208 - launch_ceph_cluster() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.3:/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf
# ./benchmarks/bench-rados:212 - launch_ceph_cluster() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.3 sudo chmod 644 /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:31:52,530510572-07:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.3 sudo chmod 644 /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring
[1] 23:31:52 [SUCCESS] ljishen@10.10.2.3
# ./benchmarks/bench-rados:214 - launch_ceph_cluster() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.3:/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:31:52,858804275-07:00] STAGE: Update local ceph.conf as a client...[0m


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:31:52,862626590-07:00] STAGE: Configure Ceph pool...[0m
# ./benchmarks/bench-rados:245 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_max_backfills 32
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:31:52,888031502-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:31:52,891530059-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '2ce98762-36ef-11ec-b561-09a495c6360a', '--', 'ceph', 'config', 'set', 'osd', 'osd_max_backfills', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 2ce98762-36ef-11ec-b561-09a495c6360a -- ceph config set osd osd_max_backfills 32
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:246 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_active 32
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:31:56,079522562-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:31:56,082971024-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '2ce98762-36ef-11ec-b561-09a495c6360a', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_active', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 2ce98762-36ef-11ec-b561-09a495c6360a -- ceph config set osd osd_recovery_max_active 32
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:247 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_single_start 8
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:31:59,346536881-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:31:59,349860849-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '2ce98762-36ef-11ec-b561-09a495c6360a', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_single_start', '8'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 2ce98762-36ef-11ec-b561-09a495c6360a -- ceph config set osd osd_recovery_max_single_start 8
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:248 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_op_priority 63
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:32:02,507384622-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:32:02,510658796-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '2ce98762-36ef-11ec-b561-09a495c6360a', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_op_priority', '63'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 2ce98762-36ef-11ec-b561-09a495c6360a -- ceph config set osd osd_recovery_op_priority 63
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:250 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config show-with-defaults osd.0
# ./benchmarks/bench-rados:251 - configure_ceph_pool() > grep 'osd_max_backfills\|osd_recovery'
# ./benchmarks/bench-rados:251 - configure_ceph_pool() > column -t -s ' '
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
osd_max_backfills                        32         mon
osd_recovery_cost                        20971520   default
osd_recovery_delay_start                 0.000000   default
osd_recovery_max_active                  32         mon
osd_recovery_max_active_hdd              3          default
osd_recovery_max_active_ssd              10         default
osd_recovery_max_chunk                   8388608    default
osd_recovery_max_omap_entries_per_chunk  8096       default
osd_recovery_max_single_start            8          mon
osd_recovery_op_priority                 63         mon
osd_recovery_op_warn_multiple            16         default
osd_recovery_priority                    5          default
osd_recovery_retry_interval              30.000000  default
osd_recovery_sleep                       0.000000   default
osd_recovery_sleep_hdd                   0.100000   default
osd_recovery_sleep_hybrid                0.025000   default
osd_recovery_sleep_ssd                   0.000000   default
# ./benchmarks/bench-rados:253 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool create bench_rados 128 128 replicated --size 1 --pg-num-min 128
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:32:08,949031663-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:32:08,952473964-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '2ce98762-36ef-11ec-b561-09a495c6360a', '--', 'ceph', 'osd', 'pool', 'create', 'bench_rados', '128', '128', 'replicated', '--size', '1', '--pg-num-min', '128'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 2ce98762-36ef-11ec-b561-09a495c6360a -- ceph osd pool create bench_rados 128 128 replicated --size 1 --pg-num-min 128
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
pool 'bench_rados' created
# ./benchmarks/bench-rados:255 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados min_size 1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:32:13,297527026-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:32:13,301227782-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '2ce98762-36ef-11ec-b561-09a495c6360a', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'min_size', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 2ce98762-36ef-11ec-b561-09a495c6360a -- ceph osd pool set bench_rados min_size 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 min_size to 1
# ./benchmarks/bench-rados:256 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados pg_autoscale_mode off
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:32:17,674670358-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:32:17,678159697-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '2ce98762-36ef-11ec-b561-09a495c6360a', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'pg_autoscale_mode', 'off'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 2ce98762-36ef-11ec-b561-09a495c6360a -- ceph osd pool set bench_rados pg_autoscale_mode off
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 pg_autoscale_mode to off
# ./benchmarks/bench-rados:257 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool application enable bench_rados benchmark
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:32:22,125970661-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:32:22,129745536-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '2ce98762-36ef-11ec-b561-09a495c6360a', '--', 'ceph', 'osd', 'pool', 'application', 'enable', 'bench_rados', 'benchmark'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 2ce98762-36ef-11ec-b561-09a495c6360a -- ceph osd pool application enable bench_rados benchmark
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
enabled application 'benchmark' on pool 'bench_rados'
# ./benchmarks/bench-rados:258 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados noscrub 1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:32:26,359010206-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:32:26,362607588-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '2ce98762-36ef-11ec-b561-09a495c6360a', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'noscrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 2ce98762-36ef-11ec-b561-09a495c6360a -- ceph osd pool set bench_rados noscrub 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 noscrub to 1
# ./benchmarks/bench-rados:259 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados nodeep-scrub 1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:32:29,818137043-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:32:29,821733463-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '2ce98762-36ef-11ec-b561-09a495c6360a', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'nodeep-scrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 2ce98762-36ef-11ec-b561-09a495c6360a -- ceph osd pool set bench_rados nodeep-scrub 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 nodeep-scrub to 1
# ./benchmarks/bench-rados:260 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool ls detail
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:32:34,117852978-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:32:34,121768579-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '2ce98762-36ef-11ec-b561-09a495c6360a', '--', 'ceph', 'osd', 'pool', 'ls', 'detail'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 2ce98762-36ef-11ec-b561-09a495c6360a -- ceph osd pool ls detail
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
pool 1 'device_health_metrics' replicated size 1 min_size 1 crush_rule 0 object_hash rjenkins pg_num 128 pgp_num 128 autoscale_mode on last_change 13 lfor 0/0/11 flags hashpspool stripe_width 0 pg_num_min 1 application mgr_devicehealth
pool 2 'bench_rados' replicated size 1 min_size 1 crush_rule 0 object_hash rjenkins pg_num 128 pgp_num 128 autoscale_mode off last_change 21 flags hashpspool,noscrub,nodeep-scrub stripe_width 0 pg_num_min 128 application benchmark

# ./benchmarks/bench-rados:261 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd df tree
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:32:37,351082234-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:32:37,354670148-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '2ce98762-36ef-11ec-b561-09a495c6360a', '--', 'ceph', 'osd', 'df', 'tree'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 2ce98762-36ef-11ec-b561-09a495c6360a -- ceph osd df tree
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
ID  CLASS  WEIGHT   REWEIGHT  SIZE     RAW USE  DATA     OMAP  META     AVAIL    %USE  VAR   PGS  STATUS  TYPE NAME    
-1         0.09769         -  100 GiB  6.0 MiB  176 KiB   0 B  5.8 MiB  100 GiB  0.01  1.00    -          root default 
-3         0.09769         -  100 GiB  6.0 MiB  176 KiB   0 B  5.8 MiB  100 GiB  0.01  1.00    -              host sm1 
 0    ssd  0.09769   1.00000  100 GiB  6.0 MiB  176 KiB   0 B  5.8 MiB  100 GiB  0.01  1.00  256      up          osd.0
                       TOTAL  100 GiB  6.0 MiB  176 KiB   0 B  5.8 MiB  100 GiB  0.01                                  
MIN/MAX VAR: 1.00/1.00  STDDEV: 0


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:32:40,610767858-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:33:03,776605447-07:00] INFO: >>   data:
    pools:   2 pools, 256 pgs
    objects: 0 objects, 0 B
    usage:   6.0 MiB used, 100 GiB / 100 GiB avail
    pgs:     256 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:33:03,785544610-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:33:12,116654426-07:00] INFO: >>   data:
    pools:   2 pools, 256 pgs
    objects: 0 objects, 0 B
    usage:   6.0 MiB used, 100 GiB / 100 GiB avail
    pgs:     256 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:33:12,125335022-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:33:20,523411525-07:00] INFO: >>   data:
    pools:   2 pools, 256 pgs
    objects: 0 objects, 0 B
    usage:   6.0 MiB used, 100 GiB / 100 GiB avail
    pgs:     256 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:33:20,532376796-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:33:28,828807378-07:00] INFO: >>   data:
    pools:   2 pools, 256 pgs
    objects: 0 objects, 0 B
    usage:   6.0 MiB used, 100 GiB / 100 GiB avail
    pgs:     256 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:33:28,836874131-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:33:37,069859106-07:00] INFO: >>   data:
    pools:   2 pools, 256 pgs
    objects: 0 objects, 0 B
    usage:   6.0 MiB used, 100 GiB / 100 GiB avail
    pgs:     256 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:33:37,078205094-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:33:37,085557154-07:00] INFO: > The cluster is idle now.[0m


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:33:37,089575657-07:00] STAGE: Start benchmarking of mode write ...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:33:37,097541931-07:00] INFO: > Start system activity collection services[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:33:37,104288492-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:329 - rados_bench() > rm --force /mnt/sda8/ceph-research/scripts/output/sys_activity_4MB_write.dat.rados_bench_host.3
# ./benchmarks/bench-rados:330 - rados_bench() > sar_pid_rados_bench_host=889956
# ./benchmarks/bench-rados:330 - rados_bench() > S_TIME_FORMAT=ISO
# ./benchmarks/bench-rados:330 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/output/sys_activity_4MB_write.dat.rados_bench_host.3 2

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:33:37,112116336-07:00] INFO: >> for OSD host[0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:33:37,120491728-07:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.5 bash
10.10.2.5: b'123930\n'
[1] 23:33:38 [SUCCESS] ljishen@10.10.2.5
123930

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:33:38,212468768-07:00] INFO: > Run rados bench[0m
# ./benchmarks/bench-rados:358 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/output/rados_bench_4MB_write.log.3
## ./benchmarks/bench-rados:349 - rados_bench() > count_cpus 32-63,96-127
# ./benchmarks/bench-rados:349 - rados_bench() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 30 write --pool bench_rados -b 4194304 -O 4194304 --concurrent-ios 128 --show-time --write-object --write-omap --write-xattr --no-cleanup
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:33:38,234043261-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:33:38,237919477-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '2ce98762-36ef-11ec-b561-09a495c6360a', '--', 'rados', 'bench', '30', 'write', '--pool', 'bench_rados', '-b', '4194304', '-O', '4194304', '--concurrent-ios', '128', '--show-time', '--write-object', '--write-omap', '--write-xattr', '--no-cleanup'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 2ce98762-36ef-11ec-b561-09a495c6360a -- rados bench 30 write --pool bench_rados -b 4194304 -O 4194304 --concurrent-ios 128 --show-time --write-object --write-omap --write-xattr --no-cleanup
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
hints = 1
2021-10-27T06:33:40.737657+0000 Maintaining 128 concurrent writes of 4194304 bytes to objects of size 4194304 for up to 30 seconds or 0 objects
2021-10-27T06:33:40.737669+0000 Object prefix: benchmark_data_node-0.bluefield2.ucsc-cmps10_7
2021-10-27T06:33:40.849257+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-27T06:33:40.849257+0000     0       0         0         0         0         0           -           0
2021-10-27T06:33:41.849538+0000     1      20        20         0         0         0           -           0
2021-10-27T06:33:42.849603+0000     2      39        39         0         0         0           -           0
2021-10-27T06:33:43.849667+0000     3      55        55         0         0         0           -           0
2021-10-27T06:33:44.849731+0000     4      73        73         0         0         0           -           0
2021-10-27T06:33:45.849821+0000     5      89        89         0         0         0           -           0
2021-10-27T06:33:46.849891+0000     6     110       110         0         0         0           -           0
2021-10-27T06:33:47.849955+0000     7     127       127         0         0         0           -           0
2021-10-27T06:33:48.850020+0000     8     127       138        11   5.49952       5.5     7.28903     7.31812
2021-10-27T06:33:49.850112+0000     9     127       146        19    8.4437        32     7.80792     7.49949
2021-10-27T06:33:50.850179+0000    10     127       154        27   10.7991        32     8.27825     7.71787
2021-10-27T06:33:51.850378+0000    11     127       166        39   14.1804        48     9.06772     8.06167
2021-10-27T06:33:52.850449+0000    12     127       174        47   15.6652        32       9.562     8.29813
2021-10-27T06:33:53.850680+0000    13     127       182        55   16.9213        32     9.98429     8.53504
2021-10-27T06:33:54.850747+0000    14     127       201        74   21.1407        76     10.0332     8.92312
2021-10-27T06:33:55.850923+0000    15     127       220        93   24.7973        76     9.95019     9.14461
2021-10-27T06:33:56.850991+0000    16     127       235       108   26.9972        60     10.0257     9.27239
2021-10-27T06:33:57.851083+0000    17     127       254       127   29.8792        76     10.1239     9.39763
2021-10-27T06:33:58.851153+0000    18     127       264       137   30.4413        40     9.91836     9.44824
2021-10-27T06:33:59.851244+0000    19     127       272       145   30.5232        32     9.94452     9.48798
2021-10-27T06:34:00.851312+0000 min lat: 7.03087 max lat: 10.897 avg lat: 9.52545
2021-10-27T06:34:00.851312+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-27T06:34:00.851312+0000    20     127       277       150    29.997        20     10.4217     9.52545
2021-10-27T06:34:01.851561+0000    21     127       280       153   29.1397        12      10.662     9.55082
2021-10-27T06:34:02.851629+0000    22     127       286       159   28.9061        24     11.8739     9.62966
2021-10-27T06:34:03.851698+0000    23     127       286       159   27.6493         0           -     9.62966
2021-10-27T06:34:04.851764+0000    24     127       294       167   27.8305        16     12.6363     9.76719
2021-10-27T06:34:05.851857+0000    25     127       300       173   27.6772        24     12.8619     9.88185
2021-10-27T06:34:06.851925+0000    26     127       308       181   27.8434        32     12.7145     10.0166
2021-10-27T06:34:07.851994+0000    27     127       320       193   28.5898        48     13.2103     10.2133
2021-10-27T06:34:08.852043+0000    28     127       333       206   29.4257        52     13.7193     10.4257
2021-10-27T06:34:09.852226+0000    29     127       341       214   29.5143        32     14.0586     10.5615
2021-10-27T06:34:10.852368+0000    30     127       353       226   30.1303        48     14.5111      10.767
2021-10-27T06:34:11.852476+0000 Total time run:         30.5791
Total writes made:      354
Write size:             4194304
Object size:            4194304
Bandwidth (MB/sec):     46.3061
Stddev Bandwidth:       24.3815
Max bandwidth (MB/sec): 76
Min bandwidth (MB/sec): 0
Average IOPS:           11
Stddev IOPS:            6.10784
Max IOPS:               19
Min IOPS:               0
Average Latency(s):     9.65302
Stddev Latency(s):      3.65881
Max latency(s):         14.5917
Min latency(s):         0.692209

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:34:12,426276122-07:00] INFO: > Stop system activity collection services[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:34:12,432971296-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:379 - rados_bench() > kill -INT 889956

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:34:12,439181469-07:00] INFO: >> for OSD host[0m
# ./benchmarks/bench-rados:384 - rados_bench() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.5 kill -INT 123930
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:34:12,448218115-07:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.5 kill -INT 123930
[1] 23:34:13 [SUCCESS] ljishen@10.10.2.5

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:34:13,809707754-07:00] INFO: >>> Copy system activity file from OSD host[0m
## ./benchmarks/bench-rados:391 - rados_bench() > basename -- /tmp/bench-rados/sys_activity_4MB_write.dat.osd_host.3
# ./benchmarks/bench-rados:391 - rados_bench() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.5:/tmp/bench-rados/sys_activity_4MB_write.dat.osd_host.3 /mnt/sda8/ceph-research/scripts/output/sys_activity_4MB_write.dat.osd_host.3


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:34:15,075951973-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:34:38,278359951-07:00] INFO: >>   data:
    pools:   2 pools, 256 pgs
    objects: 355 objects, 1.4 GiB
    usage:   4.2 GiB used, 96 GiB / 100 GiB avail
    pgs:     256 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:34:38,287239601-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:34:46,602756856-07:00] INFO: >>   data:
    pools:   2 pools, 256 pgs
    objects: 355 objects, 1.4 GiB
    usage:   4.2 GiB used, 96 GiB / 100 GiB avail
    pgs:     256 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:34:46,611464504-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:34:54,894714837-07:00] INFO: >>   data:
    pools:   2 pools, 256 pgs
    objects: 355 objects, 1.4 GiB
    usage:   4.2 GiB used, 96 GiB / 100 GiB avail
    pgs:     256 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:34:54,903605118-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:35:03,658743986-07:00] INFO: >>   data:
    pools:   2 pools, 256 pgs
    objects: 355 objects, 1.4 GiB
    usage:   4.2 GiB used, 96 GiB / 100 GiB avail
    pgs:     256 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:35:03,666598610-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:35:12,014377628-07:00] INFO: >>   data:
    pools:   2 pools, 256 pgs
    objects: 355 objects, 1.4 GiB
    usage:   4.2 GiB used, 96 GiB / 100 GiB avail
    pgs:     256 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:35:12,022700853-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:35:12,030073641-07:00] INFO: > The cluster is idle now.[0m


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:35:12,034206349-07:00] STAGE: Start benchmarking of mode seq ...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:35:12,042355407-07:00] INFO: > Start system activity collection services[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:35:12,048686928-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:329 - rados_bench() > rm --force /mnt/sda8/ceph-research/scripts/output/sys_activity_4MB_seq.dat.rados_bench_host.3
# ./benchmarks/bench-rados:330 - rados_bench() > sar_pid_rados_bench_host=891816
# ./benchmarks/bench-rados:330 - rados_bench() > S_TIME_FORMAT=ISO
# ./benchmarks/bench-rados:330 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/output/sys_activity_4MB_seq.dat.rados_bench_host.3 2

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:35:12,056406989-07:00] INFO: >> for OSD host[0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:35:12,064974042-07:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.5 bash
10.10.2.5: b'124342\n'
[1] 23:35:13 [SUCCESS] ljishen@10.10.2.5
124342

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:35:13,173885689-07:00] INFO: > Run rados bench[0m
# ./benchmarks/bench-rados:367 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/output/rados_bench_4MB_seq.log.3
## ./benchmarks/bench-rados:364 - rados_bench() > count_cpus 32-63,96-127
# ./benchmarks/bench-rados:364 - rados_bench() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 128 --show-time
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:35:13,195632345-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:35:13,199075056-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '2ce98762-36ef-11ec-b561-09a495c6360a', '--', 'rados', 'bench', '99999999', 'seq', '--pool', 'bench_rados', '--concurrent-ios', '128', '--show-time'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 2ce98762-36ef-11ec-b561-09a495c6360a -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 128 --show-time
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
hints = 1
2021-10-27T06:35:15.697416+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-27T06:35:15.697416+0000     0       1         1         0         0         0           -           0
2021-10-27T06:35:16.697525+0000     1     127       220        93   371.933       372    0.605463    0.623239
2021-10-27T06:35:17.697674+0000 Total time run:       1.74097
Total reads made:     354
Read size:            4194304
Object size:          4194304
Bandwidth (MB/sec):   813.34
Average IOPS:         203
Stddev IOPS:          0
Max IOPS:             93
Min IOPS:             93
Average Latency(s):   0.51275
Max latency(s):       0.684493
Min latency(s):       0.093455

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:35:18,338576907-07:00] INFO: > Stop system activity collection services[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:35:18,345712289-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:379 - rados_bench() > kill -INT 891816

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:35:18,351756480-07:00] INFO: >> for OSD host[0m
# ./benchmarks/bench-rados:384 - rados_bench() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.5 kill -INT 124342
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:35:18,360807663-07:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.5 kill -INT 124342
[1] 23:35:19 [SUCCESS] ljishen@10.10.2.5

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:35:19,464041688-07:00] INFO: >>> Copy system activity file from OSD host[0m
## ./benchmarks/bench-rados:391 - rados_bench() > basename -- /tmp/bench-rados/sys_activity_4MB_seq.dat.osd_host.3
# ./benchmarks/bench-rados:391 - rados_bench() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.5:/tmp/bench-rados/sys_activity_4MB_seq.dat.osd_host.3 /mnt/sda8/ceph-research/scripts/output/sys_activity_4MB_seq.dat.osd_host.3


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:35:20,541225657-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:35:43,976204365-07:00] INFO: >>   data:
    pools:   2 pools, 256 pgs
    objects: 355 objects, 1.4 GiB
    usage:   4.2 GiB used, 96 GiB / 100 GiB avail
    pgs:     256 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:35:43,983905721-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:35:52,274374928-07:00] INFO: >>   data:
    pools:   2 pools, 256 pgs
    objects: 355 objects, 1.4 GiB
    usage:   4.2 GiB used, 96 GiB / 100 GiB avail
    pgs:     256 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:35:52,283293703-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:36:00,464995765-07:00] INFO: >>   data:
    pools:   2 pools, 256 pgs
    objects: 355 objects, 1.4 GiB
    usage:   4.2 GiB used, 96 GiB / 100 GiB avail
    pgs:     256 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:36:00,473468320-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:36:08,737434692-07:00] INFO: >>   data:
    pools:   2 pools, 256 pgs
    objects: 355 objects, 1.4 GiB
    usage:   4.2 GiB used, 96 GiB / 100 GiB avail
    pgs:     256 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:36:08,745768717-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:36:16,937991176-07:00] INFO: >>   data:
    pools:   2 pools, 256 pgs
    objects: 355 objects, 1.4 GiB
    usage:   4.2 GiB used, 96 GiB / 100 GiB avail
    pgs:     256 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:36:16,946193283-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:36:16,953702948-07:00] INFO: > The cluster is idle now.[0m


[1;7;39;49m[2021-10-26T23:36:16,957623517-07:00][RUNNING][ROUND 1/7/21] object_size=16MB[0m


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:36:16,961633836-07:00] STAGE: Launch a Ceph cluster (CEPH_HOST: 10.10.2.3, OSD_HOST: 10.10.2.5) ...[0m
# ./benchmarks/bench-rados:172 - launch_ceph_cluster() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.3 bash -euo pipefail
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:36:16,970801157-07:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.3 bash -euo pipefail
10.10.2.3: b'\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T02:36:17,381730250-04:00] INFO: > Remove the existing cluster\x1b[0m\n'
10.10.2.3: b'## bash:13 -  > ls /var/lib/ceph\n'
10.10.2.3: b'## bash:13 -  > tail -n1\n'
10.10.2.3: b'# bash:13 -  > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm --host 10.10.2.3 --host 10.10.2.5 rm-cluster --force --fsid 2ce98762-36ef-11ec-b561-09a495c6360a\n'
10.10.2.3: b'\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T02:36:26,443624839-04:00] INFO: > Deploy a new cluster\x1b[0m\n'
10.10.2.3: b'## bash:22 -  > /tmp/bench-rados/ceph-research/scripts/parallel-ssh --host 10.10.2.5 uname --nodename\n## bash:22 -  > tail -n1\n'
10.10.2.3: b'# bash:22 -  > osd_hostname=sm1\n# bash:23 -  > /tmp/bench-rados/ceph-research/scripts/ceph-deploy -m 10.10.2.3 -o sm1/10.10.2.5:/dev/nvme0n1\n'
10.10.2.3: b'\n\n\x1b[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T02:36:27,584966162-04:00] STAGE: Verify passwordless SSH to hosts, passwordless sudo via ljishen, and the existence of devices on hosts...\x1b[0m\n'
10.10.2.3: b'\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T02:36:27,589280552-04:00] INFO: > Check the host for the monitor\x1b[0m\n# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:181 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@10.10.2.3 sudo --non-interactive --validate\n'
10.10.2.3: b'\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T02:36:27,740888283-04:00] INFO: > Check hosts for OSDs\x1b[0m\n'
10.10.2.3: b"\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T02:36:27,744839187-04:00] INFO: >> Check host 'sm1'\x1b[0m\n# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:200 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 sudo --non-interactive --validate\n"
10.10.2.3: b"\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T02:36:28,808013439-04:00] INFO: >>> Check device '/dev/nvme0n1'\x1b[0m\n"
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:215 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 test -b /dev/nvme0n1\n'
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:226 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 lsblk --nodeps --noheadings --output TYPE /dev/nvme0n1\n'
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:227 - verify_parameters() > grep --word-regexp disk\n'
10.10.2.3: b'\n\n\x1b[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T02:36:30,977616627-04:00] STAGE: Reset devices on hosts...\x1b[0m\n'
10.10.2.3: b"\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T02:36:30,981679542-04:00] INFO: > Resetting device '/dev/nvme0n1' on host 'sm1'\x1b[0m\n"
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:288 - reset_host_devices() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 sudo sh\n'
10.10.2.3: b'  Removing ceph--6768858f--7db0--411e--9b90--023e8fc0df1c-osd--block--cbc78f53--1b72--42c6--a3dd--16ca754f2058 (252:0)\n'
10.10.2.3: b'  Archiving volume group "ceph-6768858f-7db0-411e-9b90-023e8fc0df1c" metadata (seqno 5).\n'
10.10.2.3: b'  Releasing logical volume "osd-block-cbc78f53-1b72-42c6-a3dd-16ca754f2058"\n'
10.10.2.3: b'  Creating volume group backup "/etc/lvm/backup/ceph-6768858f-7db0-411e-9b90-023e8fc0df1c" (seqno 6).\n'
10.10.2.3: b'  Logical volume "osd-block-cbc78f53-1b72-42c6-a3dd-16ca754f2058" successfully removed\n'
10.10.2.3: b'  Removing physical volume "/dev/nvme0n1" from volume group "ceph-6768858f-7db0-411e-9b90-023e8fc0df1c"\n'
10.10.2.3: b'  Volume group "ceph-6768858f-7db0-411e-9b90-023e8fc0df1c" successfully removed\n'
10.10.2.3: b'\n\n\x1b[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T02:36:33,284085713-04:00] STAGE: Bootstrap a new cluster...\x1b[0m\n# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:312 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.3 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults\n'
10.10.2.3: b'\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T02:36:33,292594957-04:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.3: b"\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T02:36:33,295417456-04:00] DEBUG: CEPHADM_PARAMS: ['bootstrap', '--config', '/tmp/bench-rados/ceph-research/scripts/ceph.conf', '--mon-ip', '10.10.2.3', '--output-dir', '/tmp/bench-rados/deployment_data_root/etc/ceph', '--ssh-user', 'ljishen', '--skip-dashboard', '--no-minimize-config', '--allow-overwrite', '--allow-fqdn-hostname', '--orphan-initial-daemons', '--skip-monitoring-stack', '--single-host-defaults']\x1b[0m\n"
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.3 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults\n'
10.10.2.3: b'Verifying podman|docker is present...\nVerifying lvm2 is present...\nVerifying time synchronization is in place...\n'
10.10.2.3: b'Unit ntp.service is enabled and running\n'
10.10.2.3: b'Repeating the final host check...\npodman|docker (/usr/bin/docker) is present\n'
10.10.2.3: b'systemctl is present\nlvcreate is present\n'
10.10.2.3: b'Unit ntp.service is enabled and running\nHost looks OK\n'
10.10.2.3: b'Cluster fsid: 3a077bf6-36f0-11ec-b561-09a495c6360a\n'
10.10.2.3: b'Verifying IP 10.10.2.3 port 3300 ...\n'
10.10.2.3: b'Verifying IP 10.10.2.3 port 6789 ...\n'
10.10.2.3: b'Mon IP `10.10.2.3` is in CIDR network `10.10.2.0/24`\n- internal network (--cluster-network) has not been provided, OSD replication will default to the public_network\n'
10.10.2.3: b'Adjusting default settings to suit single-host cluster...\n'
10.10.2.3: b'Pulling container image quay.io/ceph/ceph:v16...\n'
10.10.2.3: b'Ceph version: ceph version 16.2.6 (ee28fb57e47e9f88813e24bbf4c14496ca299d31) pacific (stable)\nExtracting ceph user uid/gid from container image...\n'
10.10.2.3: b'Creating initial keys...\n'
10.10.2.3: b'Creating initial monmap...\n'
10.10.2.3: b'Creating mon...\n'
10.10.2.3: b'Waiting for mon to start...\nWaiting for mon...\n'
10.10.2.3: b'mon is available\nSetting mon public_network to 10.10.2.0/24\n'
10.10.2.3: b'Wrote config to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf\nWrote keyring to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring\n'
10.10.2.3: b'Creating mgr...\n'
10.10.2.3: b'Verifying port 9283 ...\n'
10.10.2.3: b'Waiting for mgr to start...\nWaiting for mgr...\n'
10.10.2.3: b'mgr not available, waiting (1/15)...\n'
10.10.2.3: b'mgr not available, waiting (2/15)...\n'
10.10.2.3: b'mgr is available\n'
10.10.2.3: b'Enabling cephadm module...\n'
10.10.2.3: b'Waiting for the mgr to restart...\nWaiting for mgr epoch 5...\n'
10.10.2.3: b'mgr epoch 5 is available\nSetting orchestrator backend to cephadm...\n'
10.10.2.3: b'Generating ssh key...\n'
10.10.2.3: b'Wrote public SSH key to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub\nAdding key to ljishen@localhost authorized_keys...\n'
10.10.2.3: b'Adding host node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us...\n'
10.10.2.3: b'Deploying unmanaged mon service...\n'
10.10.2.3: b'Deploying unmanaged mgr service...\n'
10.10.2.3: b'You can access the Ceph CLI with:\n\n\tsudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --fsid 3a077bf6-36f0-11ec-b561-09a495c6360a -c /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf -k /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring\n\n'
10.10.2.3: b'Please consider enabling telemetry to help improve Ceph:\n\n\tceph telemetry on\n\nFor more information see:\n\n\thttps://docs.ceph.com/docs/pacific/mgr/telemetry/\n\nBootstrap complete.\n'
10.10.2.3: b'\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T02:37:37,770919525-04:00] INFO: > Set the default number of monitor daemons to 1\x1b[0m\n# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:330 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1\n'
10.10.2.3: b'\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T02:37:37,779564124-04:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.3: b"\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T02:37:37,782893137-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mon', '1']\x1b[0m\n"
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1\n'
10.10.2.3: b'Inferring fsid 3a077bf6-36f0-11ec-b561-09a495c6360a\n'
10.10.2.3: b'Inferring config /var/lib/ceph/3a077bf6-36f0-11ec-b561-09a495c6360a/mon.node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.3: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.3: b'Scheduled mon update...\n'
10.10.2.3: b'\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T02:37:47,428288308-04:00] INFO: > Set the default number of manager daemons to 1\x1b[0m\n# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:335 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1\n'
10.10.2.3: b'\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T02:37:47,437441767-04:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.3: b"\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T02:37:47,440668867-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mgr', '1']\x1b[0m\n"
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1\n'
10.10.2.3: b'Inferring fsid 3a077bf6-36f0-11ec-b561-09a495c6360a\n'
10.10.2.3: b'Inferring config /var/lib/ceph/3a077bf6-36f0-11ec-b561-09a495c6360a/mon.node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.3: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.3: b'Scheduled mgr update...\n'
10.10.2.3: b'\n\n\x1b[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T02:37:56,666227614-04:00] STAGE: Add hosts to the cluster...\x1b[0m\n'
10.10.2.3: b"\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T02:37:56,671199072-04:00] INFO: > Adding host 'sm1'\x1b[0m\n"
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:362 - add_hosts() > ssh-copy-id -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -f -i /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub ljishen@sm1\n'
10.10.2.3: b'/usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: "/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub"\n'
10.10.2.3: b'\nNumber of key(s) added: 1\n\nNow try logging into the machine, with:   "ssh -o \'GlobalKnownHostsFile=/dev/null\' -o \'LogLevel=ERROR\' -o \'PasswordAuthentication=no\' -o \'StrictHostKeyChecking=no\' -o \'UserKnownHostsFile=/dev/null\' \'ljishen@sm1\'"\nand check to make sure that only the key(s) you wanted were added.\n\n'
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:368 - add_hosts() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add sm1 10.10.2.5\n'
10.10.2.3: b'\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T02:37:57,812171906-04:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.3: b"\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T02:37:57,815104923-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'host', 'add', 'sm1', '10.10.2.5']\x1b[0m\n"
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add sm1 10.10.2.5\n'
10.10.2.3: b'Inferring fsid 3a077bf6-36f0-11ec-b561-09a495c6360a\n'
10.10.2.3: b'Inferring config /var/lib/ceph/3a077bf6-36f0-11ec-b561-09a495c6360a/mon.node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.3: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.3: b"Added host 'sm1' with addr '10.10.2.5'\n"
10.10.2.3: b'\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T02:38:08,808136136-04:00] INFO: Sleep for 20 seconds\x1b[0m\n'
10.10.2.3: b'\n\n\x1b[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T02:38:28,812544756-04:00] STAGE: Add OSDs to the cluster...\x1b[0m\n'
10.10.2.3: b"\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T02:38:28,818089484-04:00] INFO: > Adding devices ['/dev/nvme0n1'] on host 'sm1' as OSDs\x1b[0m\n"
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:387 - add_host_devices_as_osds() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd sm1:/dev/nvme0n1\n'
10.10.2.3: b'\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T02:38:28,826833941-04:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.3: b"\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T02:38:28,829674384-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'daemon', 'add', 'osd', 'sm1:/dev/nvme0n1']\x1b[0m\n"
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd sm1:/dev/nvme0n1\n'
10.10.2.3: b'Inferring fsid 3a077bf6-36f0-11ec-b561-09a495c6360a\n'
10.10.2.3: b'Inferring config /var/lib/ceph/3a077bf6-36f0-11ec-b561-09a495c6360a/mon.node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.3: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.3: b"Created osd(s) 0 on host 'sm1'\n"
10.10.2.3: b'\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T02:38:51,349337120-04:00] INFO: > Sleep for 20 seconds\x1b[0m\n'
10.10.2.3: b'\n\n\x1b[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T02:39:11,353968965-04:00] STAGE: Show Cluster Status\x1b[0m\n# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:343 - show_status() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status\n'
10.10.2.3: b'\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T02:39:11,363033155-04:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.3: b"\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T02:39:11,365981881-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', '--status']\x1b[0m\n"
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status\n'
10.10.2.3: b'Inferring fsid 3a077bf6-36f0-11ec-b561-09a495c6360a\n'
10.10.2.3: b'Inferring config /var/lib/ceph/3a077bf6-36f0-11ec-b561-09a495c6360a/mon.node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.3: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.3: b'  cluster:\n    id:     3a077bf6-36f0-11ec-b561-09a495c6360a\n    health: HEALTH_WARN\n            1 pool(s) have no replicas configured\n \n  services:\n    mon: 1 daemons, quorum node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us (age 2m)\n    mgr: node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us.mqubhm(active, since 2m)\n    osd: 1 osds: 1 up (since 21s), 1 in (since 40s)\n \n  data:\n    pools:   1 pools, 128 pgs\n    objects: 0 objects, 0 B\n    usage:   4.7 MiB used, 100 GiB / 100 GiB avail\n    pgs:     99.219% pgs unknown\n             127 unknown\n             1   active+clean\n \n  progress:\n \n'
[1] 23:39:21 [SUCCESS] ljishen@10.10.2.3

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T02:36:17,381730250-04:00] INFO: > Remove the existing cluster[0m
## bash:13 -  > ls /var/lib/ceph
## bash:13 -  > tail -n1
# bash:13 -  > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm --host 10.10.2.3 --host 10.10.2.5 rm-cluster --force --fsid 2ce98762-36ef-11ec-b561-09a495c6360a

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T02:36:26,443624839-04:00] INFO: > Deploy a new cluster[0m
## bash:22 -  > /tmp/bench-rados/ceph-research/scripts/parallel-ssh --host 10.10.2.5 uname --nodename
## bash:22 -  > tail -n1
# bash:22 -  > osd_hostname=sm1
# bash:23 -  > /tmp/bench-rados/ceph-research/scripts/ceph-deploy -m 10.10.2.3 -o sm1/10.10.2.5:/dev/nvme0n1


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T02:36:27,584966162-04:00] STAGE: Verify passwordless SSH to hosts, passwordless sudo via ljishen, and the existence of devices on hosts...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T02:36:27,589280552-04:00] INFO: > Check the host for the monitor[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:181 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@10.10.2.3 sudo --non-interactive --validate

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T02:36:27,740888283-04:00] INFO: > Check hosts for OSDs[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T02:36:27,744839187-04:00] INFO: >> Check host 'sm1'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:200 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 sudo --non-interactive --validate

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T02:36:28,808013439-04:00] INFO: >>> Check device '/dev/nvme0n1'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:215 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 test -b /dev/nvme0n1
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:226 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 lsblk --nodeps --noheadings --output TYPE /dev/nvme0n1
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:227 - verify_parameters() > grep --word-regexp disk


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T02:36:30,977616627-04:00] STAGE: Reset devices on hosts...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T02:36:30,981679542-04:00] INFO: > Resetting device '/dev/nvme0n1' on host 'sm1'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:288 - reset_host_devices() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 sudo sh
  Removing ceph--6768858f--7db0--411e--9b90--023e8fc0df1c-osd--block--cbc78f53--1b72--42c6--a3dd--16ca754f2058 (252:0)
  Archiving volume group "ceph-6768858f-7db0-411e-9b90-023e8fc0df1c" metadata (seqno 5).
  Releasing logical volume "osd-block-cbc78f53-1b72-42c6-a3dd-16ca754f2058"
  Creating volume group backup "/etc/lvm/backup/ceph-6768858f-7db0-411e-9b90-023e8fc0df1c" (seqno 6).
  Logical volume "osd-block-cbc78f53-1b72-42c6-a3dd-16ca754f2058" successfully removed
  Removing physical volume "/dev/nvme0n1" from volume group "ceph-6768858f-7db0-411e-9b90-023e8fc0df1c"
  Volume group "ceph-6768858f-7db0-411e-9b90-023e8fc0df1c" successfully removed


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T02:36:33,284085713-04:00] STAGE: Bootstrap a new cluster...[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:312 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.3 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T02:36:33,292594957-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T02:36:33,295417456-04:00] DEBUG: CEPHADM_PARAMS: ['bootstrap', '--config', '/tmp/bench-rados/ceph-research/scripts/ceph.conf', '--mon-ip', '10.10.2.3', '--output-dir', '/tmp/bench-rados/deployment_data_root/etc/ceph', '--ssh-user', 'ljishen', '--skip-dashboard', '--no-minimize-config', '--allow-overwrite', '--allow-fqdn-hostname', '--orphan-initial-daemons', '--skip-monitoring-stack', '--single-host-defaults'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.3 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults
Verifying podman|docker is present...
Verifying lvm2 is present...
Verifying time synchronization is in place...
Unit ntp.service is enabled and running
Repeating the final host check...
podman|docker (/usr/bin/docker) is present
systemctl is present
lvcreate is present
Unit ntp.service is enabled and running
Host looks OK
Cluster fsid: 3a077bf6-36f0-11ec-b561-09a495c6360a
Verifying IP 10.10.2.3 port 3300 ...
Verifying IP 10.10.2.3 port 6789 ...
Mon IP `10.10.2.3` is in CIDR network `10.10.2.0/24`
- internal network (--cluster-network) has not been provided, OSD replication will default to the public_network
Adjusting default settings to suit single-host cluster...
Pulling container image quay.io/ceph/ceph:v16...
Ceph version: ceph version 16.2.6 (ee28fb57e47e9f88813e24bbf4c14496ca299d31) pacific (stable)
Extracting ceph user uid/gid from container image...
Creating initial keys...
Creating initial monmap...
Creating mon...
Waiting for mon to start...
Waiting for mon...
mon is available
Setting mon public_network to 10.10.2.0/24
Wrote config to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf
Wrote keyring to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring
Creating mgr...
Verifying port 9283 ...
Waiting for mgr to start...
Waiting for mgr...
mgr not available, waiting (1/15)...
mgr not available, waiting (2/15)...
mgr is available
Enabling cephadm module...
Waiting for the mgr to restart...
Waiting for mgr epoch 5...
mgr epoch 5 is available
Setting orchestrator backend to cephadm...
Generating ssh key...
Wrote public SSH key to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub
Adding key to ljishen@localhost authorized_keys...
Adding host node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us...
Deploying unmanaged mon service...
Deploying unmanaged mgr service...
You can access the Ceph CLI with:

	sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --fsid 3a077bf6-36f0-11ec-b561-09a495c6360a -c /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf -k /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring

Please consider enabling telemetry to help improve Ceph:

	ceph telemetry on

For more information see:

	https://docs.ceph.com/docs/pacific/mgr/telemetry/

Bootstrap complete.

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T02:37:37,770919525-04:00] INFO: > Set the default number of monitor daemons to 1[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:330 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T02:37:37,779564124-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T02:37:37,782893137-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mon', '1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1
Inferring fsid 3a077bf6-36f0-11ec-b561-09a495c6360a
Inferring config /var/lib/ceph/3a077bf6-36f0-11ec-b561-09a495c6360a/mon.node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Scheduled mon update...

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T02:37:47,428288308-04:00] INFO: > Set the default number of manager daemons to 1[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:335 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T02:37:47,437441767-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T02:37:47,440668867-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mgr', '1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1
Inferring fsid 3a077bf6-36f0-11ec-b561-09a495c6360a
Inferring config /var/lib/ceph/3a077bf6-36f0-11ec-b561-09a495c6360a/mon.node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Scheduled mgr update...


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T02:37:56,666227614-04:00] STAGE: Add hosts to the cluster...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T02:37:56,671199072-04:00] INFO: > Adding host 'sm1'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:362 - add_hosts() > ssh-copy-id -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -f -i /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub ljishen@sm1
/usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: "/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub"

Number of key(s) added: 1

Now try logging into the machine, with:   "ssh -o 'GlobalKnownHostsFile=/dev/null' -o 'LogLevel=ERROR' -o 'PasswordAuthentication=no' -o 'StrictHostKeyChecking=no' -o 'UserKnownHostsFile=/dev/null' 'ljishen@sm1'"
and check to make sure that only the key(s) you wanted were added.

# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:368 - add_hosts() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add sm1 10.10.2.5
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T02:37:57,812171906-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T02:37:57,815104923-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'host', 'add', 'sm1', '10.10.2.5'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add sm1 10.10.2.5
Inferring fsid 3a077bf6-36f0-11ec-b561-09a495c6360a
Inferring config /var/lib/ceph/3a077bf6-36f0-11ec-b561-09a495c6360a/mon.node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Added host 'sm1' with addr '10.10.2.5'

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T02:38:08,808136136-04:00] INFO: Sleep for 20 seconds[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T02:38:28,812544756-04:00] STAGE: Add OSDs to the cluster...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T02:38:28,818089484-04:00] INFO: > Adding devices ['/dev/nvme0n1'] on host 'sm1' as OSDs[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:387 - add_host_devices_as_osds() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd sm1:/dev/nvme0n1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T02:38:28,826833941-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T02:38:28,829674384-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'daemon', 'add', 'osd', 'sm1:/dev/nvme0n1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd sm1:/dev/nvme0n1
Inferring fsid 3a077bf6-36f0-11ec-b561-09a495c6360a
Inferring config /var/lib/ceph/3a077bf6-36f0-11ec-b561-09a495c6360a/mon.node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Created osd(s) 0 on host 'sm1'

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T02:38:51,349337120-04:00] INFO: > Sleep for 20 seconds[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T02:39:11,353968965-04:00] STAGE: Show Cluster Status[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:343 - show_status() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T02:39:11,363033155-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T02:39:11,365981881-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', '--status'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status
Inferring fsid 3a077bf6-36f0-11ec-b561-09a495c6360a
Inferring config /var/lib/ceph/3a077bf6-36f0-11ec-b561-09a495c6360a/mon.node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
  cluster:
    id:     3a077bf6-36f0-11ec-b561-09a495c6360a
    health: HEALTH_WARN
            1 pool(s) have no replicas configured
 
  services:
    mon: 1 daemons, quorum node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us (age 2m)
    mgr: node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us.mqubhm(active, since 2m)
    osd: 1 osds: 1 up (since 21s), 1 in (since 40s)
 
  data:
    pools:   1 pools, 128 pgs
    objects: 0 objects, 0 B
    usage:   4.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     99.219% pgs unknown
             127 unknown
             1   active+clean
 
  progress:
 

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:39:21,443194757-07:00] INFO: > Copy configuration files to local[0m
# ./benchmarks/bench-rados:208 - launch_ceph_cluster() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.3:/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf
# ./benchmarks/bench-rados:212 - launch_ceph_cluster() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.3 sudo chmod 644 /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:39:21,599260910-07:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.3 sudo chmod 644 /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring
[1] 23:39:21 [SUCCESS] ljishen@10.10.2.3
# ./benchmarks/bench-rados:214 - launch_ceph_cluster() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.3:/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:39:21,930573979-07:00] STAGE: Update local ceph.conf as a client...[0m


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:39:21,934895813-07:00] STAGE: Configure Ceph pool...[0m
# ./benchmarks/bench-rados:245 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_max_backfills 32
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:39:21,960877841-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:39:21,964205005-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '3a077bf6-36f0-11ec-b561-09a495c6360a', '--', 'ceph', 'config', 'set', 'osd', 'osd_max_backfills', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 3a077bf6-36f0-11ec-b561-09a495c6360a -- ceph config set osd osd_max_backfills 32
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:246 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_active 32
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:39:25,203080765-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:39:25,206196662-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '3a077bf6-36f0-11ec-b561-09a495c6360a', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_active', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 3a077bf6-36f0-11ec-b561-09a495c6360a -- ceph config set osd osd_recovery_max_active 32
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:247 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_single_start 8
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:39:28,502088553-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:39:28,505939892-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '3a077bf6-36f0-11ec-b561-09a495c6360a', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_single_start', '8'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 3a077bf6-36f0-11ec-b561-09a495c6360a -- ceph config set osd osd_recovery_max_single_start 8
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:248 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_op_priority 63
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:39:31,751831750-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:39:31,755421598-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '3a077bf6-36f0-11ec-b561-09a495c6360a', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_op_priority', '63'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 3a077bf6-36f0-11ec-b561-09a495c6360a -- ceph config set osd osd_recovery_op_priority 63
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:251 - configure_ceph_pool() > grep 'osd_max_backfills\|osd_recovery'
# ./benchmarks/bench-rados:250 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config show-with-defaults osd.0
# ./benchmarks/bench-rados:251 - configure_ceph_pool() > column -t -s ' '
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
osd_max_backfills                        32         mon
osd_recovery_cost                        20971520   default
osd_recovery_delay_start                 0.000000   default
osd_recovery_max_active                  32         mon
osd_recovery_max_active_hdd              3          default
osd_recovery_max_active_ssd              10         default
osd_recovery_max_chunk                   8388608    default
osd_recovery_max_omap_entries_per_chunk  8096       default
osd_recovery_max_single_start            8          mon
osd_recovery_op_priority                 63         mon
osd_recovery_op_warn_multiple            16         default
osd_recovery_priority                    5          default
osd_recovery_retry_interval              30.000000  default
osd_recovery_sleep                       0.000000   default
osd_recovery_sleep_hdd                   0.100000   default
osd_recovery_sleep_hybrid                0.025000   default
osd_recovery_sleep_ssd                   0.000000   default
# ./benchmarks/bench-rados:253 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool create bench_rados 128 128 replicated --size 1 --pg-num-min 128
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:39:38,158060913-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:39:38,161687851-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '3a077bf6-36f0-11ec-b561-09a495c6360a', '--', 'ceph', 'osd', 'pool', 'create', 'bench_rados', '128', '128', 'replicated', '--size', '1', '--pg-num-min', '128'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 3a077bf6-36f0-11ec-b561-09a495c6360a -- ceph osd pool create bench_rados 128 128 replicated --size 1 --pg-num-min 128
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
pool 'bench_rados' created
# ./benchmarks/bench-rados:255 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados min_size 1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:39:41,558541688-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:39:41,562070090-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '3a077bf6-36f0-11ec-b561-09a495c6360a', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'min_size', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 3a077bf6-36f0-11ec-b561-09a495c6360a -- ceph osd pool set bench_rados min_size 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 min_size to 1
# ./benchmarks/bench-rados:256 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados pg_autoscale_mode off
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:39:45,851228206-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:39:45,854481642-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '3a077bf6-36f0-11ec-b561-09a495c6360a', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'pg_autoscale_mode', 'off'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 3a077bf6-36f0-11ec-b561-09a495c6360a -- ceph osd pool set bench_rados pg_autoscale_mode off
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 pg_autoscale_mode to off
# ./benchmarks/bench-rados:257 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool application enable bench_rados benchmark
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:39:49,373662193-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:39:49,377259876-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '3a077bf6-36f0-11ec-b561-09a495c6360a', '--', 'ceph', 'osd', 'pool', 'application', 'enable', 'bench_rados', 'benchmark'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 3a077bf6-36f0-11ec-b561-09a495c6360a -- ceph osd pool application enable bench_rados benchmark
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
enabled application 'benchmark' on pool 'bench_rados'
# ./benchmarks/bench-rados:258 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados noscrub 1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:39:53,664080237-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:39:53,667656490-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '3a077bf6-36f0-11ec-b561-09a495c6360a', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'noscrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 3a077bf6-36f0-11ec-b561-09a495c6360a -- ceph osd pool set bench_rados noscrub 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 noscrub to 1
# ./benchmarks/bench-rados:259 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados nodeep-scrub 1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:39:57,623027316-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:39:57,626658611-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '3a077bf6-36f0-11ec-b561-09a495c6360a', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'nodeep-scrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 3a077bf6-36f0-11ec-b561-09a495c6360a -- ceph osd pool set bench_rados nodeep-scrub 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 nodeep-scrub to 1
# ./benchmarks/bench-rados:260 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool ls detail
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:40:02,014015739-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:40:02,017930007-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '3a077bf6-36f0-11ec-b561-09a495c6360a', '--', 'ceph', 'osd', 'pool', 'ls', 'detail'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 3a077bf6-36f0-11ec-b561-09a495c6360a -- ceph osd pool ls detail
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
pool 1 'device_health_metrics' replicated size 1 min_size 1 crush_rule 0 object_hash rjenkins pg_num 128 pgp_num 128 autoscale_mode on last_change 13 lfor 0/0/11 flags hashpspool stripe_width 0 pg_num_min 1 application mgr_devicehealth
pool 2 'bench_rados' replicated size 1 min_size 1 crush_rule 0 object_hash rjenkins pg_num 128 pgp_num 128 autoscale_mode off last_change 21 flags hashpspool,noscrub,nodeep-scrub stripe_width 0 pg_num_min 128 application benchmark

# ./benchmarks/bench-rados:261 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd df tree
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:40:05,381264941-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:40:05,384695168-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '3a077bf6-36f0-11ec-b561-09a495c6360a', '--', 'ceph', 'osd', 'df', 'tree'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 3a077bf6-36f0-11ec-b561-09a495c6360a -- ceph osd df tree
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
ID  CLASS  WEIGHT   REWEIGHT  SIZE     RAW USE  DATA     OMAP  META     AVAIL    %USE  VAR   PGS  STATUS  TYPE NAME    
-1         0.09769         -  100 GiB  6.0 MiB  176 KiB   0 B  5.8 MiB  100 GiB  0.01  1.00    -          root default 
-3         0.09769         -  100 GiB  6.0 MiB  176 KiB   0 B  5.8 MiB  100 GiB  0.01  1.00    -              host sm1 
 0    ssd  0.09769   1.00000  100 GiB  6.0 MiB  176 KiB   0 B  5.8 MiB  100 GiB  0.01  1.00  256      up          osd.0
                       TOTAL  100 GiB  6.0 MiB  176 KiB   0 B  5.8 MiB  100 GiB  0.01                                  
MIN/MAX VAR: 1.00/1.00  STDDEV: 0


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:40:08,822559953-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:40:31,913843274-07:00] INFO: >>   data:
    pools:   2 pools, 256 pgs
    objects: 0 objects, 0 B
    usage:   6.0 MiB used, 100 GiB / 100 GiB avail
    pgs:     256 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:40:31,922603881-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:40:40,127239290-07:00] INFO: >>   data:
    pools:   2 pools, 256 pgs
    objects: 0 objects, 0 B
    usage:   6.0 MiB used, 100 GiB / 100 GiB avail
    pgs:     256 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:40:40,135931989-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:40:48,409111909-07:00] INFO: >>   data:
    pools:   2 pools, 256 pgs
    objects: 0 objects, 0 B
    usage:   6.0 MiB used, 100 GiB / 100 GiB avail
    pgs:     256 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:40:48,417561262-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:40:56,631846279-07:00] INFO: >>   data:
    pools:   2 pools, 256 pgs
    objects: 0 objects, 0 B
    usage:   6.0 MiB used, 100 GiB / 100 GiB avail
    pgs:     256 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:40:56,640583542-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:41:04,873865940-07:00] INFO: >>   data:
    pools:   2 pools, 256 pgs
    objects: 0 objects, 0 B
    usage:   6.0 MiB used, 100 GiB / 100 GiB avail
    pgs:     256 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:41:04,882684034-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:41:04,889672330-07:00] INFO: > The cluster is idle now.[0m


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:41:04,893749113-07:00] STAGE: Start benchmarking of mode write ...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:41:04,901593759-07:00] INFO: > Start system activity collection services[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:41:04,908348776-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:329 - rados_bench() > rm --force /mnt/sda8/ceph-research/scripts/output/sys_activity_16MB_write.dat.rados_bench_host.1
# ./benchmarks/bench-rados:330 - rados_bench() > sar_pid_rados_bench_host=898673
# ./benchmarks/bench-rados:330 - rados_bench() > S_TIME_FORMAT=ISO
# ./benchmarks/bench-rados:330 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/output/sys_activity_16MB_write.dat.rados_bench_host.1 2

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:41:04,916442610-07:00] INFO: >> for OSD host[0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:41:04,925217684-07:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.5 bash
10.10.2.5: b'129107\n'
[1] 23:41:05 [SUCCESS] ljishen@10.10.2.5
129107

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:41:06,013676439-07:00] INFO: > Run rados bench[0m
# ./benchmarks/bench-rados:358 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/output/rados_bench_16MB_write.log.1
## ./benchmarks/bench-rados:349 - rados_bench() > count_cpus 32-63,96-127
# ./benchmarks/bench-rados:349 - rados_bench() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 30 write --pool bench_rados -b 16777216 -O 16777216 --concurrent-ios 128 --show-time --write-object --write-omap --write-xattr --no-cleanup
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:41:06,035786558-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:41:06,039163967-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '3a077bf6-36f0-11ec-b561-09a495c6360a', '--', 'rados', 'bench', '30', 'write', '--pool', 'bench_rados', '-b', '16777216', '-O', '16777216', '--concurrent-ios', '128', '--show-time', '--write-object', '--write-omap', '--write-xattr', '--no-cleanup'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 3a077bf6-36f0-11ec-b561-09a495c6360a -- rados bench 30 write --pool bench_rados -b 16777216 -O 16777216 --concurrent-ios 128 --show-time --write-object --write-omap --write-xattr --no-cleanup
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
hints = 1
2021-10-27T06:41:08.405398+0000 Maintaining 128 concurrent writes of 16777216 bytes to objects of size 16777216 for up to 30 seconds or 0 objects
2021-10-27T06:41:08.405427+0000 Object prefix: benchmark_data_node-0.bluefield2.ucsc-cmps10_7
2021-10-27T06:41:09.013791+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-27T06:41:09.013791+0000     0       0         0         0         0         0           -           0
2021-10-27T06:41:10.013926+0000     1       5         5         0         0         0           -           0
2021-10-27T06:41:11.013995+0000     2      10        10         0         0         0           -           0
2021-10-27T06:41:12.014102+0000     3      14        14         0         0         0           -           0
2021-10-27T06:41:13.014205+0000     4      19        19         0         0         0           -           0
2021-10-27T06:41:14.014310+0000     5      24        24         0         0         0           -           0
2021-10-27T06:41:15.014374+0000     6      29        29         0         0         0           -           0
2021-10-27T06:41:16.014479+0000     7      33        33         0         0         0           -           0
2021-10-27T06:41:17.014696+0000     8      35        35         0         0         0           -           0
2021-10-27T06:41:18.014802+0000     9      38        38         0         0         0           -           0
2021-10-27T06:41:19.014867+0000    10      40        40         0         0         0           -           0
2021-10-27T06:41:20.015000+0000    11      42        42         0         0         0           -           0
2021-10-27T06:41:21.015069+0000    12      44        44         0         0         0           -           0
2021-10-27T06:41:22.015178+0000    13      48        48         0         0         0           -           0
2021-10-27T06:41:23.015243+0000    14      52        52         0         0         0           -           0
2021-10-27T06:41:24.015347+0000    15      57        57         0         0         0           -           0
2021-10-27T06:41:25.015560+0000    16      61        61         0         0         0           -           0
2021-10-27T06:41:26.015665+0000    17      66        66         0         0         0           -           0
2021-10-27T06:41:27.015759+0000    18      68        68         0         0         0           -           0
2021-10-27T06:41:28.015983+0000    19      69        69         0         0         0           -           0
2021-10-27T06:41:29.016049+0000 min lat: 9999 max lat: 0 avg lat: 0
2021-10-27T06:41:29.016049+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-27T06:41:29.016049+0000    20      70        70         0         0         0           -           0
2021-10-27T06:41:30.016172+0000    21      71        71         0         0         0           -           0
2021-10-27T06:41:31.016241+0000    22      72        72         0         0         0           -           0
2021-10-27T06:41:32.016344+0000    23      73        73         0         0         0           -           0
2021-10-27T06:41:33.016411+0000    24      76        76         0         0         0           -           0
2021-10-27T06:41:34.016489+0000    25      79        79         0         0         0           -           0
2021-10-27T06:41:35.016558+0000    26      81        81         0         0         0           -           0
2021-10-27T06:41:36.016623+0000    27      84        84         0         0         0           -           0
2021-10-27T06:41:37.016718+0000    28      87        87         0         0         0           -           0
2021-10-27T06:41:38.016922+0000    29      90        90         0         0         0           -           0
2021-10-27T06:41:39.017016+0000    30      93        93         0         0         0           -           0
2021-10-27T06:41:40.017229+0000    31      96        96         0         0         0           -           0
2021-10-27T06:41:41.017323+0000    32      98        98         0         0         0           -           0
2021-10-27T06:41:42.017542+0000    33      99        99         0         0         0           -           0
2021-10-27T06:41:43.017637+0000    34     100       100         0         0         0           -           0
2021-10-27T06:41:44.017717+0000    35     102       102         0         0         0           -           0
2021-10-27T06:41:45.017929+0000    36     104       104         0         0         0           -           0
2021-10-27T06:41:46.018006+0000    37     107       107         0         0         0           -           0
2021-10-27T06:41:47.018102+0000    38     111       111         0         0         0           -           0
2021-10-27T06:41:48.018211+0000    39     115       115         0         0         0           -           0
2021-10-27T06:41:49.018305+0000 min lat: 9999 max lat: 0 avg lat: 0
2021-10-27T06:41:49.018305+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-27T06:41:49.018305+0000    40     119       119         0         0         0           -           0
2021-10-27T06:41:50.018393+0000    41     124       124         0         0         0           -           0
2021-10-27T06:41:51.018465+0000    42       2       128       126   47.9947        48    0.688165      22.929
2021-10-27T06:41:52.018728+0000 Total time run:         42.2343
Total writes made:      128
Write size:             16777216
Object size:            16777216
Bandwidth (MB/sec):     48.4914
Stddev Bandwidth:       7.40656
Max bandwidth (MB/sec): 48
Min bandwidth (MB/sec): 0
Average IOPS:           3
Stddev IOPS:            0.46291
Max IOPS:               3
Min IOPS:               0
Average Latency(s):     22.5806
Stddev Latency(s):      13.3392
Max latency(s):         41.8373
Min latency(s):         0.622381

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:41:52,632928582-07:00] INFO: > Stop system activity collection services[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:41:52,639882914-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:379 - rados_bench() > kill -INT 898673

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:41:52,646978491-07:00] INFO: >> for OSD host[0m
# ./benchmarks/bench-rados:384 - rados_bench() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.5 kill -INT 129107
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:41:52,654883691-07:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.5 kill -INT 129107
[1] 23:41:54 [SUCCESS] ljishen@10.10.2.5

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:41:54,215205473-07:00] INFO: >>> Copy system activity file from OSD host[0m
## ./benchmarks/bench-rados:391 - rados_bench() > basename -- /tmp/bench-rados/sys_activity_16MB_write.dat.osd_host.1
# ./benchmarks/bench-rados:391 - rados_bench() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.5:/tmp/bench-rados/sys_activity_16MB_write.dat.osd_host.1 /mnt/sda8/ceph-research/scripts/output/sys_activity_16MB_write.dat.osd_host.1


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:41:55,849319131-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:42:18,984875953-07:00] INFO: >>   data:
    pools:   2 pools, 256 pgs
    objects: 129 objects, 2.0 GiB
    usage:   6.4 GiB used, 94 GiB / 100 GiB avail
    pgs:     256 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:42:18,993513118-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:42:27,201602595-07:00] INFO: >>   data:
    pools:   2 pools, 256 pgs
    objects: 129 objects, 2.0 GiB
    usage:   6.0 GiB used, 94 GiB / 100 GiB avail
    pgs:     256 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:42:27,210374293-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:42:35,435521063-07:00] INFO: >>   data:
    pools:   2 pools, 256 pgs
    objects: 129 objects, 2.0 GiB
    usage:   6.0 GiB used, 94 GiB / 100 GiB avail
    pgs:     256 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:42:35,444734622-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:42:43,674833621-07:00] INFO: >>   data:
    pools:   2 pools, 256 pgs
    objects: 129 objects, 2.0 GiB
    usage:   6.0 GiB used, 94 GiB / 100 GiB avail
    pgs:     256 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:42:43,683659211-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:42:51,951543775-07:00] INFO: >>   data:
    pools:   2 pools, 256 pgs
    objects: 129 objects, 2.0 GiB
    usage:   6.0 GiB used, 94 GiB / 100 GiB avail
    pgs:     256 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:42:51,960777382-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:42:51,967602290-07:00] INFO: > The cluster is idle now.[0m


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:42:51,971789701-07:00] STAGE: Start benchmarking of mode seq ...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:42:51,979358107-07:00] INFO: > Start system activity collection services[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:42:51,985627471-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:329 - rados_bench() > rm --force /mnt/sda8/ceph-research/scripts/output/sys_activity_16MB_seq.dat.rados_bench_host.1
# ./benchmarks/bench-rados:330 - rados_bench() > sar_pid_rados_bench_host=900605
# ./benchmarks/bench-rados:330 - rados_bench() > S_TIME_FORMAT=ISO
# ./benchmarks/bench-rados:330 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/output/sys_activity_16MB_seq.dat.rados_bench_host.1 2

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:42:51,993309902-07:00] INFO: >> for OSD host[0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:42:52,001740589-07:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.5 bash
10.10.2.5: b'129651\n'
[1] 23:42:53 [SUCCESS] ljishen@10.10.2.5
129651

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:42:53,189202208-07:00] INFO: > Run rados bench[0m
# ./benchmarks/bench-rados:367 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/output/rados_bench_16MB_seq.log.1
## ./benchmarks/bench-rados:364 - rados_bench() > count_cpus 32-63,96-127
# ./benchmarks/bench-rados:364 - rados_bench() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 128 --show-time
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:42:53,210242467-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:42:53,213720404-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '3a077bf6-36f0-11ec-b561-09a495c6360a', '--', 'rados', 'bench', '99999999', 'seq', '--pool', 'bench_rados', '--concurrent-ios', '128', '--show-time'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 3a077bf6-36f0-11ec-b561-09a495c6360a -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 128 --show-time
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
hints = 1
2021-10-27T06:42:55.678614+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-27T06:42:55.678614+0000     0       0         0         0         0         0           -           0
2021-10-27T06:42:56.678735+0000     1      57        57         0         0         0           -           0
2021-10-27T06:42:57.678835+0000     2     113       113         0         0         0           -           0
2021-10-27T06:42:58.678998+0000 Total time run:       2.51495
Total reads made:     128
Read size:            16777216
Object size:          16777216
Bandwidth (MB/sec):   814.332
Average IOPS:         50
Stddev IOPS:          0
Max IOPS:             0
Min IOPS:             0
Average Latency(s):   1.29987
Max latency(s):       2.27457
Min latency(s):       0.268605

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:42:59,340018909-07:00] INFO: > Stop system activity collection services[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:42:59,346926042-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:379 - rados_bench() > kill -INT 900605

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:42:59,353800063-07:00] INFO: >> for OSD host[0m
# ./benchmarks/bench-rados:384 - rados_bench() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.5 kill -INT 129651
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:42:59,361849743-07:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.5 kill -INT 129651
[1] 23:43:00 [SUCCESS] ljishen@10.10.2.5

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:43:00,475073770-07:00] INFO: >>> Copy system activity file from OSD host[0m
## ./benchmarks/bench-rados:391 - rados_bench() > basename -- /tmp/bench-rados/sys_activity_16MB_seq.dat.osd_host.1
# ./benchmarks/bench-rados:391 - rados_bench() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.5:/tmp/bench-rados/sys_activity_16MB_seq.dat.osd_host.1 /mnt/sda8/ceph-research/scripts/output/sys_activity_16MB_seq.dat.osd_host.1


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:43:01,565932938-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:43:24,918426581-07:00] INFO: >>   data:
    pools:   2 pools, 256 pgs
    objects: 129 objects, 2.0 GiB
    usage:   6.0 GiB used, 94 GiB / 100 GiB avail
    pgs:     256 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:43:24,927248644-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:43:33,201745530-07:00] INFO: >>   data:
    pools:   2 pools, 256 pgs
    objects: 129 objects, 2.0 GiB
    usage:   6.0 GiB used, 94 GiB / 100 GiB avail
    pgs:     256 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:43:33,210978705-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:43:41,551109512-07:00] INFO: >>   data:
    pools:   2 pools, 256 pgs
    objects: 129 objects, 2.0 GiB
    usage:   6.0 GiB used, 94 GiB / 100 GiB avail
    pgs:     256 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:43:41,560071939-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:43:49,766898689-07:00] INFO: >>   data:
    pools:   2 pools, 256 pgs
    objects: 129 objects, 2.0 GiB
    usage:   6.0 GiB used, 94 GiB / 100 GiB avail
    pgs:     256 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:43:49,775939853-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:43:58,117162824-07:00] INFO: >>   data:
    pools:   2 pools, 256 pgs
    objects: 129 objects, 2.0 GiB
    usage:   6.0 GiB used, 94 GiB / 100 GiB avail
    pgs:     256 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:43:58,125965891-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:43:58,132718764-07:00] INFO: > The cluster is idle now.[0m


[1;7;39;49m[2021-10-26T23:43:58,134918379-07:00][RUNNING][ROUND 2/7/21] object_size=16MB[0m


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:43:58,138686552-07:00] STAGE: Launch a Ceph cluster (CEPH_HOST: 10.10.2.3, OSD_HOST: 10.10.2.5) ...[0m
# ./benchmarks/bench-rados:172 - launch_ceph_cluster() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.3 bash -euo pipefail
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:43:58,147243536-07:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.3 bash -euo pipefail
10.10.2.3: b'\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T02:43:58,528263091-04:00] INFO: > Remove the existing cluster\x1b[0m\n'
10.10.2.3: b'## bash:13 -  > ls /var/lib/ceph\n'
10.10.2.3: b'## bash:13 -  > tail -n1\n'
10.10.2.3: b'# bash:13 -  > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm --host 10.10.2.3 --host 10.10.2.5 rm-cluster --force --fsid 3a077bf6-36f0-11ec-b561-09a495c6360a\n'
10.10.2.3: b'\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T02:44:09,572970871-04:00] INFO: > Deploy a new cluster\x1b[0m\n'
10.10.2.3: b'## bash:22 -  > /tmp/bench-rados/ceph-research/scripts/parallel-ssh --host 10.10.2.5 uname --nodename\n## bash:22 -  > tail -n1\n'
10.10.2.3: b'# bash:22 -  > osd_hostname=sm1\n# bash:23 -  > /tmp/bench-rados/ceph-research/scripts/ceph-deploy -m 10.10.2.3 -o sm1/10.10.2.5:/dev/nvme0n1\n'
10.10.2.3: b'\n\n\x1b[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T02:44:10,705568208-04:00] STAGE: Verify passwordless SSH to hosts, passwordless sudo via ljishen, and the existence of devices on hosts...\x1b[0m\n'
10.10.2.3: b'\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T02:44:10,709065408-04:00] INFO: > Check the host for the monitor\x1b[0m\n# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:181 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@10.10.2.3 sudo --non-interactive --validate\n'
10.10.2.3: b'\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T02:44:10,860354255-04:00] INFO: > Check hosts for OSDs\x1b[0m\n'
10.10.2.3: b"\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T02:44:10,864053385-04:00] INFO: >> Check host 'sm1'\x1b[0m\n# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:200 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 sudo --non-interactive --validate\n"
10.10.2.3: b"\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T02:44:11,924475521-04:00] INFO: >>> Check device '/dev/nvme0n1'\x1b[0m\n# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:215 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 test -b /dev/nvme0n1\n"
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:226 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 lsblk --nodeps --noheadings --output TYPE /dev/nvme0n1\n# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:227 - verify_parameters() > grep --word-regexp disk\n'
10.10.2.3: b'\n\n\x1b[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T02:44:14,059065195-04:00] STAGE: Reset devices on hosts...\x1b[0m\n'
10.10.2.3: b"\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T02:44:14,062946950-04:00] INFO: > Resetting device '/dev/nvme0n1' on host 'sm1'\x1b[0m\n"
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:288 - reset_host_devices() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 sudo sh\n'
10.10.2.3: b'  Removing ceph--76f86bf0--1af3--485b--a14a--9299baa02288-osd--block--b9e8f052--056a--42ad--a84d--0a540b0bf40c (252:0)\n'
10.10.2.3: b'  Archiving volume group "ceph-76f86bf0-1af3-485b-a14a-9299baa02288" metadata (seqno 5).\n'
10.10.2.3: b'  Releasing logical volume "osd-block-b9e8f052-056a-42ad-a84d-0a540b0bf40c"\n'
10.10.2.3: b'  Creating volume group backup "/etc/lvm/backup/ceph-76f86bf0-1af3-485b-a14a-9299baa02288" (seqno 6).\n'
10.10.2.3: b'  Logical volume "osd-block-b9e8f052-056a-42ad-a84d-0a540b0bf40c" successfully removed\n'
10.10.2.3: b'  Removing physical volume "/dev/nvme0n1" from volume group "ceph-76f86bf0-1af3-485b-a14a-9299baa02288"\n'
10.10.2.3: b'  Volume group "ceph-76f86bf0-1af3-485b-a14a-9299baa02288" successfully removed\n'
10.10.2.3: b'\n\n\x1b[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T02:44:16,603734190-04:00] STAGE: Bootstrap a new cluster...\x1b[0m\n# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:312 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.3 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults\n'
10.10.2.3: b'\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T02:44:16,612102870-04:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.3: b"\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T02:44:16,615221716-04:00] DEBUG: CEPHADM_PARAMS: ['bootstrap', '--config', '/tmp/bench-rados/ceph-research/scripts/ceph.conf', '--mon-ip', '10.10.2.3', '--output-dir', '/tmp/bench-rados/deployment_data_root/etc/ceph', '--ssh-user', 'ljishen', '--skip-dashboard', '--no-minimize-config', '--allow-overwrite', '--allow-fqdn-hostname', '--orphan-initial-daemons', '--skip-monitoring-stack', '--single-host-defaults']\x1b[0m\n"
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.3 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults\n'
10.10.2.3: b'Verifying podman|docker is present...\nVerifying lvm2 is present...\nVerifying time synchronization is in place...\n'
10.10.2.3: b'Unit ntp.service is enabled and running\nRepeating the final host check...\npodman|docker (/usr/bin/docker) is present\nsystemctl is present\nlvcreate is present\n'
10.10.2.3: b'Unit ntp.service is enabled and running\nHost looks OK\n'
10.10.2.3: b'Cluster fsid: 4e312aa4-36f1-11ec-b561-09a495c6360a\n'
10.10.2.3: b'Verifying IP 10.10.2.3 port 3300 ...\nVerifying IP 10.10.2.3 port 6789 ...\n'
10.10.2.3: b'Mon IP `10.10.2.3` is in CIDR network `10.10.2.0/24`\n- internal network (--cluster-network) has not been provided, OSD replication will default to the public_network\n'
10.10.2.3: b'Adjusting default settings to suit single-host cluster...\n'
10.10.2.3: b'Pulling container image quay.io/ceph/ceph:v16...\n'
10.10.2.3: b'Ceph version: ceph version 16.2.6 (ee28fb57e47e9f88813e24bbf4c14496ca299d31) pacific (stable)\nExtracting ceph user uid/gid from container image...\n'
10.10.2.3: b'Creating initial keys...\n'
10.10.2.3: b'Creating initial monmap...\n'
10.10.2.3: b'Creating mon...\n'
10.10.2.3: b'Waiting for mon to start...\nWaiting for mon...\n'
10.10.2.3: b'mon is available\nSetting mon public_network to 10.10.2.0/24\n'
10.10.2.3: b'Wrote config to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf\nWrote keyring to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring\nCreating mgr...\n'
10.10.2.3: b'Verifying port 9283 ...\n'
10.10.2.3: b'Waiting for mgr to start...\nWaiting for mgr...\n'
10.10.2.3: b'mgr not available, waiting (1/15)...\n'
10.10.2.3: b'mgr not available, waiting (2/15)...\n'
10.10.2.3: b'mgr is available\n'
10.10.2.3: b'Enabling cephadm module...\n'
10.10.2.3: b'Waiting for the mgr to restart...\nWaiting for mgr epoch 5...\n'
10.10.2.3: b'mgr epoch 5 is available\nSetting orchestrator backend to cephadm...\n'
10.10.2.3: b'Generating ssh key...\n'
10.10.2.3: b'Wrote public SSH key to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub\nAdding key to ljishen@localhost authorized_keys...\n'
10.10.2.3: b'Adding host node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us...\n'
10.10.2.3: b'Deploying unmanaged mon service...\n'
10.10.2.3: b'Deploying unmanaged mgr service...\n'
10.10.2.3: b'You can access the Ceph CLI with:\n\n\tsudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --fsid 4e312aa4-36f1-11ec-b561-09a495c6360a -c /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf -k /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring\n\nPlease consider enabling telemetry to help improve Ceph:\n\n\tceph telemetry on\n\nFor more information see:\n\n\thttps://docs.ceph.com/docs/pacific/mgr/telemetry/\n\nBootstrap complete.\n'
10.10.2.3: b'\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T02:45:21,871321846-04:00] INFO: > Set the default number of monitor daemons to 1\x1b[0m\n# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:330 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1\n'
10.10.2.3: b'\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T02:45:21,879672842-04:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.3: b"\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T02:45:21,882814592-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mon', '1']\x1b[0m\n"
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1\n'
10.10.2.3: b'Inferring fsid 4e312aa4-36f1-11ec-b561-09a495c6360a\n'
10.10.2.3: b'Inferring config /var/lib/ceph/4e312aa4-36f1-11ec-b561-09a495c6360a/mon.node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.3: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.3: b'Scheduled mon update...\n'
10.10.2.3: b'\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T02:45:32,739122818-04:00] INFO: > Set the default number of manager daemons to 1\x1b[0m\n# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:335 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1\n'
10.10.2.3: b'\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T02:45:32,747636200-04:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.3: b"\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T02:45:32,750991252-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mgr', '1']\x1b[0m\n# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1\n"
10.10.2.3: b'Inferring fsid 4e312aa4-36f1-11ec-b561-09a495c6360a\n'
10.10.2.3: b'Inferring config /var/lib/ceph/4e312aa4-36f1-11ec-b561-09a495c6360a/mon.node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.3: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.3: b'Scheduled mgr update...\n'
10.10.2.3: b'\n\n\x1b[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T02:45:42,025533416-04:00] STAGE: Add hosts to the cluster...\x1b[0m\n'
10.10.2.3: b"\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T02:45:42,030506867-04:00] INFO: > Adding host 'sm1'\x1b[0m\n# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:362 - add_hosts() > ssh-copy-id -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -f -i /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub ljishen@sm1\n"
10.10.2.3: b'/usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: "/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub"\n'
10.10.2.3: b'\nNumber of key(s) added: 1\n\nNow try logging into the machine, with:   "ssh -o \'GlobalKnownHostsFile=/dev/null\' -o \'LogLevel=ERROR\' -o \'PasswordAuthentication=no\' -o \'StrictHostKeyChecking=no\' -o \'UserKnownHostsFile=/dev/null\' \'ljishen@sm1\'"\nand check to make sure that only the key(s) you wanted were added.\n\n'
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:368 - add_hosts() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add sm1 10.10.2.5\n'
10.10.2.3: b'\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T02:45:43,176549281-04:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.3: b"\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T02:45:43,179617462-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'host', 'add', 'sm1', '10.10.2.5']\x1b[0m\n"
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add sm1 10.10.2.5\n'
10.10.2.3: b'Inferring fsid 4e312aa4-36f1-11ec-b561-09a495c6360a\n'
10.10.2.3: b'Inferring config /var/lib/ceph/4e312aa4-36f1-11ec-b561-09a495c6360a/mon.node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.3: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.3: b"Added host 'sm1' with addr '10.10.2.5'\n"
10.10.2.3: b'\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T02:45:54,356127422-04:00] INFO: Sleep for 20 seconds\x1b[0m\n'
10.10.2.3: b'\n\n\x1b[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T02:46:14,360185809-04:00] STAGE: Add OSDs to the cluster...\x1b[0m\n'
10.10.2.3: b"\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T02:46:14,365582388-04:00] INFO: > Adding devices ['/dev/nvme0n1'] on host 'sm1' as OSDs\x1b[0m\n# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:387 - add_host_devices_as_osds() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd sm1:/dev/nvme0n1\n"
10.10.2.3: b'\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T02:46:14,373605837-04:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.3: b"\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T02:46:14,376575954-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'daemon', 'add', 'osd', 'sm1:/dev/nvme0n1']\x1b[0m\n"
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd sm1:/dev/nvme0n1\n'
10.10.2.3: b'Inferring fsid 4e312aa4-36f1-11ec-b561-09a495c6360a\n'
10.10.2.3: b'Inferring config /var/lib/ceph/4e312aa4-36f1-11ec-b561-09a495c6360a/mon.node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.3: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.3: b"Created osd(s) 0 on host 'sm1'\n"
10.10.2.3: b'\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T02:46:38,537446221-04:00] INFO: > Sleep for 20 seconds\x1b[0m\n'
10.10.2.3: b'\n\n\x1b[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T02:46:58,542323751-04:00] STAGE: Show Cluster Status\x1b[0m\n# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:343 - show_status() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status\n'
10.10.2.3: b'\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T02:46:58,551667578-04:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.3: b"\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T02:46:58,554558435-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', '--status']\x1b[0m\n# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status\n"
10.10.2.3: b'Inferring fsid 4e312aa4-36f1-11ec-b561-09a495c6360a\n'
10.10.2.3: b'Inferring config /var/lib/ceph/4e312aa4-36f1-11ec-b561-09a495c6360a/mon.node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.3: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.3: b'  cluster:\n    id:     4e312aa4-36f1-11ec-b561-09a495c6360a\n    health: HEALTH_WARN\n            1 pool(s) have no replicas configured\n \n  services:\n    mon: 1 daemons, quorum node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us (age 2m)\n    mgr: node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us.rahwab(active, since 2m)\n    osd: 1 osds: 1 up (since 20s), 1 in (since 40s)\n \n  data:\n    pools:   1 pools, 128 pgs\n    objects: 0 objects, 0 B\n    usage:   4.9 MiB used, 100 GiB / 100 GiB avail\n    pgs:     99.219% pgs unknown\n             0.781% pgs not active\n             127 unknown\n             1   peering\n \n  progress:\n    Global Recovery Event (0s)\n      [............................] \n \n'
[1] 23:47:07 [SUCCESS] ljishen@10.10.2.3

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T02:43:58,528263091-04:00] INFO: > Remove the existing cluster[0m
## bash:13 -  > ls /var/lib/ceph
## bash:13 -  > tail -n1
# bash:13 -  > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm --host 10.10.2.3 --host 10.10.2.5 rm-cluster --force --fsid 3a077bf6-36f0-11ec-b561-09a495c6360a

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T02:44:09,572970871-04:00] INFO: > Deploy a new cluster[0m
## bash:22 -  > /tmp/bench-rados/ceph-research/scripts/parallel-ssh --host 10.10.2.5 uname --nodename
## bash:22 -  > tail -n1
# bash:22 -  > osd_hostname=sm1
# bash:23 -  > /tmp/bench-rados/ceph-research/scripts/ceph-deploy -m 10.10.2.3 -o sm1/10.10.2.5:/dev/nvme0n1


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T02:44:10,705568208-04:00] STAGE: Verify passwordless SSH to hosts, passwordless sudo via ljishen, and the existence of devices on hosts...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T02:44:10,709065408-04:00] INFO: > Check the host for the monitor[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:181 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@10.10.2.3 sudo --non-interactive --validate

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T02:44:10,860354255-04:00] INFO: > Check hosts for OSDs[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T02:44:10,864053385-04:00] INFO: >> Check host 'sm1'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:200 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 sudo --non-interactive --validate

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T02:44:11,924475521-04:00] INFO: >>> Check device '/dev/nvme0n1'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:215 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 test -b /dev/nvme0n1
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:226 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 lsblk --nodeps --noheadings --output TYPE /dev/nvme0n1
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:227 - verify_parameters() > grep --word-regexp disk


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T02:44:14,059065195-04:00] STAGE: Reset devices on hosts...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T02:44:14,062946950-04:00] INFO: > Resetting device '/dev/nvme0n1' on host 'sm1'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:288 - reset_host_devices() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 sudo sh
  Removing ceph--76f86bf0--1af3--485b--a14a--9299baa02288-osd--block--b9e8f052--056a--42ad--a84d--0a540b0bf40c (252:0)
  Archiving volume group "ceph-76f86bf0-1af3-485b-a14a-9299baa02288" metadata (seqno 5).
  Releasing logical volume "osd-block-b9e8f052-056a-42ad-a84d-0a540b0bf40c"
  Creating volume group backup "/etc/lvm/backup/ceph-76f86bf0-1af3-485b-a14a-9299baa02288" (seqno 6).
  Logical volume "osd-block-b9e8f052-056a-42ad-a84d-0a540b0bf40c" successfully removed
  Removing physical volume "/dev/nvme0n1" from volume group "ceph-76f86bf0-1af3-485b-a14a-9299baa02288"
  Volume group "ceph-76f86bf0-1af3-485b-a14a-9299baa02288" successfully removed


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T02:44:16,603734190-04:00] STAGE: Bootstrap a new cluster...[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:312 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.3 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T02:44:16,612102870-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T02:44:16,615221716-04:00] DEBUG: CEPHADM_PARAMS: ['bootstrap', '--config', '/tmp/bench-rados/ceph-research/scripts/ceph.conf', '--mon-ip', '10.10.2.3', '--output-dir', '/tmp/bench-rados/deployment_data_root/etc/ceph', '--ssh-user', 'ljishen', '--skip-dashboard', '--no-minimize-config', '--allow-overwrite', '--allow-fqdn-hostname', '--orphan-initial-daemons', '--skip-monitoring-stack', '--single-host-defaults'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.3 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults
Verifying podman|docker is present...
Verifying lvm2 is present...
Verifying time synchronization is in place...
Unit ntp.service is enabled and running
Repeating the final host check...
podman|docker (/usr/bin/docker) is present
systemctl is present
lvcreate is present
Unit ntp.service is enabled and running
Host looks OK
Cluster fsid: 4e312aa4-36f1-11ec-b561-09a495c6360a
Verifying IP 10.10.2.3 port 3300 ...
Verifying IP 10.10.2.3 port 6789 ...
Mon IP `10.10.2.3` is in CIDR network `10.10.2.0/24`
- internal network (--cluster-network) has not been provided, OSD replication will default to the public_network
Adjusting default settings to suit single-host cluster...
Pulling container image quay.io/ceph/ceph:v16...
Ceph version: ceph version 16.2.6 (ee28fb57e47e9f88813e24bbf4c14496ca299d31) pacific (stable)
Extracting ceph user uid/gid from container image...
Creating initial keys...
Creating initial monmap...
Creating mon...
Waiting for mon to start...
Waiting for mon...
mon is available
Setting mon public_network to 10.10.2.0/24
Wrote config to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf
Wrote keyring to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring
Creating mgr...
Verifying port 9283 ...
Waiting for mgr to start...
Waiting for mgr...
mgr not available, waiting (1/15)...
mgr not available, waiting (2/15)...
mgr is available
Enabling cephadm module...
Waiting for the mgr to restart...
Waiting for mgr epoch 5...
mgr epoch 5 is available
Setting orchestrator backend to cephadm...
Generating ssh key...
Wrote public SSH key to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub
Adding key to ljishen@localhost authorized_keys...
Adding host node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us...
Deploying unmanaged mon service...
Deploying unmanaged mgr service...
You can access the Ceph CLI with:

	sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --fsid 4e312aa4-36f1-11ec-b561-09a495c6360a -c /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf -k /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring

Please consider enabling telemetry to help improve Ceph:

	ceph telemetry on

For more information see:

	https://docs.ceph.com/docs/pacific/mgr/telemetry/

Bootstrap complete.

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T02:45:21,871321846-04:00] INFO: > Set the default number of monitor daemons to 1[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:330 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T02:45:21,879672842-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T02:45:21,882814592-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mon', '1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1
Inferring fsid 4e312aa4-36f1-11ec-b561-09a495c6360a
Inferring config /var/lib/ceph/4e312aa4-36f1-11ec-b561-09a495c6360a/mon.node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Scheduled mon update...

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T02:45:32,739122818-04:00] INFO: > Set the default number of manager daemons to 1[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:335 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T02:45:32,747636200-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T02:45:32,750991252-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mgr', '1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1
Inferring fsid 4e312aa4-36f1-11ec-b561-09a495c6360a
Inferring config /var/lib/ceph/4e312aa4-36f1-11ec-b561-09a495c6360a/mon.node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Scheduled mgr update...


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T02:45:42,025533416-04:00] STAGE: Add hosts to the cluster...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T02:45:42,030506867-04:00] INFO: > Adding host 'sm1'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:362 - add_hosts() > ssh-copy-id -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -f -i /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub ljishen@sm1
/usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: "/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub"

Number of key(s) added: 1

Now try logging into the machine, with:   "ssh -o 'GlobalKnownHostsFile=/dev/null' -o 'LogLevel=ERROR' -o 'PasswordAuthentication=no' -o 'StrictHostKeyChecking=no' -o 'UserKnownHostsFile=/dev/null' 'ljishen@sm1'"
and check to make sure that only the key(s) you wanted were added.

# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:368 - add_hosts() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add sm1 10.10.2.5
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T02:45:43,176549281-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T02:45:43,179617462-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'host', 'add', 'sm1', '10.10.2.5'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add sm1 10.10.2.5
Inferring fsid 4e312aa4-36f1-11ec-b561-09a495c6360a
Inferring config /var/lib/ceph/4e312aa4-36f1-11ec-b561-09a495c6360a/mon.node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Added host 'sm1' with addr '10.10.2.5'

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T02:45:54,356127422-04:00] INFO: Sleep for 20 seconds[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T02:46:14,360185809-04:00] STAGE: Add OSDs to the cluster...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T02:46:14,365582388-04:00] INFO: > Adding devices ['/dev/nvme0n1'] on host 'sm1' as OSDs[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:387 - add_host_devices_as_osds() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd sm1:/dev/nvme0n1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T02:46:14,373605837-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T02:46:14,376575954-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'daemon', 'add', 'osd', 'sm1:/dev/nvme0n1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd sm1:/dev/nvme0n1
Inferring fsid 4e312aa4-36f1-11ec-b561-09a495c6360a
Inferring config /var/lib/ceph/4e312aa4-36f1-11ec-b561-09a495c6360a/mon.node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Created osd(s) 0 on host 'sm1'

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T02:46:38,537446221-04:00] INFO: > Sleep for 20 seconds[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T02:46:58,542323751-04:00] STAGE: Show Cluster Status[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:343 - show_status() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T02:46:58,551667578-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T02:46:58,554558435-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', '--status'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status
Inferring fsid 4e312aa4-36f1-11ec-b561-09a495c6360a
Inferring config /var/lib/ceph/4e312aa4-36f1-11ec-b561-09a495c6360a/mon.node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
  cluster:
    id:     4e312aa4-36f1-11ec-b561-09a495c6360a
    health: HEALTH_WARN
            1 pool(s) have no replicas configured
 
  services:
    mon: 1 daemons, quorum node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us (age 2m)
    mgr: node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us.rahwab(active, since 2m)
    osd: 1 osds: 1 up (since 20s), 1 in (since 40s)
 
  data:
    pools:   1 pools, 128 pgs
    objects: 0 objects, 0 B
    usage:   4.9 MiB used, 100 GiB / 100 GiB avail
    pgs:     99.219% pgs unknown
             0.781% pgs not active
             127 unknown
             1   peering
 
  progress:
    Global Recovery Event (0s)
      [............................] 
 

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:47:07,965883410-07:00] INFO: > Copy configuration files to local[0m
# ./benchmarks/bench-rados:208 - launch_ceph_cluster() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.3:/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf
# ./benchmarks/bench-rados:212 - launch_ceph_cluster() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.3 sudo chmod 644 /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:47:08,123479453-07:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.3 sudo chmod 644 /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring
[1] 23:47:08 [SUCCESS] ljishen@10.10.2.3
# ./benchmarks/bench-rados:214 - launch_ceph_cluster() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.3:/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:47:08,450693775-07:00] STAGE: Update local ceph.conf as a client...[0m


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:47:08,454595058-07:00] STAGE: Configure Ceph pool...[0m
# ./benchmarks/bench-rados:245 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_max_backfills 32
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:47:08,479273635-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:47:08,482304912-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '4e312aa4-36f1-11ec-b561-09a495c6360a', '--', 'ceph', 'config', 'set', 'osd', 'osd_max_backfills', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 4e312aa4-36f1-11ec-b561-09a495c6360a -- ceph config set osd osd_max_backfills 32
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:246 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_active 32
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:47:11,698651139-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:47:11,702033827-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '4e312aa4-36f1-11ec-b561-09a495c6360a', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_active', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 4e312aa4-36f1-11ec-b561-09a495c6360a -- ceph config set osd osd_recovery_max_active 32
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:247 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_single_start 8
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:47:15,013228554-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:47:15,016093189-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '4e312aa4-36f1-11ec-b561-09a495c6360a', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_single_start', '8'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 4e312aa4-36f1-11ec-b561-09a495c6360a -- ceph config set osd osd_recovery_max_single_start 8
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:248 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_op_priority 63
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:47:18,157249737-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:47:18,160759163-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '4e312aa4-36f1-11ec-b561-09a495c6360a', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_op_priority', '63'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 4e312aa4-36f1-11ec-b561-09a495c6360a -- ceph config set osd osd_recovery_op_priority 63
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:250 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config show-with-defaults osd.0
# ./benchmarks/bench-rados:251 - configure_ceph_pool() > grep 'osd_max_backfills\|osd_recovery'
# ./benchmarks/bench-rados:251 - configure_ceph_pool() > column -t -s ' '
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
osd_max_backfills                        32         mon
osd_recovery_cost                        20971520   default
osd_recovery_delay_start                 0.000000   default
osd_recovery_max_active                  32         mon
osd_recovery_max_active_hdd              3          default
osd_recovery_max_active_ssd              10         default
osd_recovery_max_chunk                   8388608    default
osd_recovery_max_omap_entries_per_chunk  8096       default
osd_recovery_max_single_start            8          mon
osd_recovery_op_priority                 63         mon
osd_recovery_op_warn_multiple            16         default
osd_recovery_priority                    5          default
osd_recovery_retry_interval              30.000000  default
osd_recovery_sleep                       0.000000   default
osd_recovery_sleep_hdd                   0.100000   default
osd_recovery_sleep_hybrid                0.025000   default
osd_recovery_sleep_ssd                   0.000000   default
# ./benchmarks/bench-rados:253 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool create bench_rados 128 128 replicated --size 1 --pg-num-min 128
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:47:24,650269193-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:47:24,653809868-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '4e312aa4-36f1-11ec-b561-09a495c6360a', '--', 'ceph', 'osd', 'pool', 'create', 'bench_rados', '128', '128', 'replicated', '--size', '1', '--pg-num-min', '128'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 4e312aa4-36f1-11ec-b561-09a495c6360a -- ceph osd pool create bench_rados 128 128 replicated --size 1 --pg-num-min 128
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
pool 'bench_rados' created
# ./benchmarks/bench-rados:255 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados min_size 1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:47:28,890921328-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:47:28,894780732-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '4e312aa4-36f1-11ec-b561-09a495c6360a', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'min_size', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 4e312aa4-36f1-11ec-b561-09a495c6360a -- ceph osd pool set bench_rados min_size 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 min_size to 1
# ./benchmarks/bench-rados:256 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados pg_autoscale_mode off
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:47:32,382407106-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:47:32,385900132-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '4e312aa4-36f1-11ec-b561-09a495c6360a', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'pg_autoscale_mode', 'off'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 4e312aa4-36f1-11ec-b561-09a495c6360a -- ceph osd pool set bench_rados pg_autoscale_mode off
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 pg_autoscale_mode to off
# ./benchmarks/bench-rados:257 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool application enable bench_rados benchmark
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:47:36,034665789-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:47:36,038183821-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '4e312aa4-36f1-11ec-b561-09a495c6360a', '--', 'ceph', 'osd', 'pool', 'application', 'enable', 'bench_rados', 'benchmark'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 4e312aa4-36f1-11ec-b561-09a495c6360a -- ceph osd pool application enable bench_rados benchmark
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
enabled application 'benchmark' on pool 'bench_rados'
# ./benchmarks/bench-rados:258 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados noscrub 1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:47:39,310169841-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:47:39,313485373-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '4e312aa4-36f1-11ec-b561-09a495c6360a', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'noscrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 4e312aa4-36f1-11ec-b561-09a495c6360a -- ceph osd pool set bench_rados noscrub 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 noscrub to 1
# ./benchmarks/bench-rados:259 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados nodeep-scrub 1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:47:42,877821103-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:47:42,881403557-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '4e312aa4-36f1-11ec-b561-09a495c6360a', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'nodeep-scrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 4e312aa4-36f1-11ec-b561-09a495c6360a -- ceph osd pool set bench_rados nodeep-scrub 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 nodeep-scrub to 1
# ./benchmarks/bench-rados:260 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool ls detail
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:47:47,259509535-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:47:47,262528780-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '4e312aa4-36f1-11ec-b561-09a495c6360a', '--', 'ceph', 'osd', 'pool', 'ls', 'detail'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 4e312aa4-36f1-11ec-b561-09a495c6360a -- ceph osd pool ls detail
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
pool 1 'device_health_metrics' replicated size 1 min_size 1 crush_rule 0 object_hash rjenkins pg_num 128 pgp_num 128 autoscale_mode on last_change 13 lfor 0/0/11 flags hashpspool stripe_width 0 pg_num_min 1 application mgr_devicehealth
pool 2 'bench_rados' replicated size 1 min_size 1 crush_rule 0 object_hash rjenkins pg_num 128 pgp_num 128 autoscale_mode off last_change 21 flags hashpspool,noscrub,nodeep-scrub stripe_width 0 pg_num_min 128 application benchmark

# ./benchmarks/bench-rados:261 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd df tree
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:47:50,529259012-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:47:50,532766645-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '4e312aa4-36f1-11ec-b561-09a495c6360a', '--', 'ceph', 'osd', 'df', 'tree'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 4e312aa4-36f1-11ec-b561-09a495c6360a -- ceph osd df tree
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
ID  CLASS  WEIGHT   REWEIGHT  SIZE     RAW USE  DATA     OMAP  META     AVAIL    %USE  VAR   PGS  STATUS  TYPE NAME    
-1         0.09769         -  100 GiB  6.0 MiB  176 KiB   0 B  5.8 MiB  100 GiB  0.01  1.00    -          root default 
-3         0.09769         -  100 GiB  6.0 MiB  176 KiB   0 B  5.8 MiB  100 GiB  0.01  1.00    -              host sm1 
 0    ssd  0.09769   1.00000  100 GiB  6.0 MiB  176 KiB   0 B  5.8 MiB  100 GiB  0.01  1.00  256      up          osd.0
                       TOTAL  100 GiB  6.0 MiB  176 KiB   0 B  5.8 MiB  100 GiB  0.01                                  
MIN/MAX VAR: 1.00/1.00  STDDEV: 0


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:47:53,853153221-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:48:17,076985246-07:00] INFO: >>   data:
    pools:   2 pools, 256 pgs
    objects: 0 objects, 0 B
    usage:   6.0 MiB used, 100 GiB / 100 GiB avail
    pgs:     256 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:48:17,085704145-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:48:25,318775995-07:00] INFO: >>   data:
    pools:   2 pools, 256 pgs
    objects: 0 objects, 0 B
    usage:   6.0 MiB used, 100 GiB / 100 GiB avail
    pgs:     256 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:48:25,327607936-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:48:33,626731791-07:00] INFO: >>   data:
    pools:   2 pools, 256 pgs
    objects: 0 objects, 0 B
    usage:   6.0 MiB used, 100 GiB / 100 GiB avail
    pgs:     256 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:48:33,636203604-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:48:41,859965225-07:00] INFO: >>   data:
    pools:   2 pools, 256 pgs
    objects: 0 objects, 0 B
    usage:   6.0 MiB used, 100 GiB / 100 GiB avail
    pgs:     256 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:48:41,868879360-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:48:50,125075364-07:00] INFO: >>   data:
    pools:   2 pools, 256 pgs
    objects: 0 objects, 0 B
    usage:   6.0 MiB used, 100 GiB / 100 GiB avail
    pgs:     256 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:48:50,133643970-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:48:50,140071582-07:00] INFO: > The cluster is idle now.[0m


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:48:50,144615984-07:00] STAGE: Start benchmarking of mode write ...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:48:50,153439078-07:00] INFO: > Start system activity collection services[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:48:50,159320713-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:329 - rados_bench() > rm --force /mnt/sda8/ceph-research/scripts/output/sys_activity_16MB_write.dat.rados_bench_host.2
# ./benchmarks/bench-rados:330 - rados_bench() > sar_pid_rados_bench_host=907634
# ./benchmarks/bench-rados:330 - rados_bench() > S_TIME_FORMAT=ISO
# ./benchmarks/bench-rados:330 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/output/sys_activity_16MB_write.dat.rados_bench_host.2 2

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:48:50,167601919-07:00] INFO: >> for OSD host[0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:48:50,176294277-07:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.5 bash
10.10.2.5: b'134435\n'
[1] 23:48:51 [SUCCESS] ljishen@10.10.2.5
134435

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:48:51,287965481-07:00] INFO: > Run rados bench[0m
# ./benchmarks/bench-rados:358 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/output/rados_bench_16MB_write.log.2
## ./benchmarks/bench-rados:349 - rados_bench() > count_cpus 32-63,96-127
# ./benchmarks/bench-rados:349 - rados_bench() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 30 write --pool bench_rados -b 16777216 -O 16777216 --concurrent-ios 128 --show-time --write-object --write-omap --write-xattr --no-cleanup
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:48:51,309825860-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:48:51,313421338-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '4e312aa4-36f1-11ec-b561-09a495c6360a', '--', 'rados', 'bench', '30', 'write', '--pool', 'bench_rados', '-b', '16777216', '-O', '16777216', '--concurrent-ios', '128', '--show-time', '--write-object', '--write-omap', '--write-xattr', '--no-cleanup'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 4e312aa4-36f1-11ec-b561-09a495c6360a -- rados bench 30 write --pool bench_rados -b 16777216 -O 16777216 --concurrent-ios 128 --show-time --write-object --write-omap --write-xattr --no-cleanup
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
hints = 1
2021-10-27T06:48:53.821892+0000 Maintaining 128 concurrent writes of 16777216 bytes to objects of size 16777216 for up to 30 seconds or 0 objects
2021-10-27T06:48:53.821922+0000 Object prefix: benchmark_data_node-0.bluefield2.ucsc-cmps10_7
2021-10-27T06:48:54.441958+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-27T06:48:54.441958+0000     0       0         0         0         0         0           -           0
2021-10-27T06:48:55.442066+0000     1       5         5         0         0         0           -           0
2021-10-27T06:48:56.442148+0000     2       9         9         0         0         0           -           0
2021-10-27T06:48:57.442254+0000     3      14        14         0         0         0           -           0
2021-10-27T06:48:58.442382+0000     4      18        18         0         0         0           -           0
2021-10-27T06:48:59.442464+0000     5      23        23         0         0         0           -           0
2021-10-27T06:49:00.442545+0000     6      28        28         0         0         0           -           0
2021-10-27T06:49:01.442629+0000     7      32        32         0         0         0           -           0
2021-10-27T06:49:02.442710+0000     8      35        35         0         0         0           -           0
2021-10-27T06:49:03.442793+0000     9      37        37         0         0         0           -           0
2021-10-27T06:49:04.443022+0000    10      39        39         0         0         0           -           0
2021-10-27T06:49:05.443104+0000    11      41        41         0         0         0           -           0
2021-10-27T06:49:06.443312+0000    12      43        43         0         0         0           -           0
2021-10-27T06:49:07.443456+0000    13      47        47         0         0         0           -           0
2021-10-27T06:49:08.443594+0000    14      51        51         0         0         0           -           0
2021-10-27T06:49:09.443677+0000    15      56        56         0         0         0           -           0
2021-10-27T06:49:10.443760+0000    16      61        61         0         0         0           -           0
2021-10-27T06:49:11.443921+0000    17      65        65         0         0         0           -           0
2021-10-27T06:49:12.444005+0000    18      68        68         0         0         0           -           0
2021-10-27T06:49:13.444089+0000    19      69        69         0         0         0           -           0
2021-10-27T06:49:14.444172+0000 min lat: 9999 max lat: 0 avg lat: 0
2021-10-27T06:49:14.444172+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-27T06:49:14.444172+0000    20      70        70         0         0         0           -           0
2021-10-27T06:49:15.444267+0000    21      71        71         0         0         0           -           0
2021-10-27T06:49:16.444350+0000    22      72        72         0         0         0           -           0
2021-10-27T06:49:17.444433+0000    23      73        73         0         0         0           -           0
2021-10-27T06:49:18.444690+0000    24      76        76         0         0         0           -           0
2021-10-27T06:49:19.444949+0000    25      78        78         0         0         0           -           0
2021-10-27T06:49:20.445181+0000    26      81        81         0         0         0           -           0
2021-10-27T06:49:21.445410+0000    27      83        83         0         0         0           -           0
2021-10-27T06:49:22.445495+0000    28      86        86         0         0         0           -           0
2021-10-27T06:49:23.445578+0000    29      89        89         0         0         0           -           0
2021-10-27T06:49:24.445762+0000    30      92        92         0         0         0           -           0
2021-10-27T06:49:25.445847+0000    31      95        95         0         0         0           -           0
2021-10-27T06:49:26.446061+0000    32      98        98         0         0         0           -           0
2021-10-27T06:49:27.446144+0000    33      99        99         0         0         0           -           0
2021-10-27T06:49:28.446226+0000    34     100       100         0         0         0           -           0
2021-10-27T06:49:29.446311+0000    35     101       101         0         0         0           -           0
2021-10-27T06:49:30.446395+0000    36     103       103         0         0         0           -           0
2021-10-27T06:49:31.446487+0000    37     106       106         0         0         0           -           0
2021-10-27T06:49:32.446570+0000    38     109       109         0         0         0           -           0
2021-10-27T06:49:33.446655+0000    39     113       113         0         0         0           -           0
2021-10-27T06:49:34.446880+0000 min lat: 9999 max lat: 0 avg lat: 0
2021-10-27T06:49:34.446880+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-27T06:49:34.446880+0000    40     118       118         0         0         0           -           0
2021-10-27T06:49:35.446973+0000    41     123       123         0         0         0           -           0
2021-10-27T06:49:36.447062+0000    42     127       127         0         0         0           -           0
2021-10-27T06:49:37.447198+0000 Total time run:         42.4801
Total writes made:      128
Write size:             16777216
Object size:            16777216
Bandwidth (MB/sec):     48.2108
Stddev Bandwidth:       0
Max bandwidth (MB/sec): 0
Min bandwidth (MB/sec): 0
Average IOPS:           3
Stddev IOPS:            0
Max IOPS:               0
Min IOPS:               0
Average Latency(s):     22.6091
Stddev Latency(s):      13.4286
Max latency(s):         42.0675
Min latency(s):         0.638464

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:49:38,028227415-07:00] INFO: > Stop system activity collection services[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:49:38,034666769-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:379 - rados_bench() > kill -INT 907634

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:49:38,041500033-07:00] INFO: >> for OSD host[0m
# ./benchmarks/bench-rados:384 - rados_bench() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.5 kill -INT 134435
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:49:38,050007995-07:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.5 kill -INT 134435
[1] 23:49:39 [SUCCESS] ljishen@10.10.2.5

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:49:39,666331661-07:00] INFO: >>> Copy system activity file from OSD host[0m
## ./benchmarks/bench-rados:391 - rados_bench() > basename -- /tmp/bench-rados/sys_activity_16MB_write.dat.osd_host.2
# ./benchmarks/bench-rados:391 - rados_bench() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.5:/tmp/bench-rados/sys_activity_16MB_write.dat.osd_host.2 /mnt/sda8/ceph-research/scripts/output/sys_activity_16MB_write.dat.osd_host.2


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:49:41,249482965-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:50:04,756400146-07:00] INFO: >>   data:
    pools:   2 pools, 256 pgs
    objects: 129 objects, 2.0 GiB
    usage:   6.1 GiB used, 94 GiB / 100 GiB avail
    pgs:     256 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:50:04,765148179-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:50:12,996987172-07:00] INFO: >>   data:
    pools:   2 pools, 256 pgs
    objects: 129 objects, 2.0 GiB
    usage:   6.0 GiB used, 94 GiB / 100 GiB avail
    pgs:     256 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:50:13,005455299-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:50:21,372833166-07:00] INFO: >>   data:
    pools:   2 pools, 256 pgs
    objects: 129 objects, 2.0 GiB
    usage:   6.0 GiB used, 94 GiB / 100 GiB avail
    pgs:     256 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:50:21,381337331-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:50:29,677652657-07:00] INFO: >>   data:
    pools:   2 pools, 256 pgs
    objects: 129 objects, 2.0 GiB
    usage:   6.0 GiB used, 94 GiB / 100 GiB avail
    pgs:     256 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:50:29,687268912-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:50:37,976407289-07:00] INFO: >>   data:
    pools:   2 pools, 256 pgs
    objects: 129 objects, 2.0 GiB
    usage:   6.0 GiB used, 94 GiB / 100 GiB avail
    pgs:     256 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:50:37,985588636-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:50:37,992718818-07:00] INFO: > The cluster is idle now.[0m


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:50:37,996592941-07:00] STAGE: Start benchmarking of mode seq ...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:50:38,004537954-07:00] INFO: > Start system activity collection services[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:50:38,011899121-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:329 - rados_bench() > rm --force /mnt/sda8/ceph-research/scripts/output/sys_activity_16MB_seq.dat.rados_bench_host.2
# ./benchmarks/bench-rados:330 - rados_bench() > sar_pid_rados_bench_host=909540
# ./benchmarks/bench-rados:330 - rados_bench() > S_TIME_FORMAT=ISO
# ./benchmarks/bench-rados:330 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/output/sys_activity_16MB_seq.dat.rados_bench_host.2 2

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:50:38,019796425-07:00] INFO: >> for OSD host[0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:50:38,028154024-07:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.5 bash
10.10.2.5: b'134968\n'
[1] 23:50:39 [SUCCESS] ljishen@10.10.2.5
134968

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:50:39,319117683-07:00] INFO: > Run rados bench[0m
# ./benchmarks/bench-rados:367 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/output/rados_bench_16MB_seq.log.2
## ./benchmarks/bench-rados:364 - rados_bench() > count_cpus 32-63,96-127
# ./benchmarks/bench-rados:364 - rados_bench() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 128 --show-time
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:50:39,341269951-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:50:39,344434450-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '4e312aa4-36f1-11ec-b561-09a495c6360a', '--', 'rados', 'bench', '99999999', 'seq', '--pool', 'bench_rados', '--concurrent-ios', '128', '--show-time'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 4e312aa4-36f1-11ec-b561-09a495c6360a -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 128 --show-time
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
hints = 1
2021-10-27T06:50:41.848504+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-27T06:50:41.848504+0000     0       0         0         0         0         0           -           0
2021-10-27T06:50:42.848635+0000     1      59        59         0         0         0           -           0
2021-10-27T06:50:43.848708+0000     2     111       111         0         0         0           -           0
2021-10-27T06:50:44.849011+0000 Total time run:       2.53556
Total reads made:     128
Read size:            16777216
Object size:          16777216
Bandwidth (MB/sec):   807.711
Average IOPS:         50
Stddev IOPS:          0
Max IOPS:             0
Min IOPS:             0
Average Latency(s):   1.32986
Max latency(s):       2.30383
Min latency(s):       0.256603

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:50:45,572534900-07:00] INFO: > Stop system activity collection services[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:50:45,580058722-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:379 - rados_bench() > kill -INT 909540

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:50:45,586371377-07:00] INFO: >> for OSD host[0m
# ./benchmarks/bench-rados:384 - rados_bench() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.5 kill -INT 134968
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:50:45,594865493-07:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.5 kill -INT 134968
[1] 23:50:46 [SUCCESS] ljishen@10.10.2.5

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:50:46,699800456-07:00] INFO: >>> Copy system activity file from OSD host[0m
## ./benchmarks/bench-rados:391 - rados_bench() > basename -- /tmp/bench-rados/sys_activity_16MB_seq.dat.osd_host.2
# ./benchmarks/bench-rados:391 - rados_bench() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.5:/tmp/bench-rados/sys_activity_16MB_seq.dat.osd_host.2 /mnt/sda8/ceph-research/scripts/output/sys_activity_16MB_seq.dat.osd_host.2


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:50:47,793923277-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:51:11,048762171-07:00] INFO: >>   data:
    pools:   2 pools, 256 pgs
    objects: 129 objects, 2.0 GiB
    usage:   6.0 GiB used, 94 GiB / 100 GiB avail
    pgs:     256 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:51:11,056788087-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:51:19,373483665-07:00] INFO: >>   data:
    pools:   2 pools, 256 pgs
    objects: 129 objects, 2.0 GiB
    usage:   6.0 GiB used, 94 GiB / 100 GiB avail
    pgs:     256 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:51:19,382344119-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:51:27,644702494-07:00] INFO: >>   data:
    pools:   2 pools, 256 pgs
    objects: 129 objects, 2.0 GiB
    usage:   6.0 GiB used, 94 GiB / 100 GiB avail
    pgs:     256 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:51:27,653519937-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:51:36,009424296-07:00] INFO: >>   data:
    pools:   2 pools, 256 pgs
    objects: 129 objects, 2.0 GiB
    usage:   6.0 GiB used, 94 GiB / 100 GiB avail
    pgs:     256 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:51:36,017921317-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:51:44,257779962-07:00] INFO: >>   data:
    pools:   2 pools, 256 pgs
    objects: 129 objects, 2.0 GiB
    usage:   6.0 GiB used, 94 GiB / 100 GiB avail
    pgs:     256 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:51:44,266473824-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:51:44,274368012-07:00] INFO: > The cluster is idle now.[0m


[1;7;39;49m[2021-10-26T23:51:44,277484600-07:00][RUNNING][ROUND 3/7/21] object_size=16MB[0m


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:51:44,281767560-07:00] STAGE: Launch a Ceph cluster (CEPH_HOST: 10.10.2.3, OSD_HOST: 10.10.2.5) ...[0m
# ./benchmarks/bench-rados:172 - launch_ceph_cluster() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.3 bash -euo pipefail
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:51:44,290515543-07:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.3 bash -euo pipefail
10.10.2.3: b'\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T02:51:44,726926718-04:00] INFO: > Remove the existing cluster\x1b[0m\n'
10.10.2.3: b'## bash:13 -  > ls /var/lib/ceph\n## bash:13 -  > tail -n1\n'
10.10.2.3: b'# bash:13 -  > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm --host 10.10.2.3 --host 10.10.2.5 rm-cluster --force --fsid 4e312aa4-36f1-11ec-b561-09a495c6360a\n'
10.10.2.3: b'\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T02:51:56,053668288-04:00] INFO: > Deploy a new cluster\x1b[0m\n'
10.10.2.3: b'## bash:22 -  > /tmp/bench-rados/ceph-research/scripts/parallel-ssh --host 10.10.2.5 uname --nodename\n## bash:22 -  > tail -n1\n'
10.10.2.3: b'# bash:22 -  > osd_hostname=sm1\n# bash:23 -  > /tmp/bench-rados/ceph-research/scripts/ceph-deploy -m 10.10.2.3 -o sm1/10.10.2.5:/dev/nvme0n1\n'
10.10.2.3: b'\n\n\x1b[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T02:51:57,181045606-04:00] STAGE: Verify passwordless SSH to hosts, passwordless sudo via ljishen, and the existence of devices on hosts...\x1b[0m\n'
10.10.2.3: b'\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T02:51:57,185264866-04:00] INFO: > Check the host for the monitor\x1b[0m\n# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:181 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@10.10.2.3 sudo --non-interactive --validate\n'
10.10.2.3: b'\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T02:51:57,336860111-04:00] INFO: > Check hosts for OSDs\x1b[0m\n'
10.10.2.3: b"\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T02:51:57,341058111-04:00] INFO: >> Check host 'sm1'\x1b[0m\n# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:200 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 sudo --non-interactive --validate\n"
10.10.2.3: b"\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T02:51:58,408370547-04:00] INFO: >>> Check device '/dev/nvme0n1'\x1b[0m\n"
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:215 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 test -b /dev/nvme0n1\n'
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:226 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 lsblk --nodeps --noheadings --output TYPE /dev/nvme0n1\n'
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:227 - verify_parameters() > grep --word-regexp disk\n'
10.10.2.3: b'\n\n\x1b[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T02:52:00,515040862-04:00] STAGE: Reset devices on hosts...\x1b[0m\n'
10.10.2.3: b"\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T02:52:00,519149755-04:00] INFO: > Resetting device '/dev/nvme0n1' on host 'sm1'\x1b[0m\n"
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:288 - reset_host_devices() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 sudo sh\n'
10.10.2.3: b'  Removing ceph--d1a7471a--4b08--406a--9efc--39259e29af80-osd--block--95b7b89b--d0fd--4e23--ac5d--d995c37ff400 (252:0)\n'
10.10.2.3: b'  Archiving volume group "ceph-d1a7471a-4b08-406a-9efc-39259e29af80" metadata (seqno 5).\n'
10.10.2.3: b'  Releasing logical volume "osd-block-95b7b89b-d0fd-4e23-ac5d-d995c37ff400"\n'
10.10.2.3: b'  Creating volume group backup "/etc/lvm/backup/ceph-d1a7471a-4b08-406a-9efc-39259e29af80" (seqno 6).\n'
10.10.2.3: b'  Logical volume "osd-block-95b7b89b-d0fd-4e23-ac5d-d995c37ff400" successfully removed\n'
10.10.2.3: b'  Removing physical volume "/dev/nvme0n1" from volume group "ceph-d1a7471a-4b08-406a-9efc-39259e29af80"\n'
10.10.2.3: b'  Volume group "ceph-d1a7471a-4b08-406a-9efc-39259e29af80" successfully removed\n'
10.10.2.3: b'\n\n\x1b[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T02:52:02,904120770-04:00] STAGE: Bootstrap a new cluster...\x1b[0m\n# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:312 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.3 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults\n'
10.10.2.3: b'\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T02:52:02,912482987-04:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.3: b"\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T02:52:02,915209875-04:00] DEBUG: CEPHADM_PARAMS: ['bootstrap', '--config', '/tmp/bench-rados/ceph-research/scripts/ceph.conf', '--mon-ip', '10.10.2.3', '--output-dir', '/tmp/bench-rados/deployment_data_root/etc/ceph', '--ssh-user', 'ljishen', '--skip-dashboard', '--no-minimize-config', '--allow-overwrite', '--allow-fqdn-hostname', '--orphan-initial-daemons', '--skip-monitoring-stack', '--single-host-defaults']\x1b[0m\n"
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.3 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults\n'
10.10.2.3: b'Verifying podman|docker is present...\nVerifying lvm2 is present...\nVerifying time synchronization is in place...\n'
10.10.2.3: b'Unit ntp.service is enabled and running\nRepeating the final host check...\npodman|docker (/usr/bin/docker) is present\n'
10.10.2.3: b'systemctl is present\n'
10.10.2.3: b'lvcreate is present\n'
10.10.2.3: b'Unit ntp.service is enabled and running\nHost looks OK\n'
10.10.2.3: b'Cluster fsid: 6420b284-36f2-11ec-b561-09a495c6360a\n'
10.10.2.3: b'Verifying IP 10.10.2.3 port 3300 ...\nVerifying IP 10.10.2.3 port 6789 ...\n'
10.10.2.3: b'Mon IP `10.10.2.3` is in CIDR network `10.10.2.0/24`\n- internal network (--cluster-network) has not been provided, OSD replication will default to the public_network\n'
10.10.2.3: b'Adjusting default settings to suit single-host cluster...\nPulling container image quay.io/ceph/ceph:v16...\n'
10.10.2.3: b'Ceph version: ceph version 16.2.6 (ee28fb57e47e9f88813e24bbf4c14496ca299d31) pacific (stable)\nExtracting ceph user uid/gid from container image...\n'
10.10.2.3: b'Creating initial keys...\n'
10.10.2.3: b'Creating initial monmap...\n'
10.10.2.3: b'Creating mon...\n'
10.10.2.3: b'Waiting for mon to start...\nWaiting for mon...\n'
10.10.2.3: b'mon is available\nSetting mon public_network to 10.10.2.0/24\n'
10.10.2.3: b'Wrote config to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf\nWrote keyring to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring\n'
10.10.2.3: b'Creating mgr...\n'
10.10.2.3: b'Verifying port 9283 ...\n'
10.10.2.3: b'Waiting for mgr to start...\nWaiting for mgr...\n'
10.10.2.3: b'mgr not available, waiting (1/15)...\n'
10.10.2.3: b'mgr not available, waiting (2/15)...\n'
10.10.2.3: b'mgr is available\n'
10.10.2.3: b'Enabling cephadm module...\n'
10.10.2.3: b'Waiting for the mgr to restart...\nWaiting for mgr epoch 5...\n'
10.10.2.3: b'mgr epoch 5 is available\nSetting orchestrator backend to cephadm...\n'
10.10.2.3: b'Generating ssh key...\n'
10.10.2.3: b'Wrote public SSH key to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub\nAdding key to ljishen@localhost authorized_keys...\n'
10.10.2.3: b'Adding host node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us...\n'
10.10.2.3: b'Deploying unmanaged mon service...\n'
10.10.2.3: b'Deploying unmanaged mgr service...\n'
10.10.2.3: b'You can access the Ceph CLI with:\n\n\tsudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --fsid 6420b284-36f2-11ec-b561-09a495c6360a -c /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf -k /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring\n\nPlease consider enabling telemetry to help improve Ceph:\n\n\tceph telemetry on\n\nFor more information see:\n\n\thttps://docs.ceph.com/docs/pacific/mgr/telemetry/\n\nBootstrap complete.\n'
10.10.2.3: b'\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T02:53:07,240612792-04:00] INFO: > Set the default number of monitor daemons to 1\x1b[0m\n# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:330 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1\n'
10.10.2.3: b'\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T02:53:07,249561624-04:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.3: b"\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T02:53:07,252638201-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mon', '1']\x1b[0m\n# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1\n"
10.10.2.3: b'Inferring fsid 6420b284-36f2-11ec-b561-09a495c6360a\n'
10.10.2.3: b'Inferring config /var/lib/ceph/6420b284-36f2-11ec-b561-09a495c6360a/mon.node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.3: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.3: b'Scheduled mon update...\n'
10.10.2.3: b'\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T02:53:16,678752510-04:00] INFO: > Set the default number of manager daemons to 1\x1b[0m\n# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:335 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1\n'
10.10.2.3: b'\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T02:53:16,687325344-04:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.3: b"\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T02:53:16,690603902-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mgr', '1']\x1b[0m\n"
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1\n'
10.10.2.3: b'Inferring fsid 6420b284-36f2-11ec-b561-09a495c6360a\n'
10.10.2.3: b'Inferring config /var/lib/ceph/6420b284-36f2-11ec-b561-09a495c6360a/mon.node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.3: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.3: b'Scheduled mgr update...\n'
10.10.2.3: b'\n\n\x1b[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T02:53:26,346109796-04:00] STAGE: Add hosts to the cluster...\x1b[0m\n'
10.10.2.3: b"\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T02:53:26,351171354-04:00] INFO: > Adding host 'sm1'\x1b[0m\n# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:362 - add_hosts() > ssh-copy-id -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -f -i /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub ljishen@sm1\n"
10.10.2.3: b'/usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: "/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub"\n'
10.10.2.3: b'\nNumber of key(s) added: 1\n\nNow try logging into the machine, with:   "ssh -o \'GlobalKnownHostsFile=/dev/null\' -o \'LogLevel=ERROR\' -o \'PasswordAuthentication=no\' -o \'StrictHostKeyChecking=no\' -o \'UserKnownHostsFile=/dev/null\' \'ljishen@sm1\'"\nand check to make sure that only the key(s) you wanted were added.\n\n'
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:368 - add_hosts() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add sm1 10.10.2.5\n'
10.10.2.3: b'\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T02:53:27,505905642-04:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.3: b"\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T02:53:27,509051620-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'host', 'add', 'sm1', '10.10.2.5']\x1b[0m\n"
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add sm1 10.10.2.5\n'
10.10.2.3: b'Inferring fsid 6420b284-36f2-11ec-b561-09a495c6360a\n'
10.10.2.3: b'Inferring config /var/lib/ceph/6420b284-36f2-11ec-b561-09a495c6360a/mon.node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.3: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.3: b"Added host 'sm1' with addr '10.10.2.5'\n"
10.10.2.3: b'\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T02:53:38,527577750-04:00] INFO: Sleep for 20 seconds\x1b[0m\n'
10.10.2.3: b'\n\n\x1b[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T02:53:58,531528394-04:00] STAGE: Add OSDs to the cluster...\x1b[0m\n'
10.10.2.3: b"\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T02:53:58,537231119-04:00] INFO: > Adding devices ['/dev/nvme0n1'] on host 'sm1' as OSDs\x1b[0m\n# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:387 - add_host_devices_as_osds() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd sm1:/dev/nvme0n1\n"
10.10.2.3: b'\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T02:53:58,545838088-04:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.3: b"\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T02:53:58,548513830-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'daemon', 'add', 'osd', 'sm1:/dev/nvme0n1']\x1b[0m\n"
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd sm1:/dev/nvme0n1\n'
10.10.2.3: b'Inferring fsid 6420b284-36f2-11ec-b561-09a495c6360a\n'
10.10.2.3: b'Inferring config /var/lib/ceph/6420b284-36f2-11ec-b561-09a495c6360a/mon.node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.3: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.3: b"Created osd(s) 0 on host 'sm1'\n"
10.10.2.3: b'\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T02:54:22,616580136-04:00] INFO: > Sleep for 20 seconds\x1b[0m\n'
10.10.2.3: b'\n\n\x1b[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T02:54:42,621477373-04:00] STAGE: Show Cluster Status\x1b[0m\n# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:343 - show_status() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status\n'
10.10.2.3: b'\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T02:54:42,630850656-04:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.3: b"\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-27T02:54:42,633592713-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', '--status']\x1b[0m\n"
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status\n'
10.10.2.3: b'Inferring fsid 6420b284-36f2-11ec-b561-09a495c6360a\n'
10.10.2.3: b'Inferring config /var/lib/ceph/6420b284-36f2-11ec-b561-09a495c6360a/mon.node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.3: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.3: b'  cluster:\n    id:     6420b284-36f2-11ec-b561-09a495c6360a\n    health: HEALTH_WARN\n            1 pool(s) have no replicas configured\n \n  services:\n    mon: 1 daemons, quorum node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us (age 2m)\n    mgr: node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us.wmfabw(active, since 2m)\n    osd: 1 osds: 1 up (since 20s), 1 in (since 40s)\n \n  data:\n    pools:   1 pools, 128 pgs\n    objects: 0 objects, 0 B\n    usage:   4.7 MiB used, 100 GiB / 100 GiB avail\n    pgs:     99.219% pgs unknown\n             127 unknown\n             1   active+clean\n \n  progress:\n \n'
[1] 23:54:51 [SUCCESS] ljishen@10.10.2.3

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T02:51:44,726926718-04:00] INFO: > Remove the existing cluster[0m
## bash:13 -  > ls /var/lib/ceph
## bash:13 -  > tail -n1
# bash:13 -  > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm --host 10.10.2.3 --host 10.10.2.5 rm-cluster --force --fsid 4e312aa4-36f1-11ec-b561-09a495c6360a

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T02:51:56,053668288-04:00] INFO: > Deploy a new cluster[0m
## bash:22 -  > /tmp/bench-rados/ceph-research/scripts/parallel-ssh --host 10.10.2.5 uname --nodename
## bash:22 -  > tail -n1
# bash:22 -  > osd_hostname=sm1
# bash:23 -  > /tmp/bench-rados/ceph-research/scripts/ceph-deploy -m 10.10.2.3 -o sm1/10.10.2.5:/dev/nvme0n1


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T02:51:57,181045606-04:00] STAGE: Verify passwordless SSH to hosts, passwordless sudo via ljishen, and the existence of devices on hosts...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T02:51:57,185264866-04:00] INFO: > Check the host for the monitor[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:181 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@10.10.2.3 sudo --non-interactive --validate

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T02:51:57,336860111-04:00] INFO: > Check hosts for OSDs[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T02:51:57,341058111-04:00] INFO: >> Check host 'sm1'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:200 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 sudo --non-interactive --validate

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T02:51:58,408370547-04:00] INFO: >>> Check device '/dev/nvme0n1'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:215 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 test -b /dev/nvme0n1
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:226 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 lsblk --nodeps --noheadings --output TYPE /dev/nvme0n1
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:227 - verify_parameters() > grep --word-regexp disk


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T02:52:00,515040862-04:00] STAGE: Reset devices on hosts...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T02:52:00,519149755-04:00] INFO: > Resetting device '/dev/nvme0n1' on host 'sm1'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:288 - reset_host_devices() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 sudo sh
  Removing ceph--d1a7471a--4b08--406a--9efc--39259e29af80-osd--block--95b7b89b--d0fd--4e23--ac5d--d995c37ff400 (252:0)
  Archiving volume group "ceph-d1a7471a-4b08-406a-9efc-39259e29af80" metadata (seqno 5).
  Releasing logical volume "osd-block-95b7b89b-d0fd-4e23-ac5d-d995c37ff400"
  Creating volume group backup "/etc/lvm/backup/ceph-d1a7471a-4b08-406a-9efc-39259e29af80" (seqno 6).
  Logical volume "osd-block-95b7b89b-d0fd-4e23-ac5d-d995c37ff400" successfully removed
  Removing physical volume "/dev/nvme0n1" from volume group "ceph-d1a7471a-4b08-406a-9efc-39259e29af80"
  Volume group "ceph-d1a7471a-4b08-406a-9efc-39259e29af80" successfully removed


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T02:52:02,904120770-04:00] STAGE: Bootstrap a new cluster...[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:312 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.3 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T02:52:02,912482987-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T02:52:02,915209875-04:00] DEBUG: CEPHADM_PARAMS: ['bootstrap', '--config', '/tmp/bench-rados/ceph-research/scripts/ceph.conf', '--mon-ip', '10.10.2.3', '--output-dir', '/tmp/bench-rados/deployment_data_root/etc/ceph', '--ssh-user', 'ljishen', '--skip-dashboard', '--no-minimize-config', '--allow-overwrite', '--allow-fqdn-hostname', '--orphan-initial-daemons', '--skip-monitoring-stack', '--single-host-defaults'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.3 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults
Verifying podman|docker is present...
Verifying lvm2 is present...
Verifying time synchronization is in place...
Unit ntp.service is enabled and running
Repeating the final host check...
podman|docker (/usr/bin/docker) is present
systemctl is present
lvcreate is present
Unit ntp.service is enabled and running
Host looks OK
Cluster fsid: 6420b284-36f2-11ec-b561-09a495c6360a
Verifying IP 10.10.2.3 port 3300 ...
Verifying IP 10.10.2.3 port 6789 ...
Mon IP `10.10.2.3` is in CIDR network `10.10.2.0/24`
- internal network (--cluster-network) has not been provided, OSD replication will default to the public_network
Adjusting default settings to suit single-host cluster...
Pulling container image quay.io/ceph/ceph:v16...
Ceph version: ceph version 16.2.6 (ee28fb57e47e9f88813e24bbf4c14496ca299d31) pacific (stable)
Extracting ceph user uid/gid from container image...
Creating initial keys...
Creating initial monmap...
Creating mon...
Waiting for mon to start...
Waiting for mon...
mon is available
Setting mon public_network to 10.10.2.0/24
Wrote config to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf
Wrote keyring to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring
Creating mgr...
Verifying port 9283 ...
Waiting for mgr to start...
Waiting for mgr...
mgr not available, waiting (1/15)...
mgr not available, waiting (2/15)...
mgr is available
Enabling cephadm module...
Waiting for the mgr to restart...
Waiting for mgr epoch 5...
mgr epoch 5 is available
Setting orchestrator backend to cephadm...
Generating ssh key...
Wrote public SSH key to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub
Adding key to ljishen@localhost authorized_keys...
Adding host node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us...
Deploying unmanaged mon service...
Deploying unmanaged mgr service...
You can access the Ceph CLI with:

	sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --fsid 6420b284-36f2-11ec-b561-09a495c6360a -c /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf -k /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring

Please consider enabling telemetry to help improve Ceph:

	ceph telemetry on

For more information see:

	https://docs.ceph.com/docs/pacific/mgr/telemetry/

Bootstrap complete.

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T02:53:07,240612792-04:00] INFO: > Set the default number of monitor daemons to 1[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:330 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T02:53:07,249561624-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T02:53:07,252638201-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mon', '1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1
Inferring fsid 6420b284-36f2-11ec-b561-09a495c6360a
Inferring config /var/lib/ceph/6420b284-36f2-11ec-b561-09a495c6360a/mon.node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Scheduled mon update...

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T02:53:16,678752510-04:00] INFO: > Set the default number of manager daemons to 1[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:335 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T02:53:16,687325344-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T02:53:16,690603902-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mgr', '1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1
Inferring fsid 6420b284-36f2-11ec-b561-09a495c6360a
Inferring config /var/lib/ceph/6420b284-36f2-11ec-b561-09a495c6360a/mon.node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Scheduled mgr update...


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T02:53:26,346109796-04:00] STAGE: Add hosts to the cluster...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T02:53:26,351171354-04:00] INFO: > Adding host 'sm1'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:362 - add_hosts() > ssh-copy-id -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -f -i /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub ljishen@sm1
/usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: "/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub"

Number of key(s) added: 1

Now try logging into the machine, with:   "ssh -o 'GlobalKnownHostsFile=/dev/null' -o 'LogLevel=ERROR' -o 'PasswordAuthentication=no' -o 'StrictHostKeyChecking=no' -o 'UserKnownHostsFile=/dev/null' 'ljishen@sm1'"
and check to make sure that only the key(s) you wanted were added.

# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:368 - add_hosts() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add sm1 10.10.2.5
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T02:53:27,505905642-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T02:53:27,509051620-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'host', 'add', 'sm1', '10.10.2.5'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add sm1 10.10.2.5
Inferring fsid 6420b284-36f2-11ec-b561-09a495c6360a
Inferring config /var/lib/ceph/6420b284-36f2-11ec-b561-09a495c6360a/mon.node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Added host 'sm1' with addr '10.10.2.5'

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T02:53:38,527577750-04:00] INFO: Sleep for 20 seconds[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T02:53:58,531528394-04:00] STAGE: Add OSDs to the cluster...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T02:53:58,537231119-04:00] INFO: > Adding devices ['/dev/nvme0n1'] on host 'sm1' as OSDs[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:387 - add_host_devices_as_osds() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd sm1:/dev/nvme0n1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T02:53:58,545838088-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T02:53:58,548513830-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'daemon', 'add', 'osd', 'sm1:/dev/nvme0n1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd sm1:/dev/nvme0n1
Inferring fsid 6420b284-36f2-11ec-b561-09a495c6360a
Inferring config /var/lib/ceph/6420b284-36f2-11ec-b561-09a495c6360a/mon.node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Created osd(s) 0 on host 'sm1'

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T02:54:22,616580136-04:00] INFO: > Sleep for 20 seconds[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T02:54:42,621477373-04:00] STAGE: Show Cluster Status[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:343 - show_status() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T02:54:42,630850656-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-27T02:54:42,633592713-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', '--status'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status
Inferring fsid 6420b284-36f2-11ec-b561-09a495c6360a
Inferring config /var/lib/ceph/6420b284-36f2-11ec-b561-09a495c6360a/mon.node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
  cluster:
    id:     6420b284-36f2-11ec-b561-09a495c6360a
    health: HEALTH_WARN
            1 pool(s) have no replicas configured
 
  services:
    mon: 1 daemons, quorum node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us (age 2m)
    mgr: node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us.wmfabw(active, since 2m)
    osd: 1 osds: 1 up (since 20s), 1 in (since 40s)
 
  data:
    pools:   1 pools, 128 pgs
    objects: 0 objects, 0 B
    usage:   4.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     99.219% pgs unknown
             127 unknown
             1   active+clean
 
  progress:
 

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:54:51,854380871-07:00] INFO: > Copy configuration files to local[0m
# ./benchmarks/bench-rados:208 - launch_ceph_cluster() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.3:/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf
# ./benchmarks/bench-rados:212 - launch_ceph_cluster() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.3 sudo chmod 644 /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:54:52,011576942-07:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.3 sudo chmod 644 /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring
[1] 23:54:52 [SUCCESS] ljishen@10.10.2.3
# ./benchmarks/bench-rados:214 - launch_ceph_cluster() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.3:/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:54:52,338097061-07:00] STAGE: Update local ceph.conf as a client...[0m


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:54:52,342271538-07:00] STAGE: Configure Ceph pool...[0m
# ./benchmarks/bench-rados:245 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_max_backfills 32
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:54:52,366083926-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:54:52,369556794-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '6420b284-36f2-11ec-b561-09a495c6360a', '--', 'ceph', 'config', 'set', 'osd', 'osd_max_backfills', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 6420b284-36f2-11ec-b561-09a495c6360a -- ceph config set osd osd_max_backfills 32
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:246 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_active 32
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:54:55,604846203-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:54:55,608032612-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '6420b284-36f2-11ec-b561-09a495c6360a', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_active', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 6420b284-36f2-11ec-b561-09a495c6360a -- ceph config set osd osd_recovery_max_active 32
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:247 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_single_start 8
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:54:59,211218864-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:54:59,214650013-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '6420b284-36f2-11ec-b561-09a495c6360a', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_single_start', '8'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 6420b284-36f2-11ec-b561-09a495c6360a -- ceph config set osd osd_recovery_max_single_start 8
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:248 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_op_priority 63
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:55:02,633739310-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:55:02,637234750-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '6420b284-36f2-11ec-b561-09a495c6360a', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_op_priority', '63'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 6420b284-36f2-11ec-b561-09a495c6360a -- ceph config set osd osd_recovery_op_priority 63
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:250 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config show-with-defaults osd.0
# ./benchmarks/bench-rados:251 - configure_ceph_pool() > grep 'osd_max_backfills\|osd_recovery'
# ./benchmarks/bench-rados:251 - configure_ceph_pool() > column -t -s ' '
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
osd_max_backfills                        32         mon
osd_recovery_cost                        20971520   default
osd_recovery_delay_start                 0.000000   default
osd_recovery_max_active                  32         mon
osd_recovery_max_active_hdd              3          default
osd_recovery_max_active_ssd              10         default
osd_recovery_max_chunk                   8388608    default
osd_recovery_max_omap_entries_per_chunk  8096       default
osd_recovery_max_single_start            8          mon
osd_recovery_op_priority                 63         mon
osd_recovery_op_warn_multiple            16         default
osd_recovery_priority                    5          default
osd_recovery_retry_interval              30.000000  default
osd_recovery_sleep                       0.000000   default
osd_recovery_sleep_hdd                   0.100000   default
osd_recovery_sleep_hybrid                0.025000   default
osd_recovery_sleep_ssd                   0.000000   default
# ./benchmarks/bench-rados:253 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool create bench_rados 128 128 replicated --size 1 --pg-num-min 128
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:55:08,841800721-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:55:08,845372906-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '6420b284-36f2-11ec-b561-09a495c6360a', '--', 'ceph', 'osd', 'pool', 'create', 'bench_rados', '128', '128', 'replicated', '--size', '1', '--pg-num-min', '128'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 6420b284-36f2-11ec-b561-09a495c6360a -- ceph osd pool create bench_rados 128 128 replicated --size 1 --pg-num-min 128
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
pool 'bench_rados' created
# ./benchmarks/bench-rados:255 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados min_size 1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:55:13,256410946-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:55:13,259668630-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '6420b284-36f2-11ec-b561-09a495c6360a', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'min_size', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 6420b284-36f2-11ec-b561-09a495c6360a -- ceph osd pool set bench_rados min_size 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 min_size to 1
# ./benchmarks/bench-rados:256 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados pg_autoscale_mode off
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:55:17,439742692-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:55:17,443240567-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '6420b284-36f2-11ec-b561-09a495c6360a', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'pg_autoscale_mode', 'off'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 6420b284-36f2-11ec-b561-09a495c6360a -- ceph osd pool set bench_rados pg_autoscale_mode off
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 pg_autoscale_mode to off
# ./benchmarks/bench-rados:257 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool application enable bench_rados benchmark
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:55:21,754002615-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:55:21,757202400-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '6420b284-36f2-11ec-b561-09a495c6360a', '--', 'ceph', 'osd', 'pool', 'application', 'enable', 'bench_rados', 'benchmark'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 6420b284-36f2-11ec-b561-09a495c6360a -- ceph osd pool application enable bench_rados benchmark
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
enabled application 'benchmark' on pool 'bench_rados'
# ./benchmarks/bench-rados:258 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados noscrub 1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:55:25,036261391-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:55:25,039736804-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '6420b284-36f2-11ec-b561-09a495c6360a', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'noscrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 6420b284-36f2-11ec-b561-09a495c6360a -- ceph osd pool set bench_rados noscrub 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 noscrub to 1
# ./benchmarks/bench-rados:259 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados nodeep-scrub 1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:55:28,445010563-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:55:28,448294275-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '6420b284-36f2-11ec-b561-09a495c6360a', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'nodeep-scrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 6420b284-36f2-11ec-b561-09a495c6360a -- ceph osd pool set bench_rados nodeep-scrub 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 nodeep-scrub to 1
# ./benchmarks/bench-rados:260 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool ls detail
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:55:32,303113426-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:55:32,306396518-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '6420b284-36f2-11ec-b561-09a495c6360a', '--', 'ceph', 'osd', 'pool', 'ls', 'detail'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 6420b284-36f2-11ec-b561-09a495c6360a -- ceph osd pool ls detail
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
pool 1 'device_health_metrics' replicated size 1 min_size 1 crush_rule 0 object_hash rjenkins pg_num 128 pgp_num 128 autoscale_mode on last_change 13 lfor 0/0/11 flags hashpspool stripe_width 0 pg_num_min 1 application mgr_devicehealth
pool 2 'bench_rados' replicated size 1 min_size 1 crush_rule 0 object_hash rjenkins pg_num 128 pgp_num 128 autoscale_mode off last_change 21 flags hashpspool,noscrub,nodeep-scrub stripe_width 0 pg_num_min 128 application benchmark

# ./benchmarks/bench-rados:261 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd df tree
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:55:35,708131386-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:55:35,711481904-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '6420b284-36f2-11ec-b561-09a495c6360a', '--', 'ceph', 'osd', 'df', 'tree'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 6420b284-36f2-11ec-b561-09a495c6360a -- ceph osd df tree
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
ID  CLASS  WEIGHT   REWEIGHT  SIZE     RAW USE  DATA     OMAP  META     AVAIL    %USE  VAR   PGS  STATUS  TYPE NAME    
-1         0.09769         -  100 GiB  6.0 MiB  176 KiB   0 B  5.8 MiB  100 GiB  0.01  1.00    -          root default 
-3         0.09769         -  100 GiB  6.0 MiB  176 KiB   0 B  5.8 MiB  100 GiB  0.01  1.00    -              host sm1 
 0    ssd  0.09769   1.00000  100 GiB  6.0 MiB  176 KiB   0 B  5.8 MiB  100 GiB  0.01  1.00  256      up          osd.0
                       TOTAL  100 GiB  6.0 MiB  176 KiB   0 B  5.8 MiB  100 GiB  0.01                                  
MIN/MAX VAR: 1.00/1.00  STDDEV: 0


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:55:39,032427527-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:56:02,313154504-07:00] INFO: >>   data:
    pools:   2 pools, 256 pgs
    objects: 0 objects, 0 B
    usage:   6.0 MiB used, 100 GiB / 100 GiB avail
    pgs:     256 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:56:02,322735823-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:56:10,594578089-07:00] INFO: >>   data:
    pools:   2 pools, 256 pgs
    objects: 0 objects, 0 B
    usage:   6.0 MiB used, 100 GiB / 100 GiB avail
    pgs:     256 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:56:10,603690377-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:56:18,901850181-07:00] INFO: >>   data:
    pools:   2 pools, 256 pgs
    objects: 0 objects, 0 B
    usage:   6.0 MiB used, 100 GiB / 100 GiB avail
    pgs:     256 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:56:18,910499439-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:56:27,215071937-07:00] INFO: >>   data:
    pools:   2 pools, 256 pgs
    objects: 0 objects, 0 B
    usage:   6.0 MiB used, 100 GiB / 100 GiB avail
    pgs:     256 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:56:27,224395031-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:56:35,664268367-07:00] INFO: >>   data:
    pools:   2 pools, 256 pgs
    objects: 0 objects, 0 B
    usage:   6.0 MiB used, 100 GiB / 100 GiB avail
    pgs:     256 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:56:35,672891826-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:56:35,679866085-07:00] INFO: > The cluster is idle now.[0m


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:56:35,683934081-07:00] STAGE: Start benchmarking of mode write ...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:56:35,692396578-07:00] INFO: > Start system activity collection services[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:56:35,699558760-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:329 - rados_bench() > rm --force /mnt/sda8/ceph-research/scripts/output/sys_activity_16MB_write.dat.rados_bench_host.3
# ./benchmarks/bench-rados:330 - rados_bench() > sar_pid_rados_bench_host=917688
# ./benchmarks/bench-rados:330 - rados_bench() > S_TIME_FORMAT=ISO
# ./benchmarks/bench-rados:330 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/output/sys_activity_16MB_write.dat.rados_bench_host.3 2

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:56:35,707960883-07:00] INFO: >> for OSD host[0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:56:35,716464507-07:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.5 bash
10.10.2.5: b'139743\n'
[1] 23:56:36 [SUCCESS] ljishen@10.10.2.5
139743

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:56:36,831499298-07:00] INFO: > Run rados bench[0m
# ./benchmarks/bench-rados:358 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/output/rados_bench_16MB_write.log.3
## ./benchmarks/bench-rados:349 - rados_bench() > count_cpus 32-63,96-127
# ./benchmarks/bench-rados:349 - rados_bench() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 30 write --pool bench_rados -b 16777216 -O 16777216 --concurrent-ios 128 --show-time --write-object --write-omap --write-xattr --no-cleanup
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:56:36,854293073-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:56:36,857961698-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '6420b284-36f2-11ec-b561-09a495c6360a', '--', 'rados', 'bench', '30', 'write', '--pool', 'bench_rados', '-b', '16777216', '-O', '16777216', '--concurrent-ios', '128', '--show-time', '--write-object', '--write-omap', '--write-xattr', '--no-cleanup'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 6420b284-36f2-11ec-b561-09a495c6360a -- rados bench 30 write --pool bench_rados -b 16777216 -O 16777216 --concurrent-ios 128 --show-time --write-object --write-omap --write-xattr --no-cleanup
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
hints = 1
2021-10-27T06:56:39.252218+0000 Maintaining 128 concurrent writes of 16777216 bytes to objects of size 16777216 for up to 30 seconds or 0 objects
2021-10-27T06:56:39.252247+0000 Object prefix: benchmark_data_node-0.bluefield2.ucsc-cmps10_7
2021-10-27T06:56:39.864735+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-27T06:56:39.864735+0000     0       0         0         0         0         0           -           0
2021-10-27T06:56:40.865001+0000     1       4         4         0         0         0           -           0
2021-10-27T06:56:41.865097+0000     2       9         9         0         0         0           -           0
2021-10-27T06:56:42.865178+0000     3      13        13         0         0         0           -           0
2021-10-27T06:56:43.865246+0000     4      18        18         0         0         0           -           0
2021-10-27T06:56:44.865483+0000     5      23        23         0         0         0           -           0
2021-10-27T06:56:45.865577+0000     6      28        28         0         0         0           -           0
2021-10-27T06:56:46.865646+0000     7      33        33         0         0         0           -           0
2021-10-27T06:56:47.865715+0000     8      35        35         0         0         0           -           0
2021-10-27T06:56:48.865782+0000     9      37        37         0         0         0           -           0
2021-10-27T06:56:49.865877+0000    10      39        39         0         0         0           -           0
2021-10-27T06:56:50.865943+0000    11      41        41         0         0         0           -           0
2021-10-27T06:56:51.866017+0000    12      44        44         0         0         0           -           0
2021-10-27T06:56:52.866088+0000    13      47        47         0         0         0           -           0
2021-10-27T06:56:53.866326+0000    14      52        52         0         0         0           -           0
2021-10-27T06:56:54.866395+0000    15      57        57         0         0         0           -           0
2021-10-27T06:56:55.866463+0000    16      62        62         0         0         0           -           0
2021-10-27T06:56:56.866616+0000    17      66        66         0         0         0           -           0
2021-10-27T06:56:57.866716+0000    18      68        68         0         0         0           -           0
2021-10-27T06:56:58.866940+0000    19      69        69         0         0         0           -           0
2021-10-27T06:56:59.867008+0000 min lat: 9999 max lat: 0 avg lat: 0
2021-10-27T06:56:59.867008+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-27T06:56:59.867008+0000    20      70        70         0         0         0           -           0
2021-10-27T06:57:00.867231+0000    21      71        71         0         0         0           -           0
2021-10-27T06:57:01.867327+0000    22      72        72         0         0         0           -           0
2021-10-27T06:57:02.867396+0000    23      73        73         0         0         0           -           0
2021-10-27T06:57:03.867464+0000    24      75        75         0         0         0           -           0
2021-10-27T06:57:04.867608+0000    25      77        77         0         0         0           -           0
2021-10-27T06:57:05.867703+0000    26      80        80         0         0         0           -           0
2021-10-27T06:57:06.867772+0000    27      83        83         0         0         0           -           0
2021-10-27T06:57:07.867981+0000    28      86        86         0         0         0           -           0
2021-10-27T06:57:08.868049+0000    29      89        89         0         0         0           -           0
2021-10-27T06:57:09.868144+0000    30      92        92         0         0         0           -           0
2021-10-27T06:57:10.868210+0000    31      95        95         0         0         0           -           0
2021-10-27T06:57:11.868277+0000    32      98        98         0         0         0           -           0
2021-10-27T06:57:12.868344+0000    33      99        99         0         0         0           -           0
2021-10-27T06:57:13.868437+0000    34     100       100         0         0         0           -           0
2021-10-27T06:57:14.868505+0000    35     101       101         0         0         0           -           0
2021-10-27T06:57:15.868571+0000    36     104       104         0         0         0           -           0
2021-10-27T06:57:16.868706+0000    37     106       106         0         0         0           -           0
2021-10-27T06:57:17.868945+0000    38     110       110         0         0         0           -           0
2021-10-27T06:57:18.869016+0000    39     114       114         0         0         0           -           0
2021-10-27T06:57:19.869227+0000 min lat: 9999 max lat: 0 avg lat: 0
2021-10-27T06:57:19.869227+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-27T06:57:19.869227+0000    40     119       119         0         0         0           -           0
2021-10-27T06:57:20.869318+0000    41     124       124         0         0         0           -           0
2021-10-27T06:57:21.869558+0000    42       1       128       127   48.3755    48.381    0.615107     22.3826
2021-10-27T06:57:22.869685+0000 Total time run:         42.0297
Total writes made:      128
Write size:             16777216
Object size:            16777216
Bandwidth (MB/sec):     48.7274
Stddev Bandwidth:       7.46534
Max bandwidth (MB/sec): 48.381
Min bandwidth (MB/sec): 0
Average IOPS:           3
Stddev IOPS:            0.46291
Max IOPS:               3
Min IOPS:               0
Average Latency(s):     22.2122
Stddev Latency(s):      13.3792
Max latency(s):         41.6437
Min latency(s):         0.570382

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:57:23,481108264-07:00] INFO: > Stop system activity collection services[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:57:23,487873551-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:379 - rados_bench() > kill -INT 917688

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:57:23,494796223-07:00] INFO: >> for OSD host[0m
# ./benchmarks/bench-rados:384 - rados_bench() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.5 kill -INT 139743
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:57:23,503059354-07:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.5 kill -INT 139743
[1] 23:57:25 [SUCCESS] ljishen@10.10.2.5

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:57:25,027198064-07:00] INFO: >>> Copy system activity file from OSD host[0m
## ./benchmarks/bench-rados:391 - rados_bench() > basename -- /tmp/bench-rados/sys_activity_16MB_write.dat.osd_host.3
# ./benchmarks/bench-rados:391 - rados_bench() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.5:/tmp/bench-rados/sys_activity_16MB_write.dat.osd_host.3 /mnt/sda8/ceph-research/scripts/output/sys_activity_16MB_write.dat.osd_host.3


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:57:26,733452005-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:57:49,867134517-07:00] INFO: >>   data:
    pools:   2 pools, 256 pgs
    objects: 129 objects, 2.0 GiB
    usage:   6.0 GiB used, 94 GiB / 100 GiB avail
    pgs:     256 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:57:49,876868042-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:57:58,140147292-07:00] INFO: >>   data:
    pools:   2 pools, 256 pgs
    objects: 129 objects, 2.0 GiB
    usage:   6.0 GiB used, 94 GiB / 100 GiB avail
    pgs:     256 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:57:58,149331165-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:58:06,396451733-07:00] INFO: >>   data:
    pools:   2 pools, 256 pgs
    objects: 129 objects, 2.0 GiB
    usage:   6.0 GiB used, 94 GiB / 100 GiB avail
    pgs:     256 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:58:06,405320793-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:58:14,585911379-07:00] INFO: >>   data:
    pools:   2 pools, 256 pgs
    objects: 129 objects, 2.0 GiB
    usage:   6.0 GiB used, 94 GiB / 100 GiB avail
    pgs:     256 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:58:14,594916115-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:58:22,934747477-07:00] INFO: >>   data:
    pools:   2 pools, 256 pgs
    objects: 129 objects, 2.0 GiB
    usage:   6.0 GiB used, 94 GiB / 100 GiB avail
    pgs:     256 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:58:22,943442881-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:58:22,950015676-07:00] INFO: > The cluster is idle now.[0m


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:58:22,954043697-07:00] STAGE: Start benchmarking of mode seq ...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:58:22,962162798-07:00] INFO: > Start system activity collection services[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:58:22,969231755-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:329 - rados_bench() > rm --force /mnt/sda8/ceph-research/scripts/output/sys_activity_16MB_seq.dat.rados_bench_host.3
# ./benchmarks/bench-rados:330 - rados_bench() > sar_pid_rados_bench_host=919922
# ./benchmarks/bench-rados:330 - rados_bench() > S_TIME_FORMAT=ISO
# ./benchmarks/bench-rados:330 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/output/sys_activity_16MB_seq.dat.rados_bench_host.3 2

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:58:22,977388186-07:00] INFO: >> for OSD host[0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:58:22,987109729-07:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.5 bash
10.10.2.5: b'140303\n'
[1] 23:58:24 [SUCCESS] ljishen@10.10.2.5
140303

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:58:24,402003719-07:00] INFO: > Run rados bench[0m
# ./benchmarks/bench-rados:367 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/output/rados_bench_16MB_seq.log.3
## ./benchmarks/bench-rados:364 - rados_bench() > count_cpus 32-63,96-127
# ./benchmarks/bench-rados:364 - rados_bench() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 128 --show-time
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:58:24,423745636-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:58:24,427698406-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '6420b284-36f2-11ec-b561-09a495c6360a', '--', 'rados', 'bench', '99999999', 'seq', '--pool', 'bench_rados', '--concurrent-ios', '128', '--show-time'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 6420b284-36f2-11ec-b561-09a495c6360a -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 128 --show-time
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
hints = 1
2021-10-27T06:58:26.835770+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-27T06:58:26.835770+0000     0       0         0         0         0         0           -           0
2021-10-27T06:58:27.835861+0000     1      60        60         0         0         0           -           0
2021-10-27T06:58:28.835940+0000     2     115       115         0         0         0           -           0
2021-10-27T06:58:29.836081+0000 Total time run:       2.46058
Total reads made:     128
Read size:            16777216
Object size:          16777216
Bandwidth (MB/sec):   832.322
Average IOPS:         52
Stddev IOPS:          0
Max IOPS:             0
Min IOPS:             0
Average Latency(s):   1.28016
Max latency(s):       2.23197
Min latency(s):       0.256648

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:58:30,471924620-07:00] INFO: > Stop system activity collection services[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:58:30,479444204-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:379 - rados_bench() > kill -INT 919922

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:58:30,486315469-07:00] INFO: >> for OSD host[0m
# ./benchmarks/bench-rados:384 - rados_bench() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.5 kill -INT 140303
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:58:30,494195701-07:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.5 kill -INT 140303
[1] 23:58:31 [SUCCESS] ljishen@10.10.2.5

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:58:31,633471639-07:00] INFO: >>> Copy system activity file from OSD host[0m
## ./benchmarks/bench-rados:391 - rados_bench() > basename -- /tmp/bench-rados/sys_activity_16MB_seq.dat.osd_host.3
# ./benchmarks/bench-rados:391 - rados_bench() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.5:/tmp/bench-rados/sys_activity_16MB_seq.dat.osd_host.3 /mnt/sda8/ceph-research/scripts/output/sys_activity_16MB_seq.dat.osd_host.3


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:58:32,733660807-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:58:55,857576857-07:00] INFO: >>   data:
    pools:   2 pools, 256 pgs
    objects: 129 objects, 2.0 GiB
    usage:   6.0 GiB used, 94 GiB / 100 GiB avail
    pgs:     256 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:58:55,866848845-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:59:04,096846773-07:00] INFO: >>   data:
    pools:   2 pools, 256 pgs
    objects: 129 objects, 2.0 GiB
    usage:   6.0 GiB used, 94 GiB / 100 GiB avail
    pgs:     256 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:59:04,105633989-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:59:12,476796016-07:00] INFO: >>   data:
    pools:   2 pools, 256 pgs
    objects: 129 objects, 2.0 GiB
    usage:   6.0 GiB used, 94 GiB / 100 GiB avail
    pgs:     256 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:59:12,485778119-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:59:20,892233194-07:00] INFO: >>   data:
    pools:   2 pools, 256 pgs
    objects: 129 objects, 2.0 GiB
    usage:   6.0 GiB used, 94 GiB / 100 GiB avail
    pgs:     256 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:59:20,901387892-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:59:29,273674714-07:00] INFO: >>   data:
    pools:   2 pools, 256 pgs
    objects: 129 objects, 2.0 GiB
    usage:   6.0 GiB used, 94 GiB / 100 GiB avail
    pgs:     256 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:59:29,283310706-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-26T23:59:29,290156875-07:00] INFO: > The cluster is idle now.[0m
