[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T06:52:38,489318544-07:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.1 mkdir --parents /tmp/bench-rados
[1] 06:52:38 [SUCCESS] ljishen@10.10.2.1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T06:52:38,671562266-07:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.5 mkdir --parents /tmp/bench-rados
[1] 06:52:39 [SUCCESS] ljishen@10.10.2.5
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T06:52:39,769394483-07:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.5 bash
[1] 06:52:40 [SUCCESS] ljishen@10.10.2.5


[1;7;39;49m[2021-10-28T06:52:40,863262680-07:00][RUNNING][ROUND 1/1/21] object_size=4KB[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T06:52:40,865474207-07:00] STAGE: Launch a Ceph cluster (CEPH_HOST: 10.10.2.1, OSD_HOST: 10.10.2.5) ...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T06:52:40,870066912-07:00] INFO: > Get OSD hostname[0m
## ./benchmarks/bench-rados:178 - launch_ceph_cluster() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.5 uname --nodename
## ./benchmarks/bench-rados:178 - launch_ceph_cluster() > tail -n1
# ./benchmarks/bench-rados:178 - launch_ceph_cluster() > OSD_HOSTNAME=sm1
# ./benchmarks/bench-rados:183 - launch_ceph_cluster() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.1 bash -euo pipefail
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T06:52:41,949184492-07:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.1 bash -euo pipefail
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T06:52:42,822800916-07:00] INFO: > Remove existing clusters\x1b[0m\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T06:52:42,827843155-07:00] INFO: > Deploy a new cluster\x1b[0m\n'
10.10.2.1: b'# bash:24 -  > /tmp/bench-rados/ceph-research/scripts/ceph-deploy -m 10.10.2.1 -o sm1/10.10.2.5:/dev/nvme0n1\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T06:52:42,839350077-07:00] STAGE: Verify passwordless SSH to hosts, passwordless sudo via ljishen, and the existence of devices on hosts...\x1b[0m\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T06:52:42,843721695-07:00] INFO: > Check the host for the monitor\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:181 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@10.10.2.1 sudo --non-interactive --validate\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T06:52:42,987417673-07:00] INFO: > Check hosts for OSDs\x1b[0m\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T06:52:42,991637637-07:00] INFO: >> Check host 'sm1'\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:200 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 sudo --non-interactive --validate\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T06:52:44,046344561-07:00] INFO: >>> Check device '/dev/nvme0n1'\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:215 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 test -b /dev/nvme0n1\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:226 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 lsblk --nodeps --noheadings --output TYPE /dev/nvme0n1\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:227 - verify_parameters() > grep --word-regexp disk\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T06:52:46,123375721-07:00] STAGE: Reset devices on hosts...\x1b[0m\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T06:52:46,128463235-07:00] INFO: > Resetting device '/dev/nvme0n1' on host 'sm1'\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:288 - reset_host_devices() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 sudo sh\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T06:52:47,271084261-07:00] STAGE: Bootstrap a new cluster...\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:317 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.1 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T06:52:47,281047472-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T06:52:47,284606061-07:00] DEBUG: CEPHADM_PARAMS: ['bootstrap', '--config', '/tmp/bench-rados/ceph-research/scripts/ceph.conf', '--mon-ip', '10.10.2.1', '--output-dir', '/tmp/bench-rados/deployment_data_root/etc/ceph', '--ssh-user', 'ljishen', '--skip-dashboard', '--no-minimize-config', '--allow-overwrite', '--allow-fqdn-hostname', '--orphan-initial-daemons', '--skip-monitoring-stack', '--single-host-defaults']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.1 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults\n'
10.10.2.1: b'Verifying podman|docker is present...\n'
10.10.2.1: b'Verifying lvm2 is present...\n'
10.10.2.1: b'Verifying time synchronization is in place...\n'
10.10.2.1: b'Unit ntp.service is enabled and running\n'
10.10.2.1: b'Repeating the final host check...\n'
10.10.2.1: b'podman|docker (/usr/bin/docker) is present\n'
10.10.2.1: b'systemctl is present\nlvcreate is present\n'
10.10.2.1: b'Unit ntp.service is enabled and running\n'
10.10.2.1: b'Host looks OK\n'
10.10.2.1: b'Cluster fsid: 55585f2c-37f6-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Verifying IP 10.10.2.1 port 3300 ...\n'
10.10.2.1: b'Verifying IP 10.10.2.1 port 6789 ...\n'
10.10.2.1: b'Mon IP `10.10.2.1` is in CIDR network `10.10.2.0/24`\n'
10.10.2.1: b'- internal network (--cluster-network) has not been provided, OSD replication will default to the public_network\n'
10.10.2.1: b'Adjusting default settings to suit single-host cluster...\n'
10.10.2.1: b'Pulling container image quay.io/ceph/ceph:v16...\n'
10.10.2.1: b'Ceph version: ceph version 16.2.6 (ee28fb57e47e9f88813e24bbf4c14496ca299d31) pacific (stable)\n'
10.10.2.1: b'Extracting ceph user uid/gid from container image...\n'
10.10.2.1: b'Creating initial keys...\n'
10.10.2.1: b'Creating initial monmap...\n'
10.10.2.1: b'Creating mon...\n'
10.10.2.1: b'Waiting for mon to start...\n'
10.10.2.1: b'Waiting for mon...\n'
10.10.2.1: b'mon is available\n'
10.10.2.1: b'Setting mon public_network to 10.10.2.0/24\n'
10.10.2.1: b'Wrote config to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf\n'
10.10.2.1: b'Wrote keyring to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring\n'
10.10.2.1: b'Creating mgr...\n'
10.10.2.1: b'Verifying port 9283 ...\n'
10.10.2.1: b'Waiting for mgr to start...\nWaiting for mgr...\n'
10.10.2.1: b'mgr not available, waiting (1/15)...\n'
10.10.2.1: b'mgr not available, waiting (2/15)...\n'
10.10.2.1: b'mgr is available\n'
10.10.2.1: b'Enabling cephadm module...\n'
10.10.2.1: b'Waiting for the mgr to restart...\n'
10.10.2.1: b'Waiting for mgr epoch 5...\n'
10.10.2.1: b'mgr epoch 5 is available\n'
10.10.2.1: b'Setting orchestrator backend to cephadm...\n'
10.10.2.1: b'Generating ssh key...\n'
10.10.2.1: b'Wrote public SSH key to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub\n'
10.10.2.1: b'Adding key to ljishen@localhost authorized_keys...\n'
10.10.2.1: b'Adding host node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us...\n'
10.10.2.1: b'Deploying unmanaged mon service...\n'
10.10.2.1: b'Deploying unmanaged mgr service...\n'
10.10.2.1: b'You can access the Ceph CLI with:\n\n\tsudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --fsid 55585f2c-37f6-11ec-b51d-53e6e728d2d3 -c /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf -k /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring\n\n'
10.10.2.1: b'Please consider enabling telemetry to help improve Ceph:\n\n\tceph telemetry on\n\nFor more information see:\n\n\thttps://docs.ceph.com/docs/pacific/mgr/telemetry/\n\nBootstrap complete.\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T06:53:47,082923992-07:00] INFO: Sleep for 20 seconds\x1b[0m\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T06:54:07,089020615-07:00] INFO: > Set the default number of monitor daemons to 1\x1b[0m\n# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:337 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T06:54:07,098331849-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T06:54:07,101661899-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mon', '1']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1\n'
10.10.2.1: b'Inferring fsid 55585f2c-37f6-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/55585f2c-37f6-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b'Scheduled mon update...\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T06:54:15,790740297-07:00] INFO: > Set the default number of manager daemons to 1\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:342 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T06:54:15,800960059-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T06:54:15,805049587-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mgr', '1']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1\n'
10.10.2.1: b'Inferring fsid 55585f2c-37f6-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/55585f2c-37f6-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b'Scheduled mgr update...\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T06:54:25,147640371-07:00] STAGE: Add hosts to the cluster...\x1b[0m\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T06:54:25,153765726-07:00] INFO: > Adding host 'sm1'\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:361 - add_hosts() > ssh-copy-id -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -f -i /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub ljishen@sm1\n'
10.10.2.1: b'/usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: "/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub"\n'
10.10.2.1: b'\nNumber of key(s) added: 1\n\nNow try logging into the machine, with:   "ssh -o \'GlobalKnownHostsFile=/dev/null\' -o \'LogLevel=ERROR\' -o \'PasswordAuthentication=no\' -o \'StrictHostKeyChecking=no\' -o \'UserKnownHostsFile=/dev/null\' \'ljishen@sm1\'"\nand check to make sure that only the key(s) you wanted were added.\n\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:367 - add_hosts() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add sm1 10.10.2.5\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T06:54:26,288509313-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T06:54:26,292678090-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'host', 'add', 'sm1', '10.10.2.5']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add sm1 10.10.2.5\n'
10.10.2.1: b'Inferring fsid 55585f2c-37f6-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/55585f2c-37f6-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b"Added host 'sm1' with addr '10.10.2.5'\n"
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T06:54:36,952604575-07:00] INFO: Sleep for 20 seconds\x1b[0m\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T06:54:56,957074924-07:00] STAGE: Add OSDs to the cluster...\x1b[0m\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T06:54:56,962963594-07:00] INFO: > Adding devices ['/dev/nvme0n1'] on host 'sm1' as OSDs\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:388 - add_host_devices_as_osds() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd sm1:/dev/nvme0n1\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T06:54:56,972820404-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T06:54:56,976867943-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'daemon', 'add', 'osd', 'sm1:/dev/nvme0n1']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd sm1:/dev/nvme0n1\n'
10.10.2.1: b'Inferring fsid 55585f2c-37f6-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/55585f2c-37f6-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b"Created osd(s) 0 on host 'sm1'\n"
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T06:55:21,412622274-07:00] INFO: > Sleep for 20 seconds\x1b[0m\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T06:55:41,417953323-07:00] STAGE: Show Cluster Status\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:408 - show_status() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T06:55:41,427407436-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T06:55:41,430967108-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', '--status']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status\n'
10.10.2.1: b'Inferring fsid 55585f2c-37f6-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/55585f2c-37f6-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b'  cluster:\n    id:     55585f2c-37f6-11ec-b51d-53e6e728d2d3\n    health: HEALTH_WARN\n            1 pool(s) have no replicas configured\n \n  services:\n    mon: 1 daemons, quorum node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us (age 2m)\n    mgr: node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us.szwbpr(active, since 2m)\n    osd: 1 osds: 1 up (since 19s), 1 in (since 40s)\n \n  data:\n    pools:   1 pools, 1 pgs\n    objects: 0 objects, 0 B\n    usage:   4.7 MiB used, 100 GiB / 100 GiB avail\n    pgs:     1 active+clean\n \n'
[1] 06:55:50 [SUCCESS] ljishen@10.10.2.1

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T06:52:42,822800916-07:00] INFO: > Remove existing clusters[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T06:52:42,827843155-07:00] INFO: > Deploy a new cluster[0m
# bash:24 -  > /tmp/bench-rados/ceph-research/scripts/ceph-deploy -m 10.10.2.1 -o sm1/10.10.2.5:/dev/nvme0n1


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T06:52:42,839350077-07:00] STAGE: Verify passwordless SSH to hosts, passwordless sudo via ljishen, and the existence of devices on hosts...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T06:52:42,843721695-07:00] INFO: > Check the host for the monitor[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:181 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@10.10.2.1 sudo --non-interactive --validate

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T06:52:42,987417673-07:00] INFO: > Check hosts for OSDs[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T06:52:42,991637637-07:00] INFO: >> Check host 'sm1'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:200 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 sudo --non-interactive --validate

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T06:52:44,046344561-07:00] INFO: >>> Check device '/dev/nvme0n1'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:215 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 test -b /dev/nvme0n1
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:226 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 lsblk --nodeps --noheadings --output TYPE /dev/nvme0n1
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:227 - verify_parameters() > grep --word-regexp disk


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T06:52:46,123375721-07:00] STAGE: Reset devices on hosts...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T06:52:46,128463235-07:00] INFO: > Resetting device '/dev/nvme0n1' on host 'sm1'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:288 - reset_host_devices() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 sudo sh


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T06:52:47,271084261-07:00] STAGE: Bootstrap a new cluster...[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:317 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.1 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T06:52:47,281047472-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T06:52:47,284606061-07:00] DEBUG: CEPHADM_PARAMS: ['bootstrap', '--config', '/tmp/bench-rados/ceph-research/scripts/ceph.conf', '--mon-ip', '10.10.2.1', '--output-dir', '/tmp/bench-rados/deployment_data_root/etc/ceph', '--ssh-user', 'ljishen', '--skip-dashboard', '--no-minimize-config', '--allow-overwrite', '--allow-fqdn-hostname', '--orphan-initial-daemons', '--skip-monitoring-stack', '--single-host-defaults'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.1 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults
Verifying podman|docker is present...
Verifying lvm2 is present...
Verifying time synchronization is in place...
Unit ntp.service is enabled and running
Repeating the final host check...
podman|docker (/usr/bin/docker) is present
systemctl is present
lvcreate is present
Unit ntp.service is enabled and running
Host looks OK
Cluster fsid: 55585f2c-37f6-11ec-b51d-53e6e728d2d3
Verifying IP 10.10.2.1 port 3300 ...
Verifying IP 10.10.2.1 port 6789 ...
Mon IP `10.10.2.1` is in CIDR network `10.10.2.0/24`
- internal network (--cluster-network) has not been provided, OSD replication will default to the public_network
Adjusting default settings to suit single-host cluster...
Pulling container image quay.io/ceph/ceph:v16...
Ceph version: ceph version 16.2.6 (ee28fb57e47e9f88813e24bbf4c14496ca299d31) pacific (stable)
Extracting ceph user uid/gid from container image...
Creating initial keys...
Creating initial monmap...
Creating mon...
Waiting for mon to start...
Waiting for mon...
mon is available
Setting mon public_network to 10.10.2.0/24
Wrote config to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf
Wrote keyring to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring
Creating mgr...
Verifying port 9283 ...
Waiting for mgr to start...
Waiting for mgr...
mgr not available, waiting (1/15)...
mgr not available, waiting (2/15)...
mgr is available
Enabling cephadm module...
Waiting for the mgr to restart...
Waiting for mgr epoch 5...
mgr epoch 5 is available
Setting orchestrator backend to cephadm...
Generating ssh key...
Wrote public SSH key to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub
Adding key to ljishen@localhost authorized_keys...
Adding host node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us...
Deploying unmanaged mon service...
Deploying unmanaged mgr service...
You can access the Ceph CLI with:

	sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --fsid 55585f2c-37f6-11ec-b51d-53e6e728d2d3 -c /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf -k /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring

Please consider enabling telemetry to help improve Ceph:

	ceph telemetry on

For more information see:

	https://docs.ceph.com/docs/pacific/mgr/telemetry/

Bootstrap complete.

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T06:53:47,082923992-07:00] INFO: Sleep for 20 seconds[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T06:54:07,089020615-07:00] INFO: > Set the default number of monitor daemons to 1[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:337 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T06:54:07,098331849-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T06:54:07,101661899-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mon', '1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1
Inferring fsid 55585f2c-37f6-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/55585f2c-37f6-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Scheduled mon update...

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T06:54:15,790740297-07:00] INFO: > Set the default number of manager daemons to 1[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:342 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T06:54:15,800960059-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T06:54:15,805049587-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mgr', '1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1
Inferring fsid 55585f2c-37f6-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/55585f2c-37f6-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Scheduled mgr update...


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T06:54:25,147640371-07:00] STAGE: Add hosts to the cluster...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T06:54:25,153765726-07:00] INFO: > Adding host 'sm1'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:361 - add_hosts() > ssh-copy-id -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -f -i /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub ljishen@sm1
/usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: "/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub"

Number of key(s) added: 1

Now try logging into the machine, with:   "ssh -o 'GlobalKnownHostsFile=/dev/null' -o 'LogLevel=ERROR' -o 'PasswordAuthentication=no' -o 'StrictHostKeyChecking=no' -o 'UserKnownHostsFile=/dev/null' 'ljishen@sm1'"
and check to make sure that only the key(s) you wanted were added.

# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:367 - add_hosts() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add sm1 10.10.2.5
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T06:54:26,288509313-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T06:54:26,292678090-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'host', 'add', 'sm1', '10.10.2.5'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add sm1 10.10.2.5
Inferring fsid 55585f2c-37f6-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/55585f2c-37f6-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Added host 'sm1' with addr '10.10.2.5'

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T06:54:36,952604575-07:00] INFO: Sleep for 20 seconds[0m


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T06:54:56,957074924-07:00] STAGE: Add OSDs to the cluster...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T06:54:56,962963594-07:00] INFO: > Adding devices ['/dev/nvme0n1'] on host 'sm1' as OSDs[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:388 - add_host_devices_as_osds() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd sm1:/dev/nvme0n1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T06:54:56,972820404-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T06:54:56,976867943-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'daemon', 'add', 'osd', 'sm1:/dev/nvme0n1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd sm1:/dev/nvme0n1
Inferring fsid 55585f2c-37f6-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/55585f2c-37f6-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Created osd(s) 0 on host 'sm1'

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T06:55:21,412622274-07:00] INFO: > Sleep for 20 seconds[0m


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T06:55:41,417953323-07:00] STAGE: Show Cluster Status[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:408 - show_status() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T06:55:41,427407436-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T06:55:41,430967108-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', '--status'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status
Inferring fsid 55585f2c-37f6-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/55585f2c-37f6-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
  cluster:
    id:     55585f2c-37f6-11ec-b51d-53e6e728d2d3
    health: HEALTH_WARN
            1 pool(s) have no replicas configured
 
  services:
    mon: 1 daemons, quorum node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us (age 2m)
    mgr: node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us.szwbpr(active, since 2m)
    osd: 1 osds: 1 up (since 19s), 1 in (since 40s)
 
  data:
    pools:   1 pools, 1 pgs
    objects: 0 objects, 0 B
    usage:   4.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     1 active+clean
 

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T06:55:50,168811199-07:00] INFO: > Copy configuration files to local[0m
# ./benchmarks/bench-rados:220 - launch_ceph_cluster() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.1 sudo chown --changes --recursive ljishen /tmp/bench-rados/deployment_data_root/etc/ceph
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T06:55:50,175891503-07:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.1 sudo chown --changes --recursive ljishen /tmp/bench-rados/deployment_data_root/etc/ceph
[1] 06:55:50 [SUCCESS] ljishen@10.10.2.1
# ./benchmarks/bench-rados:223 - launch_ceph_cluster() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.1:/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf
# ./benchmarks/bench-rados:226 - launch_ceph_cluster() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.1:/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T06:55:50,651993817-07:00] STAGE: Update local ceph.conf as a client...[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T06:55:50,654503286-07:00] STAGE: Configure Ceph pool...[0m
# ./benchmarks/bench-rados:257 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_max_backfills 32
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T06:55:50,677297041-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T06:55:50,680279201-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '55585f2c-37f6-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'config', 'set', 'osd', 'osd_max_backfills', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 55585f2c-37f6-11ec-b51d-53e6e728d2d3 -- ceph config set osd osd_max_backfills 32
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:258 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_active 32
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T06:55:55,108734377-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T06:55:55,111707490-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '55585f2c-37f6-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_active', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 55585f2c-37f6-11ec-b51d-53e6e728d2d3 -- ceph config set osd osd_recovery_max_active 32
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:259 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_single_start 8
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T06:55:59,550109801-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T06:55:59,553177071-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '55585f2c-37f6-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_single_start', '8'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 55585f2c-37f6-11ec-b51d-53e6e728d2d3 -- ceph config set osd osd_recovery_max_single_start 8
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:260 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_op_priority 63
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T06:56:03,953876790-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T06:56:03,956688239-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '55585f2c-37f6-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_op_priority', '63'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 55585f2c-37f6-11ec-b51d-53e6e728d2d3 -- ceph config set osd osd_recovery_op_priority 63
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:262 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config show-with-defaults osd.0
# ./benchmarks/bench-rados:263 - configure_ceph_pool() > grep 'osd_max_backfills\|osd_recovery'
# ./benchmarks/bench-rados:263 - configure_ceph_pool() > column -t -s ' '
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
osd_max_backfills                        32         mon
osd_recovery_cost                        20971520   default
osd_recovery_delay_start                 0.000000   default
osd_recovery_max_active                  32         mon
osd_recovery_max_active_hdd              3          default
osd_recovery_max_active_ssd              10         default
osd_recovery_max_chunk                   8388608    default
osd_recovery_max_omap_entries_per_chunk  8096       default
osd_recovery_max_single_start            8          mon
osd_recovery_op_priority                 63         mon
osd_recovery_op_warn_multiple            16         default
osd_recovery_priority                    5          default
osd_recovery_retry_interval              30.000000  default
osd_recovery_sleep                       0.000000   default
osd_recovery_sleep_hdd                   0.100000   default
osd_recovery_sleep_hybrid                0.025000   default
osd_recovery_sleep_ssd                   0.000000   default
# ./benchmarks/bench-rados:265 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool create bench_rados 128 128 replicated --size 1 --pg-num-min 128
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T06:56:12,706844996-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T06:56:12,710058192-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '55585f2c-37f6-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'create', 'bench_rados', '128', '128', 'replicated', '--size', '1', '--pg-num-min', '128'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 55585f2c-37f6-11ec-b51d-53e6e728d2d3 -- ceph osd pool create bench_rados 128 128 replicated --size 1 --pg-num-min 128
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
pool 'bench_rados' created
# ./benchmarks/bench-rados:267 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados min_size 1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T06:56:17,897476238-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T06:56:17,900829167-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '55585f2c-37f6-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'min_size', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 55585f2c-37f6-11ec-b51d-53e6e728d2d3 -- ceph osd pool set bench_rados min_size 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 min_size to 1
# ./benchmarks/bench-rados:268 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados pg_autoscale_mode off
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T06:56:23,179631138-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T06:56:23,182667791-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '55585f2c-37f6-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'pg_autoscale_mode', 'off'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 55585f2c-37f6-11ec-b51d-53e6e728d2d3 -- ceph osd pool set bench_rados pg_autoscale_mode off
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 pg_autoscale_mode to off
# ./benchmarks/bench-rados:269 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool application enable bench_rados benchmark
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T06:56:27,506424129-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T06:56:27,509331638-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '55585f2c-37f6-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'application', 'enable', 'bench_rados', 'benchmark'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 55585f2c-37f6-11ec-b51d-53e6e728d2d3 -- ceph osd pool application enable bench_rados benchmark
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
enabled application 'benchmark' on pool 'bench_rados'
# ./benchmarks/bench-rados:270 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados noscrub 1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T06:56:32,456413777-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T06:56:32,459470538-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '55585f2c-37f6-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'noscrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 55585f2c-37f6-11ec-b51d-53e6e728d2d3 -- ceph osd pool set bench_rados noscrub 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 noscrub to 1
# ./benchmarks/bench-rados:271 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados nodeep-scrub 1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T06:56:37,266276382-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T06:56:37,269335607-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '55585f2c-37f6-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'nodeep-scrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 55585f2c-37f6-11ec-b51d-53e6e728d2d3 -- ceph osd pool set bench_rados nodeep-scrub 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 nodeep-scrub to 1
# ./benchmarks/bench-rados:272 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool ls detail
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T06:56:42,716971981-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T06:56:42,720045142-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '55585f2c-37f6-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'ls', 'detail'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 55585f2c-37f6-11ec-b51d-53e6e728d2d3 -- ceph osd pool ls detail
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
pool 1 'device_health_metrics' replicated size 1 min_size 1 crush_rule 0 object_hash rjenkins pg_num 64 pgp_num 64 autoscale_mode on last_change 19 lfor 0/0/16 flags hashpspool stripe_width 0 pg_num_min 1 application mgr_devicehealth
pool 2 'bench_rados' replicated size 1 min_size 1 crush_rule 0 object_hash rjenkins pg_num 128 pgp_num 128 autoscale_mode off last_change 21 flags hashpspool,noscrub,nodeep-scrub stripe_width 0 pg_num_min 128 application benchmark

# ./benchmarks/bench-rados:273 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd df tree
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T06:56:47,059285360-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T06:56:47,062173292-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '55585f2c-37f6-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'df', 'tree'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 55585f2c-37f6-11ec-b51d-53e6e728d2d3 -- ceph osd df tree
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
ID  CLASS  WEIGHT   REWEIGHT  SIZE     RAW USE  DATA     OMAP  META     AVAIL    %USE  VAR   PGS  STATUS  TYPE NAME    
-1         0.09769         -  100 GiB  5.7 MiB  176 KiB   0 B  5.6 MiB  100 GiB  0.01  1.00    -          root default 
-3         0.09769         -  100 GiB  5.7 MiB  176 KiB   0 B  5.6 MiB  100 GiB  0.01  1.00    -              host sm1 
 0    ssd  0.09769   1.00000  100 GiB  5.7 MiB  176 KiB   0 B  5.6 MiB  100 GiB  0.01  1.00  192      up          osd.0
                       TOTAL  100 GiB  5.7 MiB  176 KiB   0 B  5.6 MiB  100 GiB  0.01                                  
MIN/MAX VAR: 1.00/1.00  STDDEV: 0


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T06:56:51,376401375-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T06:57:15,584497192-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T06:57:24,858471328-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T06:57:34,292206508-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T06:57:34,298080067-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T06:57:43,716830697-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T06:57:43,723146289-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T06:57:53,036142814-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T06:57:53,042403643-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T06:58:02,385826126-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T06:58:02,392105099-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T06:58:11,692560540-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T06:58:11,698714097-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T06:58:11,703521516-07:00] INFO: > The cluster is idle now.[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T06:58:11,706245860-07:00] STAGE: Start benchmarking of mode write ...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T06:58:11,711094317-07:00] INFO: > Start system activity collection services[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T06:58:11,715560604-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:341 - rados_bench() > rm --force /mnt/sda8/ceph-research/scripts/output/sys_activity_4KB_write.dat.rados_bench_host.1
# ./benchmarks/bench-rados:342 - rados_bench() > sar_pid_rados_bench_host=369969
# ./benchmarks/bench-rados:342 - rados_bench() > S_TIME_FORMAT=ISO
# ./benchmarks/bench-rados:342 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/output/sys_activity_4KB_write.dat.rados_bench_host.1 2

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T06:58:11,720994895-07:00] INFO: >> for OSD host[0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T06:58:11,729260152-07:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.5 bash
10.10.2.5: b'87258\n'
[1] 06:58:12 [SUCCESS] ljishen@10.10.2.5
87258

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T06:58:12,830195948-07:00] INFO: > Run rados bench[0m
# ./benchmarks/bench-rados:370 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/output/rados_bench_4KB_write.log.1
## ./benchmarks/bench-rados:361 - rados_bench() > count_cpus 32-63,96-127
# ./benchmarks/bench-rados:361 - rados_bench() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 60 write --pool bench_rados -b 4096 -O 4096 --concurrent-ios 128 --show-time --write-object --write-omap --write-xattr --no-cleanup
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T06:58:12,849643918-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T06:58:12,852594718-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '55585f2c-37f6-11ec-b51d-53e6e728d2d3', '--', 'rados', 'bench', '60', 'write', '--pool', 'bench_rados', '-b', '4096', '-O', '4096', '--concurrent-ios', '128', '--show-time', '--write-object', '--write-omap', '--write-xattr', '--no-cleanup'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 55585f2c-37f6-11ec-b51d-53e6e728d2d3 -- rados bench 60 write --pool bench_rados -b 4096 -O 4096 --concurrent-ios 128 --show-time --write-object --write-omap --write-xattr --no-cleanup
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
hints = 1
2021-10-28T13:58:16.139906+0000 Maintaining 128 concurrent writes of 4096 bytes to objects of size 4096 for up to 60 seconds or 0 objects
2021-10-28T13:58:16.139918+0000 Object prefix: benchmark_data_node-2.bluefield2.ucsc-cmps10_7
2021-10-28T13:58:16.140453+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-28T13:58:16.140453+0000     0       0         0         0         0         0           -           0
2021-10-28T13:58:17.140565+0000     1     128      6262      6134   23.9597   23.9609   0.0189853   0.0206915
2021-10-28T13:58:18.140642+0000     2     128     12430     12302   24.0258   24.0938   0.0139575   0.0207128
2021-10-28T13:58:19.140714+0000     3     128     18581     18453   24.0257   24.0273   0.0198275   0.0207576
2021-10-28T13:58:20.140792+0000     4     128     24365     24237   23.6673   22.5938    0.025755   0.0210727
2021-10-28T13:58:21.140867+0000     5     128     30172     30044   23.4702   22.6836   0.0228241   0.0212494
2021-10-28T13:58:22.140944+0000     6     128     36032     35904   23.3733   22.8906   0.0202241    0.021351
2021-10-28T13:58:23.141028+0000     7     127     41821     41694    23.265   22.6172   0.0219689   0.0214537
2021-10-28T13:58:24.141148+0000     8     128     47498     47370   23.1281   22.1719   0.0216259    0.021591
2021-10-28T13:58:25.141260+0000     9     127     53287     53160    23.071   22.6172   0.0217373    0.021646
2021-10-28T13:58:26.141367+0000    10     128     58836     58708   22.9309   21.6719   0.0232631   0.0217804
2021-10-28T13:58:27.141482+0000    11     128     64337     64209   22.7995   21.4883   0.0266229   0.0219068
2021-10-28T13:58:28.141593+0000    12     127     70001     69874   22.7434   22.1289   0.0212524    0.021966
2021-10-28T13:58:29.141664+0000    13     128     75498     75370   22.6452   21.4688   0.0267013    0.022038
2021-10-28T13:58:30.141752+0000    14     128     80851     80723   22.5212   20.9102   0.0278953   0.0221731
2021-10-28T13:58:31.141839+0000    15     128     86112     85984   22.3897   20.5508   0.0285638   0.0223082
2021-10-28T13:58:32.141925+0000    16     128     91689     91561   22.3518   21.7852   0.0242173   0.0223496
2021-10-28T13:58:33.142003+0000    17     128     97314     97186   22.3294   21.9727    0.026008   0.0223736
2021-10-28T13:58:34.142085+0000    18     128    102808    102680    22.281   21.4609   0.0231826   0.0224189
2021-10-28T13:58:35.142215+0000    19     127    107697    107570   22.1136   19.1016   0.0406816   0.0225771
2021-10-28T13:58:36.142330+0000 min lat: 0.00873762 max lat: 0.0503592 avg lat: 0.0230815
2021-10-28T13:58:36.142330+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-28T13:58:36.142330+0000    20     128    110955    110827   21.6439   12.7227   0.0439443   0.0230815
2021-10-28T13:58:37.142451+0000    21     128    114088    113960    21.196   12.2383   0.0401944   0.0235662
2021-10-28T13:58:38.142555+0000    22     128    117175    117047   20.7806   12.0586   0.0377911   0.0240317
2021-10-28T13:58:39.142653+0000    23     128    120104    119976   20.3745   11.4414   0.0463931   0.0245192
2021-10-28T13:58:40.142774+0000    24     128    122920    122792   19.9838        11   0.0465084   0.0249942
2021-10-28T13:58:41.142888+0000    25     128    125736    125608   19.6244        11   0.0518734   0.0254504
2021-10-28T13:58:42.142997+0000    26     128    128363    128235   19.2642   10.2617   0.0517949   0.0259299
2021-10-28T13:58:43.143110+0000    27     128    131928    131800   19.0665   13.9258   0.0187191   0.0262127
2021-10-28T13:58:44.143226+0000    28     128    136147    136019    18.974   16.4805   0.0185936   0.0263397
2021-10-28T13:58:45.143344+0000    29     127    141820    141693   19.0839   22.1641   0.0236497   0.0261887
2021-10-28T13:58:46.143454+0000    30     128    147442    147314   19.1796    21.957   0.0264202   0.0260559
2021-10-28T13:58:47.143573+0000    31     128    152992    152864   19.2602   21.6797    0.018858   0.0259482
2021-10-28T13:58:48.143685+0000    32     128    158679    158551   19.3525   22.2148   0.0181255   0.0258273
2021-10-28T13:58:49.143790+0000    33     128    164352    164224   19.4375   22.1602   0.0209244   0.0257102
2021-10-28T13:58:50.143909+0000    34     128    170065    169937   19.5221   22.3164   0.0216202   0.0256021
2021-10-28T13:58:51.144030+0000    35     128    175662    175534   19.5889   21.8633   0.0223335   0.0255161
2021-10-28T13:58:52.144144+0000    36     128    181275    181147   19.6537   21.9258   0.0218468   0.0254315
2021-10-28T13:58:53.144259+0000    37     128    186670    186542   19.6921   21.0742    0.022039   0.0253824
2021-10-28T13:58:54.144349+0000    38     127    192192    192065   19.7415   21.5742    0.019934   0.0253175
2021-10-28T13:58:55.144460+0000    39     128    197630    197502   19.7798   21.2383   0.0235865   0.0252665
2021-10-28T13:58:56.144572+0000 min lat: 0.00873762 max lat: 0.290898 avg lat: 0.0252132
2021-10-28T13:58:56.144572+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-28T13:58:56.144572+0000    40     128    203149    203021   19.8243   21.5586   0.0251526   0.0252132
2021-10-28T13:58:57.144691+0000    41     128    208646    208518   19.8644   21.4727   0.0281009   0.0251592
2021-10-28T13:58:58.144803+0000    42     128    213443    213315   19.8375   18.7383    0.044801   0.0251924
2021-10-28T13:58:59.144923+0000    43     128    216643    216515   19.6669      12.5   0.0386254   0.0254048
2021-10-28T13:59:00.145045+0000    44     128    217923    217795   19.3335         5   0.0972792   0.0258182
2021-10-28T13:59:01.145156+0000    45     128    219203    219075    19.015         5    0.143619   0.0262643
2021-10-28T13:59:02.145261+0000    46     128    220355    220227   18.6994       4.5    0.103853   0.0267029
2021-10-28T13:59:03.145376+0000    47     128    221516    221388    18.398   4.53516    0.113248   0.0271298
2021-10-28T13:59:04.145493+0000    48     128    222787    222659   18.1182   4.96484     0.11073   0.0275686
2021-10-28T13:59:05.145607+0000    49     128    224067    223939   17.8504         5   0.0994305   0.0279859
2021-10-28T13:59:06.145712+0000    50     128    225292    225164   17.5891   4.78516    0.102339   0.0284042
2021-10-28T13:59:07.145826+0000    51     128    226371    226243   17.3269   4.21484    0.144838   0.0288288
2021-10-28T13:59:08.145941+0000    52     128    228419    228291   17.1475         8   0.0357548   0.0290365
2021-10-28T13:59:09.146059+0000    53     128    231786    231658   17.0721   13.1523   0.0341792   0.0292753
2021-10-28T13:59:10.146175+0000    54     128    235331    235203   17.0123   13.8477   0.0469963   0.0293797
2021-10-28T13:59:11.146287+0000    55     128    238727    238599   16.9442   13.2656   0.0357729   0.0294998
2021-10-28T13:59:12.146403+0000    56     128    242002    241874     16.87    12.793   0.0378232   0.0296237
2021-10-28T13:59:13.146513+0000    57     128    245383    245255   16.8057    13.207   0.0345908     0.02974
2021-10-28T13:59:14.146615+0000    58     128    248705    248577   16.7397   12.9766   0.0390625   0.0298594
2021-10-28T13:59:15.146727+0000    59     128    252033    251905   16.6763        13   0.0370475   0.0299722
2021-10-28T13:59:16.146847+0000 min lat: 0.00873762 max lat: 0.326478 avg lat: 0.0300817
2021-10-28T13:59:16.146847+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-28T13:59:16.146847+0000    60     128    255361    255233    16.615        13   0.0430023   0.0300817
2021-10-28T13:59:17.147013+0000 Total time run:         60.0555
Total writes made:      255361
Write size:             4096
Object size:            4096
Bandwidth (MB/sec):     16.6097
Stddev Bandwidth:       6.46078
Max bandwidth (MB/sec): 24.0938
Min bandwidth (MB/sec): 4.21484
Average IOPS:           4252
Stddev IOPS:            1653.96
Max IOPS:               6168
Min IOPS:               1079
Average Latency(s):     0.0300957
Stddev Latency(s):      0.0196211
Max latency(s):         0.326478
Min latency(s):         0.00873762

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T06:59:17,905226406-07:00] INFO: > Stop system activity collection services[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T06:59:17,909459553-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:391 - rados_bench() > kill -INT 369969

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T06:59:17,913739579-07:00] INFO: >> for OSD host[0m
# ./benchmarks/bench-rados:396 - rados_bench() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.5 kill -INT 87258
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T06:59:17,921156296-07:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.5 kill -INT 87258
[1] 06:59:19 [SUCCESS] ljishen@10.10.2.5

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T06:59:19,132249887-07:00] INFO: >>> Copy system activity file from OSD host[0m
## ./benchmarks/bench-rados:403 - rados_bench() > basename -- /tmp/bench-rados/sys_activity_4KB_write.dat.osd_host.1
# ./benchmarks/bench-rados:403 - rados_bench() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.5:/tmp/bench-rados/sys_activity_4KB_write.dat.osd_host.1 /mnt/sda8/ceph-research/scripts/output/sys_activity_4KB_write.dat.osd_host.1


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T06:59:20,322594576-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T06:59:44,566340561-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 255.36k objects, 998 MiB
    usage:   3.2 GiB used, 97 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T06:59:44,572893160-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T06:59:53,990379819-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 255.36k objects, 998 MiB
    usage:   3.2 GiB used, 97 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T06:59:53,996962204-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:00:03,655667726-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 255.36k objects, 998 MiB
    usage:   3.2 GiB used, 97 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:00:03,661757473-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:00:13,127945491-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 255.36k objects, 998 MiB
    usage:   3.2 GiB used, 97 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:00:13,134383975-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:00:22,526609730-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 255.36k objects, 998 MiB
    usage:   3.2 GiB used, 97 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:00:22,532703434-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:00:22,537185912-07:00] INFO: > The cluster is idle now.[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:00:22,539898123-07:00] STAGE: Start benchmarking of mode seq ...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:00:22,544871595-07:00] INFO: > Start system activity collection services[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:00:22,549190995-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:341 - rados_bench() > rm --force /mnt/sda8/ceph-research/scripts/output/sys_activity_4KB_seq.dat.rados_bench_host.1
# ./benchmarks/bench-rados:342 - rados_bench() > sar_pid_rados_bench_host=372258
# ./benchmarks/bench-rados:342 - rados_bench() > S_TIME_FORMAT=ISO
# ./benchmarks/bench-rados:342 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/output/sys_activity_4KB_seq.dat.rados_bench_host.1 2

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:00:22,555081025-07:00] INFO: >> for OSD host[0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:00:22,563625728-07:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.5 bash
10.10.2.5: b'87806\n'
[1] 07:00:23 [SUCCESS] ljishen@10.10.2.5
87806

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:00:23,679591849-07:00] INFO: > Run rados bench[0m
# ./benchmarks/bench-rados:379 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/output/rados_bench_4KB_seq.log.1
## ./benchmarks/bench-rados:376 - rados_bench() > count_cpus 32-63,96-127
# ./benchmarks/bench-rados:376 - rados_bench() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 128 --show-time
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:00:23,698550728-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:00:23,701351656-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '55585f2c-37f6-11ec-b51d-53e6e728d2d3', '--', 'rados', 'bench', '99999999', 'seq', '--pool', 'bench_rados', '--concurrent-ios', '128', '--show-time'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 55585f2c-37f6-11ec-b51d-53e6e728d2d3 -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 128 --show-time
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
hints = 1
2021-10-28T14:00:27.075374+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-28T14:00:27.075374+0000     0       0         0         0         0         0           -           0
2021-10-28T14:00:28.075508+0000     1     127     17963     17836   69.6578   69.6719  0.00884445  0.00713228
2021-10-28T14:00:29.075583+0000     2     128     36008     35880   70.0684   70.4844  0.00779516  0.00711992
2021-10-28T14:00:30.075672+0000     3     128     54218     54090   70.4211   71.1328  0.00820086  0.00708377
2021-10-28T14:00:31.075764+0000     4     128     72598     72470   70.7634   71.7969  0.00880219  0.00705315
2021-10-28T14:00:32.075840+0000     5     128     87083     86955   67.9264    56.582    0.009886  0.00734937
2021-10-28T14:00:33.075914+0000     6     128    104622    104494   68.0231   68.5117   0.0074163  0.00734051
2021-10-28T14:00:34.075998+0000     7     128    122334    122206   68.1886   69.1875  0.00766866  0.00732431
2021-10-28T14:00:35.076111+0000     8     127    140195    140068   68.3857   69.7734  0.00690121  0.00730285
2021-10-28T14:00:36.076215+0000     9     127    158093    157966   68.5547   69.9141  0.00545627  0.00728719
2021-10-28T14:00:37.076295+0000    10     127    175486    175359   68.4928   67.9414  0.00705628  0.00729242
2021-10-28T14:00:38.076370+0000    11     128    193271    193143   68.5811   69.4688  0.00730265  0.00728352
2021-10-28T14:00:39.076485+0000    12     128    211778    211650   68.8897    72.293  0.00674493  0.00725164
2021-10-28T14:00:40.076622+0000    13     127    229809    229682    69.008   70.4375  0.00792949  0.00723987
2021-10-28T14:00:41.076734+0000    14     128    247003    246875   68.8755   67.1602  0.00689223  0.00725313
2021-10-28T14:00:42.076877+0000 Total time run:       14.5484
Total reads made:     255361
Read size:            4096
Object size:          4096
Bandwidth (MB/sec):   68.5644
Average IOPS:         17552
Stddev IOPS:          974.513
Max IOPS:             18507
Min IOPS:             14485
Average Latency(s):   0.0072872
Max latency(s):       0.0364795
Min latency(s):       0.0010359

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:00:42,797059927-07:00] INFO: > Stop system activity collection services[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:00:42,802230962-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:391 - rados_bench() > kill -INT 372258

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:00:42,806831943-07:00] INFO: >> for OSD host[0m
# ./benchmarks/bench-rados:396 - rados_bench() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.5 kill -INT 87806
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:00:42,814620010-07:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.5 kill -INT 87806
[1] 07:00:43 [SUCCESS] ljishen@10.10.2.5

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:00:43,895474953-07:00] INFO: >>> Copy system activity file from OSD host[0m
## ./benchmarks/bench-rados:403 - rados_bench() > basename -- /tmp/bench-rados/sys_activity_4KB_seq.dat.osd_host.1
# ./benchmarks/bench-rados:403 - rados_bench() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.5:/tmp/bench-rados/sys_activity_4KB_seq.dat.osd_host.1 /mnt/sda8/ceph-research/scripts/output/sys_activity_4KB_seq.dat.osd_host.1


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:00:44,965633850-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:01:09,216185011-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 255.36k objects, 998 MiB
    usage:   3.2 GiB used, 97 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:01:09,223353361-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:01:18,615249976-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 255.36k objects, 998 MiB
    usage:   3.2 GiB used, 97 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:01:18,622027048-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:01:28,005936782-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 255.36k objects, 998 MiB
    usage:   3.2 GiB used, 97 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:01:28,012795577-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:01:37,364862373-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 255.36k objects, 998 MiB
    usage:   3.2 GiB used, 97 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:01:37,372112747-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:01:46,637620841-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 255.36k objects, 998 MiB
    usage:   3.2 GiB used, 97 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:01:46,644363779-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:01:46,649787008-07:00] INFO: > The cluster is idle now.[0m


[1;7;39;49m[2021-10-28T07:01:46,651944464-07:00][RUNNING][ROUND 2/1/21] object_size=4KB[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:01:46,654959215-07:00] STAGE: Launch a Ceph cluster (CEPH_HOST: 10.10.2.1, OSD_HOST: 10.10.2.5) ...[0m
# ./benchmarks/bench-rados:183 - launch_ceph_cluster() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.1 bash -euo pipefail
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:01:46,663839791-07:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.1 bash -euo pipefail
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T07:01:47,104311192-07:00] INFO: > Remove existing clusters\x1b[0m\n'
10.10.2.1: b'## bash:17 -  > basename -- /var/lib/ceph/55585f2c-37f6-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'# bash:17 -  > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm --host 10.10.2.1 --host 10.10.2.5 rm-cluster --force --fsid 55585f2c-37f6-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T07:01:47,115531605-07:00] DEBUG: HOST_PARAMS: ['--host', '10.10.2.1', '--host', '10.10.2.5']\x1b[0m\n"
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T07:01:47,119321489-07:00] DEBUG: CEPHADM_PARAMS: ['rm-cluster', '--force', '--fsid', '55585f2c-37f6-11ec-b51d-53e6e728d2d3']\x1b[0m\n"
10.10.2.1: b"# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:63 -  > /tmp/bench-rados/ceph-research/scripts/parallel-ssh --host 10.10.2.1 --host 10.10.2.5 'sudo python3 -u - rm-cluster --force --fsid 55585f2c-37f6-11ec-b51d-53e6e728d2d3'\n"
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T07:01:47,128864259-07:00] DEBUG: command from stdin\x1b[0m\n'
10.10.2.1: b"# /tmp/bench-rados/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host 10.10.2.1 --host 10.10.2.5 'sudo python3 -u - rm-cluster --force --fsid 55585f2c-37f6-11ec-b51d-53e6e728d2d3'\n"
10.10.2.1: b'[1] 07:01:52 [SUCCESS] 10.10.2.1\n[2] 07:01:56 [SUCCESS] 10.10.2.5\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T07:01:56,151762949-07:00] INFO: > Deploy a new cluster\x1b[0m\n'
10.10.2.1: b'# bash:24 -  > /tmp/bench-rados/ceph-research/scripts/ceph-deploy -m 10.10.2.1 -o sm1/10.10.2.5:/dev/nvme0n1\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T07:01:56,163859219-07:00] STAGE: Verify passwordless SSH to hosts, passwordless sudo via ljishen, and the existence of devices on hosts...\x1b[0m\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T07:01:56,168589471-07:00] INFO: > Check the host for the monitor\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:181 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@10.10.2.1 sudo --non-interactive --validate\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T07:01:56,319977316-07:00] INFO: > Check hosts for OSDs\x1b[0m\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T07:01:56,324680839-07:00] INFO: >> Check host 'sm1'\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:200 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 sudo --non-interactive --validate\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T07:01:57,402518995-07:00] INFO: >>> Check device '/dev/nvme0n1'\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:215 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 test -b /dev/nvme0n1\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:226 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 lsblk --nodeps --noheadings --output TYPE /dev/nvme0n1\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:227 - verify_parameters() > grep --word-regexp disk\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T07:01:59,531574745-07:00] STAGE: Reset devices on hosts...\x1b[0m\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T07:01:59,536640037-07:00] INFO: > Resetting device '/dev/nvme0n1' on host 'sm1'\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:288 - reset_host_devices() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 sudo sh\n'
10.10.2.1: b'  Removing ceph--5d71c264--e7df--4881--a84c--4d96c9579ce5-osd--block--6ee9d33b--13f9--4904--8bbd--ed0fac39a56e (252:0)\n'
10.10.2.1: b'  Archiving volume group "ceph-5d71c264-e7df-4881-a84c-4d96c9579ce5" metadata (seqno 5).\n'
10.10.2.1: b'  Releasing logical volume "osd-block-6ee9d33b-13f9-4904-8bbd-ed0fac39a56e"\n'
10.10.2.1: b'  Creating volume group backup "/etc/lvm/backup/ceph-5d71c264-e7df-4881-a84c-4d96c9579ce5" (seqno 6).\n'
10.10.2.1: b'  Logical volume "osd-block-6ee9d33b-13f9-4904-8bbd-ed0fac39a56e" successfully removed\n'
10.10.2.1: b'  '
10.10.2.1: b'Removing physical volume "/dev/nvme0n1" from volume group "ceph-5d71c264-e7df-4881-a84c-4d96c9579ce5"\n'
10.10.2.1: b'  Volume group "ceph-5d71c264-e7df-4881-a84c-4d96c9579ce5" successfully removed\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T07:02:01,831695230-07:00] STAGE: Bootstrap a new cluster...\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:317 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.1 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T07:02:01,841345301-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T07:02:01,844922936-07:00] DEBUG: CEPHADM_PARAMS: ['bootstrap', '--config', '/tmp/bench-rados/ceph-research/scripts/ceph.conf', '--mon-ip', '10.10.2.1', '--output-dir', '/tmp/bench-rados/deployment_data_root/etc/ceph', '--ssh-user', 'ljishen', '--skip-dashboard', '--no-minimize-config', '--allow-overwrite', '--allow-fqdn-hostname', '--orphan-initial-daemons', '--skip-monitoring-stack', '--single-host-defaults']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.1 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults\n'
10.10.2.1: b'Verifying podman|docker is present...\n'
10.10.2.1: b'Verifying lvm2 is present...\nVerifying time synchronization is in place...\n'
10.10.2.1: b'Unit ntp.service is enabled and running\n'
10.10.2.1: b'Repeating the final host check...\n'
10.10.2.1: b'podman|docker (/usr/bin/docker) is present\n'
10.10.2.1: b'systemctl is present\n'
10.10.2.1: b'lvcreate is present\n'
10.10.2.1: b'Unit ntp.service is enabled and running\n'
10.10.2.1: b'Host looks OK\n'
10.10.2.1: b'Cluster fsid: 9fe4309c-37f7-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Verifying IP 10.10.2.1 port 3300 ...\n'
10.10.2.1: b'Verifying IP 10.10.2.1 port 6789 ...\n'
10.10.2.1: b'Mon IP `10.10.2.1` is in CIDR network `10.10.2.0/24`\n'
10.10.2.1: b'- internal network (--cluster-network) has not been provided, OSD replication will default to the public_network\n'
10.10.2.1: b'Adjusting default settings to suit single-host cluster...\n'
10.10.2.1: b'Pulling container image quay.io/ceph/ceph:v16...\n'
10.10.2.1: b'Ceph version: ceph version 16.2.6 (ee28fb57e47e9f88813e24bbf4c14496ca299d31) pacific (stable)\n'
10.10.2.1: b'Extracting ceph user uid/gid from container image...\n'
10.10.2.1: b'Creating initial keys...\n'
10.10.2.1: b'Creating initial monmap...\n'
10.10.2.1: b'Creating mon...\n'
10.10.2.1: b'Waiting for mon to start...\n'
10.10.2.1: b'Waiting for mon...\n'
10.10.2.1: b'mon is available\nSetting mon public_network to 10.10.2.0/24\n'
10.10.2.1: b'Wrote config to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf\n'
10.10.2.1: b'Wrote keyring to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring\n'
10.10.2.1: b'Creating mgr...\n'
10.10.2.1: b'Verifying port 9283 ...\n'
10.10.2.1: b'Waiting for mgr to start...\nWaiting for mgr...\n'
10.10.2.1: b'mgr not available, waiting (1/15)...\n'
10.10.2.1: b'mgr not available, waiting (2/15)...\n'
10.10.2.1: b'mgr is available\n'
10.10.2.1: b'Enabling cephadm module...\n'
10.10.2.1: b'Waiting for the mgr to restart...\n'
10.10.2.1: b'Waiting for mgr epoch 5...\n'
10.10.2.1: b'mgr epoch 5 is available\nSetting orchestrator backend to cephadm...\n'
10.10.2.1: b'Generating ssh key...\n'
10.10.2.1: b'Wrote public SSH key to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub\n'
10.10.2.1: b'Adding key to ljishen@localhost authorized_keys...\n'
10.10.2.1: b'Adding host node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us...\n'
10.10.2.1: b'Deploying unmanaged mon service...\n'
10.10.2.1: b'Deploying unmanaged mgr service...\n'
10.10.2.1: b'You can access the Ceph CLI with:\n\n\tsudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --fsid 9fe4309c-37f7-11ec-b51d-53e6e728d2d3 -c /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf -k /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring\n\n'
10.10.2.1: b'Please consider enabling telemetry to help improve Ceph:\n\n\tceph telemetry on\n\nFor more information see:\n\n\thttps://docs.ceph.com/docs/pacific/mgr/telemetry/\n\n'
10.10.2.1: b'Bootstrap complete.\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T07:03:01,235319949-07:00] INFO: Sleep for 20 seconds\x1b[0m\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T07:03:21,242383975-07:00] INFO: > Set the default number of monitor daemons to 1\x1b[0m\n# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:337 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T07:03:21,252887380-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T07:03:21,256829631-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mon', '1']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1\n'
10.10.2.1: b'Inferring fsid 9fe4309c-37f7-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/9fe4309c-37f7-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b'Scheduled mon update...\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T07:03:29,292164714-07:00] INFO: > Set the default number of manager daemons to 1\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:342 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T07:03:29,301596646-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T07:03:29,305251337-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mgr', '1']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1\n'
10.10.2.1: b'Inferring fsid 9fe4309c-37f7-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/9fe4309c-37f7-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b'Scheduled mgr update...\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T07:03:38,871374231-07:00] STAGE: Add hosts to the cluster...\x1b[0m\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T07:03:38,877979949-07:00] INFO: > Adding host 'sm1'\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:361 - add_hosts() > ssh-copy-id -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -f -i /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub ljishen@sm1\n'
10.10.2.1: b'/usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: "/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub"\n'
10.10.2.1: b'\nNumber of key(s) added: 1\n\nNow try logging into the machine, with:   "ssh -o \'GlobalKnownHostsFile=/dev/null\' -o \'LogLevel=ERROR\' -o \'PasswordAuthentication=no\' -o \'StrictHostKeyChecking=no\' -o \'UserKnownHostsFile=/dev/null\' \'ljishen@sm1\'"\nand check to make sure that only the key(s) you wanted were added.\n\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:367 - add_hosts() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add sm1 10.10.2.5\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T07:03:40,019791911-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T07:03:40,023286039-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'host', 'add', 'sm1', '10.10.2.5']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add sm1 10.10.2.5\n'
10.10.2.1: b'Inferring fsid 9fe4309c-37f7-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/9fe4309c-37f7-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b"Added host 'sm1' with addr '10.10.2.5'\n"
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T07:03:50,749607431-07:00] INFO: Sleep for 20 seconds\x1b[0m\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T07:04:10,754720011-07:00] STAGE: Add OSDs to the cluster...\x1b[0m\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T07:04:10,761487172-07:00] INFO: > Adding devices ['/dev/nvme0n1'] on host 'sm1' as OSDs\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:388 - add_host_devices_as_osds() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd sm1:/dev/nvme0n1\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T07:04:10,771296272-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T07:04:10,775333202-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'daemon', 'add', 'osd', 'sm1:/dev/nvme0n1']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd sm1:/dev/nvme0n1\n'
10.10.2.1: b'Inferring fsid 9fe4309c-37f7-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/9fe4309c-37f7-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b"Created osd(s) 0 on host 'sm1'\n"
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T07:04:34,092132230-07:00] INFO: > Sleep for 20 seconds\x1b[0m\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T07:04:54,097655725-07:00] STAGE: Show Cluster Status\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:408 - show_status() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T07:04:54,108184498-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T07:04:54,111885416-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', '--status']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status\n'
10.10.2.1: b'Inferring fsid 9fe4309c-37f7-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/9fe4309c-37f7-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b'  cluster:\n    id:     9fe4309c-37f7-11ec-b51d-53e6e728d2d3\n    health: HEALTH_WARN\n            1 pool(s) have no replicas configured\n \n  services:\n    mon: 1 daemons, quorum node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us (age 2m)\n    mgr: node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us.inzued(active, since 2m)\n    osd: 1 osds: 1 up (since 20s), 1 in (since 39s)\n \n  data:\n    pools:   1 pools, 1 pgs\n    objects: 0 objects, 0 B\n    usage:   4.7 MiB used, 100 GiB / 100 GiB avail\n    pgs:     1 active+clean\n \n'
[1] 07:05:03 [SUCCESS] ljishen@10.10.2.1

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:01:47,104311192-07:00] INFO: > Remove existing clusters[0m
## bash:17 -  > basename -- /var/lib/ceph/55585f2c-37f6-11ec-b51d-53e6e728d2d3
# bash:17 -  > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm --host 10.10.2.1 --host 10.10.2.5 rm-cluster --force --fsid 55585f2c-37f6-11ec-b51d-53e6e728d2d3
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:01:47,115531605-07:00] DEBUG: HOST_PARAMS: ['--host', '10.10.2.1', '--host', '10.10.2.5'][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:01:47,119321489-07:00] DEBUG: CEPHADM_PARAMS: ['rm-cluster', '--force', '--fsid', '55585f2c-37f6-11ec-b51d-53e6e728d2d3'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:63 -  > /tmp/bench-rados/ceph-research/scripts/parallel-ssh --host 10.10.2.1 --host 10.10.2.5 'sudo python3 -u - rm-cluster --force --fsid 55585f2c-37f6-11ec-b51d-53e6e728d2d3'
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:01:47,128864259-07:00] DEBUG: command from stdin[0m
# /tmp/bench-rados/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host 10.10.2.1 --host 10.10.2.5 'sudo python3 -u - rm-cluster --force --fsid 55585f2c-37f6-11ec-b51d-53e6e728d2d3'
[1] 07:01:52 [SUCCESS] 10.10.2.1
[2] 07:01:56 [SUCCESS] 10.10.2.5

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:01:56,151762949-07:00] INFO: > Deploy a new cluster[0m
# bash:24 -  > /tmp/bench-rados/ceph-research/scripts/ceph-deploy -m 10.10.2.1 -o sm1/10.10.2.5:/dev/nvme0n1


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:01:56,163859219-07:00] STAGE: Verify passwordless SSH to hosts, passwordless sudo via ljishen, and the existence of devices on hosts...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:01:56,168589471-07:00] INFO: > Check the host for the monitor[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:181 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@10.10.2.1 sudo --non-interactive --validate

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:01:56,319977316-07:00] INFO: > Check hosts for OSDs[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:01:56,324680839-07:00] INFO: >> Check host 'sm1'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:200 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 sudo --non-interactive --validate

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:01:57,402518995-07:00] INFO: >>> Check device '/dev/nvme0n1'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:215 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 test -b /dev/nvme0n1
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:226 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 lsblk --nodeps --noheadings --output TYPE /dev/nvme0n1
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:227 - verify_parameters() > grep --word-regexp disk


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:01:59,531574745-07:00] STAGE: Reset devices on hosts...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:01:59,536640037-07:00] INFO: > Resetting device '/dev/nvme0n1' on host 'sm1'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:288 - reset_host_devices() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 sudo sh
  Removing ceph--5d71c264--e7df--4881--a84c--4d96c9579ce5-osd--block--6ee9d33b--13f9--4904--8bbd--ed0fac39a56e (252:0)
  Archiving volume group "ceph-5d71c264-e7df-4881-a84c-4d96c9579ce5" metadata (seqno 5).
  Releasing logical volume "osd-block-6ee9d33b-13f9-4904-8bbd-ed0fac39a56e"
  Creating volume group backup "/etc/lvm/backup/ceph-5d71c264-e7df-4881-a84c-4d96c9579ce5" (seqno 6).
  Logical volume "osd-block-6ee9d33b-13f9-4904-8bbd-ed0fac39a56e" successfully removed
  Removing physical volume "/dev/nvme0n1" from volume group "ceph-5d71c264-e7df-4881-a84c-4d96c9579ce5"
  Volume group "ceph-5d71c264-e7df-4881-a84c-4d96c9579ce5" successfully removed


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:02:01,831695230-07:00] STAGE: Bootstrap a new cluster...[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:317 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.1 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:02:01,841345301-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:02:01,844922936-07:00] DEBUG: CEPHADM_PARAMS: ['bootstrap', '--config', '/tmp/bench-rados/ceph-research/scripts/ceph.conf', '--mon-ip', '10.10.2.1', '--output-dir', '/tmp/bench-rados/deployment_data_root/etc/ceph', '--ssh-user', 'ljishen', '--skip-dashboard', '--no-minimize-config', '--allow-overwrite', '--allow-fqdn-hostname', '--orphan-initial-daemons', '--skip-monitoring-stack', '--single-host-defaults'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.1 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults
Verifying podman|docker is present...
Verifying lvm2 is present...
Verifying time synchronization is in place...
Unit ntp.service is enabled and running
Repeating the final host check...
podman|docker (/usr/bin/docker) is present
systemctl is present
lvcreate is present
Unit ntp.service is enabled and running
Host looks OK
Cluster fsid: 9fe4309c-37f7-11ec-b51d-53e6e728d2d3
Verifying IP 10.10.2.1 port 3300 ...
Verifying IP 10.10.2.1 port 6789 ...
Mon IP `10.10.2.1` is in CIDR network `10.10.2.0/24`
- internal network (--cluster-network) has not been provided, OSD replication will default to the public_network
Adjusting default settings to suit single-host cluster...
Pulling container image quay.io/ceph/ceph:v16...
Ceph version: ceph version 16.2.6 (ee28fb57e47e9f88813e24bbf4c14496ca299d31) pacific (stable)
Extracting ceph user uid/gid from container image...
Creating initial keys...
Creating initial monmap...
Creating mon...
Waiting for mon to start...
Waiting for mon...
mon is available
Setting mon public_network to 10.10.2.0/24
Wrote config to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf
Wrote keyring to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring
Creating mgr...
Verifying port 9283 ...
Waiting for mgr to start...
Waiting for mgr...
mgr not available, waiting (1/15)...
mgr not available, waiting (2/15)...
mgr is available
Enabling cephadm module...
Waiting for the mgr to restart...
Waiting for mgr epoch 5...
mgr epoch 5 is available
Setting orchestrator backend to cephadm...
Generating ssh key...
Wrote public SSH key to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub
Adding key to ljishen@localhost authorized_keys...
Adding host node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us...
Deploying unmanaged mon service...
Deploying unmanaged mgr service...
You can access the Ceph CLI with:

	sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --fsid 9fe4309c-37f7-11ec-b51d-53e6e728d2d3 -c /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf -k /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring

Please consider enabling telemetry to help improve Ceph:

	ceph telemetry on

For more information see:

	https://docs.ceph.com/docs/pacific/mgr/telemetry/

Bootstrap complete.

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:03:01,235319949-07:00] INFO: Sleep for 20 seconds[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:03:21,242383975-07:00] INFO: > Set the default number of monitor daemons to 1[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:337 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:03:21,252887380-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:03:21,256829631-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mon', '1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1
Inferring fsid 9fe4309c-37f7-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/9fe4309c-37f7-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Scheduled mon update...

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:03:29,292164714-07:00] INFO: > Set the default number of manager daemons to 1[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:342 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:03:29,301596646-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:03:29,305251337-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mgr', '1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1
Inferring fsid 9fe4309c-37f7-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/9fe4309c-37f7-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Scheduled mgr update...


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:03:38,871374231-07:00] STAGE: Add hosts to the cluster...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:03:38,877979949-07:00] INFO: > Adding host 'sm1'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:361 - add_hosts() > ssh-copy-id -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -f -i /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub ljishen@sm1
/usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: "/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub"

Number of key(s) added: 1

Now try logging into the machine, with:   "ssh -o 'GlobalKnownHostsFile=/dev/null' -o 'LogLevel=ERROR' -o 'PasswordAuthentication=no' -o 'StrictHostKeyChecking=no' -o 'UserKnownHostsFile=/dev/null' 'ljishen@sm1'"
and check to make sure that only the key(s) you wanted were added.

# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:367 - add_hosts() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add sm1 10.10.2.5
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:03:40,019791911-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:03:40,023286039-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'host', 'add', 'sm1', '10.10.2.5'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add sm1 10.10.2.5
Inferring fsid 9fe4309c-37f7-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/9fe4309c-37f7-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Added host 'sm1' with addr '10.10.2.5'

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:03:50,749607431-07:00] INFO: Sleep for 20 seconds[0m


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:04:10,754720011-07:00] STAGE: Add OSDs to the cluster...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:04:10,761487172-07:00] INFO: > Adding devices ['/dev/nvme0n1'] on host 'sm1' as OSDs[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:388 - add_host_devices_as_osds() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd sm1:/dev/nvme0n1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:04:10,771296272-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:04:10,775333202-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'daemon', 'add', 'osd', 'sm1:/dev/nvme0n1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd sm1:/dev/nvme0n1
Inferring fsid 9fe4309c-37f7-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/9fe4309c-37f7-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Created osd(s) 0 on host 'sm1'

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:04:34,092132230-07:00] INFO: > Sleep for 20 seconds[0m


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:04:54,097655725-07:00] STAGE: Show Cluster Status[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:408 - show_status() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:04:54,108184498-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:04:54,111885416-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', '--status'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status
Inferring fsid 9fe4309c-37f7-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/9fe4309c-37f7-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
  cluster:
    id:     9fe4309c-37f7-11ec-b51d-53e6e728d2d3
    health: HEALTH_WARN
            1 pool(s) have no replicas configured
 
  services:
    mon: 1 daemons, quorum node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us (age 2m)
    mgr: node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us.inzued(active, since 2m)
    osd: 1 osds: 1 up (since 20s), 1 in (since 39s)
 
  data:
    pools:   1 pools, 1 pgs
    objects: 0 objects, 0 B
    usage:   4.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     1 active+clean
 

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:05:03,123855545-07:00] INFO: > Copy configuration files to local[0m
# ./benchmarks/bench-rados:220 - launch_ceph_cluster() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.1 sudo chown --changes --recursive ljishen /tmp/bench-rados/deployment_data_root/etc/ceph
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:05:03,131651647-07:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.1 sudo chown --changes --recursive ljishen /tmp/bench-rados/deployment_data_root/etc/ceph
[1] 07:05:03 [SUCCESS] ljishen@10.10.2.1
# ./benchmarks/bench-rados:223 - launch_ceph_cluster() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.1:/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf
# ./benchmarks/bench-rados:226 - launch_ceph_cluster() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.1:/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:05:03,609252696-07:00] STAGE: Update local ceph.conf as a client...[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:05:03,612234976-07:00] STAGE: Configure Ceph pool...[0m
# ./benchmarks/bench-rados:257 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_max_backfills 32
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:05:03,633623863-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:05:03,636468504-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '9fe4309c-37f7-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'config', 'set', 'osd', 'osd_max_backfills', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 9fe4309c-37f7-11ec-b51d-53e6e728d2d3 -- ceph config set osd osd_max_backfills 32
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:258 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_active 32
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:05:08,065428443-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:05:08,068879036-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '9fe4309c-37f7-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_active', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 9fe4309c-37f7-11ec-b51d-53e6e728d2d3 -- ceph config set osd osd_recovery_max_active 32
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:259 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_single_start 8
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:05:12,506404677-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:05:12,509517122-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '9fe4309c-37f7-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_single_start', '8'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 9fe4309c-37f7-11ec-b51d-53e6e728d2d3 -- ceph config set osd osd_recovery_max_single_start 8
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:260 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_op_priority 63
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:05:16,864232265-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:05:16,867389515-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '9fe4309c-37f7-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_op_priority', '63'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 9fe4309c-37f7-11ec-b51d-53e6e728d2d3 -- ceph config set osd osd_recovery_op_priority 63
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:262 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config show-with-defaults osd.0
# ./benchmarks/bench-rados:263 - configure_ceph_pool() > grep 'osd_max_backfills\|osd_recovery'
# ./benchmarks/bench-rados:263 - configure_ceph_pool() > column -t -s ' '
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
osd_max_backfills                        32         mon
osd_recovery_cost                        20971520   default
osd_recovery_delay_start                 0.000000   default
osd_recovery_max_active                  32         mon
osd_recovery_max_active_hdd              3          default
osd_recovery_max_active_ssd              10         default
osd_recovery_max_chunk                   8388608    default
osd_recovery_max_omap_entries_per_chunk  8096       default
osd_recovery_max_single_start            8          mon
osd_recovery_op_priority                 63         mon
osd_recovery_op_warn_multiple            16         default
osd_recovery_priority                    5          default
osd_recovery_retry_interval              30.000000  default
osd_recovery_sleep                       0.000000   default
osd_recovery_sleep_hdd                   0.100000   default
osd_recovery_sleep_hybrid                0.025000   default
osd_recovery_sleep_ssd                   0.000000   default
# ./benchmarks/bench-rados:265 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool create bench_rados 128 128 replicated --size 1 --pg-num-min 128
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:05:25,782503225-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:05:25,785667849-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '9fe4309c-37f7-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'create', 'bench_rados', '128', '128', 'replicated', '--size', '1', '--pg-num-min', '128'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 9fe4309c-37f7-11ec-b51d-53e6e728d2d3 -- ceph osd pool create bench_rados 128 128 replicated --size 1 --pg-num-min 128
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
pool 'bench_rados' created
# ./benchmarks/bench-rados:267 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados min_size 1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:05:31,354384865-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:05:31,357425074-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '9fe4309c-37f7-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'min_size', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 9fe4309c-37f7-11ec-b51d-53e6e728d2d3 -- ceph osd pool set bench_rados min_size 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 min_size to 1
# ./benchmarks/bench-rados:268 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados pg_autoscale_mode off
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:05:36,697984319-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:05:36,700873083-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '9fe4309c-37f7-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'pg_autoscale_mode', 'off'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 9fe4309c-37f7-11ec-b51d-53e6e728d2d3 -- ceph osd pool set bench_rados pg_autoscale_mode off
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 pg_autoscale_mode to off
# ./benchmarks/bench-rados:269 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool application enable bench_rados benchmark
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:05:41,155021782-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:05:41,158105303-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '9fe4309c-37f7-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'application', 'enable', 'bench_rados', 'benchmark'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 9fe4309c-37f7-11ec-b51d-53e6e728d2d3 -- ceph osd pool application enable bench_rados benchmark
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
enabled application 'benchmark' on pool 'bench_rados'
# ./benchmarks/bench-rados:270 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados noscrub 1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:05:46,112126603-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:05:46,115077094-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '9fe4309c-37f7-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'noscrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 9fe4309c-37f7-11ec-b51d-53e6e728d2d3 -- ceph osd pool set bench_rados noscrub 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 noscrub to 1
# ./benchmarks/bench-rados:271 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados nodeep-scrub 1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:05:50,572819605-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:05:50,575947339-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '9fe4309c-37f7-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'nodeep-scrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 9fe4309c-37f7-11ec-b51d-53e6e728d2d3 -- ceph osd pool set bench_rados nodeep-scrub 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 nodeep-scrub to 1
# ./benchmarks/bench-rados:272 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool ls detail
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:05:55,380943104-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:05:55,384086768-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '9fe4309c-37f7-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'ls', 'detail'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 9fe4309c-37f7-11ec-b51d-53e6e728d2d3 -- ceph osd pool ls detail
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
pool 1 'device_health_metrics' replicated size 1 min_size 1 crush_rule 0 object_hash rjenkins pg_num 64 pgp_num 64 autoscale_mode on last_change 20 lfor 0/0/16 flags hashpspool stripe_width 0 pg_num_min 1 application mgr_devicehealth
pool 2 'bench_rados' replicated size 1 min_size 1 crush_rule 0 object_hash rjenkins pg_num 128 pgp_num 128 autoscale_mode off last_change 21 flags hashpspool,noscrub,nodeep-scrub stripe_width 0 pg_num_min 128 application benchmark

# ./benchmarks/bench-rados:273 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd df tree
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:05:59,921738227-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:05:59,924611481-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '9fe4309c-37f7-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'df', 'tree'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 9fe4309c-37f7-11ec-b51d-53e6e728d2d3 -- ceph osd df tree
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
ID  CLASS  WEIGHT   REWEIGHT  SIZE     RAW USE  DATA     OMAP  META     AVAIL    %USE  VAR   PGS  STATUS  TYPE NAME    
-1         0.09769         -  100 GiB  5.7 MiB  176 KiB   0 B  5.6 MiB  100 GiB  0.01  1.00    -          root default 
-3         0.09769         -  100 GiB  5.7 MiB  176 KiB   0 B  5.6 MiB  100 GiB  0.01  1.00    -              host sm1 
 0    ssd  0.09769   1.00000  100 GiB  5.7 MiB  176 KiB   0 B  5.6 MiB  100 GiB  0.01  1.00  192      up          osd.0
                       TOTAL  100 GiB  5.7 MiB  176 KiB   0 B  5.6 MiB  100 GiB  0.01                                  
MIN/MAX VAR: 1.00/1.00  STDDEV: 0


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:06:04,265608649-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:06:28,535673406-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:06:37,834731979-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:06:47,366143349-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:06:47,373235554-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:06:56,722447732-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:06:56,729253688-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:07:05,924046165-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:07:05,931087034-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:07:15,262359908-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:07:15,269077536-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:07:24,703680304-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:07:24,710452035-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:07:24,715880966-07:00] INFO: > The cluster is idle now.[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:07:24,719148885-07:00] STAGE: Start benchmarking of mode write ...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:07:24,724856540-07:00] INFO: > Start system activity collection services[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:07:24,729568079-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:341 - rados_bench() > rm --force /mnt/sda8/ceph-research/scripts/output/sys_activity_4KB_write.dat.rados_bench_host.2
# ./benchmarks/bench-rados:342 - rados_bench() > sar_pid_rados_bench_host=380206
# ./benchmarks/bench-rados:342 - rados_bench() > S_TIME_FORMAT=ISO
# ./benchmarks/bench-rados:342 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/output/sys_activity_4KB_write.dat.rados_bench_host.2 2

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:07:24,735621436-07:00] INFO: >> for OSD host[0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:07:24,744338945-07:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.5 bash
10.10.2.5: b'92676\n'
[1] 07:07:25 [SUCCESS] ljishen@10.10.2.5
92676

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:07:25,858421495-07:00] INFO: > Run rados bench[0m
# ./benchmarks/bench-rados:370 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/output/rados_bench_4KB_write.log.2
## ./benchmarks/bench-rados:361 - rados_bench() > count_cpus 32-63,96-127
# ./benchmarks/bench-rados:361 - rados_bench() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 60 write --pool bench_rados -b 4096 -O 4096 --concurrent-ios 128 --show-time --write-object --write-omap --write-xattr --no-cleanup
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:07:25,878008978-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:07:25,880908031-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '9fe4309c-37f7-11ec-b51d-53e6e728d2d3', '--', 'rados', 'bench', '60', 'write', '--pool', 'bench_rados', '-b', '4096', '-O', '4096', '--concurrent-ios', '128', '--show-time', '--write-object', '--write-omap', '--write-xattr', '--no-cleanup'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 9fe4309c-37f7-11ec-b51d-53e6e728d2d3 -- rados bench 60 write --pool bench_rados -b 4096 -O 4096 --concurrent-ios 128 --show-time --write-object --write-omap --write-xattr --no-cleanup
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
hints = 1
2021-10-28T14:07:29.257337+0000 Maintaining 128 concurrent writes of 4096 bytes to objects of size 4096 for up to 60 seconds or 0 objects
2021-10-28T14:07:29.257350+0000 Object prefix: benchmark_data_node-2.bluefield2.ucsc-cmps10_7
2021-10-28T14:07:29.257877+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-28T14:07:29.257877+0000     0       0         0         0         0         0           -           0
2021-10-28T14:07:30.258015+0000     1     128      6142      6014   23.4899   23.4922   0.0214528   0.0210214
2021-10-28T14:07:31.258132+0000     2     128     12319     12191    23.808   24.1289   0.0213153   0.0208911
2021-10-28T14:07:32.258244+0000     3     128     18222     18094   23.5573   23.0586   0.0233292   0.0211331
2021-10-28T14:07:33.258321+0000     4     128     24014     23886   23.3238    22.625   0.0183706    0.021373
2021-10-28T14:07:34.258392+0000     5     127     29854     29727    23.222   22.8164   0.0218291   0.0214863
2021-10-28T14:07:35.258520+0000     6     128     35659     35531   23.1298   22.6719   0.0207768   0.0215709
2021-10-28T14:07:36.258636+0000     7     127     41354     41227   23.0038     22.25   0.0221388   0.0216995
2021-10-28T14:07:37.258717+0000     8     128     46930     46802   22.8503   21.7773   0.0214527   0.0218426
2021-10-28T14:07:38.258832+0000     9     128     52444     52316   22.7043   21.5391   0.0214576   0.0219905
2021-10-28T14:07:39.258899+0000    10     128     58099     57971   22.6427   22.0898   0.0223718   0.0220568
2021-10-28T14:07:40.259012+0000    11     128     63647     63519   22.5542   21.6719   0.0240718   0.0221425
2021-10-28T14:07:41.259082+0000    12     128     69168     69040   22.4718   21.5664   0.0210197   0.0222187
2021-10-28T14:07:42.259197+0000    13     127     74804     74677   22.4368   22.0195   0.0260578   0.0222631
2021-10-28T14:07:43.259318+0000    14     128     80435     80307   22.4048   21.9922   0.0216682   0.0222973
2021-10-28T14:07:44.259394+0000    15     127     86002     85875   22.3611     21.75   0.0242177   0.0223442
2021-10-28T14:07:45.259483+0000    16     128     91284     91156   22.2527   20.6289   0.0314783   0.0224437
2021-10-28T14:07:46.259607+0000    17     128     96244     96116   22.0833    19.375   0.0249333   0.0226071
2021-10-28T14:07:47.259722+0000    18     128    101568    101440   22.0117   20.7969   0.0285526   0.0226946
2021-10-28T14:07:48.259838+0000    19     128    107020    106892   21.9739   21.2969    0.035606   0.0227317
2021-10-28T14:07:49.259905+0000 min lat: 0.00814751 max lat: 0.0571287 avg lat: 0.0232374
2021-10-28T14:07:49.259905+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-28T14:07:49.259905+0000    20     128    110157    110029   21.4879   12.2539   0.0415936   0.0232374
2021-10-28T14:07:50.260036+0000    21     128    113292    113164   21.0477   12.2461   0.0438839   0.0237293
2021-10-28T14:07:51.260196+0000    22     128    116301    116173   20.6252   11.7539   0.0365569   0.0242137
2021-10-28T14:07:52.260319+0000    23     128    119180    119052   20.2173   11.2461   0.0489498   0.0247097
2021-10-28T14:07:53.260394+0000    24     128    122061    121933   19.8438   11.2539   0.0412064   0.0251666
2021-10-28T14:07:54.260465+0000    25     128    124940    124812   19.4999   11.2461   0.0453021   0.0256032
2021-10-28T14:07:55.260579+0000    26     128    127809    127681   19.1809    11.207   0.0451897   0.0260499
2021-10-28T14:07:56.260691+0000    27     128    130700    130572   18.8887    11.293   0.0501057   0.0264522
2021-10-28T14:07:57.260765+0000    28     127    135692    135565   18.9106   19.5039   0.0212979   0.0264283
2021-10-28T14:07:58.260878+0000    29     128    139762    139634   18.8065   15.8945   0.0234035    0.026571
2021-10-28T14:07:59.260950+0000    30     128    145284    145156   18.8986   21.5703   0.0247669   0.0264478
2021-10-28T14:08:00.261052+0000    31     128    150991    150863    19.008    22.293   0.0186383   0.0262919
2021-10-28T14:08:01.261123+0000    32     128    156553    156425   19.0929   21.7266      0.0251   0.0261723
2021-10-28T14:08:02.261241+0000    33     128    162107    161979   19.1717   21.6953   0.0193949   0.0260721
2021-10-28T14:08:03.261362+0000    34     128    167721    167593   19.2528   21.9297   0.0214967   0.0259585
2021-10-28T14:08:04.261430+0000    35     128    173336    173208   19.3293   21.9336   0.0189392   0.0258551
2021-10-28T14:08:05.261503+0000    36     128    178986    178858   19.4054   22.0703   0.0212982    0.025758
2021-10-28T14:08:06.261616+0000    37     128    184535    184407   19.4667   21.6758   0.0224729   0.0256728
2021-10-28T14:08:07.261731+0000    38     128    190155    190027   19.5321   21.9531   0.0200265   0.0255887
2021-10-28T14:08:08.261843+0000    39     128    195602    195474   19.5768   21.2773   0.0184538   0.0255323
2021-10-28T14:08:09.261914+0000 min lat: 0.00814751 max lat: 0.297139 avg lat: 0.0254623
2021-10-28T14:08:09.261914+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-28T14:08:09.261914+0000    40     128    201166    201038   19.6307   21.7344   0.0249021   0.0254623
2021-10-28T14:08:10.262044+0000    41     128    206666    206538   19.6758   21.4844   0.0220696   0.0254044
2021-10-28T14:08:11.262159+0000    42     128    212152    212024   19.7175   21.4297   0.0313966   0.0253479
2021-10-28T14:08:12.262274+0000    43     128    215502    215374   19.5632   13.0859   0.0337111   0.0255405
2021-10-28T14:08:13.262344+0000    44     128    217490    217362   19.2951   7.76562    0.125695   0.0258801
2021-10-28T14:08:14.262416+0000    45     128    218830    218702   18.9827   5.23438   0.0844682   0.0263064
2021-10-28T14:08:15.262531+0000    46     128    219922    219794   18.6627   4.26562    0.118081   0.0267564
2021-10-28T14:08:16.262645+0000    47     127    221211    221084   18.3728   5.03906    0.120605   0.0271713
2021-10-28T14:08:17.262716+0000    48     128    222414    222286   18.0879   4.69531    0.120802   0.0276124
2021-10-28T14:08:18.262831+0000    49     128    223468    223340   17.8027   4.11719    0.119573   0.0280461
2021-10-28T14:08:19.262897+0000    50     128    224590    224462   17.5343   4.38281    0.124105   0.0284805
2021-10-28T14:08:20.263006+0000    51     128    225607    225479   17.2684   3.97266    0.112765   0.0289338
2021-10-28T14:08:21.263075+0000    52     128    227534    227406   17.0811   7.52734   0.0369786   0.0292624
2021-10-28T14:08:22.263186+0000    53     128    230022    229894   16.9422   9.71875   0.0355631   0.0295033
2021-10-28T14:08:23.263298+0000    54     127    233427    233300   16.8748   13.3047   0.0403733   0.0296145
2021-10-28T14:08:24.263366+0000    55     128    236850    236722    16.811   13.3672   0.0403399   0.0297289
2021-10-28T14:08:25.263439+0000    56     128    240078    239950   16.7359   12.6094   0.0478574   0.0298636
2021-10-28T14:08:26.263557+0000    57     128    243406    243278   16.6704        13   0.0402332   0.0299827
2021-10-28T14:08:27.263680+0000    58     128    246734    246606   16.6071        13   0.0398265   0.0300961
2021-10-28T14:08:28.263797+0000    59     127    250090    249963   16.5478   13.1133   0.0406942   0.0302031
2021-10-28T14:08:29.263871+0000 min lat: 0.00814751 max lat: 0.325675 avg lat: 0.0303156
2021-10-28T14:08:29.263871+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-28T14:08:29.263871+0000    60     128    253386    253258   16.4865   12.8711   0.0343994   0.0303156
2021-10-28T14:08:30.264035+0000 Total time run:         60.0336
Total writes made:      253386
Write size:             4096
Object size:            4096
Bandwidth (MB/sec):     16.4873
Stddev Bandwidth:       6.45595
Max bandwidth (MB/sec): 24.1289
Min bandwidth (MB/sec): 3.97266
Average IOPS:           4220
Stddev IOPS:            1652.72
Max IOPS:               6177
Min IOPS:               1017
Average Latency(s):     0.0303202
Stddev Latency(s):      0.0202032
Max latency(s):         0.325675
Min latency(s):         0.00814751

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:08:31,024213249-07:00] INFO: > Stop system activity collection services[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:08:31,029406946-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:391 - rados_bench() > kill -INT 380206

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:08:31,034175533-07:00] INFO: >> for OSD host[0m
# ./benchmarks/bench-rados:396 - rados_bench() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.5 kill -INT 92676
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:08:31,041636002-07:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.5 kill -INT 92676
[1] 07:08:32 [SUCCESS] ljishen@10.10.2.5

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:08:32,252449581-07:00] INFO: >>> Copy system activity file from OSD host[0m
## ./benchmarks/bench-rados:403 - rados_bench() > basename -- /tmp/bench-rados/sys_activity_4KB_write.dat.osd_host.2
# ./benchmarks/bench-rados:403 - rados_bench() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.5:/tmp/bench-rados/sys_activity_4KB_write.dat.osd_host.2 /mnt/sda8/ceph-research/scripts/output/sys_activity_4KB_write.dat.osd_host.2


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:08:33,443198780-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:08:57,677188488-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 253.39k objects, 990 MiB
    usage:   3.2 GiB used, 97 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:08:57,684150197-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:09:07,176420254-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 253.39k objects, 990 MiB
    usage:   3.2 GiB used, 97 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:09:07,183911932-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:09:16,499295760-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 253.39k objects, 990 MiB
    usage:   3.2 GiB used, 97 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:09:16,505990907-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:09:25,857010210-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 253.39k objects, 990 MiB
    usage:   3.2 GiB used, 97 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:09:25,863853105-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:09:35,289636190-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 253.39k objects, 990 MiB
    usage:   3.2 GiB used, 97 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:09:35,296598250-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:09:35,302049913-07:00] INFO: > The cluster is idle now.[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:09:35,305499173-07:00] STAGE: Start benchmarking of mode seq ...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:09:35,311211939-07:00] INFO: > Start system activity collection services[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:09:35,316054946-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:341 - rados_bench() > rm --force /mnt/sda8/ceph-research/scripts/output/sys_activity_4KB_seq.dat.rados_bench_host.2
# ./benchmarks/bench-rados:342 - rados_bench() > sar_pid_rados_bench_host=382787
# ./benchmarks/bench-rados:342 - rados_bench() > S_TIME_FORMAT=ISO
# ./benchmarks/bench-rados:342 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/output/sys_activity_4KB_seq.dat.rados_bench_host.2 2

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:09:35,322357472-07:00] INFO: >> for OSD host[0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:09:35,331039844-07:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.5 bash
10.10.2.5: b'93232\n'
[1] 07:09:36 [SUCCESS] ljishen@10.10.2.5
93232

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:09:36,450701329-07:00] INFO: > Run rados bench[0m
# ./benchmarks/bench-rados:379 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/output/rados_bench_4KB_seq.log.2
## ./benchmarks/bench-rados:376 - rados_bench() > count_cpus 32-63,96-127
# ./benchmarks/bench-rados:376 - rados_bench() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 128 --show-time
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:09:36,469871024-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:09:36,472732567-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '9fe4309c-37f7-11ec-b51d-53e6e728d2d3', '--', 'rados', 'bench', '99999999', 'seq', '--pool', 'bench_rados', '--concurrent-ios', '128', '--show-time'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 9fe4309c-37f7-11ec-b51d-53e6e728d2d3 -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 128 --show-time
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
hints = 1
2021-10-28T14:09:39.760740+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-28T14:09:39.760740+0000     0       0         0         0         0         0           -           0
2021-10-28T14:09:40.760885+0000     1     128     17694     17566   68.6029   68.6172  0.00661984  0.00725527
2021-10-28T14:09:41.760979+0000     2     128     36167     36039    70.378   72.1602  0.00681371  0.00708612
2021-10-28T14:09:42.761100+0000     3     128     53934     53806     70.05   69.4023  0.00926617  0.00712259
2021-10-28T14:09:43.761219+0000     4     128     70776     70648   68.9828   65.7891  0.00707582  0.00723391
2021-10-28T14:09:44.761332+0000     5     128     88619     88491   69.1245   69.6992  0.00709695  0.00722388
2021-10-28T14:09:45.761419+0000     6     127    106580    106453   69.2968   70.1641  0.00898327  0.00720276
2021-10-28T14:09:46.761534+0000     7     128    122396    122268   68.2216   61.7773  0.00988191  0.00731692
2021-10-28T14:09:47.761641+0000     8     128    136051    135923   66.3607   53.3398   0.0079931   0.0075246
2021-10-28T14:09:48.761757+0000     9     128    152399    152271   66.0819   63.8594  0.00672277  0.00755742
2021-10-28T14:09:49.761859+0000    10     127    169826    169699   66.2808   68.0781  0.00627308  0.00753602
2021-10-28T14:09:50.761986+0000    11     127    187373    187246   66.4857    68.543  0.00643211  0.00751343
2021-10-28T14:09:51.762066+0000    12     128    204382    204254   66.4812   66.4375  0.00656963  0.00751334
2021-10-28T14:09:52.762194+0000    13     128    221694    221566   66.5686    67.625   0.0102247  0.00750336
2021-10-28T14:09:53.762293+0000    14     128    238285    238157   66.4424   64.8086  0.00826206  0.00751899
2021-10-28T14:09:54.762435+0000 Total time run:       14.8936
Total reads made:     253386
Read size:            4096
Object size:          4096
Bandwidth (MB/sec):   66.4571
Average IOPS:         17013
Stddev IOPS:          1193.03
Max IOPS:             18473
Min IOPS:             13655
Average Latency(s):   0.00751741
Max latency(s):       0.0446051
Min latency(s):       0.00100835

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:09:55,439644275-07:00] INFO: > Stop system activity collection services[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:09:55,444672771-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:391 - rados_bench() > kill -INT 382787

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:09:55,449973300-07:00] INFO: >> for OSD host[0m
# ./benchmarks/bench-rados:396 - rados_bench() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.5 kill -INT 93232
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:09:55,457814386-07:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.5 kill -INT 93232
[1] 07:09:56 [SUCCESS] ljishen@10.10.2.5

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:09:56,551812589-07:00] INFO: >>> Copy system activity file from OSD host[0m
## ./benchmarks/bench-rados:403 - rados_bench() > basename -- /tmp/bench-rados/sys_activity_4KB_seq.dat.osd_host.2
# ./benchmarks/bench-rados:403 - rados_bench() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.5:/tmp/bench-rados/sys_activity_4KB_seq.dat.osd_host.2 /mnt/sda8/ceph-research/scripts/output/sys_activity_4KB_seq.dat.osd_host.2


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:09:57,645107347-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:10:22,134802771-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 253.39k objects, 990 MiB
    usage:   3.2 GiB used, 97 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:10:22,142218346-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:10:31,599114919-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 253.39k objects, 990 MiB
    usage:   3.2 GiB used, 97 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:10:31,606232041-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:10:40,869438123-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 253.39k objects, 990 MiB
    usage:   3.2 GiB used, 97 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:10:40,876527362-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:10:50,371017278-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 253.39k objects, 990 MiB
    usage:   3.2 GiB used, 97 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:10:50,377872798-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:10:59,753776250-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 253.39k objects, 990 MiB
    usage:   3.2 GiB used, 97 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:10:59,760811307-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:10:59,766291465-07:00] INFO: > The cluster is idle now.[0m


[1;7;39;49m[2021-10-28T07:10:59,768317011-07:00][RUNNING][ROUND 3/1/21] object_size=4KB[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:10:59,771480673-07:00] STAGE: Launch a Ceph cluster (CEPH_HOST: 10.10.2.1, OSD_HOST: 10.10.2.5) ...[0m
# ./benchmarks/bench-rados:183 - launch_ceph_cluster() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.1 bash -euo pipefail
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:10:59,780671473-07:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.1 bash -euo pipefail
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T07:11:00,203977691-07:00] INFO: > Remove existing clusters\x1b[0m\n'
10.10.2.1: b'## bash:17 -  > basename -- /var/lib/ceph/9fe4309c-37f7-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'# bash:17 -  > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm --host 10.10.2.1 --host 10.10.2.5 rm-cluster --force --fsid 9fe4309c-37f7-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T07:11:00,214226137-07:00] DEBUG: HOST_PARAMS: ['--host', '10.10.2.1', '--host', '10.10.2.5']\x1b[0m\n"
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T07:11:00,217637250-07:00] DEBUG: CEPHADM_PARAMS: ['rm-cluster', '--force', '--fsid', '9fe4309c-37f7-11ec-b51d-53e6e728d2d3']\x1b[0m\n"
10.10.2.1: b"# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:63 -  > /tmp/bench-rados/ceph-research/scripts/parallel-ssh --host 10.10.2.1 --host 10.10.2.5 'sudo python3 -u - rm-cluster --force --fsid 9fe4309c-37f7-11ec-b51d-53e6e728d2d3'\n"
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T07:11:00,226382139-07:00] DEBUG: command from stdin\x1b[0m\n'
10.10.2.1: b"# /tmp/bench-rados/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host 10.10.2.1 --host 10.10.2.5 'sudo python3 -u - rm-cluster --force --fsid 9fe4309c-37f7-11ec-b51d-53e6e728d2d3'\n"
10.10.2.1: b'[1] 07:11:05 [SUCCESS] 10.10.2.1\n[2] 07:11:09 [SUCCESS] 10.10.2.5\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T07:11:09,283618921-07:00] INFO: > Deploy a new cluster\x1b[0m\n'
10.10.2.1: b'# bash:24 -  > /tmp/bench-rados/ceph-research/scripts/ceph-deploy -m 10.10.2.1 -o sm1/10.10.2.5:/dev/nvme0n1\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T07:11:09,297275113-07:00] STAGE: Verify passwordless SSH to hosts, passwordless sudo via ljishen, and the existence of devices on hosts...\x1b[0m\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T07:11:09,302102437-07:00] INFO: > Check the host for the monitor\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:181 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@10.10.2.1 sudo --non-interactive --validate\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T07:11:09,451994171-07:00] INFO: > Check hosts for OSDs\x1b[0m\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T07:11:09,457223401-07:00] INFO: >> Check host 'sm1'\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:200 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 sudo --non-interactive --validate\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T07:11:10,514885370-07:00] INFO: >>> Check device '/dev/nvme0n1'\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:215 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 test -b /dev/nvme0n1\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:226 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 lsblk --nodeps --noheadings --output TYPE /dev/nvme0n1\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:227 - verify_parameters() > grep --word-regexp disk\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T07:11:12,640072469-07:00] STAGE: Reset devices on hosts...\x1b[0m\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T07:11:12,644995654-07:00] INFO: > Resetting device '/dev/nvme0n1' on host 'sm1'\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:288 - reset_host_devices() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 sudo sh\n'
10.10.2.1: b'  Removing ceph--133e526c--137f--42c9--b84c--9c7a10418f39-osd--block--654dd94b--3569--41d1--9090--573b4de70f4f (252:0)\n'
10.10.2.1: b'  Archiving volume group "ceph-133e526c-137f-42c9-b84c-9c7a10418f39" metadata (seqno 5).\n'
10.10.2.1: b'  Releasing logical volume "osd-block-654dd94b-3569-41d1-9090-573b4de70f4f"\n'
10.10.2.1: b'  Creating volume group backup "/etc/lvm/backup/ceph-133e526c-137f-42c9-b84c-9c7a10418f39" (seqno 6).\n'
10.10.2.1: b'  Logical volume "osd-block-654dd94b-3569-41d1-9090-573b4de70f4f" successfully removed\n'
10.10.2.1: b'  Removing physical volume "/dev/nvme0n1" from volume group "ceph-133e526c-137f-42c9-b84c-9c7a10418f39"\n'
10.10.2.1: b'  Volume group "ceph-133e526c-137f-42c9-b84c-9c7a10418f39" successfully removed\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T07:11:14,955596003-07:00] STAGE: Bootstrap a new cluster...\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:317 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.1 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T07:11:14,965467561-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T07:11:14,969082347-07:00] DEBUG: CEPHADM_PARAMS: ['bootstrap', '--config', '/tmp/bench-rados/ceph-research/scripts/ceph.conf', '--mon-ip', '10.10.2.1', '--output-dir', '/tmp/bench-rados/deployment_data_root/etc/ceph', '--ssh-user', 'ljishen', '--skip-dashboard', '--no-minimize-config', '--allow-overwrite', '--allow-fqdn-hostname', '--orphan-initial-daemons', '--skip-monitoring-stack', '--single-host-defaults']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.1 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults\n'
10.10.2.1: b'Verifying podman|docker is present...\n'
10.10.2.1: b'Verifying lvm2 is present...\nVerifying time synchronization is in place...\n'
10.10.2.1: b'Unit ntp.service is enabled and running\n'
10.10.2.1: b'Repeating the final host check...\n'
10.10.2.1: b'podman|docker (/usr/bin/docker) is present\n'
10.10.2.1: b'systemctl is present\n'
10.10.2.1: b'lvcreate is present\n'
10.10.2.1: b'Unit ntp.service is enabled and running\n'
10.10.2.1: b'Host looks OK\n'
10.10.2.1: b'Cluster fsid: e9941c06-37f8-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Verifying IP 10.10.2.1 port 3300 ...\n'
10.10.2.1: b'Verifying IP 10.10.2.1 port 6789 ...\n'
10.10.2.1: b'Mon IP `10.10.2.1` is in CIDR network `10.10.2.0/24`\n'
10.10.2.1: b'- internal network (--cluster-network) has not been provided, OSD replication will default to the public_network\n'
10.10.2.1: b'Adjusting default settings to suit single-host cluster...\n'
10.10.2.1: b'Pulling container image quay.io/ceph/ceph:v16...\n'
10.10.2.1: b'Ceph version: ceph version 16.2.6 (ee28fb57e47e9f88813e24bbf4c14496ca299d31) pacific (stable)\n'
10.10.2.1: b'Extracting ceph user uid/gid from container image...\n'
10.10.2.1: b'Creating initial keys...\n'
10.10.2.1: b'Creating initial monmap...\n'
10.10.2.1: b'Creating mon...\n'
10.10.2.1: b'Waiting for mon to start...\n'
10.10.2.1: b'Waiting for mon...\n'
10.10.2.1: b'mon is available\nSetting mon public_network to 10.10.2.0/24\n'
10.10.2.1: b'Wrote config to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf\n'
10.10.2.1: b'Wrote keyring to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring\n'
10.10.2.1: b'Creating mgr...\n'
10.10.2.1: b'Verifying port 9283 ...\n'
10.10.2.1: b'Waiting for mgr to start...\nWaiting for mgr...\n'
10.10.2.1: b'mgr not available, waiting (1/15)...\n'
10.10.2.1: b'mgr not available, waiting (2/15)...\n'
10.10.2.1: b'mgr is available\n'
10.10.2.1: b'Enabling cephadm module...\n'
10.10.2.1: b'Waiting for the mgr to restart...\n'
10.10.2.1: b'Waiting for mgr epoch 5...\n'
10.10.2.1: b'mgr epoch 5 is available\n'
10.10.2.1: b'Setting orchestrator backend to cephadm...\n'
10.10.2.1: b'Generating ssh key...\n'
10.10.2.1: b'Wrote public SSH key to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub\n'
10.10.2.1: b'Adding key to ljishen@localhost authorized_keys...\n'
10.10.2.1: b'Adding host node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us...\n'
10.10.2.1: b'Deploying unmanaged mon service...\n'
10.10.2.1: b'Deploying unmanaged mgr service...\n'
10.10.2.1: b'You can access the Ceph CLI with:\n\n\tsudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --fsid e9941c06-37f8-11ec-b51d-53e6e728d2d3 -c /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf -k /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring\n\nPlease consider enabling telemetry to help improve Ceph:\n\n\tceph telemetry on\n\nFor more information see:\n\n\thttps://docs.ceph.com/docs/pacific/mgr/telemetry/\n\n'
10.10.2.1: b'Bootstrap complete.\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T07:12:16,331503431-07:00] INFO: Sleep for 20 seconds\x1b[0m\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T07:12:36,339126168-07:00] INFO: > Set the default number of monitor daemons to 1\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:337 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T07:12:36,349470445-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T07:12:36,353076323-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mon', '1']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1\n'
10.10.2.1: b'Inferring fsid e9941c06-37f8-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/e9941c06-37f8-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b'Scheduled mon update...\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T07:12:45,496905192-07:00] INFO: > Set the default number of manager daemons to 1\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:342 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T07:12:45,506769105-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T07:12:45,510497755-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mgr', '1']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1\n'
10.10.2.1: b'Inferring fsid e9941c06-37f8-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/e9941c06-37f8-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b'Scheduled mgr update...\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T07:12:55,172476907-07:00] STAGE: Add hosts to the cluster...\x1b[0m\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T07:12:55,178610207-07:00] INFO: > Adding host 'sm1'\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:361 - add_hosts() > ssh-copy-id -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -f -i /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub ljishen@sm1\n'
10.10.2.1: b'/usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: "/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub"\n'
10.10.2.1: b'\nNumber of key(s) added: 1\n\nNow try logging into the machine, with:   "ssh -o \'GlobalKnownHostsFile=/dev/null\' -o \'LogLevel=ERROR\' -o \'PasswordAuthentication=no\' -o \'StrictHostKeyChecking=no\' -o \'UserKnownHostsFile=/dev/null\' \'ljishen@sm1\'"\nand check to make sure that only the key(s) you wanted were added.\n\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:367 - add_hosts() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add sm1 10.10.2.5\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T07:12:56,320113674-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T07:12:56,323538322-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'host', 'add', 'sm1', '10.10.2.5']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add sm1 10.10.2.5\n'
10.10.2.1: b'Inferring fsid e9941c06-37f8-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/e9941c06-37f8-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b"Added host 'sm1' with addr '10.10.2.5'\n"
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T07:13:07,389973178-07:00] INFO: Sleep for 20 seconds\x1b[0m\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T07:13:27,394894119-07:00] STAGE: Add OSDs to the cluster...\x1b[0m\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T07:13:27,401642345-07:00] INFO: > Adding devices ['/dev/nvme0n1'] on host 'sm1' as OSDs\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:388 - add_host_devices_as_osds() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd sm1:/dev/nvme0n1\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T07:13:27,412299891-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T07:13:27,416007891-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'daemon', 'add', 'osd', 'sm1:/dev/nvme0n1']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd sm1:/dev/nvme0n1\n'
10.10.2.1: b'Inferring fsid e9941c06-37f8-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/e9941c06-37f8-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b"Created osd(s) 0 on host 'sm1'\n"
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T07:13:50,525807441-07:00] INFO: > Sleep for 20 seconds\x1b[0m\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T07:14:10,531013582-07:00] STAGE: Show Cluster Status\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:408 - show_status() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T07:14:10,541129238-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T07:14:10,544637673-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', '--status']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status\n'
10.10.2.1: b'Inferring fsid e9941c06-37f8-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/e9941c06-37f8-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b'  cluster:\n    id:     e9941c06-37f8-11ec-b51d-53e6e728d2d3\n    health: HEALTH_WARN\n            1 pool(s) have no replicas configured\n \n  services:\n    mon: 1 daemons, quorum node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us (age 2m)\n    mgr: node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us.ienqyh(active, since 2m)\n    osd: 1 osds: 1 up (since 20s), 1 in (since 39s)\n \n  data:\n    pools:   1 pools, 1 pgs\n    objects: 0 objects, 0 B\n    usage:   4.7 MiB used, 100 GiB / 100 GiB avail\n    pgs:     1 active+clean\n \n'
[1] 07:14:19 [SUCCESS] ljishen@10.10.2.1

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:11:00,203977691-07:00] INFO: > Remove existing clusters[0m
## bash:17 -  > basename -- /var/lib/ceph/9fe4309c-37f7-11ec-b51d-53e6e728d2d3
# bash:17 -  > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm --host 10.10.2.1 --host 10.10.2.5 rm-cluster --force --fsid 9fe4309c-37f7-11ec-b51d-53e6e728d2d3
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:11:00,214226137-07:00] DEBUG: HOST_PARAMS: ['--host', '10.10.2.1', '--host', '10.10.2.5'][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:11:00,217637250-07:00] DEBUG: CEPHADM_PARAMS: ['rm-cluster', '--force', '--fsid', '9fe4309c-37f7-11ec-b51d-53e6e728d2d3'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:63 -  > /tmp/bench-rados/ceph-research/scripts/parallel-ssh --host 10.10.2.1 --host 10.10.2.5 'sudo python3 -u - rm-cluster --force --fsid 9fe4309c-37f7-11ec-b51d-53e6e728d2d3'
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:11:00,226382139-07:00] DEBUG: command from stdin[0m
# /tmp/bench-rados/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host 10.10.2.1 --host 10.10.2.5 'sudo python3 -u - rm-cluster --force --fsid 9fe4309c-37f7-11ec-b51d-53e6e728d2d3'
[1] 07:11:05 [SUCCESS] 10.10.2.1
[2] 07:11:09 [SUCCESS] 10.10.2.5

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:11:09,283618921-07:00] INFO: > Deploy a new cluster[0m
# bash:24 -  > /tmp/bench-rados/ceph-research/scripts/ceph-deploy -m 10.10.2.1 -o sm1/10.10.2.5:/dev/nvme0n1


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:11:09,297275113-07:00] STAGE: Verify passwordless SSH to hosts, passwordless sudo via ljishen, and the existence of devices on hosts...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:11:09,302102437-07:00] INFO: > Check the host for the monitor[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:181 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@10.10.2.1 sudo --non-interactive --validate

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:11:09,451994171-07:00] INFO: > Check hosts for OSDs[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:11:09,457223401-07:00] INFO: >> Check host 'sm1'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:200 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 sudo --non-interactive --validate

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:11:10,514885370-07:00] INFO: >>> Check device '/dev/nvme0n1'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:215 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 test -b /dev/nvme0n1
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:226 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 lsblk --nodeps --noheadings --output TYPE /dev/nvme0n1
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:227 - verify_parameters() > grep --word-regexp disk


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:11:12,640072469-07:00] STAGE: Reset devices on hosts...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:11:12,644995654-07:00] INFO: > Resetting device '/dev/nvme0n1' on host 'sm1'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:288 - reset_host_devices() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 sudo sh
  Removing ceph--133e526c--137f--42c9--b84c--9c7a10418f39-osd--block--654dd94b--3569--41d1--9090--573b4de70f4f (252:0)
  Archiving volume group "ceph-133e526c-137f-42c9-b84c-9c7a10418f39" metadata (seqno 5).
  Releasing logical volume "osd-block-654dd94b-3569-41d1-9090-573b4de70f4f"
  Creating volume group backup "/etc/lvm/backup/ceph-133e526c-137f-42c9-b84c-9c7a10418f39" (seqno 6).
  Logical volume "osd-block-654dd94b-3569-41d1-9090-573b4de70f4f" successfully removed
  Removing physical volume "/dev/nvme0n1" from volume group "ceph-133e526c-137f-42c9-b84c-9c7a10418f39"
  Volume group "ceph-133e526c-137f-42c9-b84c-9c7a10418f39" successfully removed


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:11:14,955596003-07:00] STAGE: Bootstrap a new cluster...[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:317 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.1 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:11:14,965467561-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:11:14,969082347-07:00] DEBUG: CEPHADM_PARAMS: ['bootstrap', '--config', '/tmp/bench-rados/ceph-research/scripts/ceph.conf', '--mon-ip', '10.10.2.1', '--output-dir', '/tmp/bench-rados/deployment_data_root/etc/ceph', '--ssh-user', 'ljishen', '--skip-dashboard', '--no-minimize-config', '--allow-overwrite', '--allow-fqdn-hostname', '--orphan-initial-daemons', '--skip-monitoring-stack', '--single-host-defaults'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.1 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults
Verifying podman|docker is present...
Verifying lvm2 is present...
Verifying time synchronization is in place...
Unit ntp.service is enabled and running
Repeating the final host check...
podman|docker (/usr/bin/docker) is present
systemctl is present
lvcreate is present
Unit ntp.service is enabled and running
Host looks OK
Cluster fsid: e9941c06-37f8-11ec-b51d-53e6e728d2d3
Verifying IP 10.10.2.1 port 3300 ...
Verifying IP 10.10.2.1 port 6789 ...
Mon IP `10.10.2.1` is in CIDR network `10.10.2.0/24`
- internal network (--cluster-network) has not been provided, OSD replication will default to the public_network
Adjusting default settings to suit single-host cluster...
Pulling container image quay.io/ceph/ceph:v16...
Ceph version: ceph version 16.2.6 (ee28fb57e47e9f88813e24bbf4c14496ca299d31) pacific (stable)
Extracting ceph user uid/gid from container image...
Creating initial keys...
Creating initial monmap...
Creating mon...
Waiting for mon to start...
Waiting for mon...
mon is available
Setting mon public_network to 10.10.2.0/24
Wrote config to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf
Wrote keyring to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring
Creating mgr...
Verifying port 9283 ...
Waiting for mgr to start...
Waiting for mgr...
mgr not available, waiting (1/15)...
mgr not available, waiting (2/15)...
mgr is available
Enabling cephadm module...
Waiting for the mgr to restart...
Waiting for mgr epoch 5...
mgr epoch 5 is available
Setting orchestrator backend to cephadm...
Generating ssh key...
Wrote public SSH key to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub
Adding key to ljishen@localhost authorized_keys...
Adding host node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us...
Deploying unmanaged mon service...
Deploying unmanaged mgr service...
You can access the Ceph CLI with:

	sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --fsid e9941c06-37f8-11ec-b51d-53e6e728d2d3 -c /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf -k /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring

Please consider enabling telemetry to help improve Ceph:

	ceph telemetry on

For more information see:

	https://docs.ceph.com/docs/pacific/mgr/telemetry/

Bootstrap complete.

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:12:16,331503431-07:00] INFO: Sleep for 20 seconds[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:12:36,339126168-07:00] INFO: > Set the default number of monitor daemons to 1[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:337 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:12:36,349470445-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:12:36,353076323-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mon', '1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1
Inferring fsid e9941c06-37f8-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/e9941c06-37f8-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Scheduled mon update...

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:12:45,496905192-07:00] INFO: > Set the default number of manager daemons to 1[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:342 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:12:45,506769105-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:12:45,510497755-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mgr', '1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1
Inferring fsid e9941c06-37f8-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/e9941c06-37f8-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Scheduled mgr update...


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:12:55,172476907-07:00] STAGE: Add hosts to the cluster...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:12:55,178610207-07:00] INFO: > Adding host 'sm1'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:361 - add_hosts() > ssh-copy-id -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -f -i /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub ljishen@sm1
/usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: "/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub"

Number of key(s) added: 1

Now try logging into the machine, with:   "ssh -o 'GlobalKnownHostsFile=/dev/null' -o 'LogLevel=ERROR' -o 'PasswordAuthentication=no' -o 'StrictHostKeyChecking=no' -o 'UserKnownHostsFile=/dev/null' 'ljishen@sm1'"
and check to make sure that only the key(s) you wanted were added.

# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:367 - add_hosts() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add sm1 10.10.2.5
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:12:56,320113674-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:12:56,323538322-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'host', 'add', 'sm1', '10.10.2.5'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add sm1 10.10.2.5
Inferring fsid e9941c06-37f8-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/e9941c06-37f8-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Added host 'sm1' with addr '10.10.2.5'

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:13:07,389973178-07:00] INFO: Sleep for 20 seconds[0m


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:13:27,394894119-07:00] STAGE: Add OSDs to the cluster...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:13:27,401642345-07:00] INFO: > Adding devices ['/dev/nvme0n1'] on host 'sm1' as OSDs[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:388 - add_host_devices_as_osds() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd sm1:/dev/nvme0n1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:13:27,412299891-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:13:27,416007891-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'daemon', 'add', 'osd', 'sm1:/dev/nvme0n1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd sm1:/dev/nvme0n1
Inferring fsid e9941c06-37f8-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/e9941c06-37f8-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Created osd(s) 0 on host 'sm1'

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:13:50,525807441-07:00] INFO: > Sleep for 20 seconds[0m


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:14:10,531013582-07:00] STAGE: Show Cluster Status[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:408 - show_status() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:14:10,541129238-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:14:10,544637673-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', '--status'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status
Inferring fsid e9941c06-37f8-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/e9941c06-37f8-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
  cluster:
    id:     e9941c06-37f8-11ec-b51d-53e6e728d2d3
    health: HEALTH_WARN
            1 pool(s) have no replicas configured
 
  services:
    mon: 1 daemons, quorum node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us (age 2m)
    mgr: node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us.ienqyh(active, since 2m)
    osd: 1 osds: 1 up (since 20s), 1 in (since 39s)
 
  data:
    pools:   1 pools, 1 pgs
    objects: 0 objects, 0 B
    usage:   4.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     1 active+clean
 

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:14:19,532502000-07:00] INFO: > Copy configuration files to local[0m
# ./benchmarks/bench-rados:220 - launch_ceph_cluster() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.1 sudo chown --changes --recursive ljishen /tmp/bench-rados/deployment_data_root/etc/ceph
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:14:19,540074651-07:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.1 sudo chown --changes --recursive ljishen /tmp/bench-rados/deployment_data_root/etc/ceph
[1] 07:14:19 [SUCCESS] ljishen@10.10.2.1
# ./benchmarks/bench-rados:223 - launch_ceph_cluster() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.1:/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf
# ./benchmarks/bench-rados:226 - launch_ceph_cluster() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.1:/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:14:20,016794473-07:00] STAGE: Update local ceph.conf as a client...[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:14:20,019814244-07:00] STAGE: Configure Ceph pool...[0m
# ./benchmarks/bench-rados:257 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_max_backfills 32
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:14:20,041053819-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:14:20,043944667-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'e9941c06-37f8-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'config', 'set', 'osd', 'osd_max_backfills', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid e9941c06-37f8-11ec-b51d-53e6e728d2d3 -- ceph config set osd osd_max_backfills 32
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:258 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_active 32
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:14:24,355054776-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:14:24,358200584-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'e9941c06-37f8-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_active', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid e9941c06-37f8-11ec-b51d-53e6e728d2d3 -- ceph config set osd osd_recovery_max_active 32
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:259 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_single_start 8
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:14:28,663340172-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:14:28,666426278-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'e9941c06-37f8-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_single_start', '8'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid e9941c06-37f8-11ec-b51d-53e6e728d2d3 -- ceph config set osd osd_recovery_max_single_start 8
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:260 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_op_priority 63
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:14:33,173137675-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:14:33,176408950-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'e9941c06-37f8-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_op_priority', '63'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid e9941c06-37f8-11ec-b51d-53e6e728d2d3 -- ceph config set osd osd_recovery_op_priority 63
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:262 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config show-with-defaults osd.0
# ./benchmarks/bench-rados:263 - configure_ceph_pool() > grep 'osd_max_backfills\|osd_recovery'
# ./benchmarks/bench-rados:263 - configure_ceph_pool() > column -t -s ' '
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
osd_max_backfills                        32         mon
osd_recovery_cost                        20971520   default
osd_recovery_delay_start                 0.000000   default
osd_recovery_max_active                  32         mon
osd_recovery_max_active_hdd              3          default
osd_recovery_max_active_ssd              10         default
osd_recovery_max_chunk                   8388608    default
osd_recovery_max_omap_entries_per_chunk  8096       default
osd_recovery_max_single_start            8          mon
osd_recovery_op_priority                 63         mon
osd_recovery_op_warn_multiple            16         default
osd_recovery_priority                    5          default
osd_recovery_retry_interval              30.000000  default
osd_recovery_sleep                       0.000000   default
osd_recovery_sleep_hdd                   0.100000   default
osd_recovery_sleep_hybrid                0.025000   default
osd_recovery_sleep_ssd                   0.000000   default
# ./benchmarks/bench-rados:265 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool create bench_rados 128 128 replicated --size 1 --pg-num-min 128
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:14:42,039948382-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:14:42,042815174-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'e9941c06-37f8-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'create', 'bench_rados', '128', '128', 'replicated', '--size', '1', '--pg-num-min', '128'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid e9941c06-37f8-11ec-b51d-53e6e728d2d3 -- ceph osd pool create bench_rados 128 128 replicated --size 1 --pg-num-min 128
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
pool 'bench_rados' created
# ./benchmarks/bench-rados:267 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados min_size 1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:14:47,082627583-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:14:47,085504495-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'e9941c06-37f8-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'min_size', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid e9941c06-37f8-11ec-b51d-53e6e728d2d3 -- ceph osd pool set bench_rados min_size 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 min_size to 1
# ./benchmarks/bench-rados:268 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados pg_autoscale_mode off
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:14:51,391475099-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:14:51,394394671-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'e9941c06-37f8-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'pg_autoscale_mode', 'off'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid e9941c06-37f8-11ec-b51d-53e6e728d2d3 -- ceph osd pool set bench_rados pg_autoscale_mode off
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 pg_autoscale_mode to off
# ./benchmarks/bench-rados:269 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool application enable bench_rados benchmark
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:14:56,499759293-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:14:56,502727788-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'e9941c06-37f8-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'application', 'enable', 'bench_rados', 'benchmark'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid e9941c06-37f8-11ec-b51d-53e6e728d2d3 -- ceph osd pool application enable bench_rados benchmark
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
enabled application 'benchmark' on pool 'bench_rados'
# ./benchmarks/bench-rados:270 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados noscrub 1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:15:00,894457624-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:15:00,897627868-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'e9941c06-37f8-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'noscrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid e9941c06-37f8-11ec-b51d-53e6e728d2d3 -- ceph osd pool set bench_rados noscrub 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 noscrub to 1
# ./benchmarks/bench-rados:271 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados nodeep-scrub 1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:15:06,559756635-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:15:06,562769764-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'e9941c06-37f8-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'nodeep-scrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid e9941c06-37f8-11ec-b51d-53e6e728d2d3 -- ceph osd pool set bench_rados nodeep-scrub 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 nodeep-scrub to 1
# ./benchmarks/bench-rados:272 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool ls detail
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:15:10,886420006-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:15:10,889437272-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'e9941c06-37f8-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'ls', 'detail'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid e9941c06-37f8-11ec-b51d-53e6e728d2d3 -- ceph osd pool ls detail
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
pool 1 'device_health_metrics' replicated size 1 min_size 1 crush_rule 0 object_hash rjenkins pg_num 64 pgp_num 64 autoscale_mode on last_change 18 lfor 0/0/15 flags hashpspool stripe_width 0 pg_num_min 1 application mgr_devicehealth
pool 2 'bench_rados' replicated size 1 min_size 1 crush_rule 0 object_hash rjenkins pg_num 128 pgp_num 128 autoscale_mode off last_change 20 flags hashpspool,noscrub,nodeep-scrub stripe_width 0 pg_num_min 128 application benchmark

# ./benchmarks/bench-rados:273 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd df tree
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:15:15,036664707-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:15:15,039735854-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'e9941c06-37f8-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'df', 'tree'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid e9941c06-37f8-11ec-b51d-53e6e728d2d3 -- ceph osd df tree
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
ID  CLASS  WEIGHT   REWEIGHT  SIZE     RAW USE  DATA     OMAP  META     AVAIL    %USE  VAR   PGS  STATUS  TYPE NAME    
-1         0.09769         -  100 GiB  5.7 MiB  168 KiB   0 B  5.6 MiB  100 GiB  0.01  1.00    -          root default 
-3         0.09769         -  100 GiB  5.7 MiB  168 KiB   0 B  5.6 MiB  100 GiB  0.01  1.00    -              host sm1 
 0    ssd  0.09769   1.00000  100 GiB  5.7 MiB  168 KiB   0 B  5.6 MiB  100 GiB  0.01  1.00  192      up          osd.0
                       TOTAL  100 GiB  5.7 MiB  168 KiB   0 B  5.6 MiB  100 GiB  0.01                                  
MIN/MAX VAR: 1.00/1.00  STDDEV: 0


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:15:19,129490413-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:15:43,420089190-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:15:52,693177835-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:16:02,034041703-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:16:02,041360886-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:16:11,391401868-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:16:11,398184229-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:16:20,785255996-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:16:20,792122376-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:16:30,051904778-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:16:30,058874943-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:16:39,367316243-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:16:39,374418096-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:16:39,380013792-07:00] INFO: > The cluster is idle now.[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:16:39,383210305-07:00] STAGE: Start benchmarking of mode write ...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:16:39,389033098-07:00] INFO: > Start system activity collection services[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:16:39,393929405-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:341 - rados_bench() > rm --force /mnt/sda8/ceph-research/scripts/output/sys_activity_4KB_write.dat.rados_bench_host.3
# ./benchmarks/bench-rados:342 - rados_bench() > sar_pid_rados_bench_host=389447
# ./benchmarks/bench-rados:342 - rados_bench() > S_TIME_FORMAT=ISO
# ./benchmarks/bench-rados:342 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/output/sys_activity_4KB_write.dat.rados_bench_host.3 2

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:16:39,400111976-07:00] INFO: >> for OSD host[0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:16:39,408697866-07:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.5 bash
10.10.2.5: b'98153\n'
[1] 07:16:40 [SUCCESS] ljishen@10.10.2.5
98153

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:16:40,510777561-07:00] INFO: > Run rados bench[0m
# ./benchmarks/bench-rados:370 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/output/rados_bench_4KB_write.log.3
## ./benchmarks/bench-rados:361 - rados_bench() > count_cpus 32-63,96-127
# ./benchmarks/bench-rados:361 - rados_bench() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 60 write --pool bench_rados -b 4096 -O 4096 --concurrent-ios 128 --show-time --write-object --write-omap --write-xattr --no-cleanup
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:16:40,530250989-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:16:40,533230083-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'e9941c06-37f8-11ec-b51d-53e6e728d2d3', '--', 'rados', 'bench', '60', 'write', '--pool', 'bench_rados', '-b', '4096', '-O', '4096', '--concurrent-ios', '128', '--show-time', '--write-object', '--write-omap', '--write-xattr', '--no-cleanup'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid e9941c06-37f8-11ec-b51d-53e6e728d2d3 -- rados bench 60 write --pool bench_rados -b 4096 -O 4096 --concurrent-ios 128 --show-time --write-object --write-omap --write-xattr --no-cleanup
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
hints = 1
2021-10-28T14:16:43.861225+0000 Maintaining 128 concurrent writes of 4096 bytes to objects of size 4096 for up to 60 seconds or 0 objects
2021-10-28T14:16:43.861237+0000 Object prefix: benchmark_data_node-2.bluefield2.ucsc-cmps10_7
2021-10-28T14:16:43.861751+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-28T14:16:43.861751+0000     0       0         0         0         0         0           -           0
2021-10-28T14:16:44.861870+0000     1     128      6301      6173   24.1119   24.1133   0.0199651   0.0204892
2021-10-28T14:16:45.861948+0000     2     128     12449     12321   24.0628   24.0156   0.0207482   0.0206617
2021-10-28T14:16:46.862024+0000     3     128     18486     18358    23.902    23.582   0.0229156   0.0208229
2021-10-28T14:16:47.862098+0000     4     128     24372     24244   23.6741   22.9922   0.0245917   0.0210392
2021-10-28T14:16:48.862169+0000     5     128     30381     30253   23.6335   23.4727   0.0188472   0.0211131
2021-10-28T14:16:49.862254+0000     6     128     36250     36122   23.5152   22.9258   0.0221974   0.0212334
2021-10-28T14:16:50.862349+0000     7     128     42013     41885   23.3715   22.5117   0.0228805     0.02135
2021-10-28T14:16:51.862425+0000     8     127     47379     47252   23.0705   20.9648   0.0213188   0.0216434
2021-10-28T14:16:52.862513+0000     9     128     53151     53023   23.0117    22.543   0.0258777   0.0216925
2021-10-28T14:16:53.862582+0000    10     128     58845     58717   22.9346   22.2422    0.015963   0.0217721
2021-10-28T14:16:54.862661+0000    11     128     64609     64481   22.8963   22.5156   0.0234014   0.0218069
2021-10-28T14:16:55.862741+0000    12     128     70345     70217   22.8553   22.4062   0.0228373   0.0218584
2021-10-28T14:16:56.862818+0000    13     127     75939     75812   22.7783   21.8555   0.0260463   0.0219338
2021-10-28T14:16:57.862892+0000    14     128     81392     81264   22.6724   21.2969   0.0220743   0.0220305
2021-10-28T14:16:58.862960+0000    15     128     87004     86876   22.6222   21.9219   0.0240366    0.022078
2021-10-28T14:16:59.863038+0000    16     128     92532     92404   22.5578   21.5938   0.0218214   0.0221462
2021-10-28T14:17:00.863127+0000    17     128     98090     97962   22.5079   21.7109    0.022064   0.0222035
2021-10-28T14:17:01.863207+0000    18     128    103189    103061   22.3639    19.918   0.0255907   0.0223408
2021-10-28T14:17:02.863286+0000    19     128    107810    107682   22.1369   18.0508   0.0331851   0.0225697
2021-10-28T14:17:03.863354+0000 min lat: 0.00529007 max lat: 0.0514448 avg lat: 0.023087
2021-10-28T14:17:03.863354+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-28T14:17:03.863354+0000    20     128    110947    110819   21.6427   12.2539   0.0409487    0.023087
2021-10-28T14:17:04.863446+0000    21     128    113891    113763   21.1596      11.5   0.0456426   0.0235999
2021-10-28T14:17:05.863530+0000    22     128    116835    116707   20.7205      11.5   0.0465272   0.0240965
2021-10-28T14:17:06.863606+0000    23     128    119714    119586   20.3085   11.2461   0.0506973   0.0245957
2021-10-28T14:17:07.863684+0000    24     128    122530    122402   19.9206        11   0.0488933   0.0250742
2021-10-28T14:17:08.863761+0000    25     128    125281    125153   19.5536   10.7461   0.0461514   0.0255478
2021-10-28T14:17:09.863837+0000    26     128    127969    127841   19.2054      10.5   0.0477966   0.0260096
2021-10-28T14:17:10.863909+0000    27     128    130992    130864   18.9314   11.8086   0.0380642   0.0263881
2021-10-28T14:17:11.863989+0000    28     128    136236    136108   18.9868   20.4844   0.0261471   0.0262441
2021-10-28T14:17:12.864069+0000    29     127    140904    140777   18.9609   18.2383   0.0227585    0.026355
2021-10-28T14:17:13.864140+0000    30     128    146547    146419   19.0635   22.0391   0.0179426   0.0262141
2021-10-28T14:17:14.864232+0000    31     128    152091    151963   19.1471   21.6562   0.0230583   0.0261045
2021-10-28T14:17:15.864313+0000    32     128    157695    157567   19.2327   21.8906   0.0167932   0.0259869
2021-10-28T14:17:16.864405+0000    33     128    163446    163318   19.3306   22.4648   0.0240208   0.0258514
2021-10-28T14:17:17.864492+0000    34     128    169030    168902   19.4036   21.8125   0.0231729   0.0257556
2021-10-28T14:17:18.864569+0000    35     128    174570    174442   19.4674   21.6406   0.0214619   0.0256714
2021-10-28T14:17:19.864649+0000    36     128    180068    179940   19.5232   21.4766    0.025635   0.0255971
2021-10-28T14:17:20.864719+0000    37     127    185590    185463   19.5786   21.5742   0.0217364   0.0255291
2021-10-28T14:17:21.864799+0000    38     128    190932    190804   19.6124   20.8633    0.021784   0.0254866
2021-10-28T14:17:22.864876+0000    39     128    196226    196098   19.6397   20.6797   0.0235927   0.0254481
2021-10-28T14:17:23.864940+0000 min lat: 0.00529007 max lat: 0.28921 avg lat: 0.0254125
2021-10-28T14:17:23.864940+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-28T14:17:23.864940+0000    40     128    201550    201422   19.6686   20.7969   0.0239167   0.0254125
2021-10-28T14:17:24.865025+0000    41     128    206913    206785   19.6998   20.9492   0.0237343   0.0253721
2021-10-28T14:17:25.865106+0000    42     128    212276    212148   19.7295   20.9492   0.0173787   0.0253342
2021-10-28T14:17:26.865184+0000    43     128    215313    215185   19.5465   11.8633   0.0546153   0.0255662
2021-10-28T14:17:27.865258+0000    44     128    217038    216910   19.2554   6.73828    0.148441   0.0259206
2021-10-28T14:17:28.865326+0000    45     128    218190    218062   18.9275       4.5   0.0998095   0.0263726
2021-10-28T14:17:29.865393+0000    46     128    219342    219214   18.6139       4.5     0.10637    0.026827
2021-10-28T14:17:30.865461+0000    47     128    220494    220366   18.3136       4.5    0.125148   0.0272729
2021-10-28T14:17:31.865535+0000    48     128    221585    221457   18.0208   4.26172    0.119681   0.0277036
2021-10-28T14:17:32.865606+0000    49     128    222798    222670   17.7497   4.73828     0.11337   0.0281468
2021-10-28T14:17:33.865670+0000    50     128    223889    223761     17.48   4.26172    0.103715   0.0285684
2021-10-28T14:17:34.865744+0000    51     128    224913    224785   17.2157         4    0.117457   0.0289983
2021-10-28T14:17:35.865820+0000    52     128    226281    226153   16.9873   5.34375   0.0829727   0.0294069
2021-10-28T14:17:36.865893+0000    53     128    228686    228558   16.8441   9.39453   0.0353628   0.0296731
2021-10-28T14:17:37.865969+0000    54     128    232014    231886   16.7729        13   0.0446527   0.0297955
2021-10-28T14:17:38.866033+0000    55     128    235473    235345   16.7136   13.5117   0.0395402   0.0299034
2021-10-28T14:17:39.866102+0000    56     128    238774    238646   16.6453   12.8945   0.0426148   0.0300295
2021-10-28T14:17:40.866168+0000    57     127    242222    242095   16.5897   13.4727   0.0323573   0.0301272
2021-10-28T14:17:41.866245+0000    58     128    245519    245391   16.5256    12.875    0.041166   0.0302401
2021-10-28T14:17:42.866319+0000    59     128    248877    248749   16.4678   13.1172   0.0366363   0.0303532
2021-10-28T14:17:43.866383+0000 min lat: 0.00529007 max lat: 0.329562 avg lat: 0.0304583
2021-10-28T14:17:43.866383+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-28T14:17:43.866383+0000    60     128    252142    252014   16.4059   12.7539    0.038807   0.0304583
2021-10-28T14:17:44.866500+0000 Total time run:         60.0224
Total writes made:      252142
Write size:             4096
Object size:            4096
Bandwidth (MB/sec):     16.4094
Stddev Bandwidth:       6.63634
Max bandwidth (MB/sec): 24.1133
Min bandwidth (MB/sec): 4
Average IOPS:           4200
Stddev IOPS:            1698.9
Max IOPS:               6173
Min IOPS:               1024
Average Latency(s):     0.0304647
Stddev Latency(s):      0.0207616
Max latency(s):         0.329562
Min latency(s):         0.00529007

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:17:45,579434417-07:00] INFO: > Stop system activity collection services[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:17:45,585093091-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:391 - rados_bench() > kill -INT 389447

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:17:45,590097371-07:00] INFO: >> for OSD host[0m
# ./benchmarks/bench-rados:396 - rados_bench() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.5 kill -INT 98153
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:17:45,597825455-07:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.5 kill -INT 98153
[1] 07:17:46 [SUCCESS] ljishen@10.10.2.5

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:17:46,893283079-07:00] INFO: >>> Copy system activity file from OSD host[0m
## ./benchmarks/bench-rados:403 - rados_bench() > basename -- /tmp/bench-rados/sys_activity_4KB_write.dat.osd_host.3
# ./benchmarks/bench-rados:403 - rados_bench() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.5:/tmp/bench-rados/sys_activity_4KB_write.dat.osd_host.3 /mnt/sda8/ceph-research/scripts/output/sys_activity_4KB_write.dat.osd_host.3


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:17:48,179851101-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:18:12,392370569-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 252.14k objects, 985 MiB
    usage:   3.2 GiB used, 97 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:18:12,399571989-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:18:21,619321326-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 252.14k objects, 985 MiB
    usage:   3.2 GiB used, 97 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:18:21,626165162-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:18:30,873901811-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 252.14k objects, 985 MiB
    usage:   3.2 GiB used, 97 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:18:30,880903786-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:18:40,265459509-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 252.14k objects, 985 MiB
    usage:   3.2 GiB used, 97 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:18:40,272222934-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:18:49,630990410-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 252.14k objects, 985 MiB
    usage:   3.2 GiB used, 97 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:18:49,637850457-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:18:49,643210238-07:00] INFO: > The cluster is idle now.[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:18:49,646655611-07:00] STAGE: Start benchmarking of mode seq ...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:18:49,652610563-07:00] INFO: > Start system activity collection services[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:18:49,657883930-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:341 - rados_bench() > rm --force /mnt/sda8/ceph-research/scripts/output/sys_activity_4KB_seq.dat.rados_bench_host.3
# ./benchmarks/bench-rados:342 - rados_bench() > sar_pid_rados_bench_host=390803
# ./benchmarks/bench-rados:342 - rados_bench() > S_TIME_FORMAT=ISO
# ./benchmarks/bench-rados:342 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/output/sys_activity_4KB_seq.dat.rados_bench_host.3 2

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:18:49,664098852-07:00] INFO: >> for OSD host[0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:18:49,672673050-07:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.5 bash
10.10.2.5: b'98703\n'
[1] 07:18:50 [SUCCESS] ljishen@10.10.2.5
98703

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:18:50,786825076-07:00] INFO: > Run rados bench[0m
# ./benchmarks/bench-rados:379 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/output/rados_bench_4KB_seq.log.3
## ./benchmarks/bench-rados:376 - rados_bench() > count_cpus 32-63,96-127
# ./benchmarks/bench-rados:376 - rados_bench() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 128 --show-time
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:18:50,806230425-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:18:50,809105133-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'e9941c06-37f8-11ec-b51d-53e6e728d2d3', '--', 'rados', 'bench', '99999999', 'seq', '--pool', 'bench_rados', '--concurrent-ios', '128', '--show-time'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid e9941c06-37f8-11ec-b51d-53e6e728d2d3 -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 128 --show-time
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
hints = 1
2021-10-28T14:18:54.011460+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-28T14:18:54.011460+0000     0       0         0         0         0         0           -           0
2021-10-28T14:18:55.011595+0000     1     127     16998     16871   65.8885   65.9023  0.00733247  0.00754671
2021-10-28T14:18:56.011716+0000     2     128     35104     34976   68.3012   70.7227  0.00905912  0.00730091
2021-10-28T14:18:57.011786+0000     3     127     52502     52375   68.1875   67.9648  0.00674732  0.00732012
2021-10-28T14:18:58.011864+0000     4     128     70565     70437   68.7779   70.5547  0.00690691  0.00725587
2021-10-28T14:18:59.011931+0000     5     128     87425     87297   68.1933   65.8594  0.00898797  0.00732077
2021-10-28T14:19:00.011998+0000     6     127    104546    104419   67.9742   66.8828   0.0096001  0.00734697
2021-10-28T14:19:01.012065+0000     7     128    121323    121195   67.6246   65.5312  0.00883499  0.00738266
2021-10-28T14:19:02.012132+0000     8     128    137190    137062   66.9186   61.9805  0.00709552  0.00746509
2021-10-28T14:19:03.012206+0000     9     128    152827    152699   66.2696    61.082  0.00930233  0.00753775
2021-10-28T14:19:04.012288+0000    10     128    168217    168089   65.6538   60.1172  0.00836826   0.0076097
2021-10-28T14:19:05.012353+0000    11     128    186055    185927   66.0194   69.6797  0.00780322  0.00756601
2021-10-28T14:19:06.012420+0000    12     127    202527    202400   65.8797   64.3477  0.00602832  0.00758247
2021-10-28T14:19:07.012493+0000    13     128    220118    219990   66.0971   68.7109  0.00474541  0.00755896
2021-10-28T14:19:08.012560+0000    14     128    237311    237183   66.1727   67.1602  0.00742502  0.00754973
2021-10-28T14:19:09.012650+0000 Total time run:       14.9052
Total reads made:     252142
Read size:            4096
Object size:          4096
Bandwidth (MB/sec):   66.0795
Average IOPS:         16916
Stddev IOPS:          862.045
Max IOPS:             18105
Min IOPS:             15390
Average Latency(s):   0.00756111
Max latency(s):       0.05525
Min latency(s):       0.0010462

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:19:09,743655038-07:00] INFO: > Stop system activity collection services[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:19:09,749236025-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:391 - rados_bench() > kill -INT 390803

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:19:09,754373797-07:00] INFO: >> for OSD host[0m
# ./benchmarks/bench-rados:396 - rados_bench() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.5 kill -INT 98703
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:19:09,762388270-07:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.5 kill -INT 98703
[1] 07:19:10 [SUCCESS] ljishen@10.10.2.5

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:19:10,855581290-07:00] INFO: >>> Copy system activity file from OSD host[0m
## ./benchmarks/bench-rados:403 - rados_bench() > basename -- /tmp/bench-rados/sys_activity_4KB_seq.dat.osd_host.3
# ./benchmarks/bench-rados:403 - rados_bench() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.5:/tmp/bench-rados/sys_activity_4KB_seq.dat.osd_host.3 /mnt/sda8/ceph-research/scripts/output/sys_activity_4KB_seq.dat.osd_host.3


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:19:11,929066550-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:19:36,138064803-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 252.14k objects, 985 MiB
    usage:   3.2 GiB used, 97 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:19:36,144862182-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:19:45,496780544-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 252.14k objects, 985 MiB
    usage:   3.2 GiB used, 97 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:19:45,504080380-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:19:54,861687893-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 252.14k objects, 985 MiB
    usage:   3.2 GiB used, 97 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:19:54,869032534-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:20:04,521334386-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 252.14k objects, 985 MiB
    usage:   3.2 GiB used, 97 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:20:04,528572916-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:20:13,777121691-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 252.14k objects, 985 MiB
    usage:   3.2 GiB used, 97 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:20:13,784336707-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:20:13,790014317-07:00] INFO: > The cluster is idle now.[0m


[1;7;39;49m[2021-10-28T07:20:13,793455101-07:00][RUNNING][ROUND 1/2/21] object_size=16KB[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:20:13,796630544-07:00] STAGE: Launch a Ceph cluster (CEPH_HOST: 10.10.2.1, OSD_HOST: 10.10.2.5) ...[0m
# ./benchmarks/bench-rados:183 - launch_ceph_cluster() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.1 bash -euo pipefail
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:20:13,805705296-07:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.1 bash -euo pipefail
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T07:20:14,199683862-07:00] INFO: > Remove existing clusters\x1b[0m\n'
10.10.2.1: b'## bash:17 -  > basename -- /var/lib/ceph/e9941c06-37f8-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'# bash:17 -  > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm --host 10.10.2.1 --host 10.10.2.5 rm-cluster --force --fsid e9941c06-37f8-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T07:20:14,210608109-07:00] DEBUG: HOST_PARAMS: ['--host', '10.10.2.1', '--host', '10.10.2.5']\x1b[0m\n"
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T07:20:14,214197647-07:00] DEBUG: CEPHADM_PARAMS: ['rm-cluster', '--force', '--fsid', 'e9941c06-37f8-11ec-b51d-53e6e728d2d3']\x1b[0m\n"
10.10.2.1: b"# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:63 -  > /tmp/bench-rados/ceph-research/scripts/parallel-ssh --host 10.10.2.1 --host 10.10.2.5 'sudo python3 -u - rm-cluster --force --fsid e9941c06-37f8-11ec-b51d-53e6e728d2d3'\n"
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T07:20:14,222297714-07:00] DEBUG: command from stdin\x1b[0m\n'
10.10.2.1: b"# /tmp/bench-rados/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host 10.10.2.1 --host 10.10.2.5 'sudo python3 -u - rm-cluster --force --fsid e9941c06-37f8-11ec-b51d-53e6e728d2d3'\n"
10.10.2.1: b'[1] 07:20:19 [SUCCESS] 10.10.2.1\n[2] 07:20:23 [SUCCESS] 10.10.2.5\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T07:20:23,381463779-07:00] INFO: > Deploy a new cluster\x1b[0m\n'
10.10.2.1: b'# bash:24 -  > /tmp/bench-rados/ceph-research/scripts/ceph-deploy -m 10.10.2.1 -o sm1/10.10.2.5:/dev/nvme0n1\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T07:20:23,392910809-07:00] STAGE: Verify passwordless SSH to hosts, passwordless sudo via ljishen, and the existence of devices on hosts...\x1b[0m\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T07:20:23,397970030-07:00] INFO: > Check the host for the monitor\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:181 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@10.10.2.1 sudo --non-interactive --validate\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T07:20:23,547899243-07:00] INFO: > Check hosts for OSDs\x1b[0m\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T07:20:23,553006254-07:00] INFO: >> Check host 'sm1'\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:200 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 sudo --non-interactive --validate\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T07:20:24,630982568-07:00] INFO: >>> Check device '/dev/nvme0n1'\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:215 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 test -b /dev/nvme0n1\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:226 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 lsblk --nodeps --noheadings --output TYPE /dev/nvme0n1\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:227 - verify_parameters() > grep --word-regexp disk\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T07:20:26,755655194-07:00] STAGE: Reset devices on hosts...\x1b[0m\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T07:20:26,760551078-07:00] INFO: > Resetting device '/dev/nvme0n1' on host 'sm1'\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:288 - reset_host_devices() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 sudo sh\n'
10.10.2.1: b'  Removing ceph--eee34568--4e09--4562--99e3--35dc5517127a-osd--block--b8bff468--601e--4060--baa9--d9aa93486fea (252:0)\n'
10.10.2.1: b'  Archiving volume group "ceph-eee34568-4e09-4562-99e3-35dc5517127a" metadata (seqno 5).\n'
10.10.2.1: b'  Releasing logical volume "osd-block-b8bff468-601e-4060-baa9-d9aa93486fea"\n'
10.10.2.1: b'  Creating volume group backup "/etc/lvm/backup/ceph-eee34568-4e09-4562-99e3-35dc5517127a" (seqno 6).\n'
10.10.2.1: b'  Logical volume "osd-block-b8bff468-601e-4060-baa9-d9aa93486fea" successfully removed\n'
10.10.2.1: b'  Removing physical volume "/dev/nvme0n1" from volume group "ceph-eee34568-4e09-4562-99e3-35dc5517127a"\n'
10.10.2.1: b'  Volume group "ceph-eee34568-4e09-4562-99e3-35dc5517127a" successfully removed\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T07:20:29,076220199-07:00] STAGE: Bootstrap a new cluster...\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:317 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.1 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T07:20:29,086583271-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T07:20:29,090112776-07:00] DEBUG: CEPHADM_PARAMS: ['bootstrap', '--config', '/tmp/bench-rados/ceph-research/scripts/ceph.conf', '--mon-ip', '10.10.2.1', '--output-dir', '/tmp/bench-rados/deployment_data_root/etc/ceph', '--ssh-user', 'ljishen', '--skip-dashboard', '--no-minimize-config', '--allow-overwrite', '--allow-fqdn-hostname', '--orphan-initial-daemons', '--skip-monitoring-stack', '--single-host-defaults']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.1 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults\n'
10.10.2.1: b'Verifying podman|docker is present...\nVerifying lvm2 is present...\n'
10.10.2.1: b'Verifying time synchronization is in place...\n'
10.10.2.1: b'Unit ntp.service is enabled and running\n'
10.10.2.1: b'Repeating the final host check...\n'
10.10.2.1: b'podman|docker (/usr/bin/docker) is present\n'
10.10.2.1: b'systemctl is present\nlvcreate is present\n'
10.10.2.1: b'Unit ntp.service is enabled and running\n'
10.10.2.1: b'Host looks OK\nCluster fsid: 33dc3dc4-37fa-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Verifying IP 10.10.2.1 port 3300 ...\n'
10.10.2.1: b'Verifying IP 10.10.2.1 port 6789 ...\n'
10.10.2.1: b'Mon IP `10.10.2.1` is in CIDR network `10.10.2.0/24`\n'
10.10.2.1: b'- internal network (--cluster-network) has not been provided, OSD replication will default to the public_network\n'
10.10.2.1: b'Adjusting default settings to suit single-host cluster...\n'
10.10.2.1: b'Pulling container image quay.io/ceph/ceph:v16...\n'
10.10.2.1: b'Ceph version: ceph version 16.2.6 (ee28fb57e47e9f88813e24bbf4c14496ca299d31) pacific (stable)\n'
10.10.2.1: b'Extracting ceph user uid/gid from container image...\n'
10.10.2.1: b'Creating initial keys...\n'
10.10.2.1: b'Creating initial monmap...\n'
10.10.2.1: b'Creating mon...\n'
10.10.2.1: b'Waiting for mon to start...\n'
10.10.2.1: b'Waiting for mon...\n'
10.10.2.1: b'mon is available\nSetting mon public_network to 10.10.2.0/24\n'
10.10.2.1: b'Wrote config to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf\n'
10.10.2.1: b'Wrote keyring to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring\n'
10.10.2.1: b'Creating mgr...\n'
10.10.2.1: b'Verifying port 9283 ...\n'
10.10.2.1: b'Waiting for mgr to start...\nWaiting for mgr...\n'
10.10.2.1: b'mgr not available, waiting (1/15)...\n'
10.10.2.1: b'mgr not available, waiting (2/15)...\n'
10.10.2.1: b'mgr is available\n'
10.10.2.1: b'Enabling cephadm module...\n'
10.10.2.1: b'Waiting for the mgr to restart...\n'
10.10.2.1: b'Waiting for mgr epoch 5...\n'
10.10.2.1: b'mgr epoch 5 is available\nSetting orchestrator backend to cephadm...\n'
10.10.2.1: b'Generating ssh key...\n'
10.10.2.1: b'Wrote public SSH key to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub\n'
10.10.2.1: b'Adding key to ljishen@localhost authorized_keys...\n'
10.10.2.1: b'Adding host node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us...\n'
10.10.2.1: b'Deploying unmanaged mon service...\n'
10.10.2.1: b'Deploying unmanaged mgr service...\n'
10.10.2.1: b'You can access the Ceph CLI with:\n\n\tsudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --fsid 33dc3dc4-37fa-11ec-b51d-53e6e728d2d3 -c /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf -k /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring\n\n'
10.10.2.1: b'Please consider enabling telemetry to help improve Ceph:\n\n\tceph telemetry on\n\nFor more information see:\n\n\thttps://docs.ceph.com/docs/pacific/mgr/telemetry/\n\n'
10.10.2.1: b'Bootstrap complete.\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T07:21:31,215344160-07:00] INFO: Sleep for 20 seconds\x1b[0m\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T07:21:51,222319428-07:00] INFO: > Set the default number of monitor daemons to 1\x1b[0m\n# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:337 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T07:21:51,232562724-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T07:21:51,236528429-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mon', '1']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1\n'
10.10.2.1: b'Inferring fsid 33dc3dc4-37fa-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/33dc3dc4-37fa-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b'Scheduled mon update...\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T07:22:00,169669662-07:00] INFO: > Set the default number of manager daemons to 1\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:342 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T07:22:00,180140196-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T07:22:00,183757847-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mgr', '1']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1\n'
10.10.2.1: b'Inferring fsid 33dc3dc4-37fa-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/33dc3dc4-37fa-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b'Scheduled mgr update...\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T07:22:09,737627902-07:00] STAGE: Add hosts to the cluster...\x1b[0m\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T07:22:09,743750982-07:00] INFO: > Adding host 'sm1'\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:361 - add_hosts() > ssh-copy-id -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -f -i /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub ljishen@sm1\n'
10.10.2.1: b'/usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: "/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub"\n'
10.10.2.1: b'\nNumber of key(s) added: 1\n\nNow try logging into the machine, with:   "ssh -o \'GlobalKnownHostsFile=/dev/null\' -o \'LogLevel=ERROR\' -o \'PasswordAuthentication=no\' -o \'StrictHostKeyChecking=no\' -o \'UserKnownHostsFile=/dev/null\' \'ljishen@sm1\'"\nand check to make sure that only the key(s) you wanted were added.\n\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:367 - add_hosts() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add sm1 10.10.2.5\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T07:22:10,900382302-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T07:22:10,904724605-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'host', 'add', 'sm1', '10.10.2.5']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add sm1 10.10.2.5\n'
10.10.2.1: b'Inferring fsid 33dc3dc4-37fa-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/33dc3dc4-37fa-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b"Added host 'sm1' with addr '10.10.2.5'\n"
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T07:22:21,400703898-07:00] INFO: Sleep for 20 seconds\x1b[0m\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T07:22:41,405715928-07:00] STAGE: Add OSDs to the cluster...\x1b[0m\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T07:22:41,412214524-07:00] INFO: > Adding devices ['/dev/nvme0n1'] on host 'sm1' as OSDs\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:388 - add_host_devices_as_osds() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd sm1:/dev/nvme0n1\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T07:22:41,421744450-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T07:22:41,425330071-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'daemon', 'add', 'osd', 'sm1:/dev/nvme0n1']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd sm1:/dev/nvme0n1\n'
10.10.2.1: b'Inferring fsid 33dc3dc4-37fa-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/33dc3dc4-37fa-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b"Created osd(s) 0 on host 'sm1'\n"
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T07:23:04,691524321-07:00] INFO: > Sleep for 20 seconds\x1b[0m\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T07:23:24,696769483-07:00] STAGE: Show Cluster Status\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:408 - show_status() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T07:23:24,706605123-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T07:23:24,710172769-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', '--status']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status\n'
10.10.2.1: b'Inferring fsid 33dc3dc4-37fa-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/33dc3dc4-37fa-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b'  cluster:\n    id:     33dc3dc4-37fa-11ec-b51d-53e6e728d2d3\n    health: HEALTH_WARN\n            1 pool(s) have no replicas configured\n \n  services:\n    mon: 1 daemons, quorum node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us (age 2m)\n    mgr: node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us.piljcm(active, since 2m)\n    osd: 1 osds: 1 up (since 20s), 1 in (since 39s)\n \n  data:\n    pools:   1 pools, 1 pgs\n    objects: 0 objects, 0 B\n    usage:   4.7 MiB used, 100 GiB / 100 GiB avail\n    pgs:     1 active+clean\n \n'
[1] 07:23:32 [SUCCESS] ljishen@10.10.2.1

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:20:14,199683862-07:00] INFO: > Remove existing clusters[0m
## bash:17 -  > basename -- /var/lib/ceph/e9941c06-37f8-11ec-b51d-53e6e728d2d3
# bash:17 -  > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm --host 10.10.2.1 --host 10.10.2.5 rm-cluster --force --fsid e9941c06-37f8-11ec-b51d-53e6e728d2d3
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:20:14,210608109-07:00] DEBUG: HOST_PARAMS: ['--host', '10.10.2.1', '--host', '10.10.2.5'][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:20:14,214197647-07:00] DEBUG: CEPHADM_PARAMS: ['rm-cluster', '--force', '--fsid', 'e9941c06-37f8-11ec-b51d-53e6e728d2d3'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:63 -  > /tmp/bench-rados/ceph-research/scripts/parallel-ssh --host 10.10.2.1 --host 10.10.2.5 'sudo python3 -u - rm-cluster --force --fsid e9941c06-37f8-11ec-b51d-53e6e728d2d3'
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:20:14,222297714-07:00] DEBUG: command from stdin[0m
# /tmp/bench-rados/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host 10.10.2.1 --host 10.10.2.5 'sudo python3 -u - rm-cluster --force --fsid e9941c06-37f8-11ec-b51d-53e6e728d2d3'
[1] 07:20:19 [SUCCESS] 10.10.2.1
[2] 07:20:23 [SUCCESS] 10.10.2.5

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:20:23,381463779-07:00] INFO: > Deploy a new cluster[0m
# bash:24 -  > /tmp/bench-rados/ceph-research/scripts/ceph-deploy -m 10.10.2.1 -o sm1/10.10.2.5:/dev/nvme0n1


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:20:23,392910809-07:00] STAGE: Verify passwordless SSH to hosts, passwordless sudo via ljishen, and the existence of devices on hosts...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:20:23,397970030-07:00] INFO: > Check the host for the monitor[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:181 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@10.10.2.1 sudo --non-interactive --validate

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:20:23,547899243-07:00] INFO: > Check hosts for OSDs[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:20:23,553006254-07:00] INFO: >> Check host 'sm1'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:200 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 sudo --non-interactive --validate

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:20:24,630982568-07:00] INFO: >>> Check device '/dev/nvme0n1'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:215 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 test -b /dev/nvme0n1
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:226 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 lsblk --nodeps --noheadings --output TYPE /dev/nvme0n1
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:227 - verify_parameters() > grep --word-regexp disk


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:20:26,755655194-07:00] STAGE: Reset devices on hosts...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:20:26,760551078-07:00] INFO: > Resetting device '/dev/nvme0n1' on host 'sm1'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:288 - reset_host_devices() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 sudo sh
  Removing ceph--eee34568--4e09--4562--99e3--35dc5517127a-osd--block--b8bff468--601e--4060--baa9--d9aa93486fea (252:0)
  Archiving volume group "ceph-eee34568-4e09-4562-99e3-35dc5517127a" metadata (seqno 5).
  Releasing logical volume "osd-block-b8bff468-601e-4060-baa9-d9aa93486fea"
  Creating volume group backup "/etc/lvm/backup/ceph-eee34568-4e09-4562-99e3-35dc5517127a" (seqno 6).
  Logical volume "osd-block-b8bff468-601e-4060-baa9-d9aa93486fea" successfully removed
  Removing physical volume "/dev/nvme0n1" from volume group "ceph-eee34568-4e09-4562-99e3-35dc5517127a"
  Volume group "ceph-eee34568-4e09-4562-99e3-35dc5517127a" successfully removed


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:20:29,076220199-07:00] STAGE: Bootstrap a new cluster...[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:317 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.1 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:20:29,086583271-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:20:29,090112776-07:00] DEBUG: CEPHADM_PARAMS: ['bootstrap', '--config', '/tmp/bench-rados/ceph-research/scripts/ceph.conf', '--mon-ip', '10.10.2.1', '--output-dir', '/tmp/bench-rados/deployment_data_root/etc/ceph', '--ssh-user', 'ljishen', '--skip-dashboard', '--no-minimize-config', '--allow-overwrite', '--allow-fqdn-hostname', '--orphan-initial-daemons', '--skip-monitoring-stack', '--single-host-defaults'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.1 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults
Verifying podman|docker is present...
Verifying lvm2 is present...
Verifying time synchronization is in place...
Unit ntp.service is enabled and running
Repeating the final host check...
podman|docker (/usr/bin/docker) is present
systemctl is present
lvcreate is present
Unit ntp.service is enabled and running
Host looks OK
Cluster fsid: 33dc3dc4-37fa-11ec-b51d-53e6e728d2d3
Verifying IP 10.10.2.1 port 3300 ...
Verifying IP 10.10.2.1 port 6789 ...
Mon IP `10.10.2.1` is in CIDR network `10.10.2.0/24`
- internal network (--cluster-network) has not been provided, OSD replication will default to the public_network
Adjusting default settings to suit single-host cluster...
Pulling container image quay.io/ceph/ceph:v16...
Ceph version: ceph version 16.2.6 (ee28fb57e47e9f88813e24bbf4c14496ca299d31) pacific (stable)
Extracting ceph user uid/gid from container image...
Creating initial keys...
Creating initial monmap...
Creating mon...
Waiting for mon to start...
Waiting for mon...
mon is available
Setting mon public_network to 10.10.2.0/24
Wrote config to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf
Wrote keyring to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring
Creating mgr...
Verifying port 9283 ...
Waiting for mgr to start...
Waiting for mgr...
mgr not available, waiting (1/15)...
mgr not available, waiting (2/15)...
mgr is available
Enabling cephadm module...
Waiting for the mgr to restart...
Waiting for mgr epoch 5...
mgr epoch 5 is available
Setting orchestrator backend to cephadm...
Generating ssh key...
Wrote public SSH key to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub
Adding key to ljishen@localhost authorized_keys...
Adding host node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us...
Deploying unmanaged mon service...
Deploying unmanaged mgr service...
You can access the Ceph CLI with:

	sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --fsid 33dc3dc4-37fa-11ec-b51d-53e6e728d2d3 -c /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf -k /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring

Please consider enabling telemetry to help improve Ceph:

	ceph telemetry on

For more information see:

	https://docs.ceph.com/docs/pacific/mgr/telemetry/

Bootstrap complete.

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:21:31,215344160-07:00] INFO: Sleep for 20 seconds[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:21:51,222319428-07:00] INFO: > Set the default number of monitor daemons to 1[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:337 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:21:51,232562724-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:21:51,236528429-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mon', '1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1
Inferring fsid 33dc3dc4-37fa-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/33dc3dc4-37fa-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Scheduled mon update...

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:22:00,169669662-07:00] INFO: > Set the default number of manager daemons to 1[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:342 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:22:00,180140196-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:22:00,183757847-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mgr', '1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1
Inferring fsid 33dc3dc4-37fa-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/33dc3dc4-37fa-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Scheduled mgr update...


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:22:09,737627902-07:00] STAGE: Add hosts to the cluster...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:22:09,743750982-07:00] INFO: > Adding host 'sm1'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:361 - add_hosts() > ssh-copy-id -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -f -i /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub ljishen@sm1
/usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: "/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub"

Number of key(s) added: 1

Now try logging into the machine, with:   "ssh -o 'GlobalKnownHostsFile=/dev/null' -o 'LogLevel=ERROR' -o 'PasswordAuthentication=no' -o 'StrictHostKeyChecking=no' -o 'UserKnownHostsFile=/dev/null' 'ljishen@sm1'"
and check to make sure that only the key(s) you wanted were added.

# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:367 - add_hosts() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add sm1 10.10.2.5
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:22:10,900382302-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:22:10,904724605-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'host', 'add', 'sm1', '10.10.2.5'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add sm1 10.10.2.5
Inferring fsid 33dc3dc4-37fa-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/33dc3dc4-37fa-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Added host 'sm1' with addr '10.10.2.5'

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:22:21,400703898-07:00] INFO: Sleep for 20 seconds[0m


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:22:41,405715928-07:00] STAGE: Add OSDs to the cluster...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:22:41,412214524-07:00] INFO: > Adding devices ['/dev/nvme0n1'] on host 'sm1' as OSDs[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:388 - add_host_devices_as_osds() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd sm1:/dev/nvme0n1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:22:41,421744450-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:22:41,425330071-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'daemon', 'add', 'osd', 'sm1:/dev/nvme0n1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd sm1:/dev/nvme0n1
Inferring fsid 33dc3dc4-37fa-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/33dc3dc4-37fa-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Created osd(s) 0 on host 'sm1'

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:23:04,691524321-07:00] INFO: > Sleep for 20 seconds[0m


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:23:24,696769483-07:00] STAGE: Show Cluster Status[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:408 - show_status() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:23:24,706605123-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:23:24,710172769-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', '--status'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status
Inferring fsid 33dc3dc4-37fa-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/33dc3dc4-37fa-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
  cluster:
    id:     33dc3dc4-37fa-11ec-b51d-53e6e728d2d3
    health: HEALTH_WARN
            1 pool(s) have no replicas configured
 
  services:
    mon: 1 daemons, quorum node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us (age 2m)
    mgr: node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us.piljcm(active, since 2m)
    osd: 1 osds: 1 up (since 20s), 1 in (since 39s)
 
  data:
    pools:   1 pools, 1 pgs
    objects: 0 objects, 0 B
    usage:   4.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     1 active+clean
 

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:23:32,938201598-07:00] INFO: > Copy configuration files to local[0m
# ./benchmarks/bench-rados:220 - launch_ceph_cluster() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.1 sudo chown --changes --recursive ljishen /tmp/bench-rados/deployment_data_root/etc/ceph
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:23:32,945813412-07:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.1 sudo chown --changes --recursive ljishen /tmp/bench-rados/deployment_data_root/etc/ceph
[1] 07:23:33 [SUCCESS] ljishen@10.10.2.1
# ./benchmarks/bench-rados:223 - launch_ceph_cluster() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.1:/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf
# ./benchmarks/bench-rados:226 - launch_ceph_cluster() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.1:/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:23:33,416840407-07:00] STAGE: Update local ceph.conf as a client...[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:23:33,420047371-07:00] STAGE: Configure Ceph pool...[0m
# ./benchmarks/bench-rados:257 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_max_backfills 32
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:23:33,441860237-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:23:33,444636499-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '33dc3dc4-37fa-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'config', 'set', 'osd', 'osd_max_backfills', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 33dc3dc4-37fa-11ec-b51d-53e6e728d2d3 -- ceph config set osd osd_max_backfills 32
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:258 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_active 32
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:23:37,764548037-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:23:37,767557459-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '33dc3dc4-37fa-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_active', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 33dc3dc4-37fa-11ec-b51d-53e6e728d2d3 -- ceph config set osd osd_recovery_max_active 32
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:259 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_single_start 8
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:23:42,123077100-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:23:42,126083696-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '33dc3dc4-37fa-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_single_start', '8'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 33dc3dc4-37fa-11ec-b51d-53e6e728d2d3 -- ceph config set osd osd_recovery_max_single_start 8
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:260 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_op_priority 63
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:23:46,348086526-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:23:46,350909346-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '33dc3dc4-37fa-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_op_priority', '63'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 33dc3dc4-37fa-11ec-b51d-53e6e728d2d3 -- ceph config set osd osd_recovery_op_priority 63
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:262 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config show-with-defaults osd.0
# ./benchmarks/bench-rados:263 - configure_ceph_pool() > grep 'osd_max_backfills\|osd_recovery'
# ./benchmarks/bench-rados:263 - configure_ceph_pool() > column -t -s ' '
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
osd_max_backfills                        32         mon
osd_recovery_cost                        20971520   default
osd_recovery_delay_start                 0.000000   default
osd_recovery_max_active                  32         mon
osd_recovery_max_active_hdd              3          default
osd_recovery_max_active_ssd              10         default
osd_recovery_max_chunk                   8388608    default
osd_recovery_max_omap_entries_per_chunk  8096       default
osd_recovery_max_single_start            8          mon
osd_recovery_op_priority                 63         mon
osd_recovery_op_warn_multiple            16         default
osd_recovery_priority                    5          default
osd_recovery_retry_interval              30.000000  default
osd_recovery_sleep                       0.000000   default
osd_recovery_sleep_hdd                   0.100000   default
osd_recovery_sleep_hybrid                0.025000   default
osd_recovery_sleep_ssd                   0.000000   default
# ./benchmarks/bench-rados:265 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool create bench_rados 128 128 replicated --size 1 --pg-num-min 128
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:23:54,939126152-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:23:54,942295965-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '33dc3dc4-37fa-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'create', 'bench_rados', '128', '128', 'replicated', '--size', '1', '--pg-num-min', '128'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 33dc3dc4-37fa-11ec-b51d-53e6e728d2d3 -- ceph osd pool create bench_rados 128 128 replicated --size 1 --pg-num-min 128
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
pool 'bench_rados' created
# ./benchmarks/bench-rados:267 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados min_size 1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:23:59,496541234-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:23:59,499411783-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '33dc3dc4-37fa-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'min_size', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 33dc3dc4-37fa-11ec-b51d-53e6e728d2d3 -- ceph osd pool set bench_rados min_size 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 min_size to 1
# ./benchmarks/bench-rados:268 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados pg_autoscale_mode off
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:24:03,846707147-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:24:03,849863164-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '33dc3dc4-37fa-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'pg_autoscale_mode', 'off'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 33dc3dc4-37fa-11ec-b51d-53e6e728d2d3 -- ceph osd pool set bench_rados pg_autoscale_mode off
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 pg_autoscale_mode to off
# ./benchmarks/bench-rados:269 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool application enable bench_rados benchmark
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:24:09,229342598-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:24:09,232529253-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '33dc3dc4-37fa-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'application', 'enable', 'bench_rados', 'benchmark'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 33dc3dc4-37fa-11ec-b51d-53e6e728d2d3 -- ceph osd pool application enable bench_rados benchmark
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
enabled application 'benchmark' on pool 'bench_rados'
# ./benchmarks/bench-rados:270 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados noscrub 1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:24:14,611871760-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:24:14,615147993-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '33dc3dc4-37fa-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'noscrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 33dc3dc4-37fa-11ec-b51d-53e6e728d2d3 -- ceph osd pool set bench_rados noscrub 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 noscrub to 1
# ./benchmarks/bench-rados:271 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados nodeep-scrub 1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:24:19,936369611-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:24:19,939293692-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '33dc3dc4-37fa-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'nodeep-scrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 33dc3dc4-37fa-11ec-b51d-53e6e728d2d3 -- ceph osd pool set bench_rados nodeep-scrub 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 nodeep-scrub to 1
# ./benchmarks/bench-rados:272 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool ls detail
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:24:24,886315835-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:24:24,889215549-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '33dc3dc4-37fa-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'ls', 'detail'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 33dc3dc4-37fa-11ec-b51d-53e6e728d2d3 -- ceph osd pool ls detail
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
pool 1 'device_health_metrics' replicated size 1 min_size 1 crush_rule 0 object_hash rjenkins pg_num 64 pgp_num 64 autoscale_mode on last_change 19 lfor 0/0/16 flags hashpspool stripe_width 0 pg_num_min 1 application mgr_devicehealth
pool 2 'bench_rados' replicated size 1 min_size 1 crush_rule 0 object_hash rjenkins pg_num 128 pgp_num 128 autoscale_mode off last_change 21 flags hashpspool,noscrub,nodeep-scrub stripe_width 0 pg_num_min 128 application benchmark

# ./benchmarks/bench-rados:273 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd df tree
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:24:29,110038317-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:24:29,113039262-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '33dc3dc4-37fa-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'df', 'tree'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 33dc3dc4-37fa-11ec-b51d-53e6e728d2d3 -- ceph osd df tree
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
ID  CLASS  WEIGHT   REWEIGHT  SIZE     RAW USE  DATA     OMAP  META     AVAIL    %USE  VAR   PGS  STATUS  TYPE NAME    
-1         0.09769         -  100 GiB  5.7 MiB  176 KiB   0 B  5.6 MiB  100 GiB  0.01  1.00    -          root default 
-3         0.09769         -  100 GiB  5.7 MiB  176 KiB   0 B  5.6 MiB  100 GiB  0.01  1.00    -              host sm1 
 0    ssd  0.09769   1.00000  100 GiB  5.7 MiB  176 KiB   0 B  5.6 MiB  100 GiB  0.01  1.00  192      up          osd.0
                       TOTAL  100 GiB  5.7 MiB  176 KiB   0 B  5.6 MiB  100 GiB  0.01                                  
MIN/MAX VAR: 1.00/1.00  STDDEV: 0


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:24:33,319056750-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:24:57,608694140-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:25:07,099184364-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:25:16,521335063-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:25:16,528400919-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:25:25,861401105-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:25:25,868365540-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:25:35,225734219-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:25:35,232168826-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:25:44,791198674-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:25:44,798240995-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:25:54,031984696-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:25:54,038938189-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:25:54,044265689-07:00] INFO: > The cluster is idle now.[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:25:54,047547353-07:00] STAGE: Start benchmarking of mode write ...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:25:54,053586574-07:00] INFO: > Start system activity collection services[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:25:54,058497999-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:341 - rados_bench() > rm --force /mnt/sda8/ceph-research/scripts/output/sys_activity_16KB_write.dat.rados_bench_host.1
# ./benchmarks/bench-rados:342 - rados_bench() > sar_pid_rados_bench_host=396464
# ./benchmarks/bench-rados:342 - rados_bench() > S_TIME_FORMAT=ISO
# ./benchmarks/bench-rados:342 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/output/sys_activity_16KB_write.dat.rados_bench_host.1 2

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:25:54,064801037-07:00] INFO: >> for OSD host[0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:25:54,073792742-07:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.5 bash
10.10.2.5: b'103542\n'
[1] 07:25:55 [SUCCESS] ljishen@10.10.2.5
103542

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:25:55,179094473-07:00] INFO: > Run rados bench[0m
# ./benchmarks/bench-rados:370 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/output/rados_bench_16KB_write.log.1
## ./benchmarks/bench-rados:361 - rados_bench() > count_cpus 32-63,96-127
# ./benchmarks/bench-rados:361 - rados_bench() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 60 write --pool bench_rados -b 16384 -O 16384 --concurrent-ios 128 --show-time --write-object --write-omap --write-xattr --no-cleanup
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:25:55,198521743-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:25:55,201550621-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '33dc3dc4-37fa-11ec-b51d-53e6e728d2d3', '--', 'rados', 'bench', '60', 'write', '--pool', 'bench_rados', '-b', '16384', '-O', '16384', '--concurrent-ios', '128', '--show-time', '--write-object', '--write-omap', '--write-xattr', '--no-cleanup'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 33dc3dc4-37fa-11ec-b51d-53e6e728d2d3 -- rados bench 60 write --pool bench_rados -b 16384 -O 16384 --concurrent-ios 128 --show-time --write-object --write-omap --write-xattr --no-cleanup
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
hints = 1
2021-10-28T14:25:58.364120+0000 Maintaining 128 concurrent writes of 16384 bytes to objects of size 16384 for up to 60 seconds or 0 objects
2021-10-28T14:25:58.364131+0000 Object prefix: benchmark_data_node-2.bluefield2.ucsc-cmps10_7
2021-10-28T14:25:58.365134+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-28T14:25:58.365134+0000     0       0         0         0         0         0           -           0
2021-10-28T14:25:59.365221+0000     1     128      3186      3058   47.7789   47.7812   0.0354803   0.0413234
2021-10-28T14:26:00.365293+0000     2     128      6250      6122   47.8252    47.875   0.0435413   0.0414052
2021-10-28T14:26:01.365361+0000     3     128      9263      9135   47.5751   47.0781   0.0451158   0.0418528
2021-10-28T14:26:02.365428+0000     4     128     12203     12075    47.165   45.9375   0.0446217   0.0422097
2021-10-28T14:26:03.365494+0000     5     128     15111     14983   46.8189   45.4375   0.0427061   0.0424943
2021-10-28T14:26:04.365559+0000     6     128     18026     17898   46.6064   45.5469   0.0407481   0.0427392
2021-10-28T14:26:05.365626+0000     7     128     20907     20779   46.3787   45.0156   0.0482987   0.0430078
2021-10-28T14:26:06.365692+0000     8     128     23786     23658    46.204   44.9844    0.042526   0.0431532
2021-10-28T14:26:07.365759+0000     9     128     26667     26539   46.0717   45.0156   0.0432234   0.0433017
2021-10-28T14:26:08.365823+0000    10     128     29546     29418   45.9626   44.9844   0.0456731   0.0433913
2021-10-28T14:26:09.365896+0000    11     128     31787     31659   44.9672   35.0156   0.0870858   0.0443099
2021-10-28T14:26:10.365974+0000    12     128     33218     33090   43.0831   22.3594   0.0825803   0.0462207
2021-10-28T14:26:11.366042+0000    13     128     34666     34538   41.5092    22.625   0.0882711   0.0480135
2021-10-28T14:26:12.366108+0000    14     128     36074     35946   40.1156        22   0.0906361   0.0496969
2021-10-28T14:26:13.366172+0000    15     128     37419     37291   38.8422   21.0156    0.108384   0.0512632
2021-10-28T14:26:14.366217+0000    16     128     38827     38699   37.7895        22   0.0788932   0.0528378
2021-10-28T14:26:15.366284+0000    17     128     40107     39979    36.743        20    0.104439   0.0542544
2021-10-28T14:26:16.366350+0000    18     128     41450     41322   35.8674   20.9844     0.09534   0.0556494
2021-10-28T14:26:17.366416+0000    19     128     43051     42923   35.2962   25.0156   0.0387353   0.0565871
2021-10-28T14:26:18.366481+0000 min lat: 0.026326 max lat: 0.258016 avg lat: 0.0564831
2021-10-28T14:26:18.366481+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-28T14:26:18.366481+0000    20     128     45418     45290   35.3805   36.9844   0.0462955   0.0564831
2021-10-28T14:26:19.366554+0000    21     128     48362     48234    35.886        46   0.0437748   0.0556724
2021-10-28T14:26:20.366620+0000    22     128     51243     51115   36.3009   45.0156   0.0437702   0.0550453
2021-10-28T14:26:21.366688+0000    23     128     54122     53994   36.6783   44.9844   0.0438355   0.0544839
2021-10-28T14:26:22.366755+0000    24     128     57003     56875   37.0256   45.0156     0.03992   0.0539705
2021-10-28T14:26:23.366821+0000    25     128     59882     59754   37.3438   44.9844   0.0423068   0.0534974
2021-10-28T14:26:24.366887+0000    26     128     62442     62314   37.4458        40   0.0834302   0.0533587
2021-10-28T14:26:25.366955+0000    27     128     63878     63750   36.8899   22.4375    0.090422   0.0541145
2021-10-28T14:26:26.367021+0000    28     128     64362     64234   35.8425    7.5625    0.313914   0.0554807
2021-10-28T14:26:27.367087+0000    29     128     64874     64746   34.8824         8    0.263683   0.0570071
2021-10-28T14:26:28.367153+0000    30     127     65354     65227   33.9702   7.51562    0.265479   0.0586005
2021-10-28T14:26:29.367216+0000    31     128     65835     65707   33.1163       7.5    0.296399    0.060188
2021-10-28T14:26:30.367286+0000    32     128     66282     66154   32.2996   6.98438     0.24601   0.0616136
2021-10-28T14:26:31.367352+0000    33     128     66794     66666   31.5633         8    0.260411    0.063055
2021-10-28T14:26:32.367419+0000    34     128     67296     67168   30.8656   7.84375     0.26044   0.0646307
2021-10-28T14:26:33.367483+0000    35     128     67755     67627   30.1886   7.17188    0.233156   0.0659395
2021-10-28T14:26:34.367552+0000    36     128     68616     68488   29.7237   13.4531   0.0800972   0.0672074
2021-10-28T14:26:35.367619+0000    37     128     70250     70122   29.6104   25.5312   0.0906805   0.0674812
2021-10-28T14:26:36.367686+0000    38     128     71930     71802   29.5219     26.25   0.0779891   0.0676536
2021-10-28T14:26:37.367752+0000    39     128     73643     73515   29.4512   26.7656    0.067883   0.0678606
2021-10-28T14:26:38.367817+0000 min lat: 0.026326 max lat: 0.346355 avg lat: 0.0680975
2021-10-28T14:26:38.367817+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-28T14:26:38.367817+0000    40     128     75242     75114   29.3395   24.9844   0.0741205   0.0680975
2021-10-28T14:26:39.367892+0000    41     127     76843     76716   29.2343   25.0312   0.0751178   0.0683259
2021-10-28T14:26:40.367958+0000    42     128     78442     78314   29.1327   24.9688   0.0827181   0.0685776
2021-10-28T14:26:41.368025+0000    43     128     80106     79978   29.0598        26   0.0817003   0.0687716
2021-10-28T14:26:42.368092+0000    44     128     81707     81579   28.9679   25.0156   0.0799871   0.0689712
2021-10-28T14:26:43.368157+0000    45     128     83371     83243   28.9019        26   0.0785361   0.0691302
2021-10-28T14:26:44.368216+0000    46     128     85035     84907   28.8388        26   0.0852927    0.069288
2021-10-28T14:26:45.368282+0000    47     128     86634     86506   28.7567   24.9844   0.0797336   0.0694733
2021-10-28T14:26:46.368358+0000    48     128     88170     88042   28.6576        24    0.079021   0.0697009
2021-10-28T14:26:47.368433+0000    49     128     90346     90218   28.7666        34   0.0469655   0.0694898
2021-10-28T14:26:48.368497+0000    50     128     93098     92970   29.0512        43    0.069545   0.0688002
2021-10-28T14:26:49.368566+0000    51     128     94549     94421   28.9261   22.6719   0.0873916   0.0691005
2021-10-28T14:26:50.368634+0000    52     128     95957     95829   28.7929        22   0.0957432   0.0693986
2021-10-28T14:26:51.368703+0000    53     128     97365     97237   28.6647        22     0.10925   0.0696848
2021-10-28T14:26:52.368771+0000    54     128     98773     98645   28.5412        22    0.085669   0.0699881
2021-10-28T14:26:53.368836+0000    55     128    100181    100053   28.4223        22   0.0828941   0.0703012
2021-10-28T14:26:54.368903+0000    56     128    101546    101418   28.2955   21.3281    0.100458   0.0706237
2021-10-28T14:26:55.368970+0000    57     128    102869    102741   28.1618   20.6719   0.0956421   0.0709525
2021-10-28T14:26:56.369036+0000    58     128    104149    104021    28.021        20   0.0865135   0.0713084
2021-10-28T14:26:57.369108+0000    59     128    106761    106633   28.2378   40.8125    0.043934   0.0707702
2021-10-28T14:26:58.369182+0000 min lat: 0.0249916 max lat: 0.346355 avg lat: 0.0704737
2021-10-28T14:26:58.369182+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-28T14:26:58.369182+0000    60     128    109067    108939   28.3676   36.0312   0.0500612   0.0704737
2021-10-28T14:26:59.369246+0000 Total time run:         60.0316
Total writes made:      109067
Write size:             16384
Object size:            16384
Bandwidth (MB/sec):     28.3879
Stddev Bandwidth:       12.8744
Max bandwidth (MB/sec): 47.875
Min bandwidth (MB/sec): 6.98438
Average IOPS:           1816
Stddev IOPS:            823.961
Max IOPS:               3064
Min IOPS:               447
Average Latency(s):     0.0704384
Stddev Latency(s):      0.0456176
Max latency(s):         0.346355
Min latency(s):         0.0249916

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:27:00,094766037-07:00] INFO: > Stop system activity collection services[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:27:00,100254429-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:391 - rados_bench() > kill -INT 396464

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:27:00,105529670-07:00] INFO: >> for OSD host[0m
# ./benchmarks/bench-rados:396 - rados_bench() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.5 kill -INT 103542
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:27:00,113200326-07:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.5 kill -INT 103542
[1] 07:27:01 [SUCCESS] ljishen@10.10.2.5

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:27:01,223890781-07:00] INFO: >>> Copy system activity file from OSD host[0m
## ./benchmarks/bench-rados:403 - rados_bench() > basename -- /tmp/bench-rados/sys_activity_16KB_write.dat.osd_host.1
# ./benchmarks/bench-rados:403 - rados_bench() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.5:/tmp/bench-rados/sys_activity_16KB_write.dat.osd_host.1 /mnt/sda8/ceph-research/scripts/output/sys_activity_16KB_write.dat.osd_host.1


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:27:02,314347747-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:27:26,772816855-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 109.07k objects, 1.7 GiB
    usage:   5.1 GiB used, 95 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:27:26,780120308-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:27:36,171541655-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 109.07k objects, 1.7 GiB
    usage:   5.1 GiB used, 95 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:27:36,178641444-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:27:45,537325078-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 109.07k objects, 1.7 GiB
    usage:   5.1 GiB used, 95 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:27:45,544288561-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:27:54,953285800-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 109.07k objects, 1.7 GiB
    usage:   5.1 GiB used, 95 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:27:54,960450582-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:28:04,242321484-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 109.07k objects, 1.7 GiB
    usage:   5.1 GiB used, 95 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:28:04,249463763-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:28:04,254929283-07:00] INFO: > The cluster is idle now.[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:28:04,258256293-07:00] STAGE: Start benchmarking of mode seq ...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:28:04,264500871-07:00] INFO: > Start system activity collection services[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:28:04,269480254-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:341 - rados_bench() > rm --force /mnt/sda8/ceph-research/scripts/output/sys_activity_16KB_seq.dat.rados_bench_host.1
# ./benchmarks/bench-rados:342 - rados_bench() > sar_pid_rados_bench_host=397807
# ./benchmarks/bench-rados:342 - rados_bench() > S_TIME_FORMAT=ISO
# ./benchmarks/bench-rados:342 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/output/sys_activity_16KB_seq.dat.rados_bench_host.1 2

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:28:04,275673455-07:00] INFO: >> for OSD host[0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:28:04,284697731-07:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.5 bash
10.10.2.5: b'104228\n'
[1] 07:28:05 [SUCCESS] ljishen@10.10.2.5
104228

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:28:05,410678131-07:00] INFO: > Run rados bench[0m
# ./benchmarks/bench-rados:379 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/output/rados_bench_16KB_seq.log.1
## ./benchmarks/bench-rados:376 - rados_bench() > count_cpus 32-63,96-127
# ./benchmarks/bench-rados:376 - rados_bench() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 128 --show-time
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:28:05,429896598-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:28:05,432663512-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '33dc3dc4-37fa-11ec-b51d-53e6e728d2d3', '--', 'rados', 'bench', '99999999', 'seq', '--pool', 'bench_rados', '--concurrent-ios', '128', '--show-time'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 33dc3dc4-37fa-11ec-b51d-53e6e728d2d3 -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 128 --show-time
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
hints = 1
2021-10-28T14:28:08.760531+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-28T14:28:08.760531+0000     0       0         0         0         0         0           -           0
2021-10-28T14:28:09.760645+0000     1     127     14983     14856   232.083   232.125  0.00971039  0.00858199
2021-10-28T14:28:10.760768+0000     2     128     29664     29536   230.715   229.375   0.0103979  0.00864718
2021-10-28T14:28:11.760850+0000     3     127     43504     43377   225.893   216.266  0.00932862  0.00883073
2021-10-28T14:28:12.760916+0000     4     128     57710     57582   224.904   221.953  0.00888971  0.00888088
2021-10-28T14:28:13.760979+0000     5     128     72532     72404   226.239   231.594  0.00795959   0.0088258
2021-10-28T14:28:14.761042+0000     6     128     86582     86454   225.119   219.531  0.00724335  0.00887189
2021-10-28T14:28:15.761107+0000     7     128     99642     99514   222.109   204.062  0.00769867  0.00899454
2021-10-28T14:28:16.761202+0000 Total time run:       7.70915
Total reads made:     109067
Read size:            16384
Object size:          16384
Bandwidth (MB/sec):   221.058
Average IOPS:         14147
Stddev IOPS:          645.08
Max IOPS:             14856
Min IOPS:             13060
Average Latency(s):   0.00903833
Max latency(s):       0.0426026
Min latency(s):       0.00212388

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:28:17,478328519-07:00] INFO: > Stop system activity collection services[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:28:17,483597539-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:391 - rados_bench() > kill -INT 397807

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:28:17,489124414-07:00] INFO: >> for OSD host[0m
# ./benchmarks/bench-rados:396 - rados_bench() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.5 kill -INT 104228
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:28:17,497079014-07:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.5 kill -INT 104228
[1] 07:28:18 [SUCCESS] ljishen@10.10.2.5

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:28:18,599642526-07:00] INFO: >>> Copy system activity file from OSD host[0m
## ./benchmarks/bench-rados:403 - rados_bench() > basename -- /tmp/bench-rados/sys_activity_16KB_seq.dat.osd_host.1
# ./benchmarks/bench-rados:403 - rados_bench() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.5:/tmp/bench-rados/sys_activity_16KB_seq.dat.osd_host.1 /mnt/sda8/ceph-research/scripts/output/sys_activity_16KB_seq.dat.osd_host.1


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:28:19,652741361-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:28:43,840528119-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 109.07k objects, 1.7 GiB
    usage:   5.1 GiB used, 95 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:28:43,847617538-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:28:53,148149529-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 109.07k objects, 1.7 GiB
    usage:   5.1 GiB used, 95 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:28:53,155113412-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:29:02,374550340-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 109.07k objects, 1.7 GiB
    usage:   5.1 GiB used, 95 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:29:02,381428241-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:29:11,780517763-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 109.07k objects, 1.7 GiB
    usage:   5.1 GiB used, 95 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:29:11,787347444-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:29:21,142724299-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 109.07k objects, 1.7 GiB
    usage:   5.1 GiB used, 95 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:29:21,149720133-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:29:21,155109459-07:00] INFO: > The cluster is idle now.[0m


[1;7;39;49m[2021-10-28T07:29:21,157322309-07:00][RUNNING][ROUND 2/2/21] object_size=16KB[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:29:21,160708300-07:00] STAGE: Launch a Ceph cluster (CEPH_HOST: 10.10.2.1, OSD_HOST: 10.10.2.5) ...[0m
# ./benchmarks/bench-rados:183 - launch_ceph_cluster() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.1 bash -euo pipefail
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:29:21,170288604-07:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.1 bash -euo pipefail
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T07:29:21,589381219-07:00] INFO: > Remove existing clusters\x1b[0m\n'
10.10.2.1: b'## bash:17 -  > basename -- /var/lib/ceph/33dc3dc4-37fa-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'# bash:17 -  > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm --host 10.10.2.1 --host 10.10.2.5 rm-cluster --force --fsid 33dc3dc4-37fa-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T07:29:21,600473442-07:00] DEBUG: HOST_PARAMS: ['--host', '10.10.2.1', '--host', '10.10.2.5']\x1b[0m\n"
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T07:29:21,604348757-07:00] DEBUG: CEPHADM_PARAMS: ['rm-cluster', '--force', '--fsid', '33dc3dc4-37fa-11ec-b51d-53e6e728d2d3']\x1b[0m\n"
10.10.2.1: b"# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:63 -  > /tmp/bench-rados/ceph-research/scripts/parallel-ssh --host 10.10.2.1 --host 10.10.2.5 'sudo python3 -u - rm-cluster --force --fsid 33dc3dc4-37fa-11ec-b51d-53e6e728d2d3'\n"
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T07:29:21,612843065-07:00] DEBUG: command from stdin\x1b[0m\n'
10.10.2.1: b"# /tmp/bench-rados/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host 10.10.2.1 --host 10.10.2.5 'sudo python3 -u - rm-cluster --force --fsid 33dc3dc4-37fa-11ec-b51d-53e6e728d2d3'\n"
10.10.2.1: b'[1] 07:29:27 [SUCCESS] 10.10.2.1\n[2] 07:29:31 [SUCCESS] 10.10.2.5\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T07:29:31,565525133-07:00] INFO: > Deploy a new cluster\x1b[0m\n'
10.10.2.1: b'# bash:24 -  > /tmp/bench-rados/ceph-research/scripts/ceph-deploy -m 10.10.2.1 -o sm1/10.10.2.5:/dev/nvme0n1\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T07:29:31,577192457-07:00] STAGE: Verify passwordless SSH to hosts, passwordless sudo via ljishen, and the existence of devices on hosts...\x1b[0m\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T07:29:31,581994675-07:00] INFO: > Check the host for the monitor\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:181 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@10.10.2.1 sudo --non-interactive --validate\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T07:29:31,730894313-07:00] INFO: > Check hosts for OSDs\x1b[0m\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T07:29:31,735412406-07:00] INFO: >> Check host 'sm1'\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:200 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 sudo --non-interactive --validate\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T07:29:32,799024118-07:00] INFO: >>> Check device '/dev/nvme0n1'\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:215 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 test -b /dev/nvme0n1\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:226 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 lsblk --nodeps --noheadings --output TYPE /dev/nvme0n1\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:227 - verify_parameters() > grep --word-regexp disk\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T07:29:34,899487717-07:00] STAGE: Reset devices on hosts...\x1b[0m\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T07:29:34,904833216-07:00] INFO: > Resetting device '/dev/nvme0n1' on host 'sm1'\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:288 - reset_host_devices() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 sudo sh\n'
10.10.2.1: b'  Removing ceph--c234e253--ea78--4adc--9ce2--03ada115cdb3-osd--block--b6bc116d--1e38--48bd--82e4--f65f34731ed4 (252:0)\n'
10.10.2.1: b'  Archiving volume group "ceph-c234e253-ea78-4adc-9ce2-03ada115cdb3" metadata (seqno 5).\n'
10.10.2.1: b'  Releasing logical volume "osd-block-b6bc116d-1e38-48bd-82e4-f65f34731ed4"\n'
10.10.2.1: b'  Creating volume group backup "/etc/lvm/backup/ceph-c234e253-ea78-4adc-9ce2-03ada115cdb3" (seqno 6).\n'
10.10.2.1: b'  Logical volume "osd-block-b6bc116d-1e38-48bd-82e4-f65f34731ed4" successfully removed\n'
10.10.2.1: b'  Removing physical volume "/dev/nvme0n1" from volume group "ceph-c234e253-ea78-4adc-9ce2-03ada115cdb3"\n'
10.10.2.1: b'  Volume group "ceph-c234e253-ea78-4adc-9ce2-03ada115cdb3" successfully removed\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T07:29:37,225838082-07:00] STAGE: Bootstrap a new cluster...\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:317 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.1 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T07:29:37,236348912-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T07:29:37,240283548-07:00] DEBUG: CEPHADM_PARAMS: ['bootstrap', '--config', '/tmp/bench-rados/ceph-research/scripts/ceph.conf', '--mon-ip', '10.10.2.1', '--output-dir', '/tmp/bench-rados/deployment_data_root/etc/ceph', '--ssh-user', 'ljishen', '--skip-dashboard', '--no-minimize-config', '--allow-overwrite', '--allow-fqdn-hostname', '--orphan-initial-daemons', '--skip-monitoring-stack', '--single-host-defaults']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.1 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults\n'
10.10.2.1: b'Verifying podman|docker is present...\nVerifying lvm2 is present...\n'
10.10.2.1: b'Verifying time synchronization is in place...\n'
10.10.2.1: b'Unit ntp.service is enabled and running\n'
10.10.2.1: b'Repeating the final host check...\n'
10.10.2.1: b'podman|docker (/usr/bin/docker) is present\n'
10.10.2.1: b'systemctl is present\n'
10.10.2.1: b'lvcreate is present\n'
10.10.2.1: b'Unit ntp.service is enabled and running\n'
10.10.2.1: b'Host looks OK\n'
10.10.2.1: b'Cluster fsid: 7a953d1e-37fb-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Verifying IP 10.10.2.1 port 3300 ...\n'
10.10.2.1: b'Verifying IP 10.10.2.1 port 6789 ...\n'
10.10.2.1: b'Mon IP `10.10.2.1` is in CIDR network `10.10.2.0/24`\n'
10.10.2.1: b'- internal network (--cluster-network) has not been provided, OSD replication will default to the public_network\n'
10.10.2.1: b'Adjusting default settings to suit single-host cluster...\n'
10.10.2.1: b'Pulling container image quay.io/ceph/ceph:v16...\n'
10.10.2.1: b'Ceph version: ceph version 16.2.6 (ee28fb57e47e9f88813e24bbf4c14496ca299d31) pacific (stable)\n'
10.10.2.1: b'Extracting ceph user uid/gid from container image...\n'
10.10.2.1: b'Creating initial keys...\n'
10.10.2.1: b'Creating initial monmap...\n'
10.10.2.1: b'Creating mon...\n'
10.10.2.1: b'Waiting for mon to start...\n'
10.10.2.1: b'Waiting for mon...\n'
10.10.2.1: b'mon is available\nSetting mon public_network to 10.10.2.0/24\n'
10.10.2.1: b'Wrote config to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf\n'
10.10.2.1: b'Wrote keyring to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring\n'
10.10.2.1: b'Creating mgr...\n'
10.10.2.1: b'Verifying port 9283 ...\n'
10.10.2.1: b'Waiting for mgr to start...\nWaiting for mgr...\n'
10.10.2.1: b'mgr not available, waiting (1/15)...\n'
10.10.2.1: b'mgr not available, waiting (2/15)...\n'
10.10.2.1: b'mgr is available\n'
10.10.2.1: b'Enabling cephadm module...\n'
10.10.2.1: b'Waiting for the mgr to restart...\nWaiting for mgr epoch 5...\n'
10.10.2.1: b'mgr epoch 5 is available\n'
10.10.2.1: b'Setting orchestrator backend to cephadm...\n'
10.10.2.1: b'Generating ssh key...\n'
10.10.2.1: b'Wrote public SSH key to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub\nAdding key to ljishen@localhost authorized_keys...\n'
10.10.2.1: b'Adding host node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us...\n'
10.10.2.1: b'Deploying unmanaged mon service...\n'
10.10.2.1: b'Deploying unmanaged mgr service...\n'
10.10.2.1: b'You can access the Ceph CLI with:\n\n\tsudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --fsid 7a953d1e-37fb-11ec-b51d-53e6e728d2d3 -c /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf -k /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring\n\nPlease consider enabling telemetry to help improve Ceph:\n\n\tceph telemetry on\n\nFor more information see:\n\n\thttps://docs.ceph.com/docs/pacific/mgr/telemetry/\n\nBootstrap complete.\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T07:30:36,901316559-07:00] INFO: Sleep for 20 seconds\x1b[0m\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T07:30:56,908817052-07:00] INFO: > Set the default number of monitor daemons to 1\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:337 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T07:30:56,918785442-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T07:30:56,922443699-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mon', '1']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1\n'
10.10.2.1: b'Inferring fsid 7a953d1e-37fb-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/7a953d1e-37fb-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b'Scheduled mon update...\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T07:31:05,998964266-07:00] INFO: > Set the default number of manager daemons to 1\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:342 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T07:31:06,008862894-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T07:31:06,012928968-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mgr', '1']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1\n'
10.10.2.1: b'Inferring fsid 7a953d1e-37fb-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/7a953d1e-37fb-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b'Scheduled mgr update...\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T07:31:14,982668918-07:00] STAGE: Add hosts to the cluster...\x1b[0m\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T07:31:14,988927614-07:00] INFO: > Adding host 'sm1'\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:361 - add_hosts() > ssh-copy-id -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -f -i /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub ljishen@sm1\n'
10.10.2.1: b'/usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: "/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub"\n'
10.10.2.1: b'\nNumber of key(s) added: 1\n\nNow try logging into the machine, with:   "ssh -o \'GlobalKnownHostsFile=/dev/null\' -o \'LogLevel=ERROR\' -o \'PasswordAuthentication=no\' -o \'StrictHostKeyChecking=no\' -o \'UserKnownHostsFile=/dev/null\' \'ljishen@sm1\'"\nand check to make sure that only the key(s) you wanted were added.\n\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:367 - add_hosts() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add sm1 10.10.2.5\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T07:31:16,132332914-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T07:31:16,135751410-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'host', 'add', 'sm1', '10.10.2.5']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add sm1 10.10.2.5\n'
10.10.2.1: b'Inferring fsid 7a953d1e-37fb-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/7a953d1e-37fb-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b"Added host 'sm1' with addr '10.10.2.5'\n"
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T07:31:27,240263813-07:00] INFO: Sleep for 20 seconds\x1b[0m\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T07:31:47,245083452-07:00] STAGE: Add OSDs to the cluster...\x1b[0m\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T07:31:47,251861775-07:00] INFO: > Adding devices ['/dev/nvme0n1'] on host 'sm1' as OSDs\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:388 - add_host_devices_as_osds() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd sm1:/dev/nvme0n1\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T07:31:47,262784849-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T07:31:47,267023096-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'daemon', 'add', 'osd', 'sm1:/dev/nvme0n1']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd sm1:/dev/nvme0n1\n'
10.10.2.1: b'Inferring fsid 7a953d1e-37fb-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/7a953d1e-37fb-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b"Created osd(s) 0 on host 'sm1'\n"
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T07:32:11,008105715-07:00] INFO: > Sleep for 20 seconds\x1b[0m\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T07:32:31,012848743-07:00] STAGE: Show Cluster Status\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:408 - show_status() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T07:32:31,022893698-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T07:32:31,026637786-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', '--status']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status\n'
10.10.2.1: b'Inferring fsid 7a953d1e-37fb-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/7a953d1e-37fb-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b'  cluster:\n    id:     7a953d1e-37fb-11ec-b51d-53e6e728d2d3\n    health: HEALTH_WARN\n            1 pool(s) have no replicas configured\n \n  services:\n    mon: 1 daemons, quorum node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us (age 2m)\n    mgr: node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us.bfoafl(active, since 2m)\n    osd: 1 osds: 1 up (since 20s), 1 in (since 40s)\n \n  data:\n    pools:   1 pools, 1 pgs\n    objects: 0 objects, 0 B\n    usage:   4.7 MiB used, 100 GiB / 100 GiB avail\n    pgs:     1 active+clean\n \n'
[1] 07:32:40 [SUCCESS] ljishen@10.10.2.1

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:29:21,589381219-07:00] INFO: > Remove existing clusters[0m
## bash:17 -  > basename -- /var/lib/ceph/33dc3dc4-37fa-11ec-b51d-53e6e728d2d3
# bash:17 -  > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm --host 10.10.2.1 --host 10.10.2.5 rm-cluster --force --fsid 33dc3dc4-37fa-11ec-b51d-53e6e728d2d3
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:29:21,600473442-07:00] DEBUG: HOST_PARAMS: ['--host', '10.10.2.1', '--host', '10.10.2.5'][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:29:21,604348757-07:00] DEBUG: CEPHADM_PARAMS: ['rm-cluster', '--force', '--fsid', '33dc3dc4-37fa-11ec-b51d-53e6e728d2d3'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:63 -  > /tmp/bench-rados/ceph-research/scripts/parallel-ssh --host 10.10.2.1 --host 10.10.2.5 'sudo python3 -u - rm-cluster --force --fsid 33dc3dc4-37fa-11ec-b51d-53e6e728d2d3'
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:29:21,612843065-07:00] DEBUG: command from stdin[0m
# /tmp/bench-rados/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host 10.10.2.1 --host 10.10.2.5 'sudo python3 -u - rm-cluster --force --fsid 33dc3dc4-37fa-11ec-b51d-53e6e728d2d3'
[1] 07:29:27 [SUCCESS] 10.10.2.1
[2] 07:29:31 [SUCCESS] 10.10.2.5

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:29:31,565525133-07:00] INFO: > Deploy a new cluster[0m
# bash:24 -  > /tmp/bench-rados/ceph-research/scripts/ceph-deploy -m 10.10.2.1 -o sm1/10.10.2.5:/dev/nvme0n1


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:29:31,577192457-07:00] STAGE: Verify passwordless SSH to hosts, passwordless sudo via ljishen, and the existence of devices on hosts...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:29:31,581994675-07:00] INFO: > Check the host for the monitor[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:181 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@10.10.2.1 sudo --non-interactive --validate

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:29:31,730894313-07:00] INFO: > Check hosts for OSDs[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:29:31,735412406-07:00] INFO: >> Check host 'sm1'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:200 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 sudo --non-interactive --validate

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:29:32,799024118-07:00] INFO: >>> Check device '/dev/nvme0n1'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:215 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 test -b /dev/nvme0n1
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:226 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 lsblk --nodeps --noheadings --output TYPE /dev/nvme0n1
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:227 - verify_parameters() > grep --word-regexp disk


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:29:34,899487717-07:00] STAGE: Reset devices on hosts...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:29:34,904833216-07:00] INFO: > Resetting device '/dev/nvme0n1' on host 'sm1'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:288 - reset_host_devices() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 sudo sh
  Removing ceph--c234e253--ea78--4adc--9ce2--03ada115cdb3-osd--block--b6bc116d--1e38--48bd--82e4--f65f34731ed4 (252:0)
  Archiving volume group "ceph-c234e253-ea78-4adc-9ce2-03ada115cdb3" metadata (seqno 5).
  Releasing logical volume "osd-block-b6bc116d-1e38-48bd-82e4-f65f34731ed4"
  Creating volume group backup "/etc/lvm/backup/ceph-c234e253-ea78-4adc-9ce2-03ada115cdb3" (seqno 6).
  Logical volume "osd-block-b6bc116d-1e38-48bd-82e4-f65f34731ed4" successfully removed
  Removing physical volume "/dev/nvme0n1" from volume group "ceph-c234e253-ea78-4adc-9ce2-03ada115cdb3"
  Volume group "ceph-c234e253-ea78-4adc-9ce2-03ada115cdb3" successfully removed


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:29:37,225838082-07:00] STAGE: Bootstrap a new cluster...[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:317 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.1 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:29:37,236348912-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:29:37,240283548-07:00] DEBUG: CEPHADM_PARAMS: ['bootstrap', '--config', '/tmp/bench-rados/ceph-research/scripts/ceph.conf', '--mon-ip', '10.10.2.1', '--output-dir', '/tmp/bench-rados/deployment_data_root/etc/ceph', '--ssh-user', 'ljishen', '--skip-dashboard', '--no-minimize-config', '--allow-overwrite', '--allow-fqdn-hostname', '--orphan-initial-daemons', '--skip-monitoring-stack', '--single-host-defaults'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.1 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults
Verifying podman|docker is present...
Verifying lvm2 is present...
Verifying time synchronization is in place...
Unit ntp.service is enabled and running
Repeating the final host check...
podman|docker (/usr/bin/docker) is present
systemctl is present
lvcreate is present
Unit ntp.service is enabled and running
Host looks OK
Cluster fsid: 7a953d1e-37fb-11ec-b51d-53e6e728d2d3
Verifying IP 10.10.2.1 port 3300 ...
Verifying IP 10.10.2.1 port 6789 ...
Mon IP `10.10.2.1` is in CIDR network `10.10.2.0/24`
- internal network (--cluster-network) has not been provided, OSD replication will default to the public_network
Adjusting default settings to suit single-host cluster...
Pulling container image quay.io/ceph/ceph:v16...
Ceph version: ceph version 16.2.6 (ee28fb57e47e9f88813e24bbf4c14496ca299d31) pacific (stable)
Extracting ceph user uid/gid from container image...
Creating initial keys...
Creating initial monmap...
Creating mon...
Waiting for mon to start...
Waiting for mon...
mon is available
Setting mon public_network to 10.10.2.0/24
Wrote config to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf
Wrote keyring to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring
Creating mgr...
Verifying port 9283 ...
Waiting for mgr to start...
Waiting for mgr...
mgr not available, waiting (1/15)...
mgr not available, waiting (2/15)...
mgr is available
Enabling cephadm module...
Waiting for the mgr to restart...
Waiting for mgr epoch 5...
mgr epoch 5 is available
Setting orchestrator backend to cephadm...
Generating ssh key...
Wrote public SSH key to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub
Adding key to ljishen@localhost authorized_keys...
Adding host node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us...
Deploying unmanaged mon service...
Deploying unmanaged mgr service...
You can access the Ceph CLI with:

	sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --fsid 7a953d1e-37fb-11ec-b51d-53e6e728d2d3 -c /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf -k /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring

Please consider enabling telemetry to help improve Ceph:

	ceph telemetry on

For more information see:

	https://docs.ceph.com/docs/pacific/mgr/telemetry/

Bootstrap complete.

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:30:36,901316559-07:00] INFO: Sleep for 20 seconds[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:30:56,908817052-07:00] INFO: > Set the default number of monitor daemons to 1[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:337 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:30:56,918785442-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:30:56,922443699-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mon', '1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1
Inferring fsid 7a953d1e-37fb-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/7a953d1e-37fb-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Scheduled mon update...

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:31:05,998964266-07:00] INFO: > Set the default number of manager daemons to 1[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:342 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:31:06,008862894-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:31:06,012928968-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mgr', '1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1
Inferring fsid 7a953d1e-37fb-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/7a953d1e-37fb-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Scheduled mgr update...


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:31:14,982668918-07:00] STAGE: Add hosts to the cluster...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:31:14,988927614-07:00] INFO: > Adding host 'sm1'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:361 - add_hosts() > ssh-copy-id -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -f -i /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub ljishen@sm1
/usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: "/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub"

Number of key(s) added: 1

Now try logging into the machine, with:   "ssh -o 'GlobalKnownHostsFile=/dev/null' -o 'LogLevel=ERROR' -o 'PasswordAuthentication=no' -o 'StrictHostKeyChecking=no' -o 'UserKnownHostsFile=/dev/null' 'ljishen@sm1'"
and check to make sure that only the key(s) you wanted were added.

# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:367 - add_hosts() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add sm1 10.10.2.5
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:31:16,132332914-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:31:16,135751410-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'host', 'add', 'sm1', '10.10.2.5'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add sm1 10.10.2.5
Inferring fsid 7a953d1e-37fb-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/7a953d1e-37fb-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Added host 'sm1' with addr '10.10.2.5'

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:31:27,240263813-07:00] INFO: Sleep for 20 seconds[0m


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:31:47,245083452-07:00] STAGE: Add OSDs to the cluster...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:31:47,251861775-07:00] INFO: > Adding devices ['/dev/nvme0n1'] on host 'sm1' as OSDs[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:388 - add_host_devices_as_osds() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd sm1:/dev/nvme0n1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:31:47,262784849-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:31:47,267023096-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'daemon', 'add', 'osd', 'sm1:/dev/nvme0n1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd sm1:/dev/nvme0n1
Inferring fsid 7a953d1e-37fb-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/7a953d1e-37fb-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Created osd(s) 0 on host 'sm1'

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:32:11,008105715-07:00] INFO: > Sleep for 20 seconds[0m


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:32:31,012848743-07:00] STAGE: Show Cluster Status[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:408 - show_status() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:32:31,022893698-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:32:31,026637786-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', '--status'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status
Inferring fsid 7a953d1e-37fb-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/7a953d1e-37fb-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
  cluster:
    id:     7a953d1e-37fb-11ec-b51d-53e6e728d2d3
    health: HEALTH_WARN
            1 pool(s) have no replicas configured
 
  services:
    mon: 1 daemons, quorum node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us (age 2m)
    mgr: node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us.bfoafl(active, since 2m)
    osd: 1 osds: 1 up (since 20s), 1 in (since 40s)
 
  data:
    pools:   1 pools, 1 pgs
    objects: 0 objects, 0 B
    usage:   4.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     1 active+clean
 

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:32:40,072937321-07:00] INFO: > Copy configuration files to local[0m
# ./benchmarks/bench-rados:220 - launch_ceph_cluster() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.1 sudo chown --changes --recursive ljishen /tmp/bench-rados/deployment_data_root/etc/ceph
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:32:40,080524950-07:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.1 sudo chown --changes --recursive ljishen /tmp/bench-rados/deployment_data_root/etc/ceph
[1] 07:32:40 [SUCCESS] ljishen@10.10.2.1
# ./benchmarks/bench-rados:223 - launch_ceph_cluster() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.1:/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf
# ./benchmarks/bench-rados:226 - launch_ceph_cluster() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.1:/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:32:40,556804214-07:00] STAGE: Update local ceph.conf as a client...[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:32:40,560372910-07:00] STAGE: Configure Ceph pool...[0m
# ./benchmarks/bench-rados:257 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_max_backfills 32
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:32:40,582070629-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:32:40,584923245-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '7a953d1e-37fb-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'config', 'set', 'osd', 'osd_max_backfills', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 7a953d1e-37fb-11ec-b51d-53e6e728d2d3 -- ceph config set osd osd_max_backfills 32
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:258 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_active 32
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:32:45,029309145-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:32:45,032258453-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '7a953d1e-37fb-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_active', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 7a953d1e-37fb-11ec-b51d-53e6e728d2d3 -- ceph config set osd osd_recovery_max_active 32
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:259 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_single_start 8
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:32:49,395396543-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:32:49,398329280-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '7a953d1e-37fb-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_single_start', '8'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 7a953d1e-37fb-11ec-b51d-53e6e728d2d3 -- ceph config set osd osd_recovery_max_single_start 8
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:260 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_op_priority 63
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:32:53,736844598-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:32:53,739831046-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '7a953d1e-37fb-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_op_priority', '63'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 7a953d1e-37fb-11ec-b51d-53e6e728d2d3 -- ceph config set osd osd_recovery_op_priority 63
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:262 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config show-with-defaults osd.0
# ./benchmarks/bench-rados:263 - configure_ceph_pool() > grep 'osd_max_backfills\|osd_recovery'
# ./benchmarks/bench-rados:263 - configure_ceph_pool() > column -t -s ' '
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
osd_max_backfills                        32         mon
osd_recovery_cost                        20971520   default
osd_recovery_delay_start                 0.000000   default
osd_recovery_max_active                  32         mon
osd_recovery_max_active_hdd              3          default
osd_recovery_max_active_ssd              10         default
osd_recovery_max_chunk                   8388608    default
osd_recovery_max_omap_entries_per_chunk  8096       default
osd_recovery_max_single_start            8          mon
osd_recovery_op_priority                 63         mon
osd_recovery_op_warn_multiple            16         default
osd_recovery_priority                    5          default
osd_recovery_retry_interval              30.000000  default
osd_recovery_sleep                       0.000000   default
osd_recovery_sleep_hdd                   0.100000   default
osd_recovery_sleep_hybrid                0.025000   default
osd_recovery_sleep_ssd                   0.000000   default
# ./benchmarks/bench-rados:265 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool create bench_rados 128 128 replicated --size 1 --pg-num-min 128
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:33:02,362813961-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:33:02,365699329-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '7a953d1e-37fb-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'create', 'bench_rados', '128', '128', 'replicated', '--size', '1', '--pg-num-min', '128'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 7a953d1e-37fb-11ec-b51d-53e6e728d2d3 -- ceph osd pool create bench_rados 128 128 replicated --size 1 --pg-num-min 128
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
pool 'bench_rados' created
# ./benchmarks/bench-rados:267 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados min_size 1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:33:07,418991559-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:33:07,422194746-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '7a953d1e-37fb-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'min_size', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 7a953d1e-37fb-11ec-b51d-53e6e728d2d3 -- ceph osd pool set bench_rados min_size 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 min_size to 1
# ./benchmarks/bench-rados:268 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados pg_autoscale_mode off
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:33:11,903817700-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:33:11,906734216-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '7a953d1e-37fb-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'pg_autoscale_mode', 'off'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 7a953d1e-37fb-11ec-b51d-53e6e728d2d3 -- ceph osd pool set bench_rados pg_autoscale_mode off
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 pg_autoscale_mode to off
# ./benchmarks/bench-rados:269 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool application enable bench_rados benchmark
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:33:17,457289235-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:33:17,460402602-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '7a953d1e-37fb-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'application', 'enable', 'bench_rados', 'benchmark'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 7a953d1e-37fb-11ec-b51d-53e6e728d2d3 -- ceph osd pool application enable bench_rados benchmark
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
enabled application 'benchmark' on pool 'bench_rados'
# ./benchmarks/bench-rados:270 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados noscrub 1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:33:22,668564654-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:33:22,671461412-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '7a953d1e-37fb-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'noscrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 7a953d1e-37fb-11ec-b51d-53e6e728d2d3 -- ceph osd pool set bench_rados noscrub 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 noscrub to 1
# ./benchmarks/bench-rados:271 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados nodeep-scrub 1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:33:27,343923652-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:33:27,346968360-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '7a953d1e-37fb-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'nodeep-scrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 7a953d1e-37fb-11ec-b51d-53e6e728d2d3 -- ceph osd pool set bench_rados nodeep-scrub 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 nodeep-scrub to 1
# ./benchmarks/bench-rados:272 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool ls detail
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:33:31,816929448-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:33:31,819944309-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '7a953d1e-37fb-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'ls', 'detail'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 7a953d1e-37fb-11ec-b51d-53e6e728d2d3 -- ceph osd pool ls detail
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
pool 1 'device_health_metrics' replicated size 1 min_size 1 crush_rule 0 object_hash rjenkins pg_num 64 pgp_num 64 autoscale_mode on last_change 17 lfor 0/0/15 flags hashpspool stripe_width 0 pg_num_min 1 application mgr_devicehealth
pool 2 'bench_rados' replicated size 1 min_size 1 crush_rule 0 object_hash rjenkins pg_num 128 pgp_num 128 autoscale_mode off last_change 19 flags hashpspool,noscrub,nodeep-scrub stripe_width 0 pg_num_min 128 application benchmark

# ./benchmarks/bench-rados:273 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd df tree
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:33:36,216068162-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:33:36,218931428-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '7a953d1e-37fb-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'df', 'tree'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 7a953d1e-37fb-11ec-b51d-53e6e728d2d3 -- ceph osd df tree
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
ID  CLASS  WEIGHT   REWEIGHT  SIZE     RAW USE  DATA     OMAP  META     AVAIL    %USE  VAR   PGS  STATUS  TYPE NAME    
-1         0.09769         -  100 GiB  5.7 MiB  160 KiB   0 B  5.6 MiB  100 GiB  0.01  1.00    -          root default 
-3         0.09769         -  100 GiB  5.7 MiB  160 KiB   0 B  5.6 MiB  100 GiB  0.01  1.00    -              host sm1 
 0    ssd  0.09769   1.00000  100 GiB  5.7 MiB  160 KiB   0 B  5.6 MiB  100 GiB  0.01  1.00  192      up          osd.0
                       TOTAL  100 GiB  5.7 MiB  160 KiB   0 B  5.6 MiB  100 GiB  0.01                                  
MIN/MAX VAR: 1.00/1.00  STDDEV: 0


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:33:40,492062491-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:34:04,798383782-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:34:14,087915476-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:34:23,451947376-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:34:23,458828503-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:34:32,866517063-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:34:32,873847437-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:34:42,207445345-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:34:42,214424808-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:34:51,588289428-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:34:51,595304538-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:35:00,837363582-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:35:00,844844138-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:35:00,850186416-07:00] INFO: > The cluster is idle now.[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:35:00,853465295-07:00] STAGE: Start benchmarking of mode write ...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:35:00,859389118-07:00] INFO: > Start system activity collection services[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:35:00,864504288-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:341 - rados_bench() > rm --force /mnt/sda8/ceph-research/scripts/output/sys_activity_16KB_write.dat.rados_bench_host.2
# ./benchmarks/bench-rados:342 - rados_bench() > sar_pid_rados_bench_host=403382
# ./benchmarks/bench-rados:342 - rados_bench() > S_TIME_FORMAT=ISO
# ./benchmarks/bench-rados:342 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/output/sys_activity_16KB_write.dat.rados_bench_host.2 2

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:35:00,871056356-07:00] INFO: >> for OSD host[0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:35:00,879978659-07:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.5 bash
10.10.2.5: b'109082\n'
[1] 07:35:02 [SUCCESS] ljishen@10.10.2.5
109082

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:35:02,052348272-07:00] INFO: > Run rados bench[0m
# ./benchmarks/bench-rados:370 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/output/rados_bench_16KB_write.log.2
## ./benchmarks/bench-rados:361 - rados_bench() > count_cpus 32-63,96-127
# ./benchmarks/bench-rados:361 - rados_bench() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 60 write --pool bench_rados -b 16384 -O 16384 --concurrent-ios 128 --show-time --write-object --write-omap --write-xattr --no-cleanup
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:35:02,072450925-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:35:02,075273314-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '7a953d1e-37fb-11ec-b51d-53e6e728d2d3', '--', 'rados', 'bench', '60', 'write', '--pool', 'bench_rados', '-b', '16384', '-O', '16384', '--concurrent-ios', '128', '--show-time', '--write-object', '--write-omap', '--write-xattr', '--no-cleanup'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 7a953d1e-37fb-11ec-b51d-53e6e728d2d3 -- rados bench 60 write --pool bench_rados -b 16384 -O 16384 --concurrent-ios 128 --show-time --write-object --write-omap --write-xattr --no-cleanup
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
hints = 1
2021-10-28T14:35:05.498683+0000 Maintaining 128 concurrent writes of 16384 bytes to objects of size 16384 for up to 60 seconds or 0 objects
2021-10-28T14:35:05.498701+0000 Object prefix: benchmark_data_node-2.bluefield2.ucsc-cmps10_7
2021-10-28T14:35:05.499768+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-28T14:35:05.499768+0000     0       0         0         0         0         0           -           0
2021-10-28T14:35:06.499908+0000     1     128      3089      2961   46.2622   46.2656   0.0351873   0.0424813
2021-10-28T14:35:07.500027+0000     2     128      6098      5970   46.6361   47.0156   0.0432156   0.0425309
2021-10-28T14:35:08.500141+0000     3     127      9069      8942   46.5682   46.4375   0.0474104    0.042598
2021-10-28T14:35:09.500218+0000     4     128     12049     11921   46.5619   46.5469   0.0437424   0.0427034
2021-10-28T14:35:10.500333+0000     5     128     14993     14865   46.4485        46   0.0466151   0.0428772
2021-10-28T14:35:11.500449+0000     6     128     17937     17809   46.3729        46   0.0457659   0.0429419
2021-10-28T14:35:12.500533+0000     7     128     20881     20753    46.319        46   0.0471502   0.0430785
2021-10-28T14:35:13.500613+0000     8     128     23695     23567   46.0248   43.9688   0.0459967   0.0433477
2021-10-28T14:35:14.500726+0000     9     128     26578     26450   45.9156   45.0469   0.0481318   0.0434637
2021-10-28T14:35:15.500846+0000    10     128     29394     29266   45.7235        44   0.0430673   0.0436135
2021-10-28T14:35:16.500971+0000    11     128     31633     31505   44.7468   34.9844   0.0993396   0.0444563
2021-10-28T14:35:17.501054+0000    12     128     33041     32913   42.8511        22    0.095334   0.0464406
2021-10-28T14:35:18.501169+0000    13     128     34449     34321    41.247        22    0.090305   0.0483744
2021-10-28T14:35:19.501283+0000    14     128     35857     35729    39.872        22   0.0930742   0.0500503
2021-10-28T14:35:20.501400+0000    15     128     37265     37137   38.6803        22   0.0951693   0.0515861
2021-10-28T14:35:21.501476+0000    16     128     38545     38417   37.5128        20    0.109205   0.0531083
2021-10-28T14:35:22.501589+0000    17     128     39762     39634   36.4245   19.0156   0.0996808   0.0546737
2021-10-28T14:35:23.501703+0000    18     127     41110     40983   35.5718   21.0781   0.0952721   0.0560125
2021-10-28T14:35:24.501816+0000    19     128     42578     42450   34.9059   22.9219   0.0472003   0.0572549
2021-10-28T14:35:25.501894+0000 min lat: 0.0304627 max lat: 0.247876 avg lat: 0.0571239
2021-10-28T14:35:25.501894+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-28T14:35:25.501894+0000    20     128     44882     44754   34.9605        36   0.0469622   0.0571239
2021-10-28T14:35:26.502015+0000    21     128     47761     47633   35.4375   44.9844   0.0412063    0.056361
2021-10-28T14:35:27.502128+0000    22     128     50577     50449   35.8265        44   0.0411541   0.0557469
2021-10-28T14:35:28.502244+0000    23     128     53480     53352   36.2408   45.3594   0.0443112   0.0551242
2021-10-28T14:35:29.502324+0000    24     128     56337     56209   36.5906   44.6406    0.046003   0.0545941
2021-10-28T14:35:30.502443+0000    25     128     59153     59025   36.8868        44   0.0429277   0.0541564
2021-10-28T14:35:31.502560+0000    26     128     61969     61841   37.1602        44   0.0550918   0.0537607
2021-10-28T14:35:32.502671+0000    27     128     63505     63377   36.6727        24   0.0874751   0.0544501
2021-10-28T14:35:33.502748+0000    28     128     64338     64210   35.8277   13.0156    0.279402   0.0554288
2021-10-28T14:35:34.502862+0000    29     128     64850     64722   34.8681         8    0.268738   0.0570827
2021-10-28T14:35:35.502975+0000    30     128     65297     65169   33.9386   6.98438     0.32348   0.0586861
2021-10-28T14:35:36.503089+0000    31     128     65746     65618   33.0701   7.01562    0.254088   0.0601417
2021-10-28T14:35:37.503165+0000    32     128     66258     66130   32.2867         8     0.26578   0.0617792
2021-10-28T14:35:38.503279+0000    33     128     66642     66514   31.4901         6    0.317126   0.0631154
2021-10-28T14:35:39.503397+0000    34     128     67089     66961   30.7693   6.98438    0.332855   0.0646922
2021-10-28T14:35:40.503513+0000    35     128     67601     67473   30.1187         8    0.265057   0.0661905
2021-10-28T14:35:41.503593+0000    36     128     68306     68178   29.5881   11.0156   0.0753254   0.0674777
2021-10-28T14:35:42.503706+0000    37     128     69649     69521   29.3555   20.9844   0.0865364   0.0680847
2021-10-28T14:35:43.503823+0000    38     128     71250     71122   29.2412   25.0156   0.0821819   0.0683244
2021-10-28T14:35:44.503948+0000    39     128     72914     72786    29.158        26   0.0718688   0.0685105
2021-10-28T14:35:45.504032+0000 min lat: 0.0304627 max lat: 0.35793 avg lat: 0.0686705
2021-10-28T14:35:45.504032+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-28T14:35:45.504032+0000    40     128     74641     74513   29.1036   26.9844   0.0713635   0.0686705
2021-10-28T14:35:46.504157+0000    41     127     76242     76115   29.0042   25.0312   0.0829994   0.0688531
2021-10-28T14:35:47.504289+0000    42     128     77906     77778   28.9322   25.9844   0.0822014    0.069048
2021-10-28T14:35:48.504406+0000    43     128     79570     79442   28.8639        26   0.0832762   0.0692426
2021-10-28T14:35:49.504483+0000    44     128     81169     81041   28.7757   24.9844   0.0847861   0.0694417
2021-10-28T14:35:50.504597+0000    45     128     82705     82577   28.6695        24   0.0809291    0.069685
2021-10-28T14:35:51.504717+0000    46     128     84306     84178     28.59   25.0156   0.0989545   0.0698786
2021-10-28T14:35:52.504840+0000    47     128     85905     85777   28.5133   24.9844   0.0894962   0.0700811
2021-10-28T14:35:53.504926+0000    48     128     87506     87378   28.4403   25.0156   0.0819798   0.0702702
2021-10-28T14:35:54.505046+0000    49     128     89233     89105   28.4106   26.9844   0.0472233   0.0703509
2021-10-28T14:35:55.505162+0000    50     128     92114     91986   28.7426   45.0156   0.0439465    0.069558
2021-10-28T14:35:56.505281+0000    51     128     93969     93841   28.7472   28.9844    0.103408   0.0695122
2021-10-28T14:35:57.505359+0000    52     128     95409     95281   28.6271      22.5   0.0914271   0.0697967
2021-10-28T14:35:58.505477+0000    53     128     96850     96722   28.5117   22.5156   0.0963053   0.0700933
2021-10-28T14:35:59.505596+0000    54     128     98258     98130   28.3911        22   0.0900935   0.0704087
2021-10-28T14:36:00.505708+0000    55     128     99538     99410   28.2385        20   0.0887048   0.0707679
2021-10-28T14:36:01.505785+0000    56     128    100833    100705   28.0955   20.2344    0.100025   0.0711081
2021-10-28T14:36:02.505905+0000    57     128    102098    101970   27.9493   19.7656   0.0902719   0.0714894
2021-10-28T14:36:03.506029+0000    58     128    103441    103313   27.8292   20.9844    0.104868   0.0718025
2021-10-28T14:36:04.506152+0000    59     128    105426    105298   27.8831   31.0156   0.0436016   0.0716963
2021-10-28T14:36:05.506238+0000 min lat: 0.0304627 max lat: 0.35793 avg lat: 0.0713852
2021-10-28T14:36:05.506238+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-28T14:36:05.506238+0000    60     128    107665    107537   28.0014   34.9844   0.0455158   0.0713852
2021-10-28T14:36:06.506414+0000 Total time run:         60.0273
Total writes made:      107665
Write size:             16384
Object size:            16384
Bandwidth (MB/sec):     28.025
Stddev Bandwidth:       12.7681
Max bandwidth (MB/sec): 47.0156
Min bandwidth (MB/sec): 6
Average IOPS:           1793
Stddev IOPS:            817.158
Max IOPS:               3009
Min IOPS:               384
Average Latency(s):     0.0713501
Stddev Latency(s):      0.0471151
Max latency(s):         0.35793
Min latency(s):         0.0304627

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:36:07,249455583-07:00] INFO: > Stop system activity collection services[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:36:07,254679708-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:391 - rados_bench() > kill -INT 403382

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:36:07,260154145-07:00] INFO: >> for OSD host[0m
# ./benchmarks/bench-rados:396 - rados_bench() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.5 kill -INT 109082
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:36:07,267832565-07:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.5 kill -INT 109082
[1] 07:36:08 [SUCCESS] ljishen@10.10.2.5

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:36:08,403891801-07:00] INFO: >>> Copy system activity file from OSD host[0m
## ./benchmarks/bench-rados:403 - rados_bench() > basename -- /tmp/bench-rados/sys_activity_16KB_write.dat.osd_host.2
# ./benchmarks/bench-rados:403 - rados_bench() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.5:/tmp/bench-rados/sys_activity_16KB_write.dat.osd_host.2 /mnt/sda8/ceph-research/scripts/output/sys_activity_16KB_write.dat.osd_host.2


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:36:09,470415167-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:36:33,753247529-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 107.67k objects, 1.6 GiB
    usage:   5.1 GiB used, 95 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:36:33,760682961-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:36:42,995090319-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 107.67k objects, 1.6 GiB
    usage:   5.1 GiB used, 95 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:36:43,002758279-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:36:52,332582210-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 107.67k objects, 1.6 GiB
    usage:   5.1 GiB used, 95 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:36:52,340186481-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:37:01,659504603-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 107.67k objects, 1.6 GiB
    usage:   5.1 GiB used, 95 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:37:01,666845908-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:37:11,165770679-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 107.67k objects, 1.6 GiB
    usage:   5.1 GiB used, 95 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:37:11,172677976-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:37:11,178031244-07:00] INFO: > The cluster is idle now.[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:37:11,181356360-07:00] STAGE: Start benchmarking of mode seq ...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:37:11,187547738-07:00] INFO: > Start system activity collection services[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:37:11,192589168-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:341 - rados_bench() > rm --force /mnt/sda8/ceph-research/scripts/output/sys_activity_16KB_seq.dat.rados_bench_host.2
# ./benchmarks/bench-rados:342 - rados_bench() > sar_pid_rados_bench_host=404749
# ./benchmarks/bench-rados:342 - rados_bench() > S_TIME_FORMAT=ISO
# ./benchmarks/bench-rados:342 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/output/sys_activity_16KB_seq.dat.rados_bench_host.2 2

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:37:11,199145734-07:00] INFO: >> for OSD host[0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:37:11,207963471-07:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.5 bash
10.10.2.5: b'109630\n'
[1] 07:37:12 [SUCCESS] ljishen@10.10.2.5
109630

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:37:12,303220546-07:00] INFO: > Run rados bench[0m
# ./benchmarks/bench-rados:379 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/output/rados_bench_16KB_seq.log.2
## ./benchmarks/bench-rados:376 - rados_bench() > count_cpus 32-63,96-127
# ./benchmarks/bench-rados:376 - rados_bench() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 128 --show-time
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:37:12,322366436-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:37:12,325294143-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '7a953d1e-37fb-11ec-b51d-53e6e728d2d3', '--', 'rados', 'bench', '99999999', 'seq', '--pool', 'bench_rados', '--concurrent-ios', '128', '--show-time'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 7a953d1e-37fb-11ec-b51d-53e6e728d2d3 -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 128 --show-time
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
hints = 1
2021-10-28T14:37:15.615594+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-28T14:37:15.615594+0000     0       0         0         0         0         0           -           0
2021-10-28T14:37:16.615693+0000     1     128     13955     13827    216.01   216.047   0.0109566  0.00920671
2021-10-28T14:37:17.615780+0000     2     127     27705     27578   215.426   214.859  0.00924862  0.00926163
2021-10-28T14:37:18.615880+0000     3     128     41945     41817   217.771   222.484   0.0103945  0.00916243
2021-10-28T14:37:19.615958+0000     4     128     55158     55030   214.938   206.453  0.00969311  0.00928947
2021-10-28T14:37:20.616038+0000     5     128     68448     68320   213.478   207.656  0.00863355   0.0093527
2021-10-28T14:37:21.616114+0000     6     127     81310     81183   211.393   200.984    0.010142  0.00944992
2021-10-28T14:37:22.616190+0000     7     128     94021     93893   209.563   198.594   0.0106547  0.00953157
2021-10-28T14:37:23.616266+0000     8     127    106417    106290   207.578   193.703  0.00506125  0.00962491
2021-10-28T14:37:24.616414+0000 Total time run:       8.13478
Total reads made:     107665
Read size:            16384
Object size:          16384
Bandwidth (MB/sec):   206.799
Average IOPS:         13235
Stddev IOPS:          623.754
Max IOPS:             14239
Min IOPS:             12397
Average Latency(s):   0.009662
Max latency(s):       0.0412415
Min latency(s):       0.00146011

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:37:25,337779517-07:00] INFO: > Stop system activity collection services[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:37:25,342943248-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:391 - rados_bench() > kill -INT 404749

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:37:25,348587404-07:00] INFO: >> for OSD host[0m
# ./benchmarks/bench-rados:396 - rados_bench() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.5 kill -INT 109630
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:37:25,356529601-07:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.5 kill -INT 109630
[1] 07:37:26 [SUCCESS] ljishen@10.10.2.5

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:37:26,459652178-07:00] INFO: >>> Copy system activity file from OSD host[0m
## ./benchmarks/bench-rados:403 - rados_bench() > basename -- /tmp/bench-rados/sys_activity_16KB_seq.dat.osd_host.2
# ./benchmarks/bench-rados:403 - rados_bench() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.5:/tmp/bench-rados/sys_activity_16KB_seq.dat.osd_host.2 /mnt/sda8/ceph-research/scripts/output/sys_activity_16KB_seq.dat.osd_host.2


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:37:27,524768323-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:37:51,807796221-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 107.67k objects, 1.6 GiB
    usage:   5.1 GiB used, 95 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:37:51,814926308-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:38:01,191911587-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 107.67k objects, 1.6 GiB
    usage:   5.1 GiB used, 95 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:38:01,199216132-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:38:10,575222677-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 107.67k objects, 1.6 GiB
    usage:   5.1 GiB used, 95 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:38:10,582294413-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:38:19,883984048-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 107.67k objects, 1.6 GiB
    usage:   5.1 GiB used, 95 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:38:19,891158900-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:38:29,281629192-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 107.67k objects, 1.6 GiB
    usage:   5.1 GiB used, 95 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:38:29,288899142-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:38:29,294475170-07:00] INFO: > The cluster is idle now.[0m


[1;7;39;49m[2021-10-28T07:38:29,296823665-07:00][RUNNING][ROUND 3/2/21] object_size=16KB[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:38:29,300344941-07:00] STAGE: Launch a Ceph cluster (CEPH_HOST: 10.10.2.1, OSD_HOST: 10.10.2.5) ...[0m
# ./benchmarks/bench-rados:183 - launch_ceph_cluster() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.1 bash -euo pipefail
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:38:29,309431655-07:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.1 bash -euo pipefail
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T07:38:29,726062153-07:00] INFO: > Remove existing clusters\x1b[0m\n'
10.10.2.1: b'## bash:17 -  > basename -- /var/lib/ceph/7a953d1e-37fb-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'# bash:17 -  > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm --host 10.10.2.1 --host 10.10.2.5 rm-cluster --force --fsid 7a953d1e-37fb-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T07:38:29,736153284-07:00] DEBUG: HOST_PARAMS: ['--host', '10.10.2.1', '--host', '10.10.2.5']\x1b[0m\n"
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T07:38:29,739525183-07:00] DEBUG: CEPHADM_PARAMS: ['rm-cluster', '--force', '--fsid', '7a953d1e-37fb-11ec-b51d-53e6e728d2d3']\x1b[0m\n"
10.10.2.1: b"# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:63 -  > /tmp/bench-rados/ceph-research/scripts/parallel-ssh --host 10.10.2.1 --host 10.10.2.5 'sudo python3 -u - rm-cluster --force --fsid 7a953d1e-37fb-11ec-b51d-53e6e728d2d3'\n"
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T07:38:29,747093721-07:00] DEBUG: command from stdin\x1b[0m\n'
10.10.2.1: b"# /tmp/bench-rados/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host 10.10.2.1 --host 10.10.2.5 'sudo python3 -u - rm-cluster --force --fsid 7a953d1e-37fb-11ec-b51d-53e6e728d2d3'\n"
10.10.2.1: b'[1] 07:38:35 [SUCCESS] 10.10.2.1\n[2] 07:38:39 [SUCCESS] 10.10.2.5\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T07:38:39,551830370-07:00] INFO: > Deploy a new cluster\x1b[0m\n'
10.10.2.1: b'# bash:24 -  > /tmp/bench-rados/ceph-research/scripts/ceph-deploy -m 10.10.2.1 -o sm1/10.10.2.5:/dev/nvme0n1\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T07:38:39,562645712-07:00] STAGE: Verify passwordless SSH to hosts, passwordless sudo via ljishen, and the existence of devices on hosts...\x1b[0m\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T07:38:39,567507421-07:00] INFO: > Check the host for the monitor\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:181 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@10.10.2.1 sudo --non-interactive --validate\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T07:38:39,715718143-07:00] INFO: > Check hosts for OSDs\x1b[0m\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T07:38:39,720245174-07:00] INFO: >> Check host 'sm1'\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:200 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 sudo --non-interactive --validate\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T07:38:40,798736684-07:00] INFO: >>> Check device '/dev/nvme0n1'\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:215 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 test -b /dev/nvme0n1\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:226 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 lsblk --nodeps --noheadings --output TYPE /dev/nvme0n1\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:227 - verify_parameters() > grep --word-regexp disk\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T07:38:42,931623445-07:00] STAGE: Reset devices on hosts...\x1b[0m\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T07:38:42,936389075-07:00] INFO: > Resetting device '/dev/nvme0n1' on host 'sm1'\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:288 - reset_host_devices() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 sudo sh\n'
10.10.2.1: b'  Removing ceph--62a9d784--aac9--42d2--8717--b6f2ff1c348f-osd--block--4b39fdec--a2ef--4d6f--a69c--432aad29756c (252:0)\n'
10.10.2.1: b'  Archiving volume group "ceph-62a9d784-aac9-42d2-8717-b6f2ff1c348f" metadata (seqno 5).\n'
10.10.2.1: b'  Releasing logical volume "osd-block-4b39fdec-a2ef-4d6f-a69c-432aad29756c"\n'
10.10.2.1: b'  Creating volume group backup "/etc/lvm/backup/ceph-62a9d784-aac9-42d2-8717-b6f2ff1c348f" (seqno 6).\n'
10.10.2.1: b'  Logical volume "osd-block-4b39fdec-a2ef-4d6f-a69c-432aad29756c" successfully removed\n'
10.10.2.1: b'  Removing physical volume "/dev/nvme0n1" from volume group "ceph-62a9d784-aac9-42d2-8717-b6f2ff1c348f"\n'
10.10.2.1: b'  Volume group "ceph-62a9d784-aac9-42d2-8717-b6f2ff1c348f" successfully removed\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T07:38:45,242342555-07:00] STAGE: Bootstrap a new cluster...\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:317 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.1 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T07:38:45,251902957-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T07:38:45,255467188-07:00] DEBUG: CEPHADM_PARAMS: ['bootstrap', '--config', '/tmp/bench-rados/ceph-research/scripts/ceph.conf', '--mon-ip', '10.10.2.1', '--output-dir', '/tmp/bench-rados/deployment_data_root/etc/ceph', '--ssh-user', 'ljishen', '--skip-dashboard', '--no-minimize-config', '--allow-overwrite', '--allow-fqdn-hostname', '--orphan-initial-daemons', '--skip-monitoring-stack', '--single-host-defaults']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.1 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults\n'
10.10.2.1: b'Verifying podman|docker is present...\nVerifying lvm2 is present...\n'
10.10.2.1: b'Verifying time synchronization is in place...\n'
10.10.2.1: b'Unit ntp.service is enabled and running\n'
10.10.2.1: b'Repeating the final host check...\n'
10.10.2.1: b'podman|docker (/usr/bin/docker) is present\nsystemctl is present\n'
10.10.2.1: b'lvcreate is present\n'
10.10.2.1: b'Unit ntp.service is enabled and running\n'
10.10.2.1: b'Host looks OK\n'
10.10.2.1: b'Cluster fsid: c13940fc-37fc-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Verifying IP 10.10.2.1 port 3300 ...\n'
10.10.2.1: b'Verifying IP 10.10.2.1 port 6789 ...\n'
10.10.2.1: b'Mon IP `10.10.2.1` is in CIDR network `10.10.2.0/24`\n'
10.10.2.1: b'- internal network (--cluster-network) has not been provided, OSD replication will default to the public_network\n'
10.10.2.1: b'Adjusting default settings to suit single-host cluster...\n'
10.10.2.1: b'Pulling container image quay.io/ceph/ceph:v16...\n'
10.10.2.1: b'Ceph version: ceph version 16.2.6 (ee28fb57e47e9f88813e24bbf4c14496ca299d31) pacific (stable)\n'
10.10.2.1: b'Extracting ceph user uid/gid from container image...\n'
10.10.2.1: b'Creating initial keys...\n'
10.10.2.1: b'Creating initial monmap...\n'
10.10.2.1: b'Creating mon...\n'
10.10.2.1: b'Waiting for mon to start...\n'
10.10.2.1: b'Waiting for mon...\n'
10.10.2.1: b'mon is available\n'
10.10.2.1: b'Setting mon public_network to 10.10.2.0/24\n'
10.10.2.1: b'Wrote config to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf\n'
10.10.2.1: b'Wrote keyring to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring\n'
10.10.2.1: b'Creating mgr...\n'
10.10.2.1: b'Verifying port 9283 ...\n'
10.10.2.1: b'Waiting for mgr to start...\n'
10.10.2.1: b'Waiting for mgr...\n'
10.10.2.1: b'mgr not available, waiting (1/15)...\n'
10.10.2.1: b'mgr not available, waiting (2/15)...\n'
10.10.2.1: b'mgr is available\n'
10.10.2.1: b'Enabling cephadm module...\n'
10.10.2.1: b'Waiting for the mgr to restart...\n'
10.10.2.1: b'Waiting for mgr epoch 5...\n'
10.10.2.1: b'mgr epoch 5 is available\n'
10.10.2.1: b'Setting orchestrator backend to cephadm...\n'
10.10.2.1: b'Generating ssh key...\n'
10.10.2.1: b'Wrote public SSH key to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub\n'
10.10.2.1: b'Adding key to ljishen@localhost authorized_keys...\n'
10.10.2.1: b'Adding host node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us...\n'
10.10.2.1: b'Deploying unmanaged mon service...\n'
10.10.2.1: b'Deploying unmanaged mgr service...\n'
10.10.2.1: b'You can access the Ceph CLI with:\n\n\tsudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --fsid c13940fc-37fc-11ec-b51d-53e6e728d2d3 -c /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf -k /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring\n\n'
10.10.2.1: b'Please consider enabling telemetry to help improve Ceph:\n\n\tceph telemetry on\n\nFor more information see:\n\n\thttps://docs.ceph.com/docs/pacific/mgr/telemetry/\n\nBootstrap complete.\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T07:39:43,861043469-07:00] INFO: Sleep for 20 seconds\x1b[0m\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T07:40:03,867519092-07:00] INFO: > Set the default number of monitor daemons to 1\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:337 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T07:40:03,876605153-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T07:40:03,880334424-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mon', '1']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1\n'
10.10.2.1: b'Inferring fsid c13940fc-37fc-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/c13940fc-37fc-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b'Scheduled mon update...\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T07:40:13,010500540-07:00] INFO: > Set the default number of manager daemons to 1\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:342 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T07:40:13,020558147-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T07:40:13,024392695-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mgr', '1']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1\n'
10.10.2.1: b'Inferring fsid c13940fc-37fc-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/c13940fc-37fc-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b'Scheduled mgr update...\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T07:40:21,999934653-07:00] STAGE: Add hosts to the cluster...\x1b[0m\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T07:40:22,005876233-07:00] INFO: > Adding host 'sm1'\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:361 - add_hosts() > ssh-copy-id -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -f -i /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub ljishen@sm1\n'
10.10.2.1: b'/usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: "/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub"\n'
10.10.2.1: b'\nNumber of key(s) added: 1\n\nNow try logging into the machine, with:   "ssh -o \'GlobalKnownHostsFile=/dev/null\' -o \'LogLevel=ERROR\' -o \'PasswordAuthentication=no\' -o \'StrictHostKeyChecking=no\' -o \'UserKnownHostsFile=/dev/null\' \'ljishen@sm1\'"\nand check to make sure that only the key(s) you wanted were added.\n\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:367 - add_hosts() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add sm1 10.10.2.5\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T07:40:23,154650608-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T07:40:23,158449970-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'host', 'add', 'sm1', '10.10.2.5']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add sm1 10.10.2.5\n'
10.10.2.1: b'Inferring fsid c13940fc-37fc-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/c13940fc-37fc-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b"Added host 'sm1' with addr '10.10.2.5'\n"
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T07:40:34,392138127-07:00] INFO: Sleep for 20 seconds\x1b[0m\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T07:40:54,396272040-07:00] STAGE: Add OSDs to the cluster...\x1b[0m\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T07:40:54,402472727-07:00] INFO: > Adding devices ['/dev/nvme0n1'] on host 'sm1' as OSDs\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:388 - add_host_devices_as_osds() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd sm1:/dev/nvme0n1\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T07:40:54,411570760-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T07:40:54,415075719-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'daemon', 'add', 'osd', 'sm1:/dev/nvme0n1']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd sm1:/dev/nvme0n1\n'
10.10.2.1: b'Inferring fsid c13940fc-37fc-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/c13940fc-37fc-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b"Created osd(s) 0 on host 'sm1'\n"
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T07:41:18,324946627-07:00] INFO: > Sleep for 20 seconds\x1b[0m\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T07:41:38,330371491-07:00] STAGE: Show Cluster Status\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:408 - show_status() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T07:41:38,340560425-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T07:41:38,344196430-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', '--status']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status\n'
10.10.2.1: b'Inferring fsid c13940fc-37fc-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/c13940fc-37fc-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b'  cluster:\n    id:     c13940fc-37fc-11ec-b51d-53e6e728d2d3\n    health: HEALTH_WARN\n            1 pool(s) have no replicas configured\n \n  services:\n    mon: 1 daemons, quorum node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us (age 2m)\n    mgr: node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us.dkeqmg(active, since 2m)\n    osd: 1 osds: 1 up (since 20s), 1 in (since 39s)\n \n  data:\n    pools:   1 pools, 1 pgs\n    objects: 0 objects, 0 B\n    usage:   4.7 MiB used, 100 GiB / 100 GiB avail\n    pgs:     1 active+clean\n \n'
[1] 07:41:46 [SUCCESS] ljishen@10.10.2.1

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:38:29,726062153-07:00] INFO: > Remove existing clusters[0m
## bash:17 -  > basename -- /var/lib/ceph/7a953d1e-37fb-11ec-b51d-53e6e728d2d3
# bash:17 -  > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm --host 10.10.2.1 --host 10.10.2.5 rm-cluster --force --fsid 7a953d1e-37fb-11ec-b51d-53e6e728d2d3
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:38:29,736153284-07:00] DEBUG: HOST_PARAMS: ['--host', '10.10.2.1', '--host', '10.10.2.5'][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:38:29,739525183-07:00] DEBUG: CEPHADM_PARAMS: ['rm-cluster', '--force', '--fsid', '7a953d1e-37fb-11ec-b51d-53e6e728d2d3'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:63 -  > /tmp/bench-rados/ceph-research/scripts/parallel-ssh --host 10.10.2.1 --host 10.10.2.5 'sudo python3 -u - rm-cluster --force --fsid 7a953d1e-37fb-11ec-b51d-53e6e728d2d3'
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:38:29,747093721-07:00] DEBUG: command from stdin[0m
# /tmp/bench-rados/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host 10.10.2.1 --host 10.10.2.5 'sudo python3 -u - rm-cluster --force --fsid 7a953d1e-37fb-11ec-b51d-53e6e728d2d3'
[1] 07:38:35 [SUCCESS] 10.10.2.1
[2] 07:38:39 [SUCCESS] 10.10.2.5

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:38:39,551830370-07:00] INFO: > Deploy a new cluster[0m
# bash:24 -  > /tmp/bench-rados/ceph-research/scripts/ceph-deploy -m 10.10.2.1 -o sm1/10.10.2.5:/dev/nvme0n1


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:38:39,562645712-07:00] STAGE: Verify passwordless SSH to hosts, passwordless sudo via ljishen, and the existence of devices on hosts...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:38:39,567507421-07:00] INFO: > Check the host for the monitor[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:181 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@10.10.2.1 sudo --non-interactive --validate

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:38:39,715718143-07:00] INFO: > Check hosts for OSDs[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:38:39,720245174-07:00] INFO: >> Check host 'sm1'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:200 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 sudo --non-interactive --validate

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:38:40,798736684-07:00] INFO: >>> Check device '/dev/nvme0n1'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:215 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 test -b /dev/nvme0n1
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:226 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 lsblk --nodeps --noheadings --output TYPE /dev/nvme0n1
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:227 - verify_parameters() > grep --word-regexp disk


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:38:42,931623445-07:00] STAGE: Reset devices on hosts...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:38:42,936389075-07:00] INFO: > Resetting device '/dev/nvme0n1' on host 'sm1'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:288 - reset_host_devices() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 sudo sh
  Removing ceph--62a9d784--aac9--42d2--8717--b6f2ff1c348f-osd--block--4b39fdec--a2ef--4d6f--a69c--432aad29756c (252:0)
  Archiving volume group "ceph-62a9d784-aac9-42d2-8717-b6f2ff1c348f" metadata (seqno 5).
  Releasing logical volume "osd-block-4b39fdec-a2ef-4d6f-a69c-432aad29756c"
  Creating volume group backup "/etc/lvm/backup/ceph-62a9d784-aac9-42d2-8717-b6f2ff1c348f" (seqno 6).
  Logical volume "osd-block-4b39fdec-a2ef-4d6f-a69c-432aad29756c" successfully removed
  Removing physical volume "/dev/nvme0n1" from volume group "ceph-62a9d784-aac9-42d2-8717-b6f2ff1c348f"
  Volume group "ceph-62a9d784-aac9-42d2-8717-b6f2ff1c348f" successfully removed


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:38:45,242342555-07:00] STAGE: Bootstrap a new cluster...[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:317 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.1 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:38:45,251902957-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:38:45,255467188-07:00] DEBUG: CEPHADM_PARAMS: ['bootstrap', '--config', '/tmp/bench-rados/ceph-research/scripts/ceph.conf', '--mon-ip', '10.10.2.1', '--output-dir', '/tmp/bench-rados/deployment_data_root/etc/ceph', '--ssh-user', 'ljishen', '--skip-dashboard', '--no-minimize-config', '--allow-overwrite', '--allow-fqdn-hostname', '--orphan-initial-daemons', '--skip-monitoring-stack', '--single-host-defaults'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.1 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults
Verifying podman|docker is present...
Verifying lvm2 is present...
Verifying time synchronization is in place...
Unit ntp.service is enabled and running
Repeating the final host check...
podman|docker (/usr/bin/docker) is present
systemctl is present
lvcreate is present
Unit ntp.service is enabled and running
Host looks OK
Cluster fsid: c13940fc-37fc-11ec-b51d-53e6e728d2d3
Verifying IP 10.10.2.1 port 3300 ...
Verifying IP 10.10.2.1 port 6789 ...
Mon IP `10.10.2.1` is in CIDR network `10.10.2.0/24`
- internal network (--cluster-network) has not been provided, OSD replication will default to the public_network
Adjusting default settings to suit single-host cluster...
Pulling container image quay.io/ceph/ceph:v16...
Ceph version: ceph version 16.2.6 (ee28fb57e47e9f88813e24bbf4c14496ca299d31) pacific (stable)
Extracting ceph user uid/gid from container image...
Creating initial keys...
Creating initial monmap...
Creating mon...
Waiting for mon to start...
Waiting for mon...
mon is available
Setting mon public_network to 10.10.2.0/24
Wrote config to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf
Wrote keyring to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring
Creating mgr...
Verifying port 9283 ...
Waiting for mgr to start...
Waiting for mgr...
mgr not available, waiting (1/15)...
mgr not available, waiting (2/15)...
mgr is available
Enabling cephadm module...
Waiting for the mgr to restart...
Waiting for mgr epoch 5...
mgr epoch 5 is available
Setting orchestrator backend to cephadm...
Generating ssh key...
Wrote public SSH key to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub
Adding key to ljishen@localhost authorized_keys...
Adding host node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us...
Deploying unmanaged mon service...
Deploying unmanaged mgr service...
You can access the Ceph CLI with:

	sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --fsid c13940fc-37fc-11ec-b51d-53e6e728d2d3 -c /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf -k /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring

Please consider enabling telemetry to help improve Ceph:

	ceph telemetry on

For more information see:

	https://docs.ceph.com/docs/pacific/mgr/telemetry/

Bootstrap complete.

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:39:43,861043469-07:00] INFO: Sleep for 20 seconds[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:40:03,867519092-07:00] INFO: > Set the default number of monitor daemons to 1[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:337 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:40:03,876605153-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:40:03,880334424-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mon', '1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1
Inferring fsid c13940fc-37fc-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/c13940fc-37fc-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Scheduled mon update...

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:40:13,010500540-07:00] INFO: > Set the default number of manager daemons to 1[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:342 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:40:13,020558147-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:40:13,024392695-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mgr', '1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1
Inferring fsid c13940fc-37fc-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/c13940fc-37fc-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Scheduled mgr update...


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:40:21,999934653-07:00] STAGE: Add hosts to the cluster...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:40:22,005876233-07:00] INFO: > Adding host 'sm1'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:361 - add_hosts() > ssh-copy-id -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -f -i /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub ljishen@sm1
/usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: "/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub"

Number of key(s) added: 1

Now try logging into the machine, with:   "ssh -o 'GlobalKnownHostsFile=/dev/null' -o 'LogLevel=ERROR' -o 'PasswordAuthentication=no' -o 'StrictHostKeyChecking=no' -o 'UserKnownHostsFile=/dev/null' 'ljishen@sm1'"
and check to make sure that only the key(s) you wanted were added.

# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:367 - add_hosts() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add sm1 10.10.2.5
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:40:23,154650608-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:40:23,158449970-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'host', 'add', 'sm1', '10.10.2.5'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add sm1 10.10.2.5
Inferring fsid c13940fc-37fc-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/c13940fc-37fc-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Added host 'sm1' with addr '10.10.2.5'

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:40:34,392138127-07:00] INFO: Sleep for 20 seconds[0m


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:40:54,396272040-07:00] STAGE: Add OSDs to the cluster...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:40:54,402472727-07:00] INFO: > Adding devices ['/dev/nvme0n1'] on host 'sm1' as OSDs[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:388 - add_host_devices_as_osds() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd sm1:/dev/nvme0n1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:40:54,411570760-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:40:54,415075719-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'daemon', 'add', 'osd', 'sm1:/dev/nvme0n1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd sm1:/dev/nvme0n1
Inferring fsid c13940fc-37fc-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/c13940fc-37fc-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Created osd(s) 0 on host 'sm1'

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:41:18,324946627-07:00] INFO: > Sleep for 20 seconds[0m


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:41:38,330371491-07:00] STAGE: Show Cluster Status[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:408 - show_status() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:41:38,340560425-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:41:38,344196430-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', '--status'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status
Inferring fsid c13940fc-37fc-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/c13940fc-37fc-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
  cluster:
    id:     c13940fc-37fc-11ec-b51d-53e6e728d2d3
    health: HEALTH_WARN
            1 pool(s) have no replicas configured
 
  services:
    mon: 1 daemons, quorum node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us (age 2m)
    mgr: node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us.dkeqmg(active, since 2m)
    osd: 1 osds: 1 up (since 20s), 1 in (since 39s)
 
  data:
    pools:   1 pools, 1 pgs
    objects: 0 objects, 0 B
    usage:   4.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     1 active+clean
 

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:41:46,848169331-07:00] INFO: > Copy configuration files to local[0m
# ./benchmarks/bench-rados:220 - launch_ceph_cluster() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.1 sudo chown --changes --recursive ljishen /tmp/bench-rados/deployment_data_root/etc/ceph
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:41:46,855783960-07:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.1 sudo chown --changes --recursive ljishen /tmp/bench-rados/deployment_data_root/etc/ceph
[1] 07:41:47 [SUCCESS] ljishen@10.10.2.1
# ./benchmarks/bench-rados:223 - launch_ceph_cluster() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.1:/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf
# ./benchmarks/bench-rados:226 - launch_ceph_cluster() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.1:/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:41:47,332978185-07:00] STAGE: Update local ceph.conf as a client...[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:41:47,336255250-07:00] STAGE: Configure Ceph pool...[0m
# ./benchmarks/bench-rados:257 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_max_backfills 32
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:41:47,357982454-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:41:47,360834489-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'c13940fc-37fc-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'config', 'set', 'osd', 'osd_max_backfills', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid c13940fc-37fc-11ec-b51d-53e6e728d2d3 -- ceph config set osd osd_max_backfills 32
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:258 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_active 32
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:41:51,800790558-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:41:51,803686235-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'c13940fc-37fc-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_active', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid c13940fc-37fc-11ec-b51d-53e6e728d2d3 -- ceph config set osd osd_recovery_max_active 32
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:259 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_single_start 8
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:41:56,109226151-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:41:56,112468962-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'c13940fc-37fc-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_single_start', '8'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid c13940fc-37fc-11ec-b51d-53e6e728d2d3 -- ceph config set osd osd_recovery_max_single_start 8
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:260 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_op_priority 63
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:42:00,568251185-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:42:00,571175205-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'c13940fc-37fc-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_op_priority', '63'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid c13940fc-37fc-11ec-b51d-53e6e728d2d3 -- ceph config set osd osd_recovery_op_priority 63
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:262 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config show-with-defaults osd.0
# ./benchmarks/bench-rados:263 - configure_ceph_pool() > grep 'osd_max_backfills\|osd_recovery'
# ./benchmarks/bench-rados:263 - configure_ceph_pool() > column -t -s ' '
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
osd_max_backfills                        32         mon
osd_recovery_cost                        20971520   default
osd_recovery_delay_start                 0.000000   default
osd_recovery_max_active                  32         mon
osd_recovery_max_active_hdd              3          default
osd_recovery_max_active_ssd              10         default
osd_recovery_max_chunk                   8388608    default
osd_recovery_max_omap_entries_per_chunk  8096       default
osd_recovery_max_single_start            8          mon
osd_recovery_op_priority                 63         mon
osd_recovery_op_warn_multiple            16         default
osd_recovery_priority                    5          default
osd_recovery_retry_interval              30.000000  default
osd_recovery_sleep                       0.000000   default
osd_recovery_sleep_hdd                   0.100000   default
osd_recovery_sleep_hybrid                0.025000   default
osd_recovery_sleep_ssd                   0.000000   default
# ./benchmarks/bench-rados:265 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool create bench_rados 128 128 replicated --size 1 --pg-num-min 128
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:42:09,245720258-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:42:09,248958229-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'c13940fc-37fc-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'create', 'bench_rados', '128', '128', 'replicated', '--size', '1', '--pg-num-min', '128'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid c13940fc-37fc-11ec-b51d-53e6e728d2d3 -- ceph osd pool create bench_rados 128 128 replicated --size 1 --pg-num-min 128
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
pool 'bench_rados' created
# ./benchmarks/bench-rados:267 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados min_size 1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:42:14,694540483-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:42:14,697806719-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'c13940fc-37fc-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'min_size', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid c13940fc-37fc-11ec-b51d-53e6e728d2d3 -- ceph osd pool set bench_rados min_size 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 min_size to 1
# ./benchmarks/bench-rados:268 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados pg_autoscale_mode off
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:42:19,554664043-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:42:19,557884953-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'c13940fc-37fc-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'pg_autoscale_mode', 'off'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid c13940fc-37fc-11ec-b51d-53e6e728d2d3 -- ceph osd pool set bench_rados pg_autoscale_mode off
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 pg_autoscale_mode to off
# ./benchmarks/bench-rados:269 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool application enable bench_rados benchmark
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:42:24,428184289-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:42:24,431274522-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'c13940fc-37fc-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'application', 'enable', 'bench_rados', 'benchmark'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid c13940fc-37fc-11ec-b51d-53e6e728d2d3 -- ceph osd pool application enable bench_rados benchmark
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
enabled application 'benchmark' on pool 'bench_rados'
# ./benchmarks/bench-rados:270 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados noscrub 1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:42:30,020035632-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:42:30,022944012-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'c13940fc-37fc-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'noscrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid c13940fc-37fc-11ec-b51d-53e6e728d2d3 -- ceph osd pool set bench_rados noscrub 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 noscrub to 1
# ./benchmarks/bench-rados:271 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados nodeep-scrub 1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:42:34,420966150-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:42:34,424053508-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'c13940fc-37fc-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'nodeep-scrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid c13940fc-37fc-11ec-b51d-53e6e728d2d3 -- ceph osd pool set bench_rados nodeep-scrub 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 nodeep-scrub to 1
# ./benchmarks/bench-rados:272 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool ls detail
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:42:39,472056279-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:42:39,475018071-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'c13940fc-37fc-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'ls', 'detail'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid c13940fc-37fc-11ec-b51d-53e6e728d2d3 -- ceph osd pool ls detail
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
pool 1 'device_health_metrics' replicated size 1 min_size 1 crush_rule 0 object_hash rjenkins pg_num 64 pgp_num 64 autoscale_mode on last_change 19 lfor 0/0/16 flags hashpspool stripe_width 0 pg_num_min 1 application mgr_devicehealth
pool 2 'bench_rados' replicated size 1 min_size 1 crush_rule 0 object_hash rjenkins pg_num 128 pgp_num 128 autoscale_mode off last_change 21 flags hashpspool,noscrub,nodeep-scrub stripe_width 0 pg_num_min 128 application benchmark

# ./benchmarks/bench-rados:273 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd df tree
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:42:43,646230204-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:42:43,649457645-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'c13940fc-37fc-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'df', 'tree'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid c13940fc-37fc-11ec-b51d-53e6e728d2d3 -- ceph osd df tree
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
ID  CLASS  WEIGHT   REWEIGHT  SIZE     RAW USE  DATA     OMAP  META     AVAIL    %USE  VAR   PGS  STATUS  TYPE NAME    
-1         0.09769         -  100 GiB  5.7 MiB  176 KiB   0 B  5.6 MiB  100 GiB  0.01  1.00    -          root default 
-3         0.09769         -  100 GiB  5.7 MiB  176 KiB   0 B  5.6 MiB  100 GiB  0.01  1.00    -              host sm1 
 0    ssd  0.09769   1.00000  100 GiB  5.7 MiB  176 KiB   0 B  5.6 MiB  100 GiB  0.01  1.00  192      up          osd.0
                       TOTAL  100 GiB  5.7 MiB  176 KiB   0 B  5.6 MiB  100 GiB  0.01                                  
MIN/MAX VAR: 1.00/1.00  STDDEV: 0


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:42:47,898874693-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:43:12,137493795-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:43:21,512441555-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:43:30,836249845-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:43:30,843316030-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:43:40,071598837-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:43:40,078688337-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:43:49,385674077-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:43:49,393469036-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:43:58,795235717-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:43:58,802243263-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:44:08,076117497-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:44:08,083078585-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:44:08,088489511-07:00] INFO: > The cluster is idle now.[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:44:08,092167171-07:00] STAGE: Start benchmarking of mode write ...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:44:08,098555971-07:00] INFO: > Start system activity collection services[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:44:08,103670219-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:341 - rados_bench() > rm --force /mnt/sda8/ceph-research/scripts/output/sys_activity_16KB_write.dat.rados_bench_host.3
# ./benchmarks/bench-rados:342 - rados_bench() > sar_pid_rados_bench_host=410301
# ./benchmarks/bench-rados:342 - rados_bench() > S_TIME_FORMAT=ISO
# ./benchmarks/bench-rados:342 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/output/sys_activity_16KB_write.dat.rados_bench_host.3 2

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:44:08,110102741-07:00] INFO: >> for OSD host[0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:44:08,118921900-07:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.5 bash
10.10.2.5: b'114460\n'
[1] 07:44:09 [SUCCESS] ljishen@10.10.2.5
114460

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:44:09,211288945-07:00] INFO: > Run rados bench[0m
# ./benchmarks/bench-rados:370 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/output/rados_bench_16KB_write.log.3
## ./benchmarks/bench-rados:361 - rados_bench() > count_cpus 32-63,96-127
# ./benchmarks/bench-rados:361 - rados_bench() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 60 write --pool bench_rados -b 16384 -O 16384 --concurrent-ios 128 --show-time --write-object --write-omap --write-xattr --no-cleanup
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:44:09,231140555-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:44:09,234053294-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'c13940fc-37fc-11ec-b51d-53e6e728d2d3', '--', 'rados', 'bench', '60', 'write', '--pool', 'bench_rados', '-b', '16384', '-O', '16384', '--concurrent-ios', '128', '--show-time', '--write-object', '--write-omap', '--write-xattr', '--no-cleanup'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid c13940fc-37fc-11ec-b51d-53e6e728d2d3 -- rados bench 60 write --pool bench_rados -b 16384 -O 16384 --concurrent-ios 128 --show-time --write-object --write-omap --write-xattr --no-cleanup
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
hints = 1
2021-10-28T14:44:12.624724+0000 Maintaining 128 concurrent writes of 16384 bytes to objects of size 16384 for up to 60 seconds or 0 objects
2021-10-28T14:44:12.624735+0000 Object prefix: benchmark_data_node-2.bluefield2.ucsc-cmps10_7
2021-10-28T14:44:12.625767+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-28T14:44:12.625767+0000     0       0         0         0         0         0           -           0
2021-10-28T14:44:13.625881+0000     1     128      3143      3015   47.1069   47.1094   0.0384917   0.0415529
2021-10-28T14:44:14.625967+0000     2     128      6202      6074   47.4498   47.7969   0.0400895   0.0416192
2021-10-28T14:44:15.626041+0000     3     128      9292      9164   47.7258   48.2812   0.0435061   0.0416151
2021-10-28T14:44:16.626112+0000     4     128     12365     12237   47.7974   48.0156   0.0410884   0.0415998
2021-10-28T14:44:17.626189+0000     5     128     15347     15219    47.556   46.5938   0.0458735   0.0418819
2021-10-28T14:44:18.626275+0000     6     128     18307     18179   47.3376     46.25   0.0413148   0.0420748
2021-10-28T14:44:19.626357+0000     7     128     21237     21109   47.1147   45.7812   0.0445588   0.0422997
2021-10-28T14:44:20.626432+0000     8     128     24198     24070   47.0082   46.2656    0.037652   0.0424181
2021-10-28T14:44:21.626514+0000     9     128     27142     27014   46.8957        46   0.0451594   0.0425267
2021-10-28T14:44:22.626595+0000    10     128     30062     29934   46.7683    45.625   0.0456475   0.0426694
2021-10-28T14:44:23.626682+0000    11     128     32046     31918   45.3346        31   0.0945547   0.0438871
2021-10-28T14:44:24.626755+0000    12     128     33518     33390   43.4732        23   0.0887937   0.0457872
2021-10-28T14:44:25.626831+0000    13     128     34862     34734   41.7444        21    0.103194   0.0476612
2021-10-28T14:44:26.626913+0000    14     128     36206     36078   40.2625        21    0.115071   0.0494929
2021-10-28T14:44:27.626987+0000    15     128     37550     37422   38.9782        21   0.0866955   0.0511457
2021-10-28T14:44:28.627061+0000    16     128     38894     38766   37.8545        21   0.0898635   0.0526384
2021-10-28T14:44:29.627134+0000    17     128     40302     40174   36.9218        22   0.0991875   0.0540544
2021-10-28T14:44:30.627206+0000    18     128     41582     41454   35.9816        20   0.0950401   0.0555068
2021-10-28T14:44:31.627278+0000    19     128     44014     43886   36.0877        38   0.0397586   0.0553744
2021-10-28T14:44:32.627354+0000 min lat: 0.0259869 max lat: 0.225307 avg lat: 0.0552893
2021-10-28T14:44:32.627354+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-28T14:44:32.627354+0000    20     128     46382     46254   36.1332        37   0.0396101   0.0552893
2021-10-28T14:44:33.627435+0000    21     128     49326     49198   36.6029        46   0.0439311   0.0545644
2021-10-28T14:44:34.627516+0000    22     128     52270     52142   37.0298        46   0.0496258   0.0539429
2021-10-28T14:44:35.627601+0000    23     128     55150     55022   37.3762        45   0.0507461   0.0534493
2021-10-28T14:44:36.627688+0000    24     128     57966     57838    37.652        44   0.0391197   0.0530642
2021-10-28T14:44:37.627771+0000    25     128     60862     60734   37.9558     45.25   0.0404209    0.052639
2021-10-28T14:44:38.627844+0000    26     128     62972     62844   37.7639   32.9688   0.0777293   0.0528539
2021-10-28T14:44:39.627924+0000    27     128     64227     64099   37.0915   19.6094     0.16659   0.0535066
2021-10-28T14:44:40.628012+0000    28     128     64739     64611   36.0524         8    0.228299   0.0550309
2021-10-28T14:44:41.628090+0000    29     128     65251     65123   35.0851         8    0.243242   0.0566561
2021-10-28T14:44:42.628175+0000    30     128     65763     65635   34.1822         8     0.27215   0.0583564
2021-10-28T14:44:43.628248+0000    31     128     66275     66147   33.3376         8    0.282451   0.0598242
2021-10-28T14:44:44.628321+0000    32     128     66734     66606   32.5199   7.17188    0.268978    0.061267
2021-10-28T14:44:45.628393+0000    33     128     67246     67118   31.7769         8    0.243289   0.0627944
2021-10-28T14:44:46.628472+0000    34     128     67683     67555   31.0431   6.82812      0.2754   0.0640386
2021-10-28T14:44:47.628554+0000    35     128     68218     68090    30.395   8.35938    0.177836   0.0655864
2021-10-28T14:44:48.628629+0000    36     128     69603     69475   30.1517   21.6406   0.0676266   0.0662891
2021-10-28T14:44:49.628705+0000    37     128     71267     71139   30.0395        26   0.0698347   0.0665332
2021-10-28T14:44:50.628787+0000    38     128     73006     72878    29.964   27.1719   0.0719803    0.066695
2021-10-28T14:44:51.628863+0000    39     128     74670     74542   29.8623        26   0.0695223   0.0668963
2021-10-28T14:44:52.628934+0000 min lat: 0.0259869 max lat: 0.320979 avg lat: 0.0671406
2021-10-28T14:44:52.628934+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-28T14:44:52.628934+0000    40     128     76334     76206   29.7657        26   0.0749306   0.0671406
2021-10-28T14:44:53.629021+0000    41     128     77998     77870   29.6738        26   0.0775847   0.0673229
2021-10-28T14:44:54.629102+0000    42     128     79662     79534   29.5862        26   0.0889622   0.0675462
2021-10-28T14:44:55.629176+0000    43     128     81251     81123   29.4755   24.8281   0.0898427   0.0677687
2021-10-28T14:44:56.629251+0000    44     128     82926     82798   29.4004   26.1719   0.0810364   0.0679341
2021-10-28T14:44:57.629329+0000    45     128     84579     84451    29.321   25.8281   0.0775466   0.0681268
2021-10-28T14:44:58.629406+0000    46     128     86243     86115   29.2487        26   0.0726926   0.0683387
2021-10-28T14:44:59.629489+0000    47     128     87779     87651    29.137        24   0.0850599   0.0685439
2021-10-28T14:45:00.629563+0000    48     128     89518     89390    29.096   27.1719   0.0510541    0.068705
2021-10-28T14:45:01.629637+0000    49     128     92311     92183   29.3928   43.6406   0.0460247   0.0680122
2021-10-28T14:45:02.629712+0000    50     128     94116     93988    29.369   28.2031   0.0797258   0.0680607
2021-10-28T14:45:03.629787+0000    51     128     95524     95396   29.2244        22   0.0782224   0.0683658
2021-10-28T14:45:04.629864+0000    52     128     96995     96867   29.1044   22.9844   0.0788429   0.0686372
2021-10-28T14:45:05.629948+0000    53     128     98403     98275   28.9703        22    0.100064    0.068962
2021-10-28T14:45:06.630030+0000    54     128     99748     99620    28.823   21.0156    0.103784   0.0692989
2021-10-28T14:45:07.630105+0000    55     128    101091    100963   28.6804   20.9844   0.0897156    0.069687
2021-10-28T14:45:08.630186+0000    56     128    102370    102242   28.5251   19.9844   0.0988218   0.0700752
2021-10-28T14:45:09.630259+0000    57     128    103716    103588   28.3936   21.0312   0.0980378   0.0703764
2021-10-28T14:45:10.630351+0000    58     128    105443    105315   28.3693   26.9844   0.0432374   0.0702477
2021-10-28T14:45:11.630434+0000    59     128    108196    108068   28.6175   43.0156   0.0454114   0.0698524
2021-10-28T14:45:12.630508+0000 min lat: 0.0259869 max lat: 0.320979 avg lat: 0.0692112
2021-10-28T14:45:12.630508+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-28T14:45:12.630508+0000    60     124    111050    110926   28.8847   44.6562   0.0442601   0.0692112
2021-10-28T14:45:13.630643+0000 Total time run:         60.0297
Total writes made:      111050
Write size:             16384
Object size:            16384
Bandwidth (MB/sec):     28.905
Stddev Bandwidth:       13.0228
Max bandwidth (MB/sec): 48.2812
Min bandwidth (MB/sec): 6.82812
Average IOPS:           1849
Stddev IOPS:            833.461
Max IOPS:               3090
Min IOPS:               437
Average Latency(s):     0.0691774
Stddev Latency(s):      0.0444174
Max latency(s):         0.320979
Min latency(s):         0.0259869

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:45:14,464074330-07:00] INFO: > Stop system activity collection services[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:45:14,469708086-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:391 - rados_bench() > kill -INT 410301

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:45:14,475034153-07:00] INFO: >> for OSD host[0m
# ./benchmarks/bench-rados:396 - rados_bench() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.5 kill -INT 114460
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:45:14,482516423-07:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.5 kill -INT 114460
[1] 07:45:15 [SUCCESS] ljishen@10.10.2.5

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:45:15,584024315-07:00] INFO: >>> Copy system activity file from OSD host[0m
## ./benchmarks/bench-rados:403 - rados_bench() > basename -- /tmp/bench-rados/sys_activity_16KB_write.dat.osd_host.3
# ./benchmarks/bench-rados:403 - rados_bench() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.5:/tmp/bench-rados/sys_activity_16KB_write.dat.osd_host.3 /mnt/sda8/ceph-research/scripts/output/sys_activity_16KB_write.dat.osd_host.3


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:45:16,670591892-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:45:40,942551820-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 111.05k objects, 1.7 GiB
    usage:   5.2 GiB used, 95 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:45:40,949700331-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:45:50,340018867-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 111.05k objects, 1.7 GiB
    usage:   5.2 GiB used, 95 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:45:50,347446855-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:45:59,697352695-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 111.05k objects, 1.7 GiB
    usage:   5.2 GiB used, 95 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:45:59,704580205-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:46:08,996883957-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 111.05k objects, 1.7 GiB
    usage:   5.2 GiB used, 95 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:46:09,003834264-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:46:18,417844839-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 111.05k objects, 1.7 GiB
    usage:   5.2 GiB used, 95 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:46:18,425387813-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:46:18,430681239-07:00] INFO: > The cluster is idle now.[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:46:18,434000734-07:00] STAGE: Start benchmarking of mode seq ...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:46:18,440266462-07:00] INFO: > Start system activity collection services[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:46:18,445624008-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:341 - rados_bench() > rm --force /mnt/sda8/ceph-research/scripts/output/sys_activity_16KB_seq.dat.rados_bench_host.3
# ./benchmarks/bench-rados:342 - rados_bench() > sar_pid_rados_bench_host=411654
# ./benchmarks/bench-rados:342 - rados_bench() > S_TIME_FORMAT=ISO
# ./benchmarks/bench-rados:342 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/output/sys_activity_16KB_seq.dat.rados_bench_host.3 2

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:46:18,452340485-07:00] INFO: >> for OSD host[0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:46:18,461081016-07:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.5 bash
10.10.2.5: b'115014\n'
[1] 07:46:19 [SUCCESS] ljishen@10.10.2.5
115014

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:46:19,559255827-07:00] INFO: > Run rados bench[0m
# ./benchmarks/bench-rados:379 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/output/rados_bench_16KB_seq.log.3
## ./benchmarks/bench-rados:376 - rados_bench() > count_cpus 32-63,96-127
# ./benchmarks/bench-rados:376 - rados_bench() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 128 --show-time
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:46:19,578841486-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:46:19,581784001-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'c13940fc-37fc-11ec-b51d-53e6e728d2d3', '--', 'rados', 'bench', '99999999', 'seq', '--pool', 'bench_rados', '--concurrent-ios', '128', '--show-time'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid c13940fc-37fc-11ec-b51d-53e6e728d2d3 -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 128 --show-time
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
hints = 1
2021-10-28T14:46:22.928226+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-28T14:46:22.928226+0000     0       0         0         0         0         0           -           0
2021-10-28T14:46:23.928355+0000     1     127     14681     14554   227.359   227.406   0.0071278  0.00874445
2021-10-28T14:46:24.928435+0000     2     127     29468     29341   229.194   231.047    0.042383  0.00858771
2021-10-28T14:46:25.928515+0000     3     128     45758     45630   237.627   254.516  0.00557086  0.00840032
2021-10-28T14:46:26.928582+0000     4     128     60769     60641   236.853   234.547  0.00716112  0.00842911
2021-10-28T14:46:27.928650+0000     5     128     75017     74889   234.005   222.625  0.00754438  0.00853459
2021-10-28T14:46:28.928720+0000     6     128     88171     88043   229.257   205.531  0.00877222  0.00871305
2021-10-28T14:46:29.928795+0000     7     128    102438    102310   228.349   222.922  0.00964284  0.00874831
2021-10-28T14:46:30.928915+0000 Total time run:       7.53857
Total reads made:     111050
Read size:            16384
Object size:          16384
Bandwidth (MB/sec):   230.171
Average IOPS:         14730
Stddev IOPS:          947.163
Max IOPS:             16289
Min IOPS:             13154
Average Latency(s):   0.00868152
Max latency(s):       0.0479379
Min latency(s):       0.00137848

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:46:31,602524325-07:00] INFO: > Stop system activity collection services[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:46:31,608071348-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:391 - rados_bench() > kill -INT 411654

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:46:31,613664378-07:00] INFO: >> for OSD host[0m
# ./benchmarks/bench-rados:396 - rados_bench() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.5 kill -INT 115014
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:46:31,621336326-07:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.5 kill -INT 115014
[1] 07:46:32 [SUCCESS] ljishen@10.10.2.5

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:46:32,712076930-07:00] INFO: >>> Copy system activity file from OSD host[0m
## ./benchmarks/bench-rados:403 - rados_bench() > basename -- /tmp/bench-rados/sys_activity_16KB_seq.dat.osd_host.3
# ./benchmarks/bench-rados:403 - rados_bench() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.5:/tmp/bench-rados/sys_activity_16KB_seq.dat.osd_host.3 /mnt/sda8/ceph-research/scripts/output/sys_activity_16KB_seq.dat.osd_host.3


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:46:33,784954192-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:46:58,032761360-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 111.05k objects, 1.7 GiB
    usage:   5.2 GiB used, 95 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:46:58,040274048-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:47:07,359377286-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 111.05k objects, 1.7 GiB
    usage:   5.2 GiB used, 95 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:47:07,366553609-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:47:16,670524322-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 111.05k objects, 1.7 GiB
    usage:   5.2 GiB used, 95 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:47:16,677666431-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:47:26,093481070-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 111.05k objects, 1.7 GiB
    usage:   5.2 GiB used, 95 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:47:26,100778502-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:47:35,528023752-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 111.05k objects, 1.7 GiB
    usage:   5.2 GiB used, 95 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:47:35,535329209-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:47:35,541082460-07:00] INFO: > The cluster is idle now.[0m


[1;7;39;49m[2021-10-28T07:47:35,544613114-07:00][RUNNING][ROUND 1/3/21] object_size=64KB[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:47:35,547782657-07:00] STAGE: Launch a Ceph cluster (CEPH_HOST: 10.10.2.1, OSD_HOST: 10.10.2.5) ...[0m
# ./benchmarks/bench-rados:183 - launch_ceph_cluster() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.1 bash -euo pipefail
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:47:35,557082021-07:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.1 bash -euo pipefail
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T07:47:35,973229834-07:00] INFO: > Remove existing clusters\x1b[0m\n'
10.10.2.1: b'## bash:17 -  > basename -- /var/lib/ceph/c13940fc-37fc-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'# bash:17 -  > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm --host 10.10.2.1 --host 10.10.2.5 rm-cluster --force --fsid c13940fc-37fc-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T07:47:35,984923848-07:00] DEBUG: HOST_PARAMS: ['--host', '10.10.2.1', '--host', '10.10.2.5']\x1b[0m\n"
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T07:47:35,989157647-07:00] DEBUG: CEPHADM_PARAMS: ['rm-cluster', '--force', '--fsid', 'c13940fc-37fc-11ec-b51d-53e6e728d2d3']\x1b[0m\n"
10.10.2.1: b"# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:63 -  > /tmp/bench-rados/ceph-research/scripts/parallel-ssh --host 10.10.2.1 --host 10.10.2.5 'sudo python3 -u - rm-cluster --force --fsid c13940fc-37fc-11ec-b51d-53e6e728d2d3'\n"
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T07:47:35,997848685-07:00] DEBUG: command from stdin\x1b[0m\n'
10.10.2.1: b"# /tmp/bench-rados/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host 10.10.2.1 --host 10.10.2.5 'sudo python3 -u - rm-cluster --force --fsid c13940fc-37fc-11ec-b51d-53e6e728d2d3'\n"
10.10.2.1: b'[1] 07:47:41 [SUCCESS] 10.10.2.1\n[2] 07:47:45 [SUCCESS] 10.10.2.5\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T07:47:45,851456267-07:00] INFO: > Deploy a new cluster\x1b[0m\n'
10.10.2.1: b'# bash:24 -  > /tmp/bench-rados/ceph-research/scripts/ceph-deploy -m 10.10.2.1 -o sm1/10.10.2.5:/dev/nvme0n1\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T07:47:45,862747453-07:00] STAGE: Verify passwordless SSH to hosts, passwordless sudo via ljishen, and the existence of devices on hosts...\x1b[0m\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T07:47:45,867642826-07:00] INFO: > Check the host for the monitor\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:181 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@10.10.2.1 sudo --non-interactive --validate\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T07:47:46,019340393-07:00] INFO: > Check hosts for OSDs\x1b[0m\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T07:47:46,024478342-07:00] INFO: >> Check host 'sm1'\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:200 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 sudo --non-interactive --validate\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T07:47:47,086928889-07:00] INFO: >>> Check device '/dev/nvme0n1'\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:215 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 test -b /dev/nvme0n1\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:226 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 lsblk --nodeps --noheadings --output TYPE /dev/nvme0n1\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:227 - verify_parameters() > grep --word-regexp disk\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T07:47:49,199788295-07:00] STAGE: Reset devices on hosts...\x1b[0m\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T07:47:49,204815265-07:00] INFO: > Resetting device '/dev/nvme0n1' on host 'sm1'\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:288 - reset_host_devices() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 sudo sh\n'
10.10.2.1: b'  Removing ceph--46a3b939--fc43--4fb7--aea0--ab9ee37ba432-osd--block--c3717630--6754--4938--829e--5473c4124bb2 (252:0)\n'
10.10.2.1: b'  Archiving volume group "ceph-46a3b939-fc43-4fb7-aea0-ab9ee37ba432" metadata (seqno 5).\n'
10.10.2.1: b'  Releasing logical volume "osd-block-c3717630-6754-4938-829e-5473c4124bb2"\n'
10.10.2.1: b'  Creating volume group backup "/etc/lvm/backup/ceph-46a3b939-fc43-4fb7-aea0-ab9ee37ba432" (seqno 6).\n'
10.10.2.1: b'  Logical volume "osd-block-c3717630-6754-4938-829e-5473c4124bb2" successfully removed\n'
10.10.2.1: b'  Removing physical volume "/dev/nvme0n1" from volume group "ceph-46a3b939-fc43-4fb7-aea0-ab9ee37ba432"\n'
10.10.2.1: b'  Volume group "ceph-46a3b939-fc43-4fb7-aea0-ab9ee37ba432" successfully removed\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T07:47:51,479293890-07:00] STAGE: Bootstrap a new cluster...\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:317 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.1 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T07:47:51,489377998-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T07:47:51,493144689-07:00] DEBUG: CEPHADM_PARAMS: ['bootstrap', '--config', '/tmp/bench-rados/ceph-research/scripts/ceph.conf', '--mon-ip', '10.10.2.1', '--output-dir', '/tmp/bench-rados/deployment_data_root/etc/ceph', '--ssh-user', 'ljishen', '--skip-dashboard', '--no-minimize-config', '--allow-overwrite', '--allow-fqdn-hostname', '--orphan-initial-daemons', '--skip-monitoring-stack', '--single-host-defaults']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.1 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults\n'
10.10.2.1: b'Verifying podman|docker is present...\n'
10.10.2.1: b'Verifying lvm2 is present...\nVerifying time synchronization is in place...\n'
10.10.2.1: b'Unit ntp.service is enabled and running\n'
10.10.2.1: b'Repeating the final host check...\npodman|docker (/usr/bin/docker) is present\n'
10.10.2.1: b'systemctl is present\n'
10.10.2.1: b'lvcreate is present\n'
10.10.2.1: b'Unit ntp.service is enabled and running\n'
10.10.2.1: b'Host looks OK\n'
10.10.2.1: b'Cluster fsid: 06cf3c7e-37fe-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Verifying IP 10.10.2.1 port 3300 ...\n'
10.10.2.1: b'Verifying IP 10.10.2.1 port 6789 ...\n'
10.10.2.1: b'Mon IP `10.10.2.1` is in CIDR network `10.10.2.0/24`\n'
10.10.2.1: b'- internal network (--cluster-network) has not been provided, OSD replication will default to the public_network\n'
10.10.2.1: b'Adjusting default settings to suit single-host cluster...\n'
10.10.2.1: b'Pulling container image quay.io/ceph/ceph:v16...\n'
10.10.2.1: b'Ceph version: ceph version 16.2.6 (ee28fb57e47e9f88813e24bbf4c14496ca299d31) pacific (stable)\n'
10.10.2.1: b'Extracting ceph user uid/gid from container image...\n'
10.10.2.1: b'Creating initial keys...\n'
10.10.2.1: b'Creating initial monmap...\n'
10.10.2.1: b'Creating mon...\n'
10.10.2.1: b'Waiting for mon to start...\n'
10.10.2.1: b'Waiting for mon...\n'
10.10.2.1: b'mon is available\n'
10.10.2.1: b'Setting mon public_network to 10.10.2.0/24\n'
10.10.2.1: b'Wrote config to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf\n'
10.10.2.1: b'Wrote keyring to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring\n'
10.10.2.1: b'Creating mgr...\n'
10.10.2.1: b'Verifying port 9283 ...\n'
10.10.2.1: b'Waiting for mgr to start...\nWaiting for mgr...\n'
10.10.2.1: b'mgr not available, waiting (1/15)...\n'
10.10.2.1: b'mgr not available, waiting (2/15)...\n'
10.10.2.1: b'mgr is available\n'
10.10.2.1: b'Enabling cephadm module...\n'
10.10.2.1: b'Waiting for the mgr to restart...\n'
10.10.2.1: b'Waiting for mgr epoch 5...\n'
10.10.2.1: b'mgr epoch 5 is available\n'
10.10.2.1: b'Setting orchestrator backend to cephadm...\n'
10.10.2.1: b'Generating ssh key...\n'
10.10.2.1: b'Wrote public SSH key to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub\n'
10.10.2.1: b'Adding key to ljishen@localhost authorized_keys...\n'
10.10.2.1: b'Adding host node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us...\n'
10.10.2.1: b'Deploying unmanaged mon service...\n'
10.10.2.1: b'Deploying unmanaged mgr service...\n'
10.10.2.1: b'You can access the Ceph CLI with:\n\n\tsudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --fsid 06cf3c7e-37fe-11ec-b51d-53e6e728d2d3 -c /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf -k /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring\n\n'
10.10.2.1: b'Please consider enabling telemetry to help improve Ceph:\n\n\tceph telemetry on\n\nFor more information see:\n\n\thttps://docs.ceph.com/docs/pacific/mgr/telemetry/\n\n'
10.10.2.1: b'Bootstrap complete.\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T07:48:52,536357951-07:00] INFO: Sleep for 20 seconds\x1b[0m\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T07:49:12,543729693-07:00] INFO: > Set the default number of monitor daemons to 1\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:337 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T07:49:12,554068288-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T07:49:12,557538091-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mon', '1']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1\n'
10.10.2.1: b'Inferring fsid 06cf3c7e-37fe-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/06cf3c7e-37fe-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b'Scheduled mon update...\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T07:49:20,629001290-07:00] INFO: > Set the default number of manager daemons to 1\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:342 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T07:49:20,639460172-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T07:49:20,643161219-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mgr', '1']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1\n'
10.10.2.1: b'Inferring fsid 06cf3c7e-37fe-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/06cf3c7e-37fe-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b'Scheduled mgr update...\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T07:49:30,014859883-07:00] STAGE: Add hosts to the cluster...\x1b[0m\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T07:49:30,021540232-07:00] INFO: > Adding host 'sm1'\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:361 - add_hosts() > ssh-copy-id -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -f -i /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub ljishen@sm1\n'
10.10.2.1: b'/usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: "/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub"\n'
10.10.2.1: b'\nNumber of key(s) added: 1\n\nNow try logging into the machine, with:   "ssh -o \'GlobalKnownHostsFile=/dev/null\' -o \'LogLevel=ERROR\' -o \'PasswordAuthentication=no\' -o \'StrictHostKeyChecking=no\' -o \'UserKnownHostsFile=/dev/null\' \'ljishen@sm1\'"\nand check to make sure that only the key(s) you wanted were added.\n\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:367 - add_hosts() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add sm1 10.10.2.5\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T07:49:31,168166406-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T07:49:31,171834642-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'host', 'add', 'sm1', '10.10.2.5']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add sm1 10.10.2.5\n'
10.10.2.1: b'Inferring fsid 06cf3c7e-37fe-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/06cf3c7e-37fe-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b"Added host 'sm1' with addr '10.10.2.5'\n"
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T07:49:42,012566062-07:00] INFO: Sleep for 20 seconds\x1b[0m\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T07:50:02,017342951-07:00] STAGE: Add OSDs to the cluster...\x1b[0m\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T07:50:02,024342259-07:00] INFO: > Adding devices ['/dev/nvme0n1'] on host 'sm1' as OSDs\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:388 - add_host_devices_as_osds() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd sm1:/dev/nvme0n1\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T07:50:02,034785041-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T07:50:02,038772567-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'daemon', 'add', 'osd', 'sm1:/dev/nvme0n1']\x1b[0m\n# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd sm1:/dev/nvme0n1\n"
10.10.2.1: b'Inferring fsid 06cf3c7e-37fe-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/06cf3c7e-37fe-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b"Created osd(s) 0 on host 'sm1'\n"
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T07:50:25,691195767-07:00] INFO: > Sleep for 20 seconds\x1b[0m\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T07:50:45,696725617-07:00] STAGE: Show Cluster Status\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:408 - show_status() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T07:50:45,706788424-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T07:50:45,710227429-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', '--status']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status\n'
10.10.2.1: b'Inferring fsid 06cf3c7e-37fe-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/06cf3c7e-37fe-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b'  cluster:\n    id:     06cf3c7e-37fe-11ec-b51d-53e6e728d2d3\n    health: HEALTH_WARN\n            1 pool(s) have no replicas configured\n \n  services:\n    mon: 1 daemons, quorum node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us (age 2m)\n    mgr: node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us.ibspjq(active, since 2m)\n    osd: 1 osds: 1 up (since 20s), 1 in (since 39s)\n \n  data:\n    pools:   1 pools, 1 pgs\n    objects: 0 objects, 0 B\n    usage:   4.7 MiB used, 100 GiB / 100 GiB avail\n    pgs:     1 active+clean\n \n'
[1] 07:50:54 [SUCCESS] ljishen@10.10.2.1

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:47:35,973229834-07:00] INFO: > Remove existing clusters[0m
## bash:17 -  > basename -- /var/lib/ceph/c13940fc-37fc-11ec-b51d-53e6e728d2d3
# bash:17 -  > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm --host 10.10.2.1 --host 10.10.2.5 rm-cluster --force --fsid c13940fc-37fc-11ec-b51d-53e6e728d2d3
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:47:35,984923848-07:00] DEBUG: HOST_PARAMS: ['--host', '10.10.2.1', '--host', '10.10.2.5'][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:47:35,989157647-07:00] DEBUG: CEPHADM_PARAMS: ['rm-cluster', '--force', '--fsid', 'c13940fc-37fc-11ec-b51d-53e6e728d2d3'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:63 -  > /tmp/bench-rados/ceph-research/scripts/parallel-ssh --host 10.10.2.1 --host 10.10.2.5 'sudo python3 -u - rm-cluster --force --fsid c13940fc-37fc-11ec-b51d-53e6e728d2d3'
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:47:35,997848685-07:00] DEBUG: command from stdin[0m
# /tmp/bench-rados/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host 10.10.2.1 --host 10.10.2.5 'sudo python3 -u - rm-cluster --force --fsid c13940fc-37fc-11ec-b51d-53e6e728d2d3'
[1] 07:47:41 [SUCCESS] 10.10.2.1
[2] 07:47:45 [SUCCESS] 10.10.2.5

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:47:45,851456267-07:00] INFO: > Deploy a new cluster[0m
# bash:24 -  > /tmp/bench-rados/ceph-research/scripts/ceph-deploy -m 10.10.2.1 -o sm1/10.10.2.5:/dev/nvme0n1


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:47:45,862747453-07:00] STAGE: Verify passwordless SSH to hosts, passwordless sudo via ljishen, and the existence of devices on hosts...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:47:45,867642826-07:00] INFO: > Check the host for the monitor[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:181 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@10.10.2.1 sudo --non-interactive --validate

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:47:46,019340393-07:00] INFO: > Check hosts for OSDs[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:47:46,024478342-07:00] INFO: >> Check host 'sm1'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:200 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 sudo --non-interactive --validate

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:47:47,086928889-07:00] INFO: >>> Check device '/dev/nvme0n1'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:215 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 test -b /dev/nvme0n1
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:226 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 lsblk --nodeps --noheadings --output TYPE /dev/nvme0n1
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:227 - verify_parameters() > grep --word-regexp disk


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:47:49,199788295-07:00] STAGE: Reset devices on hosts...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:47:49,204815265-07:00] INFO: > Resetting device '/dev/nvme0n1' on host 'sm1'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:288 - reset_host_devices() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 sudo sh
  Removing ceph--46a3b939--fc43--4fb7--aea0--ab9ee37ba432-osd--block--c3717630--6754--4938--829e--5473c4124bb2 (252:0)
  Archiving volume group "ceph-46a3b939-fc43-4fb7-aea0-ab9ee37ba432" metadata (seqno 5).
  Releasing logical volume "osd-block-c3717630-6754-4938-829e-5473c4124bb2"
  Creating volume group backup "/etc/lvm/backup/ceph-46a3b939-fc43-4fb7-aea0-ab9ee37ba432" (seqno 6).
  Logical volume "osd-block-c3717630-6754-4938-829e-5473c4124bb2" successfully removed
  Removing physical volume "/dev/nvme0n1" from volume group "ceph-46a3b939-fc43-4fb7-aea0-ab9ee37ba432"
  Volume group "ceph-46a3b939-fc43-4fb7-aea0-ab9ee37ba432" successfully removed


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:47:51,479293890-07:00] STAGE: Bootstrap a new cluster...[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:317 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.1 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:47:51,489377998-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:47:51,493144689-07:00] DEBUG: CEPHADM_PARAMS: ['bootstrap', '--config', '/tmp/bench-rados/ceph-research/scripts/ceph.conf', '--mon-ip', '10.10.2.1', '--output-dir', '/tmp/bench-rados/deployment_data_root/etc/ceph', '--ssh-user', 'ljishen', '--skip-dashboard', '--no-minimize-config', '--allow-overwrite', '--allow-fqdn-hostname', '--orphan-initial-daemons', '--skip-monitoring-stack', '--single-host-defaults'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.1 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults
Verifying podman|docker is present...
Verifying lvm2 is present...
Verifying time synchronization is in place...
Unit ntp.service is enabled and running
Repeating the final host check...
podman|docker (/usr/bin/docker) is present
systemctl is present
lvcreate is present
Unit ntp.service is enabled and running
Host looks OK
Cluster fsid: 06cf3c7e-37fe-11ec-b51d-53e6e728d2d3
Verifying IP 10.10.2.1 port 3300 ...
Verifying IP 10.10.2.1 port 6789 ...
Mon IP `10.10.2.1` is in CIDR network `10.10.2.0/24`
- internal network (--cluster-network) has not been provided, OSD replication will default to the public_network
Adjusting default settings to suit single-host cluster...
Pulling container image quay.io/ceph/ceph:v16...
Ceph version: ceph version 16.2.6 (ee28fb57e47e9f88813e24bbf4c14496ca299d31) pacific (stable)
Extracting ceph user uid/gid from container image...
Creating initial keys...
Creating initial monmap...
Creating mon...
Waiting for mon to start...
Waiting for mon...
mon is available
Setting mon public_network to 10.10.2.0/24
Wrote config to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf
Wrote keyring to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring
Creating mgr...
Verifying port 9283 ...
Waiting for mgr to start...
Waiting for mgr...
mgr not available, waiting (1/15)...
mgr not available, waiting (2/15)...
mgr is available
Enabling cephadm module...
Waiting for the mgr to restart...
Waiting for mgr epoch 5...
mgr epoch 5 is available
Setting orchestrator backend to cephadm...
Generating ssh key...
Wrote public SSH key to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub
Adding key to ljishen@localhost authorized_keys...
Adding host node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us...
Deploying unmanaged mon service...
Deploying unmanaged mgr service...
You can access the Ceph CLI with:

	sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --fsid 06cf3c7e-37fe-11ec-b51d-53e6e728d2d3 -c /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf -k /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring

Please consider enabling telemetry to help improve Ceph:

	ceph telemetry on

For more information see:

	https://docs.ceph.com/docs/pacific/mgr/telemetry/

Bootstrap complete.

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:48:52,536357951-07:00] INFO: Sleep for 20 seconds[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:49:12,543729693-07:00] INFO: > Set the default number of monitor daemons to 1[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:337 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:49:12,554068288-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:49:12,557538091-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mon', '1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1
Inferring fsid 06cf3c7e-37fe-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/06cf3c7e-37fe-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Scheduled mon update...

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:49:20,629001290-07:00] INFO: > Set the default number of manager daemons to 1[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:342 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:49:20,639460172-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:49:20,643161219-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mgr', '1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1
Inferring fsid 06cf3c7e-37fe-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/06cf3c7e-37fe-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Scheduled mgr update...


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:49:30,014859883-07:00] STAGE: Add hosts to the cluster...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:49:30,021540232-07:00] INFO: > Adding host 'sm1'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:361 - add_hosts() > ssh-copy-id -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -f -i /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub ljishen@sm1
/usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: "/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub"

Number of key(s) added: 1

Now try logging into the machine, with:   "ssh -o 'GlobalKnownHostsFile=/dev/null' -o 'LogLevel=ERROR' -o 'PasswordAuthentication=no' -o 'StrictHostKeyChecking=no' -o 'UserKnownHostsFile=/dev/null' 'ljishen@sm1'"
and check to make sure that only the key(s) you wanted were added.

# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:367 - add_hosts() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add sm1 10.10.2.5
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:49:31,168166406-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:49:31,171834642-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'host', 'add', 'sm1', '10.10.2.5'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add sm1 10.10.2.5
Inferring fsid 06cf3c7e-37fe-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/06cf3c7e-37fe-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Added host 'sm1' with addr '10.10.2.5'

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:49:42,012566062-07:00] INFO: Sleep for 20 seconds[0m


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:50:02,017342951-07:00] STAGE: Add OSDs to the cluster...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:50:02,024342259-07:00] INFO: > Adding devices ['/dev/nvme0n1'] on host 'sm1' as OSDs[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:388 - add_host_devices_as_osds() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd sm1:/dev/nvme0n1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:50:02,034785041-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:50:02,038772567-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'daemon', 'add', 'osd', 'sm1:/dev/nvme0n1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd sm1:/dev/nvme0n1
Inferring fsid 06cf3c7e-37fe-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/06cf3c7e-37fe-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Created osd(s) 0 on host 'sm1'

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:50:25,691195767-07:00] INFO: > Sleep for 20 seconds[0m


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:50:45,696725617-07:00] STAGE: Show Cluster Status[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:408 - show_status() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:50:45,706788424-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:50:45,710227429-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', '--status'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status
Inferring fsid 06cf3c7e-37fe-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/06cf3c7e-37fe-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
  cluster:
    id:     06cf3c7e-37fe-11ec-b51d-53e6e728d2d3
    health: HEALTH_WARN
            1 pool(s) have no replicas configured
 
  services:
    mon: 1 daemons, quorum node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us (age 2m)
    mgr: node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us.ibspjq(active, since 2m)
    osd: 1 osds: 1 up (since 20s), 1 in (since 39s)
 
  data:
    pools:   1 pools, 1 pgs
    objects: 0 objects, 0 B
    usage:   4.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     1 active+clean
 

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:50:54,612990353-07:00] INFO: > Copy configuration files to local[0m
# ./benchmarks/bench-rados:220 - launch_ceph_cluster() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.1 sudo chown --changes --recursive ljishen /tmp/bench-rados/deployment_data_root/etc/ceph
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:50:54,620901531-07:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.1 sudo chown --changes --recursive ljishen /tmp/bench-rados/deployment_data_root/etc/ceph
[1] 07:50:54 [SUCCESS] ljishen@10.10.2.1
# ./benchmarks/bench-rados:223 - launch_ceph_cluster() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.1:/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf
# ./benchmarks/bench-rados:226 - launch_ceph_cluster() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.1:/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:50:55,101282042-07:00] STAGE: Update local ceph.conf as a client...[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:50:55,104572933-07:00] STAGE: Configure Ceph pool...[0m
# ./benchmarks/bench-rados:257 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_max_backfills 32
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:50:55,126563353-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:50:55,129557957-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '06cf3c7e-37fe-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'config', 'set', 'osd', 'osd_max_backfills', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 06cf3c7e-37fe-11ec-b51d-53e6e728d2d3 -- ceph config set osd osd_max_backfills 32
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:258 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_active 32
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:50:59,426207517-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:50:59,429238889-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '06cf3c7e-37fe-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_active', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 06cf3c7e-37fe-11ec-b51d-53e6e728d2d3 -- ceph config set osd osd_recovery_max_active 32
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:259 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_single_start 8
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:51:03,826767393-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:51:03,829730587-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '06cf3c7e-37fe-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_single_start', '8'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 06cf3c7e-37fe-11ec-b51d-53e6e728d2d3 -- ceph config set osd osd_recovery_max_single_start 8
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:260 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_op_priority 63
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:51:08,185949564-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:51:08,189023607-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '06cf3c7e-37fe-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_op_priority', '63'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 06cf3c7e-37fe-11ec-b51d-53e6e728d2d3 -- ceph config set osd osd_recovery_op_priority 63
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:262 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config show-with-defaults osd.0
# ./benchmarks/bench-rados:263 - configure_ceph_pool() > grep 'osd_max_backfills\|osd_recovery'
# ./benchmarks/bench-rados:263 - configure_ceph_pool() > column -t -s ' '
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
osd_max_backfills                        32         mon
osd_recovery_cost                        20971520   default
osd_recovery_delay_start                 0.000000   default
osd_recovery_max_active                  32         mon
osd_recovery_max_active_hdd              3          default
osd_recovery_max_active_ssd              10         default
osd_recovery_max_chunk                   8388608    default
osd_recovery_max_omap_entries_per_chunk  8096       default
osd_recovery_max_single_start            8          mon
osd_recovery_op_priority                 63         mon
osd_recovery_op_warn_multiple            16         default
osd_recovery_priority                    5          default
osd_recovery_retry_interval              30.000000  default
osd_recovery_sleep                       0.000000   default
osd_recovery_sleep_hdd                   0.100000   default
osd_recovery_sleep_hybrid                0.025000   default
osd_recovery_sleep_ssd                   0.000000   default
# ./benchmarks/bench-rados:265 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool create bench_rados 128 128 replicated --size 1 --pg-num-min 128
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:51:16,874054985-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:51:16,876994013-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '06cf3c7e-37fe-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'create', 'bench_rados', '128', '128', 'replicated', '--size', '1', '--pg-num-min', '128'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 06cf3c7e-37fe-11ec-b51d-53e6e728d2d3 -- ceph osd pool create bench_rados 128 128 replicated --size 1 --pg-num-min 128
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
pool 'bench_rados' created
# ./benchmarks/bench-rados:267 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados min_size 1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:51:21,191979057-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:51:21,194930799-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '06cf3c7e-37fe-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'min_size', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 06cf3c7e-37fe-11ec-b51d-53e6e728d2d3 -- ceph osd pool set bench_rados min_size 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 min_size to 1
# ./benchmarks/bench-rados:268 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados pg_autoscale_mode off
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:51:25,532459285-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:51:25,535344612-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '06cf3c7e-37fe-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'pg_autoscale_mode', 'off'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 06cf3c7e-37fe-11ec-b51d-53e6e728d2d3 -- ceph osd pool set bench_rados pg_autoscale_mode off
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 pg_autoscale_mode to off
# ./benchmarks/bench-rados:269 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool application enable bench_rados benchmark
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:51:30,905080689-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:51:30,908308652-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '06cf3c7e-37fe-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'application', 'enable', 'bench_rados', 'benchmark'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 06cf3c7e-37fe-11ec-b51d-53e6e728d2d3 -- ceph osd pool application enable bench_rados benchmark
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
enabled application 'benchmark' on pool 'bench_rados'
# ./benchmarks/bench-rados:270 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados noscrub 1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:51:36,362959508-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:51:36,365955063-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '06cf3c7e-37fe-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'noscrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 06cf3c7e-37fe-11ec-b51d-53e6e728d2d3 -- ceph osd pool set bench_rados noscrub 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 noscrub to 1
# ./benchmarks/bench-rados:271 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados nodeep-scrub 1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:51:40,928693229-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:51:40,931682492-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '06cf3c7e-37fe-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'nodeep-scrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 06cf3c7e-37fe-11ec-b51d-53e6e728d2d3 -- ceph osd pool set bench_rados nodeep-scrub 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 nodeep-scrub to 1
# ./benchmarks/bench-rados:272 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool ls detail
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:51:45,367128721-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:51:45,370088218-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '06cf3c7e-37fe-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'ls', 'detail'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 06cf3c7e-37fe-11ec-b51d-53e6e728d2d3 -- ceph osd pool ls detail
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
pool 1 'device_health_metrics' replicated size 1 min_size 1 crush_rule 0 object_hash rjenkins pg_num 64 pgp_num 64 autoscale_mode on last_change 20 lfor 0/0/16 flags hashpspool stripe_width 0 pg_num_min 1 application mgr_devicehealth
pool 2 'bench_rados' replicated size 1 min_size 1 crush_rule 0 object_hash rjenkins pg_num 128 pgp_num 128 autoscale_mode off last_change 21 flags hashpspool,noscrub,nodeep-scrub stripe_width 0 pg_num_min 128 application benchmark

# ./benchmarks/bench-rados:273 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd df tree
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:51:49,612333597-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:51:49,615513269-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '06cf3c7e-37fe-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'df', 'tree'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 06cf3c7e-37fe-11ec-b51d-53e6e728d2d3 -- ceph osd df tree
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
ID  CLASS  WEIGHT   REWEIGHT  SIZE     RAW USE  DATA     OMAP  META     AVAIL    %USE  VAR   PGS  STATUS  TYPE NAME    
-1         0.09769         -  100 GiB  5.7 MiB  176 KiB   0 B  5.6 MiB  100 GiB  0.01  1.00    -          root default 
-3         0.09769         -  100 GiB  5.7 MiB  176 KiB   0 B  5.6 MiB  100 GiB  0.01  1.00    -              host sm1 
 0    ssd  0.09769   1.00000  100 GiB  5.7 MiB  176 KiB   0 B  5.6 MiB  100 GiB  0.01  1.00  192      up          osd.0
                       TOTAL  100 GiB  5.7 MiB  176 KiB   0 B  5.6 MiB  100 GiB  0.01                                  
MIN/MAX VAR: 1.00/1.00  STDDEV: 0


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:51:53,795406548-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:52:18,075616741-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:52:27,373525250-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:52:36,721273189-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:52:36,728263202-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:52:46,078032200-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:52:46,085142750-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:52:55,436812441-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:52:55,444150029-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:53:04,943767383-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:53:04,950730755-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:53:14,315594371-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:53:14,322726451-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:53:14,328375016-07:00] INFO: > The cluster is idle now.[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:53:14,331943781-07:00] STAGE: Start benchmarking of mode write ...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:53:14,338691267-07:00] INFO: > Start system activity collection services[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:53:14,343910412-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:341 - rados_bench() > rm --force /mnt/sda8/ceph-research/scripts/output/sys_activity_64KB_write.dat.rados_bench_host.1
# ./benchmarks/bench-rados:342 - rados_bench() > sar_pid_rados_bench_host=417239
# ./benchmarks/bench-rados:342 - rados_bench() > S_TIME_FORMAT=ISO
# ./benchmarks/bench-rados:342 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/output/sys_activity_64KB_write.dat.rados_bench_host.1 2

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:53:14,350393900-07:00] INFO: >> for OSD host[0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:53:14,359042187-07:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.5 bash
10.10.2.5: b'120030\n'
[1] 07:53:15 [SUCCESS] ljishen@10.10.2.5
120030

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:53:15,479568231-07:00] INFO: > Run rados bench[0m
# ./benchmarks/bench-rados:370 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/output/rados_bench_64KB_write.log.1
## ./benchmarks/bench-rados:361 - rados_bench() > count_cpus 32-63,96-127
# ./benchmarks/bench-rados:361 - rados_bench() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 60 write --pool bench_rados -b 65536 -O 65536 --concurrent-ios 128 --show-time --write-object --write-omap --write-xattr --no-cleanup
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:53:15,499744012-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:53:15,502682970-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '06cf3c7e-37fe-11ec-b51d-53e6e728d2d3', '--', 'rados', 'bench', '60', 'write', '--pool', 'bench_rados', '-b', '65536', '-O', '65536', '--concurrent-ios', '128', '--show-time', '--write-object', '--write-omap', '--write-xattr', '--no-cleanup'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 06cf3c7e-37fe-11ec-b51d-53e6e728d2d3 -- rados bench 60 write --pool bench_rados -b 65536 -O 65536 --concurrent-ios 128 --show-time --write-object --write-omap --write-xattr --no-cleanup
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
hints = 1
2021-10-28T14:53:18.891056+0000 Maintaining 128 concurrent writes of 65536 bytes to objects of size 65536 for up to 60 seconds or 0 objects
2021-10-28T14:53:18.891068+0000 Object prefix: benchmark_data_node-2.bluefield2.ucsc-cmps10_7
2021-10-28T14:53:18.893822+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-28T14:53:18.893822+0000     0       0         0         0         0         0           -           0
2021-10-28T14:53:19.893920+0000     1     128       990       862   53.8723    53.875     0.13296    0.132579
2021-10-28T14:53:20.894000+0000     2     128      2014      1886   58.9337        64    0.123495    0.130758
2021-10-28T14:53:21.894082+0000     3     128      3038      2910   60.6207        64    0.126393    0.129304
2021-10-28T14:53:22.894156+0000     4     128      4035      3907   61.0425   62.3125    0.128052    0.128886
2021-10-28T14:53:23.894229+0000     5     128      5015      4887   61.0831     61.25    0.152402    0.129018
2021-10-28T14:53:24.894302+0000     6     128      6022      5894   61.3914   62.9375    0.132131    0.129025
2021-10-28T14:53:25.894374+0000     7     128      7022      6894   61.5491      62.5    0.128612    0.128578
2021-10-28T14:53:26.894450+0000     8     128      8030      7902   61.7299        63    0.116512    0.128597
2021-10-28T14:53:27.894523+0000     9     128      8599      8471   58.8221   35.5625    0.256913    0.133993
2021-10-28T14:53:28.894590+0000    10     128      9087      8959   55.9897      30.5    0.269873    0.141025
2021-10-28T14:53:29.894659+0000    11     128      9566      9438   53.6212   29.9375    0.273076    0.147381
2021-10-28T14:53:30.894730+0000    12     128     10078      9950   51.8192        32     0.27626    0.153319
2021-10-28T14:53:31.894808+0000    13     128     10519     10391   49.9531   27.5625    0.283078    0.158179
2021-10-28T14:53:32.894886+0000    14     128     11003     10875   48.5456     30.25    0.280547    0.163082
2021-10-28T14:53:33.894963+0000    15     128     11486     11358   47.3216   30.1875    0.262779    0.167698
2021-10-28T14:53:34.895035+0000    16     128     11927     11799   46.0865   27.5625    0.267772    0.171524
2021-10-28T14:53:35.895108+0000    17     128     12819     12691   46.6547     55.75    0.122479    0.170946
2021-10-28T14:53:36.895180+0000    18     128     13719     13591   47.1875     56.25    0.125314    0.168966
2021-10-28T14:53:37.895250+0000    19     128     14743     14615   48.0722        64    0.138273    0.165901
2021-10-28T14:53:38.895319+0000 min lat: 0.0855906 max lat: 0.302118 avg lat: 0.163639
2021-10-28T14:53:38.895319+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-28T14:53:38.895319+0000    20     128     15710     15582   48.6902   60.4375    0.120487    0.163639
2021-10-28T14:53:39.895401+0000    21     128     16478     16350   48.6572        48    0.267111    0.163472
2021-10-28T14:53:40.895466+0000    22     128     16919     16791   47.6982   27.5625    0.253778    0.166123
2021-10-28T14:53:41.895539+0000    23     128     17175     17047     46.32        16    0.538761    0.170472
2021-10-28T14:53:42.895614+0000    24     128     17431     17303   45.0566        16    0.506415    0.175532
2021-10-28T14:53:43.895688+0000    25     128     17687     17559   43.8943        16    0.498704    0.180358
2021-10-28T14:53:44.895765+0000    26     128     17943     17815   42.8214        16    0.507274    0.185211
2021-10-28T14:53:45.895840+0000    27     128     18199     18071    41.828        16    0.518534    0.189861
2021-10-28T14:53:46.895913+0000    28     128     18455     18327   40.9055        16    0.548639    0.194566
2021-10-28T14:53:47.895984+0000    29     128     18654     18526   39.9238   12.4375    0.552518    0.198155
2021-10-28T14:53:48.896067+0000    30     128     18910     18782   39.1263        16    0.503247    0.202548
2021-10-28T14:53:49.896140+0000    31     128     19166     19038   38.3803        16    0.506994    0.206585
2021-10-28T14:53:50.896211+0000    32     128     19654     19526   38.1339      30.5    0.198905    0.209214
2021-10-28T14:53:51.896295+0000    33     128     20247     20119   38.1014   37.0625     0.22237    0.209339
2021-10-28T14:53:52.896377+0000    34     128     20887     20759   38.1571        40    0.219215    0.209019
2021-10-28T14:53:53.896453+0000    35     128     21470     21342   38.1079   36.4375    0.208633    0.209231
2021-10-28T14:53:54.896531+0000    36     128     22039     21911   38.0371   35.5625     0.23685    0.209492
2021-10-28T14:53:55.896601+0000    37     128     22672     22544   38.0783   39.5625    0.205174    0.209698
2021-10-28T14:53:56.896672+0000    38     128     23262     23134   38.0465    36.875    0.217191    0.209844
2021-10-28T14:53:57.896743+0000    39     128     23831     23703   37.9828   35.5625    0.213628    0.209899
2021-10-28T14:53:58.896815+0000 min lat: 0.0855906 max lat: 0.558647 avg lat: 0.210184
2021-10-28T14:53:58.896815+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-28T14:53:58.896815+0000    40     128     24343     24215   37.8332        32    0.236784    0.210184
2021-10-28T14:53:59.896891+0000    41     128     24599     24471   37.3006        16    0.517472    0.212914
2021-10-28T14:54:00.896964+0000    42     128     24855     24727   36.7934        16    0.496435     0.21602
2021-10-28T14:54:01.897044+0000    43     128     25111     24983   36.3098        16    0.553116    0.219212
2021-10-28T14:54:02.897123+0000    44     128     25367     25239   35.8482        16     0.53106    0.222363
2021-10-28T14:54:03.897195+0000    45     128     25623     25495   35.4071        16    0.526013    0.225181
2021-10-28T14:54:04.897268+0000    46     128     25950     25822   35.0816   20.4375    0.271554    0.227218
2021-10-28T14:54:05.897342+0000    47     128     26462     26334    35.016        32    0.257821    0.228094
2021-10-28T14:54:06.897415+0000    48     128     26903     26775   34.8607   27.5625     0.27466     0.22891
2021-10-28T14:54:07.897485+0000    49     128     27415     27287   34.8023        32    0.257439    0.229535
2021-10-28T14:54:08.897553+0000    50     128     28055     27927   34.9062        40    0.132587    0.228911
2021-10-28T14:54:09.897624+0000    51     128     28882     28754   35.2352   51.6875    0.129342    0.226803
2021-10-28T14:54:10.897692+0000    52     128     29847     29719   35.7173   60.3125    0.144087    0.223631
2021-10-28T14:54:11.897758+0000    53     128     30871     30743   36.2509        64    0.118241    0.220467
2021-10-28T14:54:12.897833+0000    54     128     31838     31710   36.6987   60.4375    0.133682    0.217755
2021-10-28T14:54:13.897905+0000    55     128     32606     32478   36.9041        48    0.277064    0.216401
2021-10-28T14:54:14.897977+0000    56     128     33047     32919   36.7373   27.5625    0.259157    0.217108
2021-10-28T14:54:15.898048+0000    57     128     33303     33175   36.3734        16     0.51181    0.219129
2021-10-28T14:54:16.898118+0000    58     128     33559     33431   36.0221        16    0.507211    0.221335
2021-10-28T14:54:17.898189+0000    59     128     33809     33681   35.6764    15.625    0.534608    0.223572
2021-10-28T14:54:18.898284+0000 min lat: 0.0855906 max lat: 0.570161 avg lat: 0.225469
2021-10-28T14:54:18.898284+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-28T14:54:18.898284+0000    60     128     34014     33886   35.2953   12.8125     0.49314    0.225469
2021-10-28T14:54:19.898398+0000 Total time run:         60.354
Total writes made:      34014
Write size:             65536
Object size:            65536
Bandwidth (MB/sec):     35.2234
Stddev Bandwidth:       17.6664
Max bandwidth (MB/sec): 64
Min bandwidth (MB/sec): 12.4375
Average IOPS:           563
Stddev IOPS:            282.663
Max IOPS:               1024
Min IOPS:               199
Average Latency(s):     0.226597
Stddev Latency(s):      0.130001
Max latency(s):         0.570161
Min latency(s):         0.0855906

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:54:20,679337920-07:00] INFO: > Stop system activity collection services[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:54:20,685082175-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:391 - rados_bench() > kill -INT 417239

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:54:20,690477181-07:00] INFO: >> for OSD host[0m
# ./benchmarks/bench-rados:396 - rados_bench() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.5 kill -INT 120030
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:54:20,698306025-07:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.5 kill -INT 120030
[1] 07:54:22 [SUCCESS] ljishen@10.10.2.5

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:54:22,188716715-07:00] INFO: >>> Copy system activity file from OSD host[0m
## ./benchmarks/bench-rados:403 - rados_bench() > basename -- /tmp/bench-rados/sys_activity_64KB_write.dat.osd_host.1
# ./benchmarks/bench-rados:403 - rados_bench() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.5:/tmp/bench-rados/sys_activity_64KB_write.dat.osd_host.1 /mnt/sda8/ceph-research/scripts/output/sys_activity_64KB_write.dat.osd_host.1


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:54:23,567558126-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:54:47,756684268-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 34.02k objects, 2.1 GiB
    usage:   6.3 GiB used, 94 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:54:47,763882143-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:54:57,130412356-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 34.02k objects, 2.1 GiB
    usage:   6.3 GiB used, 94 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:54:57,138009923-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:55:06,569992549-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 34.02k objects, 2.1 GiB
    usage:   6.3 GiB used, 94 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:55:06,577259213-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:55:15,959662660-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 34.02k objects, 2.1 GiB
    usage:   6.3 GiB used, 94 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:55:15,966923923-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:55:25,325748527-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 34.02k objects, 2.1 GiB
    usage:   6.3 GiB used, 94 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:55:25,333075695-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:55:25,339006671-07:00] INFO: > The cluster is idle now.[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:55:25,342328241-07:00] STAGE: Start benchmarking of mode seq ...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:55:25,348603636-07:00] INFO: > Start system activity collection services[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:55:25,353899226-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:341 - rados_bench() > rm --force /mnt/sda8/ceph-research/scripts/output/sys_activity_64KB_seq.dat.rados_bench_host.1
# ./benchmarks/bench-rados:342 - rados_bench() > sar_pid_rados_bench_host=418590
# ./benchmarks/bench-rados:342 - rados_bench() > S_TIME_FORMAT=ISO
# ./benchmarks/bench-rados:342 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/output/sys_activity_64KB_seq.dat.rados_bench_host.1 2

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:55:25,360404074-07:00] INFO: >> for OSD host[0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:55:25,369268248-07:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.5 bash
10.10.2.5: b'120579\n'
[1] 07:55:26 [SUCCESS] ljishen@10.10.2.5
120579

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:55:26,557639986-07:00] INFO: > Run rados bench[0m
# ./benchmarks/bench-rados:379 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/output/rados_bench_64KB_seq.log.1
## ./benchmarks/bench-rados:376 - rados_bench() > count_cpus 32-63,96-127
# ./benchmarks/bench-rados:376 - rados_bench() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 128 --show-time
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:55:26,577328359-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:55:26,580143634-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '06cf3c7e-37fe-11ec-b51d-53e6e728d2d3', '--', 'rados', 'bench', '99999999', 'seq', '--pool', 'bench_rados', '--concurrent-ios', '128', '--show-time'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 06cf3c7e-37fe-11ec-b51d-53e6e728d2d3 -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 128 --show-time
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
hints = 1
2021-10-28T14:55:30.037273+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-28T14:55:30.037273+0000     0       0         0         0         0         0           -           0
2021-10-28T14:55:31.037424+0000     1     128      8862      8734   545.746   545.875   0.0145316   0.0145414
2021-10-28T14:55:32.037575+0000     2     128     16955     16827   525.742   505.812   0.0174928   0.0151327
2021-10-28T14:55:33.037725+0000     3     128     25219     25091   522.636     516.5   0.0169612   0.0152619
2021-10-28T14:55:34.037827+0000     4     128     33920     33792   527.916   543.812   0.0159254   0.0151198
2021-10-28T14:55:35.038011+0000 Total time run:       4.02519
Total reads made:     34014
Read size:            65536
Object size:          65536
Bandwidth (MB/sec):   528.143
Average IOPS:         8450
Stddev IOPS:          319.21
Max IOPS:             8734
Min IOPS:             8093
Average Latency(s):   0.0151132
Max latency(s):       0.0375843
Min latency(s):       0.00381018

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:55:35,746546556-07:00] INFO: > Stop system activity collection services[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:55:35,752466511-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:391 - rados_bench() > kill -INT 418590

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:55:35,758186351-07:00] INFO: >> for OSD host[0m
# ./benchmarks/bench-rados:396 - rados_bench() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.5 kill -INT 120579
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:55:35,765893685-07:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.5 kill -INT 120579
[1] 07:55:36 [SUCCESS] ljishen@10.10.2.5

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:55:36,852042397-07:00] INFO: >>> Copy system activity file from OSD host[0m
## ./benchmarks/bench-rados:403 - rados_bench() > basename -- /tmp/bench-rados/sys_activity_64KB_seq.dat.osd_host.1
# ./benchmarks/bench-rados:403 - rados_bench() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.5:/tmp/bench-rados/sys_activity_64KB_seq.dat.osd_host.1 /mnt/sda8/ceph-research/scripts/output/sys_activity_64KB_seq.dat.osd_host.1


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:55:37,928157963-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:56:02,265188327-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 34.02k objects, 2.1 GiB
    usage:   6.3 GiB used, 94 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:56:02,272678402-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:56:11,856593894-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 34.02k objects, 2.1 GiB
    usage:   6.3 GiB used, 94 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:56:11,864341905-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:56:21,244851226-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 34.02k objects, 2.1 GiB
    usage:   6.3 GiB used, 94 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:56:21,252392237-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:56:30,517774065-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 34.02k objects, 2.1 GiB
    usage:   6.3 GiB used, 94 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:56:30,525160073-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:56:39,956625801-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 34.02k objects, 2.1 GiB
    usage:   6.3 GiB used, 94 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:56:39,964404820-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:56:39,970176687-07:00] INFO: > The cluster is idle now.[0m


[1;7;39;49m[2021-10-28T07:56:39,972466953-07:00][RUNNING][ROUND 2/3/21] object_size=64KB[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:56:39,975716828-07:00] STAGE: Launch a Ceph cluster (CEPH_HOST: 10.10.2.1, OSD_HOST: 10.10.2.5) ...[0m
# ./benchmarks/bench-rados:183 - launch_ceph_cluster() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.1 bash -euo pipefail
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:56:39,985268467-07:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.1 bash -euo pipefail
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T07:56:40,475073279-07:00] INFO: > Remove existing clusters\x1b[0m\n'
10.10.2.1: b'## bash:17 -  > basename -- /var/lib/ceph/06cf3c7e-37fe-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'# bash:17 -  > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm --host 10.10.2.1 --host 10.10.2.5 rm-cluster --force --fsid 06cf3c7e-37fe-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T07:56:40,486592815-07:00] DEBUG: HOST_PARAMS: ['--host', '10.10.2.1', '--host', '10.10.2.5']\x1b[0m\n"
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T07:56:40,490786769-07:00] DEBUG: CEPHADM_PARAMS: ['rm-cluster', '--force', '--fsid', '06cf3c7e-37fe-11ec-b51d-53e6e728d2d3']\x1b[0m\n# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:63 -  > /tmp/bench-rados/ceph-research/scripts/parallel-ssh --host 10.10.2.1 --host 10.10.2.5 'sudo python3 -u - rm-cluster --force --fsid 06cf3c7e-37fe-11ec-b51d-53e6e728d2d3'\n"
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T07:56:40,499347071-07:00] DEBUG: command from stdin\x1b[0m\n'
10.10.2.1: b"# /tmp/bench-rados/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host 10.10.2.1 --host 10.10.2.5 'sudo python3 -u - rm-cluster --force --fsid 06cf3c7e-37fe-11ec-b51d-53e6e728d2d3'\n"
10.10.2.1: b'[1] 07:56:45 [SUCCESS] 10.10.2.1\n[2] 07:56:51 [SUCCESS] 10.10.2.5\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T07:56:51,667160920-07:00] INFO: > Deploy a new cluster\x1b[0m\n'
10.10.2.1: b'# bash:24 -  > /tmp/bench-rados/ceph-research/scripts/ceph-deploy -m 10.10.2.1 -o sm1/10.10.2.5:/dev/nvme0n1\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T07:56:51,680258231-07:00] STAGE: Verify passwordless SSH to hosts, passwordless sudo via ljishen, and the existence of devices on hosts...\x1b[0m\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T07:56:51,684903774-07:00] INFO: > Check the host for the monitor\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:181 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@10.10.2.1 sudo --non-interactive --validate\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T07:56:51,835481595-07:00] INFO: > Check hosts for OSDs\x1b[0m\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T07:56:51,840042018-07:00] INFO: >> Check host 'sm1'\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:200 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 sudo --non-interactive --validate\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T07:56:52,910969250-07:00] INFO: >>> Check device '/dev/nvme0n1'\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:215 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 test -b /dev/nvme0n1\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:226 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 lsblk --nodeps --noheadings --output TYPE /dev/nvme0n1\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:227 - verify_parameters() > grep --word-regexp disk\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T07:56:55,048069216-07:00] STAGE: Reset devices on hosts...\x1b[0m\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T07:56:55,052827811-07:00] INFO: > Resetting device '/dev/nvme0n1' on host 'sm1'\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:288 - reset_host_devices() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 sudo sh\n'
10.10.2.1: b'  Removing ceph--a99d9801--4673--47f0--a05a--626a361e6af5-osd--block--dd630ce7--1a59--4b9c--bcd4--3e0a1cd7d453 (252:0)\n'
10.10.2.1: b'  Archiving volume group "ceph-a99d9801-4673-47f0-a05a-626a361e6af5" metadata (seqno 5).\n'
10.10.2.1: b'  Releasing logical volume "osd-block-dd630ce7-1a59-4b9c-bcd4-3e0a1cd7d453"\n'
10.10.2.1: b'  Creating volume group backup "/etc/lvm/backup/ceph-a99d9801-4673-47f0-a05a-626a361e6af5" (seqno 6).\n'
10.10.2.1: b'  Logical volume "osd-block-dd630ce7-1a59-4b9c-bcd4-3e0a1cd7d453" successfully removed\n'
10.10.2.1: b'  Removing physical volume "/dev/nvme0n1" from volume group "ceph-a99d9801-4673-47f0-a05a-626a361e6af5"\n'
10.10.2.1: b'  Volume group "ceph-a99d9801-4673-47f0-a05a-626a361e6af5" successfully removed\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T07:56:57,602224602-07:00] STAGE: Bootstrap a new cluster...\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:317 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.1 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T07:56:57,612449664-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T07:56:57,616162584-07:00] DEBUG: CEPHADM_PARAMS: ['bootstrap', '--config', '/tmp/bench-rados/ceph-research/scripts/ceph.conf', '--mon-ip', '10.10.2.1', '--output-dir', '/tmp/bench-rados/deployment_data_root/etc/ceph', '--ssh-user', 'ljishen', '--skip-dashboard', '--no-minimize-config', '--allow-overwrite', '--allow-fqdn-hostname', '--orphan-initial-daemons', '--skip-monitoring-stack', '--single-host-defaults']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.1 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults\n'
10.10.2.1: b'Verifying podman|docker is present...\n'
10.10.2.1: b'Verifying lvm2 is present...\n'
10.10.2.1: b'Verifying time synchronization is in place...\n'
10.10.2.1: b'Unit ntp.service is enabled and running\n'
10.10.2.1: b'Repeating the final host check...\n'
10.10.2.1: b'podman|docker (/usr/bin/docker) is present\n'
10.10.2.1: b'systemctl is present\n'
10.10.2.1: b'lvcreate is present\n'
10.10.2.1: b'Unit ntp.service is enabled and running\n'
10.10.2.1: b'Host looks OK\n'
10.10.2.1: b'Cluster fsid: 4c52da84-37ff-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Verifying IP 10.10.2.1 port 3300 ...\n'
10.10.2.1: b'Verifying IP 10.10.2.1 port 6789 ...\n'
10.10.2.1: b'Mon IP `10.10.2.1` is in CIDR network `10.10.2.0/24`\n'
10.10.2.1: b'- internal network (--cluster-network) has not been provided, OSD replication will default to the public_network\n'
10.10.2.1: b'Adjusting default settings to suit single-host cluster...\n'
10.10.2.1: b'Pulling container image quay.io/ceph/ceph:v16...\n'
10.10.2.1: b'Ceph version: ceph version 16.2.6 (ee28fb57e47e9f88813e24bbf4c14496ca299d31) pacific (stable)\nExtracting ceph user uid/gid from container image...\n'
10.10.2.1: b'Creating initial keys...\n'
10.10.2.1: b'Creating initial monmap...\n'
10.10.2.1: b'Creating mon...\n'
10.10.2.1: b'Waiting for mon to start...\n'
10.10.2.1: b'Waiting for mon...\n'
10.10.2.1: b'mon is available\nSetting mon public_network to 10.10.2.0/24\n'
10.10.2.1: b'Wrote config to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf\n'
10.10.2.1: b'Wrote keyring to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring\n'
10.10.2.1: b'Creating mgr...\n'
10.10.2.1: b'Verifying port 9283 ...\n'
10.10.2.1: b'Waiting for mgr to start...\n'
10.10.2.1: b'Waiting for mgr...\n'
10.10.2.1: b'mgr not available, waiting (1/15)...\n'
10.10.2.1: b'mgr not available, waiting (2/15)...\n'
10.10.2.1: b'mgr is available\n'
10.10.2.1: b'Enabling cephadm module...\n'
10.10.2.1: b'Waiting for the mgr to restart...\nWaiting for mgr epoch 5...\n'
10.10.2.1: b'mgr epoch 5 is available\nSetting orchestrator backend to cephadm...\n'
10.10.2.1: b'Generating ssh key...\n'
10.10.2.1: b'Wrote public SSH key to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub\nAdding key to ljishen@localhost authorized_keys...\n'
10.10.2.1: b'Adding host node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us...\n'
10.10.2.1: b'Deploying unmanaged mon service...\n'
10.10.2.1: b'Deploying unmanaged mgr service...\n'
10.10.2.1: b'You can access the Ceph CLI with:\n\n\tsudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --fsid 4c52da84-37ff-11ec-b51d-53e6e728d2d3 -c /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf -k /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring\n\nPlease consider enabling telemetry to help improve Ceph:\n\n\tceph telemetry on\n\nFor more information see:\n\n\thttps://docs.ceph.com/docs/pacific/mgr/telemetry/\n\n'
10.10.2.1: b'Bootstrap complete.\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T07:57:57,563106396-07:00] INFO: Sleep for 20 seconds\x1b[0m\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T07:58:17,570620299-07:00] INFO: > Set the default number of monitor daemons to 1\x1b[0m\n# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:337 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T07:58:17,581028225-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T07:58:17,584703394-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mon', '1']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1\n'
10.10.2.1: b'Inferring fsid 4c52da84-37ff-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/4c52da84-37ff-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b'Scheduled mon update...\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T07:58:26,608526838-07:00] INFO: > Set the default number of manager daemons to 1\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:342 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T07:58:26,618864211-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T07:58:26,622427099-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mgr', '1']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1\n'
10.10.2.1: b'Inferring fsid 4c52da84-37ff-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/4c52da84-37ff-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b'Scheduled mgr update...\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T07:58:35,549278556-07:00] STAGE: Add hosts to the cluster...\x1b[0m\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T07:58:35,555048824-07:00] INFO: > Adding host 'sm1'\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:361 - add_hosts() > ssh-copy-id -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -f -i /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub ljishen@sm1\n'
10.10.2.1: b'/usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: "/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub"\n'
10.10.2.1: b'\nNumber of key(s) added: 1\n\nNow try logging into the machine, with:   "ssh -o \'GlobalKnownHostsFile=/dev/null\' -o \'LogLevel=ERROR\' -o \'PasswordAuthentication=no\' -o \'StrictHostKeyChecking=no\' -o \'UserKnownHostsFile=/dev/null\' \'ljishen@sm1\'"\nand check to make sure that only the key(s) you wanted were added.\n\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:367 - add_hosts() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add sm1 10.10.2.5\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T07:58:36,708517385-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T07:58:36,712523055-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'host', 'add', 'sm1', '10.10.2.5']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add sm1 10.10.2.5\n'
10.10.2.1: b'Inferring fsid 4c52da84-37ff-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/4c52da84-37ff-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b"Added host 'sm1' with addr '10.10.2.5'\n"
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T07:58:47,717813553-07:00] INFO: Sleep for 20 seconds\x1b[0m\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T07:59:07,722725181-07:00] STAGE: Add OSDs to the cluster...\x1b[0m\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T07:59:07,729643567-07:00] INFO: > Adding devices ['/dev/nvme0n1'] on host 'sm1' as OSDs\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:388 - add_host_devices_as_osds() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd sm1:/dev/nvme0n1\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T07:59:07,740168132-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T07:59:07,743908934-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'daemon', 'add', 'osd', 'sm1:/dev/nvme0n1']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd sm1:/dev/nvme0n1\n'
10.10.2.1: b'Inferring fsid 4c52da84-37ff-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/4c52da84-37ff-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b"Created osd(s) 0 on host 'sm1'\n"
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T07:59:31,426581935-07:00] INFO: > Sleep for 20 seconds\x1b[0m\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T07:59:51,432664478-07:00] STAGE: Show Cluster Status\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:408 - show_status() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T07:59:51,442497223-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T07:59:51,446061363-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', '--status']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status\n'
10.10.2.1: b'Inferring fsid 4c52da84-37ff-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/4c52da84-37ff-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b'  cluster:\n    id:     4c52da84-37ff-11ec-b51d-53e6e728d2d3\n    health: HEALTH_WARN\n            1 pool(s) have no replicas configured\n \n  services:\n    mon: 1 daemons, quorum node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us (age 2m)\n    mgr: node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us.rhlait(active, since 2m)\n    osd: 1 osds: 1 up (since 20s), 1 in (since 39s)\n \n  data:\n    pools:   1 pools, 1 pgs\n    objects: 0 objects, 0 B\n    usage:   4.7 MiB used, 100 GiB / 100 GiB avail\n    pgs:     1 active+clean\n \n'
[1] 07:59:59 [SUCCESS] ljishen@10.10.2.1

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:56:40,475073279-07:00] INFO: > Remove existing clusters[0m
## bash:17 -  > basename -- /var/lib/ceph/06cf3c7e-37fe-11ec-b51d-53e6e728d2d3
# bash:17 -  > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm --host 10.10.2.1 --host 10.10.2.5 rm-cluster --force --fsid 06cf3c7e-37fe-11ec-b51d-53e6e728d2d3
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:56:40,486592815-07:00] DEBUG: HOST_PARAMS: ['--host', '10.10.2.1', '--host', '10.10.2.5'][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:56:40,490786769-07:00] DEBUG: CEPHADM_PARAMS: ['rm-cluster', '--force', '--fsid', '06cf3c7e-37fe-11ec-b51d-53e6e728d2d3'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:63 -  > /tmp/bench-rados/ceph-research/scripts/parallel-ssh --host 10.10.2.1 --host 10.10.2.5 'sudo python3 -u - rm-cluster --force --fsid 06cf3c7e-37fe-11ec-b51d-53e6e728d2d3'
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:56:40,499347071-07:00] DEBUG: command from stdin[0m
# /tmp/bench-rados/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host 10.10.2.1 --host 10.10.2.5 'sudo python3 -u - rm-cluster --force --fsid 06cf3c7e-37fe-11ec-b51d-53e6e728d2d3'
[1] 07:56:45 [SUCCESS] 10.10.2.1
[2] 07:56:51 [SUCCESS] 10.10.2.5

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:56:51,667160920-07:00] INFO: > Deploy a new cluster[0m
# bash:24 -  > /tmp/bench-rados/ceph-research/scripts/ceph-deploy -m 10.10.2.1 -o sm1/10.10.2.5:/dev/nvme0n1


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:56:51,680258231-07:00] STAGE: Verify passwordless SSH to hosts, passwordless sudo via ljishen, and the existence of devices on hosts...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:56:51,684903774-07:00] INFO: > Check the host for the monitor[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:181 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@10.10.2.1 sudo --non-interactive --validate

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:56:51,835481595-07:00] INFO: > Check hosts for OSDs[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:56:51,840042018-07:00] INFO: >> Check host 'sm1'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:200 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 sudo --non-interactive --validate

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:56:52,910969250-07:00] INFO: >>> Check device '/dev/nvme0n1'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:215 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 test -b /dev/nvme0n1
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:226 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 lsblk --nodeps --noheadings --output TYPE /dev/nvme0n1
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:227 - verify_parameters() > grep --word-regexp disk


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:56:55,048069216-07:00] STAGE: Reset devices on hosts...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:56:55,052827811-07:00] INFO: > Resetting device '/dev/nvme0n1' on host 'sm1'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:288 - reset_host_devices() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 sudo sh
  Removing ceph--a99d9801--4673--47f0--a05a--626a361e6af5-osd--block--dd630ce7--1a59--4b9c--bcd4--3e0a1cd7d453 (252:0)
  Archiving volume group "ceph-a99d9801-4673-47f0-a05a-626a361e6af5" metadata (seqno 5).
  Releasing logical volume "osd-block-dd630ce7-1a59-4b9c-bcd4-3e0a1cd7d453"
  Creating volume group backup "/etc/lvm/backup/ceph-a99d9801-4673-47f0-a05a-626a361e6af5" (seqno 6).
  Logical volume "osd-block-dd630ce7-1a59-4b9c-bcd4-3e0a1cd7d453" successfully removed
  Removing physical volume "/dev/nvme0n1" from volume group "ceph-a99d9801-4673-47f0-a05a-626a361e6af5"
  Volume group "ceph-a99d9801-4673-47f0-a05a-626a361e6af5" successfully removed


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:56:57,602224602-07:00] STAGE: Bootstrap a new cluster...[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:317 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.1 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:56:57,612449664-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:56:57,616162584-07:00] DEBUG: CEPHADM_PARAMS: ['bootstrap', '--config', '/tmp/bench-rados/ceph-research/scripts/ceph.conf', '--mon-ip', '10.10.2.1', '--output-dir', '/tmp/bench-rados/deployment_data_root/etc/ceph', '--ssh-user', 'ljishen', '--skip-dashboard', '--no-minimize-config', '--allow-overwrite', '--allow-fqdn-hostname', '--orphan-initial-daemons', '--skip-monitoring-stack', '--single-host-defaults'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.1 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults
Verifying podman|docker is present...
Verifying lvm2 is present...
Verifying time synchronization is in place...
Unit ntp.service is enabled and running
Repeating the final host check...
podman|docker (/usr/bin/docker) is present
systemctl is present
lvcreate is present
Unit ntp.service is enabled and running
Host looks OK
Cluster fsid: 4c52da84-37ff-11ec-b51d-53e6e728d2d3
Verifying IP 10.10.2.1 port 3300 ...
Verifying IP 10.10.2.1 port 6789 ...
Mon IP `10.10.2.1` is in CIDR network `10.10.2.0/24`
- internal network (--cluster-network) has not been provided, OSD replication will default to the public_network
Adjusting default settings to suit single-host cluster...
Pulling container image quay.io/ceph/ceph:v16...
Ceph version: ceph version 16.2.6 (ee28fb57e47e9f88813e24bbf4c14496ca299d31) pacific (stable)
Extracting ceph user uid/gid from container image...
Creating initial keys...
Creating initial monmap...
Creating mon...
Waiting for mon to start...
Waiting for mon...
mon is available
Setting mon public_network to 10.10.2.0/24
Wrote config to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf
Wrote keyring to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring
Creating mgr...
Verifying port 9283 ...
Waiting for mgr to start...
Waiting for mgr...
mgr not available, waiting (1/15)...
mgr not available, waiting (2/15)...
mgr is available
Enabling cephadm module...
Waiting for the mgr to restart...
Waiting for mgr epoch 5...
mgr epoch 5 is available
Setting orchestrator backend to cephadm...
Generating ssh key...
Wrote public SSH key to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub
Adding key to ljishen@localhost authorized_keys...
Adding host node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us...
Deploying unmanaged mon service...
Deploying unmanaged mgr service...
You can access the Ceph CLI with:

	sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --fsid 4c52da84-37ff-11ec-b51d-53e6e728d2d3 -c /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf -k /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring

Please consider enabling telemetry to help improve Ceph:

	ceph telemetry on

For more information see:

	https://docs.ceph.com/docs/pacific/mgr/telemetry/

Bootstrap complete.

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:57:57,563106396-07:00] INFO: Sleep for 20 seconds[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:58:17,570620299-07:00] INFO: > Set the default number of monitor daemons to 1[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:337 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:58:17,581028225-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:58:17,584703394-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mon', '1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1
Inferring fsid 4c52da84-37ff-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/4c52da84-37ff-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Scheduled mon update...

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:58:26,608526838-07:00] INFO: > Set the default number of manager daemons to 1[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:342 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:58:26,618864211-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:58:26,622427099-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mgr', '1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1
Inferring fsid 4c52da84-37ff-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/4c52da84-37ff-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Scheduled mgr update...


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:58:35,549278556-07:00] STAGE: Add hosts to the cluster...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:58:35,555048824-07:00] INFO: > Adding host 'sm1'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:361 - add_hosts() > ssh-copy-id -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -f -i /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub ljishen@sm1
/usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: "/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub"

Number of key(s) added: 1

Now try logging into the machine, with:   "ssh -o 'GlobalKnownHostsFile=/dev/null' -o 'LogLevel=ERROR' -o 'PasswordAuthentication=no' -o 'StrictHostKeyChecking=no' -o 'UserKnownHostsFile=/dev/null' 'ljishen@sm1'"
and check to make sure that only the key(s) you wanted were added.

# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:367 - add_hosts() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add sm1 10.10.2.5
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:58:36,708517385-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:58:36,712523055-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'host', 'add', 'sm1', '10.10.2.5'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add sm1 10.10.2.5
Inferring fsid 4c52da84-37ff-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/4c52da84-37ff-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Added host 'sm1' with addr '10.10.2.5'

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:58:47,717813553-07:00] INFO: Sleep for 20 seconds[0m


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:59:07,722725181-07:00] STAGE: Add OSDs to the cluster...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:59:07,729643567-07:00] INFO: > Adding devices ['/dev/nvme0n1'] on host 'sm1' as OSDs[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:388 - add_host_devices_as_osds() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd sm1:/dev/nvme0n1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:59:07,740168132-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:59:07,743908934-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'daemon', 'add', 'osd', 'sm1:/dev/nvme0n1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd sm1:/dev/nvme0n1
Inferring fsid 4c52da84-37ff-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/4c52da84-37ff-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Created osd(s) 0 on host 'sm1'

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:59:31,426581935-07:00] INFO: > Sleep for 20 seconds[0m


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:59:51,432664478-07:00] STAGE: Show Cluster Status[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:408 - show_status() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:59:51,442497223-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:59:51,446061363-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', '--status'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status
Inferring fsid 4c52da84-37ff-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/4c52da84-37ff-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
  cluster:
    id:     4c52da84-37ff-11ec-b51d-53e6e728d2d3
    health: HEALTH_WARN
            1 pool(s) have no replicas configured
 
  services:
    mon: 1 daemons, quorum node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us (age 2m)
    mgr: node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us.rhlait(active, since 2m)
    osd: 1 osds: 1 up (since 20s), 1 in (since 39s)
 
  data:
    pools:   1 pools, 1 pgs
    objects: 0 objects, 0 B
    usage:   4.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     1 active+clean
 

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:59:59,956471595-07:00] INFO: > Copy configuration files to local[0m
# ./benchmarks/bench-rados:220 - launch_ceph_cluster() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.1 sudo chown --changes --recursive ljishen /tmp/bench-rados/deployment_data_root/etc/ceph
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T07:59:59,964033335-07:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.1 sudo chown --changes --recursive ljishen /tmp/bench-rados/deployment_data_root/etc/ceph
[1] 08:00:00 [SUCCESS] ljishen@10.10.2.1
# ./benchmarks/bench-rados:223 - launch_ceph_cluster() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.1:/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf
# ./benchmarks/bench-rados:226 - launch_ceph_cluster() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.1:/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:00:00,436488644-07:00] STAGE: Update local ceph.conf as a client...[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:00:00,439773133-07:00] STAGE: Configure Ceph pool...[0m
# ./benchmarks/bench-rados:257 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_max_backfills 32
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:00:00,461877217-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:00:00,464883633-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '4c52da84-37ff-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'config', 'set', 'osd', 'osd_max_backfills', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 4c52da84-37ff-11ec-b51d-53e6e728d2d3 -- ceph config set osd osd_max_backfills 32
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:258 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_active 32
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:00:04,891726632-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:00:04,894716797-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '4c52da84-37ff-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_active', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 4c52da84-37ff-11ec-b51d-53e6e728d2d3 -- ceph config set osd osd_recovery_max_active 32
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:259 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_single_start 8
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:00:09,351120131-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:00:09,354083756-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '4c52da84-37ff-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_single_start', '8'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 4c52da84-37ff-11ec-b51d-53e6e728d2d3 -- ceph config set osd osd_recovery_max_single_start 8
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:260 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_op_priority 63
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:00:13,881916236-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:00:13,884890952-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '4c52da84-37ff-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_op_priority', '63'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 4c52da84-37ff-11ec-b51d-53e6e728d2d3 -- ceph config set osd osd_recovery_op_priority 63
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:262 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config show-with-defaults osd.0
# ./benchmarks/bench-rados:263 - configure_ceph_pool() > grep 'osd_max_backfills\|osd_recovery'
# ./benchmarks/bench-rados:263 - configure_ceph_pool() > column -t -s ' '
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
osd_max_backfills                        32         mon
osd_recovery_cost                        20971520   default
osd_recovery_delay_start                 0.000000   default
osd_recovery_max_active                  32         mon
osd_recovery_max_active_hdd              3          default
osd_recovery_max_active_ssd              10         default
osd_recovery_max_chunk                   8388608    default
osd_recovery_max_omap_entries_per_chunk  8096       default
osd_recovery_max_single_start            8          mon
osd_recovery_op_priority                 63         mon
osd_recovery_op_warn_multiple            16         default
osd_recovery_priority                    5          default
osd_recovery_retry_interval              30.000000  default
osd_recovery_sleep                       0.000000   default
osd_recovery_sleep_hdd                   0.100000   default
osd_recovery_sleep_hybrid                0.025000   default
osd_recovery_sleep_ssd                   0.000000   default
# ./benchmarks/bench-rados:265 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool create bench_rados 128 128 replicated --size 1 --pg-num-min 128
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:00:22,590101808-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:00:22,593267002-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '4c52da84-37ff-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'create', 'bench_rados', '128', '128', 'replicated', '--size', '1', '--pg-num-min', '128'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 4c52da84-37ff-11ec-b51d-53e6e728d2d3 -- ceph osd pool create bench_rados 128 128 replicated --size 1 --pg-num-min 128
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
pool 'bench_rados' created
# ./benchmarks/bench-rados:267 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados min_size 1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:00:28,013141543-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:00:28,016139542-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '4c52da84-37ff-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'min_size', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 4c52da84-37ff-11ec-b51d-53e6e728d2d3 -- ceph osd pool set bench_rados min_size 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 min_size to 1
# ./benchmarks/bench-rados:268 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados pg_autoscale_mode off
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:00:32,397026901-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:00:32,400181316-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '4c52da84-37ff-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'pg_autoscale_mode', 'off'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 4c52da84-37ff-11ec-b51d-53e6e728d2d3 -- ceph osd pool set bench_rados pg_autoscale_mode off
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 pg_autoscale_mode to off
# ./benchmarks/bench-rados:269 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool application enable bench_rados benchmark
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:00:37,028355035-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:00:37,031311576-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '4c52da84-37ff-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'application', 'enable', 'bench_rados', 'benchmark'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 4c52da84-37ff-11ec-b51d-53e6e728d2d3 -- ceph osd pool application enable bench_rados benchmark
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
enabled application 'benchmark' on pool 'bench_rados'
# ./benchmarks/bench-rados:270 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados noscrub 1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:00:41,504972113-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:00:41,508189656-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '4c52da84-37ff-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'noscrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 4c52da84-37ff-11ec-b51d-53e6e728d2d3 -- ceph osd pool set bench_rados noscrub 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 noscrub to 1
# ./benchmarks/bench-rados:271 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados nodeep-scrub 1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:00:46,803631599-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:00:46,806537716-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '4c52da84-37ff-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'nodeep-scrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 4c52da84-37ff-11ec-b51d-53e6e728d2d3 -- ceph osd pool set bench_rados nodeep-scrub 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 nodeep-scrub to 1
# ./benchmarks/bench-rados:272 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool ls detail
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:00:52,326772752-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:00:52,330077289-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '4c52da84-37ff-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'ls', 'detail'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 4c52da84-37ff-11ec-b51d-53e6e728d2d3 -- ceph osd pool ls detail
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
pool 1 'device_health_metrics' replicated size 1 min_size 1 crush_rule 0 object_hash rjenkins pg_num 64 pgp_num 64 autoscale_mode on last_change 19 lfor 0/0/16 flags hashpspool stripe_width 0 pg_num_min 1 application mgr_devicehealth
pool 2 'bench_rados' replicated size 1 min_size 1 crush_rule 0 object_hash rjenkins pg_num 128 pgp_num 128 autoscale_mode off last_change 21 flags hashpspool,noscrub,nodeep-scrub stripe_width 0 pg_num_min 128 application benchmark

# ./benchmarks/bench-rados:273 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd df tree
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:00:56,667342325-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:00:56,670097708-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '4c52da84-37ff-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'df', 'tree'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 4c52da84-37ff-11ec-b51d-53e6e728d2d3 -- ceph osd df tree
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
ID  CLASS  WEIGHT   REWEIGHT  SIZE     RAW USE  DATA     OMAP  META     AVAIL    %USE  VAR   PGS  STATUS  TYPE NAME    
-1         0.09769         -  100 GiB  5.7 MiB  176 KiB   0 B  5.6 MiB  100 GiB  0.01  1.00    -          root default 
-3         0.09769         -  100 GiB  5.7 MiB  176 KiB   0 B  5.6 MiB  100 GiB  0.01  1.00    -              host sm1 
 0    ssd  0.09769   1.00000  100 GiB  5.7 MiB  176 KiB   0 B  5.6 MiB  100 GiB  0.01  1.00  192      up          osd.0
                       TOTAL  100 GiB  5.7 MiB  176 KiB   0 B  5.6 MiB  100 GiB  0.01                                  
MIN/MAX VAR: 1.00/1.00  STDDEV: 0


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:01:00,962458368-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:01:25,209391465-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:01:34,609255598-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:01:44,041516331-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:01:44,049436136-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:01:53,375209109-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:01:53,382915962-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:02:02,633309608-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:02:02,640743507-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:02:11,997144309-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:02:12,004832677-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:02:21,380245828-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:02:21,387630664-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:02:21,393360121-07:00] INFO: > The cluster is idle now.[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:02:21,396846121-07:00] STAGE: Start benchmarking of mode write ...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:02:21,403300363-07:00] INFO: > Start system activity collection services[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:02:21,408563321-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:341 - rados_bench() > rm --force /mnt/sda8/ceph-research/scripts/output/sys_activity_64KB_write.dat.rados_bench_host.2
# ./benchmarks/bench-rados:342 - rados_bench() > sar_pid_rados_bench_host=424141
# ./benchmarks/bench-rados:342 - rados_bench() > S_TIME_FORMAT=ISO
# ./benchmarks/bench-rados:342 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/output/sys_activity_64KB_write.dat.rados_bench_host.2 2

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:02:21,415119436-07:00] INFO: >> for OSD host[0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:02:21,423788643-07:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.5 bash
10.10.2.5: b'125447\n'
[1] 08:02:22 [SUCCESS] ljishen@10.10.2.5
125447

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:02:22,551524608-07:00] INFO: > Run rados bench[0m
# ./benchmarks/bench-rados:370 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/output/rados_bench_64KB_write.log.2
## ./benchmarks/bench-rados:361 - rados_bench() > count_cpus 32-63,96-127
# ./benchmarks/bench-rados:361 - rados_bench() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 60 write --pool bench_rados -b 65536 -O 65536 --concurrent-ios 128 --show-time --write-object --write-omap --write-xattr --no-cleanup
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:02:22,571732087-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:02:22,574791894-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '4c52da84-37ff-11ec-b51d-53e6e728d2d3', '--', 'rados', 'bench', '60', 'write', '--pool', 'bench_rados', '-b', '65536', '-O', '65536', '--concurrent-ios', '128', '--show-time', '--write-object', '--write-omap', '--write-xattr', '--no-cleanup'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 4c52da84-37ff-11ec-b51d-53e6e728d2d3 -- rados bench 60 write --pool bench_rados -b 65536 -O 65536 --concurrent-ios 128 --show-time --write-object --write-omap --write-xattr --no-cleanup
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
hints = 1
2021-10-28T15:02:25.905004+0000 Maintaining 128 concurrent writes of 65536 bytes to objects of size 65536 for up to 60 seconds or 0 objects
2021-10-28T15:02:25.905015+0000 Object prefix: benchmark_data_node-2.bluefield2.ucsc-cmps10_7
2021-10-28T15:02:25.908207+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-28T15:02:25.908207+0000     0       0         0         0         0         0           -           0
2021-10-28T15:02:26.908325+0000     1     128      1056       928    57.997        58    0.152102    0.129419
2021-10-28T15:02:27.908428+0000     2     128      2080      1952   60.9953        64    0.111922    0.127787
2021-10-28T15:02:28.908551+0000     3     128      3104      2976   61.9943        64    0.131632    0.127066
2021-10-28T15:02:29.908651+0000     4     128      4062      3934    61.463    59.875    0.129556    0.127326
2021-10-28T15:02:30.908762+0000     5     128      5086      4958    61.969        64    0.124269    0.127178
2021-10-28T15:02:31.908872+0000     6     127      6048      5921   61.6709   60.1875    0.121503    0.127954
2021-10-28T15:02:32.908989+0000     7     128      7072      6944   61.9937   63.9375    0.131979    0.127463
2021-10-28T15:02:33.909090+0000     8     128      8096      7968   62.2437        64    0.133617    0.127695
2021-10-28T15:02:34.909201+0000     9     128      8608      8480   58.8828        32    0.271139    0.134007
2021-10-28T15:02:35.909319+0000    10     128      9054      8926   55.7817    27.875    0.284533    0.140708
2021-10-28T15:02:36.909394+0000    11     128      9566      9438   53.6195        32    0.273125    0.147758
2021-10-28T15:02:37.909467+0000    12     128     10016      9888   51.4949    28.125      0.2619     0.15326
2021-10-28T15:02:38.909540+0000    13     128     10528     10400   49.9951        32    0.265938    0.158971
2021-10-28T15:02:39.909618+0000    14     128     10974     10846    48.415    27.875     0.30461    0.163994
2021-10-28T15:02:40.909699+0000    15     128     11424     11296   47.0622    28.125    0.275669    0.168648
2021-10-28T15:02:41.909776+0000    16     128     11870     11742   45.8629    27.875    0.305963     0.17267
2021-10-28T15:02:42.909858+0000    17     128     12576     12448   45.7604    44.125    0.136149    0.173152
2021-10-28T15:02:43.909931+0000    18     128     13534     13406   46.5443    59.875    0.119435    0.171543
2021-10-28T15:02:44.910009+0000    19     128     14496     14368   47.2588    60.125    0.129451    0.168758
2021-10-28T15:02:45.910089+0000 min lat: 0.0927214 max lat: 0.312224 avg lat: 0.166447
2021-10-28T15:02:45.910089+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-28T15:02:45.910089+0000    20     128     15454     15326   47.8894    59.875    0.124938    0.166447
2021-10-28T15:02:46.910191+0000    21     128     16350     16222   48.2754        56    0.198942    0.164803
2021-10-28T15:02:47.910273+0000    22     128     16800     16672   47.3593    28.125    0.291269    0.167887
2021-10-28T15:02:48.910354+0000    23     128     17118     16990   46.1643    19.875    0.518103    0.171678
2021-10-28T15:02:49.910439+0000    24     128     17374     17246   44.9074        16    0.515535    0.176837
2021-10-28T15:02:50.910525+0000    25     128     17630     17502   43.7511        16    0.524585    0.181867
2021-10-28T15:02:51.910646+0000    26     128     17824     17696   42.5346    12.125    0.558887    0.185605
2021-10-28T15:02:52.910762+0000    27     128     18080     17952   41.5517        16     0.55583     0.19042
2021-10-28T15:02:53.910857+0000    28     128     18336     18208   40.6391        16    0.513869    0.195093
2021-10-28T15:02:54.910969+0000    29     128     18592     18464   39.7894        16    0.513751    0.199625
2021-10-28T15:02:55.911081+0000    30     128     18848     18720   38.9964        16    0.542211    0.203992
2021-10-28T15:02:56.911190+0000    31     128     19038     18910   38.1214    11.875    0.534049    0.207275
2021-10-28T15:02:57.911285+0000    32     128     19422     19294     37.68        24     0.19514    0.211478
2021-10-28T15:02:58.911396+0000    33     128     19968     19840   37.5722    34.125    0.210232    0.212213
2021-10-28T15:02:59.911508+0000    34     128     20640     20512   37.7023        42    0.197552    0.211838
2021-10-28T15:03:00.911618+0000    35     128     21214     21086     37.65    35.875    0.217238    0.211672
2021-10-28T15:03:01.911714+0000    36     128     21792     21664   37.6075    36.125    0.198193    0.211835
2021-10-28T15:03:02.911827+0000    37     128     22366     22238   37.5606    35.875    0.218026    0.212102
2021-10-28T15:03:03.911940+0000    38     128     23006     22878   37.6247        40     0.22197    0.212125
2021-10-28T15:03:04.912051+0000    39     128     23584     23456   37.5861    36.125    0.225077    0.212275
2021-10-28T15:03:05.912148+0000 min lat: 0.0927214 max lat: 0.55966 avg lat: 0.212384
2021-10-28T15:03:05.912148+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-28T15:03:05.912148+0000    40     128     24158     24030   37.5432    35.875    0.208443    0.212384
2021-10-28T15:03:06.912275+0000    41     128     24480     24352   37.1183    20.125    0.527838    0.213737
2021-10-28T15:03:07.912389+0000    42     128     24736     24608   36.6155        16    0.528682      0.2171
2021-10-28T15:03:08.912499+0000    43     128     24992     24864    36.136        16    0.559217    0.220277
2021-10-28T15:03:09.912594+0000    44     128     25248     25120   35.6783        16    0.519777    0.223281
2021-10-28T15:03:10.912706+0000    45     128     25504     25376    35.241        16    0.541429    0.226231
2021-10-28T15:03:11.912816+0000    46     128     25760     25632   34.8226        16    0.487501    0.228956
2021-10-28T15:03:12.912925+0000    47     128     26144     26016   34.5923        24    0.296741    0.230425
2021-10-28T15:03:13.913023+0000    48     128     26590     26462   34.4523    27.875    0.286775    0.231225
2021-10-28T15:03:14.913134+0000    49     128     27102     26974   34.4022        32    0.277732    0.232078
2021-10-28T15:03:15.913245+0000    50     128     27552     27424   34.2766    28.125    0.289752    0.232598
2021-10-28T15:03:16.913356+0000    51     128     28320     28192   34.5456        48    0.123519    0.231352
2021-10-28T15:03:17.913456+0000    52     128     29278     29150   35.0326    59.875    0.128046    0.228117
2021-10-28T15:03:18.913567+0000    53     128     30240     30112   35.5059    60.125     0.13111    0.225028
2021-10-28T15:03:19.913670+0000    54     128     31227     31099   35.9906   61.6875    0.133816    0.221957
2021-10-28T15:03:20.913780+0000    55     128     32222     32094   36.4668   62.1875    0.129783    0.219113
2021-10-28T15:03:21.913877+0000    56     127     32766     32639   36.4238   34.0625    0.286892    0.219062
2021-10-28T15:03:22.913990+0000    57     128     33118     32990   36.1696   21.9375     0.35183    0.219791
2021-10-28T15:03:23.914100+0000    58     128     33374     33246   35.8218        16    0.560146    0.222271
2021-10-28T15:03:24.914210+0000    59     128     33630     33502   35.4858        16    0.529418    0.224644
2021-10-28T15:03:25.914310+0000 min lat: 0.0927214 max lat: 0.567258 avg lat: 0.22686
2021-10-28T15:03:25.914310+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-28T15:03:25.914310+0000    60     128     33886     33758    35.161        16     0.51894     0.22686
2021-10-28T15:03:26.914483+0000 Total time run:         60.4768
Total writes made:      33886
Write size:             65536
Object size:            65536
Bandwidth (MB/sec):     35.0196
Stddev Bandwidth:       17.8167
Max bandwidth (MB/sec): 64
Min bandwidth (MB/sec): 11.875
Average IOPS:           560
Stddev IOPS:            285.068
Max IOPS:               1024
Min IOPS:               190
Average Latency(s):     0.22797
Stddev Latency(s):      0.13107
Max latency(s):         0.567258
Min latency(s):         0.0927214

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:03:27,693521230-07:00] INFO: > Stop system activity collection services[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:03:27,698708816-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:391 - rados_bench() > kill -INT 424141

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:03:27,704311133-07:00] INFO: >> for OSD host[0m
# ./benchmarks/bench-rados:396 - rados_bench() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.5 kill -INT 125447
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:03:27,711882542-07:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.5 kill -INT 125447
[1] 08:03:29 [SUCCESS] ljishen@10.10.2.5

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:03:29,317332386-07:00] INFO: >>> Copy system activity file from OSD host[0m
## ./benchmarks/bench-rados:403 - rados_bench() > basename -- /tmp/bench-rados/sys_activity_64KB_write.dat.osd_host.2
# ./benchmarks/bench-rados:403 - rados_bench() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.5:/tmp/bench-rados/sys_activity_64KB_write.dat.osd_host.2 /mnt/sda8/ceph-research/scripts/output/sys_activity_64KB_write.dat.osd_host.2


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:03:30,895915683-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:03:55,196505460-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 33.89k objects, 2.1 GiB
    usage:   6.7 GiB used, 93 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:03:55,203918400-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:04:04,597334206-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 33.89k objects, 2.1 GiB
    usage:   6.3 GiB used, 94 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:04:04,604775679-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:04:13,986665816-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 33.89k objects, 2.1 GiB
    usage:   6.3 GiB used, 94 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:04:13,994159818-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:04:23,403007787-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 33.89k objects, 2.1 GiB
    usage:   6.3 GiB used, 94 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:04:23,410587430-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:04:32,769999219-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 33.89k objects, 2.1 GiB
    usage:   6.3 GiB used, 94 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:04:32,777669062-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:04:32,783610188-07:00] INFO: > The cluster is idle now.[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:04:32,787218057-07:00] STAGE: Start benchmarking of mode seq ...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:04:32,793471311-07:00] INFO: > Start system activity collection services[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:04:32,798734990-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:341 - rados_bench() > rm --force /mnt/sda8/ceph-research/scripts/output/sys_activity_64KB_seq.dat.rados_bench_host.2
# ./benchmarks/bench-rados:342 - rados_bench() > sar_pid_rados_bench_host=425481
# ./benchmarks/bench-rados:342 - rados_bench() > S_TIME_FORMAT=ISO
# ./benchmarks/bench-rados:342 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/output/sys_activity_64KB_seq.dat.rados_bench_host.2 2

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:04:32,805334367-07:00] INFO: >> for OSD host[0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:04:32,814252021-07:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.5 bash
10.10.2.5: b'125991\n'
[1] 08:04:34 [SUCCESS] ljishen@10.10.2.5
125991

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:04:34,221688610-07:00] INFO: > Run rados bench[0m
# ./benchmarks/bench-rados:379 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/output/rados_bench_64KB_seq.log.2
## ./benchmarks/bench-rados:376 - rados_bench() > count_cpus 32-63,96-127
# ./benchmarks/bench-rados:376 - rados_bench() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 128 --show-time
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:04:34,241403993-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:04:34,244305901-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '4c52da84-37ff-11ec-b51d-53e6e728d2d3', '--', 'rados', 'bench', '99999999', 'seq', '--pool', 'bench_rados', '--concurrent-ios', '128', '--show-time'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 4c52da84-37ff-11ec-b51d-53e6e728d2d3 -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 128 --show-time
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
hints = 1
2021-10-28T15:04:37.642396+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-28T15:04:37.642396+0000     0       0         0         0         0         0           -           0
2021-10-28T15:04:38.642547+0000     1     128      8765      8637   539.684   539.812   0.0155395     0.01467
2021-10-28T15:04:39.642666+0000     2     128     17087     16959   529.874   520.125   0.0133824   0.0150366
2021-10-28T15:04:40.642806+0000     3     127     25342     25215   525.225       516   0.0156727   0.0151833
2021-10-28T15:04:41.642909+0000     4      92     33886     33794   527.952   536.188   0.0146284   0.0151106
2021-10-28T15:04:42.643051+0000 Total time run:       4.00901
Total reads made:     33886
Read size:            65536
Object size:          65536
Bandwidth (MB/sec):   528.279
Average IOPS:         8452
Stddev IOPS:          187.635
Max IOPS:             8637
Min IOPS:             8256
Average Latency(s):   0.0151115
Max latency(s):       0.0343914
Min latency(s):       0.00375373

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:04:43,374765582-07:00] INFO: > Stop system activity collection services[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:04:43,380378419-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:391 - rados_bench() > kill -INT 425481

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:04:43,385955048-07:00] INFO: >> for OSD host[0m
# ./benchmarks/bench-rados:396 - rados_bench() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.5 kill -INT 125991
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:04:43,393869322-07:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.5 kill -INT 125991
[1] 08:04:44 [SUCCESS] ljishen@10.10.2.5

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:04:44,599924748-07:00] INFO: >>> Copy system activity file from OSD host[0m
## ./benchmarks/bench-rados:403 - rados_bench() > basename -- /tmp/bench-rados/sys_activity_64KB_seq.dat.osd_host.2
# ./benchmarks/bench-rados:403 - rados_bench() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.5:/tmp/bench-rados/sys_activity_64KB_seq.dat.osd_host.2 /mnt/sda8/ceph-research/scripts/output/sys_activity_64KB_seq.dat.osd_host.2


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:04:45,680016792-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:05:10,211134667-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 33.89k objects, 2.1 GiB
    usage:   6.3 GiB used, 94 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:05:10,218564117-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:05:19,502369824-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 33.89k objects, 2.1 GiB
    usage:   6.3 GiB used, 94 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:05:19,509852665-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:05:29,252562507-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 33.89k objects, 2.1 GiB
    usage:   6.3 GiB used, 94 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:05:29,260074643-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:05:38,784446678-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 33.89k objects, 2.1 GiB
    usage:   6.3 GiB used, 94 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:05:38,792432396-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:05:48,193096701-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 33.89k objects, 2.1 GiB
    usage:   6.3 GiB used, 94 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:05:48,200627823-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:05:48,206328566-07:00] INFO: > The cluster is idle now.[0m


[1;7;39;49m[2021-10-28T08:05:48,208686870-07:00][RUNNING][ROUND 3/3/21] object_size=64KB[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:05:48,212266575-07:00] STAGE: Launch a Ceph cluster (CEPH_HOST: 10.10.2.1, OSD_HOST: 10.10.2.5) ...[0m
# ./benchmarks/bench-rados:183 - launch_ceph_cluster() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.1 bash -euo pipefail
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:05:48,221831180-07:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.1 bash -euo pipefail
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T08:05:48,623271228-07:00] INFO: > Remove existing clusters\x1b[0m\n'
10.10.2.1: b'## bash:17 -  > basename -- /var/lib/ceph/4c52da84-37ff-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'# bash:17 -  > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm --host 10.10.2.1 --host 10.10.2.5 rm-cluster --force --fsid 4c52da84-37ff-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T08:05:48,634756529-07:00] DEBUG: HOST_PARAMS: ['--host', '10.10.2.1', '--host', '10.10.2.5']\x1b[0m\n"
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T08:05:48,638711634-07:00] DEBUG: CEPHADM_PARAMS: ['rm-cluster', '--force', '--fsid', '4c52da84-37ff-11ec-b51d-53e6e728d2d3']\x1b[0m\n# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:63 -  > /tmp/bench-rados/ceph-research/scripts/parallel-ssh --host 10.10.2.1 --host 10.10.2.5 'sudo python3 -u - rm-cluster --force --fsid 4c52da84-37ff-11ec-b51d-53e6e728d2d3'\n"
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T08:05:48,647463426-07:00] DEBUG: command from stdin\x1b[0m\n'
10.10.2.1: b"# /tmp/bench-rados/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host 10.10.2.1 --host 10.10.2.5 'sudo python3 -u - rm-cluster --force --fsid 4c52da84-37ff-11ec-b51d-53e6e728d2d3'\n"
10.10.2.1: b'[1] 08:05:54 [SUCCESS] 10.10.2.1\n[2] 08:06:00 [SUCCESS] 10.10.2.5\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T08:06:00,070592490-07:00] INFO: > Deploy a new cluster\x1b[0m\n'
10.10.2.1: b'# bash:24 -  > /tmp/bench-rados/ceph-research/scripts/ceph-deploy -m 10.10.2.1 -o sm1/10.10.2.5:/dev/nvme0n1\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T08:06:00,082573753-07:00] STAGE: Verify passwordless SSH to hosts, passwordless sudo via ljishen, and the existence of devices on hosts...\x1b[0m\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T08:06:00,087118968-07:00] INFO: > Check the host for the monitor\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:181 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@10.10.2.1 sudo --non-interactive --validate\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T08:06:00,238971954-07:00] INFO: > Check hosts for OSDs\x1b[0m\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T08:06:00,244162101-07:00] INFO: >> Check host 'sm1'\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:200 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 sudo --non-interactive --validate\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T08:06:01,303366715-07:00] INFO: >>> Check device '/dev/nvme0n1'\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:215 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 test -b /dev/nvme0n1\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:226 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 lsblk --nodeps --noheadings --output TYPE /dev/nvme0n1\n# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:227 - verify_parameters() > grep --word-regexp disk\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T08:06:03,447693095-07:00] STAGE: Reset devices on hosts...\x1b[0m\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T08:06:03,452675090-07:00] INFO: > Resetting device '/dev/nvme0n1' on host 'sm1'\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:288 - reset_host_devices() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 sudo sh\n'
10.10.2.1: b'  Removing ceph--46b75ec9--166b--48ba--82fc--ce4557c986af-osd--block--a257cb8f--6d7a--4260--a633--10fc79fbbae0 (252:0)\n'
10.10.2.1: b'  Archiving volume group "ceph-46b75ec9-166b-48ba-82fc-ce4557c986af" metadata (seqno 5).\n'
10.10.2.1: b'  Releasing logical volume "osd-block-a257cb8f-6d7a-4260-a633-10fc79fbbae0"\n'
10.10.2.1: b'  Creating volume group backup "/etc/lvm/backup/ceph-46b75ec9-166b-48ba-82fc-ce4557c986af" (seqno 6).\n'
10.10.2.1: b'  Logical volume "osd-block-a257cb8f-6d7a-4260-a633-10fc79fbbae0" successfully removed\n'
10.10.2.1: b'  Removing physical volume "/dev/nvme0n1" from volume group "ceph-46b75ec9-166b-48ba-82fc-ce4557c986af"\n'
10.10.2.1: b'  Volume group "ceph-46b75ec9-166b-48ba-82fc-ce4557c986af" successfully removed\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T08:06:05,906644514-07:00] STAGE: Bootstrap a new cluster...\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:317 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.1 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T08:06:05,916423337-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T08:06:05,920285578-07:00] DEBUG: CEPHADM_PARAMS: ['bootstrap', '--config', '/tmp/bench-rados/ceph-research/scripts/ceph.conf', '--mon-ip', '10.10.2.1', '--output-dir', '/tmp/bench-rados/deployment_data_root/etc/ceph', '--ssh-user', 'ljishen', '--skip-dashboard', '--no-minimize-config', '--allow-overwrite', '--allow-fqdn-hostname', '--orphan-initial-daemons', '--skip-monitoring-stack', '--single-host-defaults']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.1 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults\n'
10.10.2.1: b'Verifying podman|docker is present...\nVerifying lvm2 is present...\n'
10.10.2.1: b'Verifying time synchronization is in place...\n'
10.10.2.1: b'Unit ntp.service is enabled and running\n'
10.10.2.1: b'Repeating the final host check...\n'
10.10.2.1: b'podman|docker (/usr/bin/docker) is present\n'
10.10.2.1: b'systemctl is present\n'
10.10.2.1: b'lvcreate is present\n'
10.10.2.1: b'Unit ntp.service is enabled and running\n'
10.10.2.1: b'Host looks OK\n'
10.10.2.1: b'Cluster fsid: 93237fa8-3800-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Verifying IP 10.10.2.1 port 3300 ...\n'
10.10.2.1: b'Verifying IP 10.10.2.1 port 6789 ...\n'
10.10.2.1: b'Mon IP `10.10.2.1` is in CIDR network `10.10.2.0/24`\n'
10.10.2.1: b'- internal network (--cluster-network) has not been provided, OSD replication will default to the public_network\n'
10.10.2.1: b'Adjusting default settings to suit single-host cluster...\n'
10.10.2.1: b'Pulling container image quay.io/ceph/ceph:v16...\n'
10.10.2.1: b'Ceph version: ceph version 16.2.6 (ee28fb57e47e9f88813e24bbf4c14496ca299d31) pacific (stable)\n'
10.10.2.1: b'Extracting ceph user uid/gid from container image...\n'
10.10.2.1: b'Creating initial keys...\n'
10.10.2.1: b'Creating initial monmap...\n'
10.10.2.1: b'Creating mon...\n'
10.10.2.1: b'Waiting for mon to start...\n'
10.10.2.1: b'Waiting for mon...\n'
10.10.2.1: b'mon is available\nSetting mon public_network to 10.10.2.0/24\n'
10.10.2.1: b'Wrote config to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf\n'
10.10.2.1: b'Wrote keyring to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring\n'
10.10.2.1: b'Creating mgr...\n'
10.10.2.1: b'Verifying port 9283 ...\n'
10.10.2.1: b'Waiting for mgr to start...\nWaiting for mgr...\n'
10.10.2.1: b'mgr not available, waiting (1/15)...\n'
10.10.2.1: b'mgr not available, waiting (2/15)...\n'
10.10.2.1: b'mgr is available\n'
10.10.2.1: b'Enabling cephadm module...\n'
10.10.2.1: b'Waiting for the mgr to restart...\n'
10.10.2.1: b'Waiting for mgr epoch 5...\n'
10.10.2.1: b'mgr epoch 5 is available\nSetting orchestrator backend to cephadm...\n'
10.10.2.1: b'Generating ssh key...\n'
10.10.2.1: b'Wrote public SSH key to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub\n'
10.10.2.1: b'Adding key to ljishen@localhost authorized_keys...\n'
10.10.2.1: b'Adding host node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us...\n'
10.10.2.1: b'Deploying unmanaged mon service...\n'
10.10.2.1: b'Deploying unmanaged mgr service...\n'
10.10.2.1: b'You can access the Ceph CLI with:\n\n\tsudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --fsid 93237fa8-3800-11ec-b51d-53e6e728d2d3 -c /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf -k /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring\n\n'
10.10.2.1: b'Please consider enabling telemetry to help improve Ceph:\n\n\tceph telemetry on\n\nFor more information see:\n\n\thttps://docs.ceph.com/docs/pacific/mgr/telemetry/\n\n'
10.10.2.1: b'Bootstrap complete.\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T08:07:07,781772498-07:00] INFO: Sleep for 20 seconds\x1b[0m\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T08:07:27,788975557-07:00] INFO: > Set the default number of monitor daemons to 1\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:337 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T08:07:27,798936382-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T08:07:27,802618764-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mon', '1']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1\n'
10.10.2.1: b'Inferring fsid 93237fa8-3800-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/93237fa8-3800-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b'Scheduled mon update...\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T08:07:36,621079193-07:00] INFO: > Set the default number of manager daemons to 1\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:342 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T08:07:36,630603157-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T08:07:36,634622624-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mgr', '1']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1\n'
10.10.2.1: b'Inferring fsid 93237fa8-3800-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/93237fa8-3800-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b'Scheduled mgr update...\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T08:07:45,961176271-07:00] STAGE: Add hosts to the cluster...\x1b[0m\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T08:07:45,967607572-07:00] INFO: > Adding host 'sm1'\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:361 - add_hosts() > ssh-copy-id -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -f -i /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub ljishen@sm1\n'
10.10.2.1: b'/usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: "/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub"\n'
10.10.2.1: b'\nNumber of key(s) added: 1\n\nNow try logging into the machine, with:   "ssh -o \'GlobalKnownHostsFile=/dev/null\' -o \'LogLevel=ERROR\' -o \'PasswordAuthentication=no\' -o \'StrictHostKeyChecking=no\' -o \'UserKnownHostsFile=/dev/null\' \'ljishen@sm1\'"\nand check to make sure that only the key(s) you wanted were added.\n\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:367 - add_hosts() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add sm1 10.10.2.5\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T08:07:47,118786178-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T08:07:47,122379071-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'host', 'add', 'sm1', '10.10.2.5']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add sm1 10.10.2.5\n'
10.10.2.1: b'Inferring fsid 93237fa8-3800-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/93237fa8-3800-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b"Added host 'sm1' with addr '10.10.2.5'\n"
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T08:07:58,069687705-07:00] INFO: Sleep for 20 seconds\x1b[0m\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T08:08:18,074417090-07:00] STAGE: Add OSDs to the cluster...\x1b[0m\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T08:08:18,081163863-07:00] INFO: > Adding devices ['/dev/nvme0n1'] on host 'sm1' as OSDs\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:388 - add_host_devices_as_osds() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd sm1:/dev/nvme0n1\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T08:08:18,090802172-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T08:08:18,094261916-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'daemon', 'add', 'osd', 'sm1:/dev/nvme0n1']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd sm1:/dev/nvme0n1\n'
10.10.2.1: b'Inferring fsid 93237fa8-3800-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/93237fa8-3800-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b"Created osd(s) 0 on host 'sm1'\n"
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T08:08:41,719422830-07:00] INFO: > Sleep for 20 seconds\x1b[0m\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T08:09:01,725140030-07:00] STAGE: Show Cluster Status\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:408 - show_status() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T08:09:01,735637154-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T08:09:01,739351626-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', '--status']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status\n'
10.10.2.1: b'Inferring fsid 93237fa8-3800-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/93237fa8-3800-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b'  cluster:\n    id:     93237fa8-3800-11ec-b51d-53e6e728d2d3\n    health: HEALTH_WARN\n            1 pool(s) have no replicas configured\n \n  services:\n    mon: 1 daemons, quorum node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us (age 2m)\n    mgr: node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us.jfkdha(active, since 2m)\n    osd: 1 osds: 1 up (since 19s), 1 in (since 39s)\n \n  data:\n    pools:   1 pools, 1 pgs\n    objects: 0 objects, 0 B\n    usage:   4.7 MiB used, 100 GiB / 100 GiB avail\n    pgs:     1 active+clean\n \n'
[1] 08:09:10 [SUCCESS] ljishen@10.10.2.1

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:05:48,623271228-07:00] INFO: > Remove existing clusters[0m
## bash:17 -  > basename -- /var/lib/ceph/4c52da84-37ff-11ec-b51d-53e6e728d2d3
# bash:17 -  > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm --host 10.10.2.1 --host 10.10.2.5 rm-cluster --force --fsid 4c52da84-37ff-11ec-b51d-53e6e728d2d3
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:05:48,634756529-07:00] DEBUG: HOST_PARAMS: ['--host', '10.10.2.1', '--host', '10.10.2.5'][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:05:48,638711634-07:00] DEBUG: CEPHADM_PARAMS: ['rm-cluster', '--force', '--fsid', '4c52da84-37ff-11ec-b51d-53e6e728d2d3'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:63 -  > /tmp/bench-rados/ceph-research/scripts/parallel-ssh --host 10.10.2.1 --host 10.10.2.5 'sudo python3 -u - rm-cluster --force --fsid 4c52da84-37ff-11ec-b51d-53e6e728d2d3'
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:05:48,647463426-07:00] DEBUG: command from stdin[0m
# /tmp/bench-rados/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host 10.10.2.1 --host 10.10.2.5 'sudo python3 -u - rm-cluster --force --fsid 4c52da84-37ff-11ec-b51d-53e6e728d2d3'
[1] 08:05:54 [SUCCESS] 10.10.2.1
[2] 08:06:00 [SUCCESS] 10.10.2.5

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:06:00,070592490-07:00] INFO: > Deploy a new cluster[0m
# bash:24 -  > /tmp/bench-rados/ceph-research/scripts/ceph-deploy -m 10.10.2.1 -o sm1/10.10.2.5:/dev/nvme0n1


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:06:00,082573753-07:00] STAGE: Verify passwordless SSH to hosts, passwordless sudo via ljishen, and the existence of devices on hosts...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:06:00,087118968-07:00] INFO: > Check the host for the monitor[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:181 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@10.10.2.1 sudo --non-interactive --validate

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:06:00,238971954-07:00] INFO: > Check hosts for OSDs[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:06:00,244162101-07:00] INFO: >> Check host 'sm1'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:200 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 sudo --non-interactive --validate

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:06:01,303366715-07:00] INFO: >>> Check device '/dev/nvme0n1'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:215 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 test -b /dev/nvme0n1
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:226 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 lsblk --nodeps --noheadings --output TYPE /dev/nvme0n1
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:227 - verify_parameters() > grep --word-regexp disk


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:06:03,447693095-07:00] STAGE: Reset devices on hosts...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:06:03,452675090-07:00] INFO: > Resetting device '/dev/nvme0n1' on host 'sm1'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:288 - reset_host_devices() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 sudo sh
  Removing ceph--46b75ec9--166b--48ba--82fc--ce4557c986af-osd--block--a257cb8f--6d7a--4260--a633--10fc79fbbae0 (252:0)
  Archiving volume group "ceph-46b75ec9-166b-48ba-82fc-ce4557c986af" metadata (seqno 5).
  Releasing logical volume "osd-block-a257cb8f-6d7a-4260-a633-10fc79fbbae0"
  Creating volume group backup "/etc/lvm/backup/ceph-46b75ec9-166b-48ba-82fc-ce4557c986af" (seqno 6).
  Logical volume "osd-block-a257cb8f-6d7a-4260-a633-10fc79fbbae0" successfully removed
  Removing physical volume "/dev/nvme0n1" from volume group "ceph-46b75ec9-166b-48ba-82fc-ce4557c986af"
  Volume group "ceph-46b75ec9-166b-48ba-82fc-ce4557c986af" successfully removed


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:06:05,906644514-07:00] STAGE: Bootstrap a new cluster...[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:317 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.1 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:06:05,916423337-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:06:05,920285578-07:00] DEBUG: CEPHADM_PARAMS: ['bootstrap', '--config', '/tmp/bench-rados/ceph-research/scripts/ceph.conf', '--mon-ip', '10.10.2.1', '--output-dir', '/tmp/bench-rados/deployment_data_root/etc/ceph', '--ssh-user', 'ljishen', '--skip-dashboard', '--no-minimize-config', '--allow-overwrite', '--allow-fqdn-hostname', '--orphan-initial-daemons', '--skip-monitoring-stack', '--single-host-defaults'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.1 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults
Verifying podman|docker is present...
Verifying lvm2 is present...
Verifying time synchronization is in place...
Unit ntp.service is enabled and running
Repeating the final host check...
podman|docker (/usr/bin/docker) is present
systemctl is present
lvcreate is present
Unit ntp.service is enabled and running
Host looks OK
Cluster fsid: 93237fa8-3800-11ec-b51d-53e6e728d2d3
Verifying IP 10.10.2.1 port 3300 ...
Verifying IP 10.10.2.1 port 6789 ...
Mon IP `10.10.2.1` is in CIDR network `10.10.2.0/24`
- internal network (--cluster-network) has not been provided, OSD replication will default to the public_network
Adjusting default settings to suit single-host cluster...
Pulling container image quay.io/ceph/ceph:v16...
Ceph version: ceph version 16.2.6 (ee28fb57e47e9f88813e24bbf4c14496ca299d31) pacific (stable)
Extracting ceph user uid/gid from container image...
Creating initial keys...
Creating initial monmap...
Creating mon...
Waiting for mon to start...
Waiting for mon...
mon is available
Setting mon public_network to 10.10.2.0/24
Wrote config to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf
Wrote keyring to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring
Creating mgr...
Verifying port 9283 ...
Waiting for mgr to start...
Waiting for mgr...
mgr not available, waiting (1/15)...
mgr not available, waiting (2/15)...
mgr is available
Enabling cephadm module...
Waiting for the mgr to restart...
Waiting for mgr epoch 5...
mgr epoch 5 is available
Setting orchestrator backend to cephadm...
Generating ssh key...
Wrote public SSH key to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub
Adding key to ljishen@localhost authorized_keys...
Adding host node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us...
Deploying unmanaged mon service...
Deploying unmanaged mgr service...
You can access the Ceph CLI with:

	sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --fsid 93237fa8-3800-11ec-b51d-53e6e728d2d3 -c /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf -k /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring

Please consider enabling telemetry to help improve Ceph:

	ceph telemetry on

For more information see:

	https://docs.ceph.com/docs/pacific/mgr/telemetry/

Bootstrap complete.

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:07:07,781772498-07:00] INFO: Sleep for 20 seconds[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:07:27,788975557-07:00] INFO: > Set the default number of monitor daemons to 1[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:337 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:07:27,798936382-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:07:27,802618764-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mon', '1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1
Inferring fsid 93237fa8-3800-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/93237fa8-3800-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Scheduled mon update...

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:07:36,621079193-07:00] INFO: > Set the default number of manager daemons to 1[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:342 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:07:36,630603157-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:07:36,634622624-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mgr', '1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1
Inferring fsid 93237fa8-3800-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/93237fa8-3800-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Scheduled mgr update...


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:07:45,961176271-07:00] STAGE: Add hosts to the cluster...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:07:45,967607572-07:00] INFO: > Adding host 'sm1'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:361 - add_hosts() > ssh-copy-id -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -f -i /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub ljishen@sm1
/usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: "/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub"

Number of key(s) added: 1

Now try logging into the machine, with:   "ssh -o 'GlobalKnownHostsFile=/dev/null' -o 'LogLevel=ERROR' -o 'PasswordAuthentication=no' -o 'StrictHostKeyChecking=no' -o 'UserKnownHostsFile=/dev/null' 'ljishen@sm1'"
and check to make sure that only the key(s) you wanted were added.

# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:367 - add_hosts() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add sm1 10.10.2.5
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:07:47,118786178-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:07:47,122379071-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'host', 'add', 'sm1', '10.10.2.5'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add sm1 10.10.2.5
Inferring fsid 93237fa8-3800-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/93237fa8-3800-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Added host 'sm1' with addr '10.10.2.5'

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:07:58,069687705-07:00] INFO: Sleep for 20 seconds[0m


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:08:18,074417090-07:00] STAGE: Add OSDs to the cluster...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:08:18,081163863-07:00] INFO: > Adding devices ['/dev/nvme0n1'] on host 'sm1' as OSDs[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:388 - add_host_devices_as_osds() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd sm1:/dev/nvme0n1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:08:18,090802172-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:08:18,094261916-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'daemon', 'add', 'osd', 'sm1:/dev/nvme0n1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd sm1:/dev/nvme0n1
Inferring fsid 93237fa8-3800-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/93237fa8-3800-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Created osd(s) 0 on host 'sm1'

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:08:41,719422830-07:00] INFO: > Sleep for 20 seconds[0m


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:09:01,725140030-07:00] STAGE: Show Cluster Status[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:408 - show_status() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:09:01,735637154-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:09:01,739351626-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', '--status'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status
Inferring fsid 93237fa8-3800-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/93237fa8-3800-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
  cluster:
    id:     93237fa8-3800-11ec-b51d-53e6e728d2d3
    health: HEALTH_WARN
            1 pool(s) have no replicas configured
 
  services:
    mon: 1 daemons, quorum node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us (age 2m)
    mgr: node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us.jfkdha(active, since 2m)
    osd: 1 osds: 1 up (since 19s), 1 in (since 39s)
 
  data:
    pools:   1 pools, 1 pgs
    objects: 0 objects, 0 B
    usage:   4.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     1 active+clean
 

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:09:10,349769137-07:00] INFO: > Copy configuration files to local[0m
# ./benchmarks/bench-rados:220 - launch_ceph_cluster() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.1 sudo chown --changes --recursive ljishen /tmp/bench-rados/deployment_data_root/etc/ceph
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:09:10,357306540-07:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.1 sudo chown --changes --recursive ljishen /tmp/bench-rados/deployment_data_root/etc/ceph
[1] 08:09:10 [SUCCESS] ljishen@10.10.2.1
# ./benchmarks/bench-rados:223 - launch_ceph_cluster() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.1:/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf
# ./benchmarks/bench-rados:226 - launch_ceph_cluster() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.1:/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:09:10,832777705-07:00] STAGE: Update local ceph.conf as a client...[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:09:10,836330680-07:00] STAGE: Configure Ceph pool...[0m
# ./benchmarks/bench-rados:257 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_max_backfills 32
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:09:10,858417482-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:09:10,861304482-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '93237fa8-3800-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'config', 'set', 'osd', 'osd_max_backfills', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 93237fa8-3800-11ec-b51d-53e6e728d2d3 -- ceph config set osd osd_max_backfills 32
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:258 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_active 32
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:09:15,042867225-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:09:15,046190849-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '93237fa8-3800-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_active', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 93237fa8-3800-11ec-b51d-53e6e728d2d3 -- ceph config set osd osd_recovery_max_active 32
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:259 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_single_start 8
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:09:19,510033916-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:09:19,513087921-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '93237fa8-3800-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_single_start', '8'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 93237fa8-3800-11ec-b51d-53e6e728d2d3 -- ceph config set osd osd_recovery_max_single_start 8
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:260 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_op_priority 63
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:09:23,760477723-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:09:23,763505298-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '93237fa8-3800-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_op_priority', '63'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 93237fa8-3800-11ec-b51d-53e6e728d2d3 -- ceph config set osd osd_recovery_op_priority 63
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:262 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config show-with-defaults osd.0
# ./benchmarks/bench-rados:263 - configure_ceph_pool() > grep 'osd_max_backfills\|osd_recovery'
# ./benchmarks/bench-rados:263 - configure_ceph_pool() > column -t -s ' '
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
osd_max_backfills                        32         mon
osd_recovery_cost                        20971520   default
osd_recovery_delay_start                 0.000000   default
osd_recovery_max_active                  32         mon
osd_recovery_max_active_hdd              3          default
osd_recovery_max_active_ssd              10         default
osd_recovery_max_chunk                   8388608    default
osd_recovery_max_omap_entries_per_chunk  8096       default
osd_recovery_max_single_start            8          mon
osd_recovery_op_priority                 63         mon
osd_recovery_op_warn_multiple            16         default
osd_recovery_priority                    5          default
osd_recovery_retry_interval              30.000000  default
osd_recovery_sleep                       0.000000   default
osd_recovery_sleep_hdd                   0.100000   default
osd_recovery_sleep_hybrid                0.025000   default
osd_recovery_sleep_ssd                   0.000000   default
# ./benchmarks/bench-rados:265 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool create bench_rados 128 128 replicated --size 1 --pg-num-min 128
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:09:32,664463848-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:09:32,667657507-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '93237fa8-3800-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'create', 'bench_rados', '128', '128', 'replicated', '--size', '1', '--pg-num-min', '128'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 93237fa8-3800-11ec-b51d-53e6e728d2d3 -- ceph osd pool create bench_rados 128 128 replicated --size 1 --pg-num-min 128
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
pool 'bench_rados' created
# ./benchmarks/bench-rados:267 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados min_size 1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:09:37,364076975-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:09:37,366888734-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '93237fa8-3800-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'min_size', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 93237fa8-3800-11ec-b51d-53e6e728d2d3 -- ceph osd pool set bench_rados min_size 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 min_size to 1
# ./benchmarks/bench-rados:268 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados pg_autoscale_mode off
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:09:41,773454596-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:09:41,776564376-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '93237fa8-3800-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'pg_autoscale_mode', 'off'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 93237fa8-3800-11ec-b51d-53e6e728d2d3 -- ceph osd pool set bench_rados pg_autoscale_mode off
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 pg_autoscale_mode to off
# ./benchmarks/bench-rados:269 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool application enable bench_rados benchmark
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:09:46,386803873-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:09:46,390087551-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '93237fa8-3800-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'application', 'enable', 'bench_rados', 'benchmark'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 93237fa8-3800-11ec-b51d-53e6e728d2d3 -- ceph osd pool application enable bench_rados benchmark
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
enabled application 'benchmark' on pool 'bench_rados'
# ./benchmarks/bench-rados:270 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados noscrub 1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:09:51,771902459-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:09:51,775078935-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '93237fa8-3800-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'noscrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 93237fa8-3800-11ec-b51d-53e6e728d2d3 -- ceph osd pool set bench_rados noscrub 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 noscrub to 1
# ./benchmarks/bench-rados:271 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados nodeep-scrub 1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:09:56,223575144-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:09:56,226551813-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '93237fa8-3800-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'nodeep-scrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 93237fa8-3800-11ec-b51d-53e6e728d2d3 -- ceph osd pool set bench_rados nodeep-scrub 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 nodeep-scrub to 1
# ./benchmarks/bench-rados:272 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool ls detail
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:10:01,415062127-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:10:01,418076397-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '93237fa8-3800-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'ls', 'detail'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 93237fa8-3800-11ec-b51d-53e6e728d2d3 -- ceph osd pool ls detail
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
pool 1 'device_health_metrics' replicated size 1 min_size 1 crush_rule 0 object_hash rjenkins pg_num 64 pgp_num 64 autoscale_mode on last_change 19 lfor 0/0/16 flags hashpspool stripe_width 0 pg_num_min 1 application mgr_devicehealth
pool 2 'bench_rados' replicated size 1 min_size 1 crush_rule 0 object_hash rjenkins pg_num 128 pgp_num 128 autoscale_mode off last_change 21 flags hashpspool,noscrub,nodeep-scrub stripe_width 0 pg_num_min 128 application benchmark

# ./benchmarks/bench-rados:273 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd df tree
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:10:06,105332199-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:10:06,108457097-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '93237fa8-3800-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'df', 'tree'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 93237fa8-3800-11ec-b51d-53e6e728d2d3 -- ceph osd df tree
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
ID  CLASS  WEIGHT   REWEIGHT  SIZE     RAW USE  DATA     OMAP  META     AVAIL    %USE  VAR   PGS  STATUS  TYPE NAME    
-1         0.09769         -  100 GiB  5.7 MiB  176 KiB   0 B  5.6 MiB  100 GiB  0.01  1.00    -          root default 
-3         0.09769         -  100 GiB  5.7 MiB  176 KiB   0 B  5.6 MiB  100 GiB  0.01  1.00    -              host sm1 
 0    ssd  0.09769   1.00000  100 GiB  5.7 MiB  176 KiB   0 B  5.6 MiB  100 GiB  0.01  1.00  192      up          osd.0
                       TOTAL  100 GiB  5.7 MiB  176 KiB   0 B  5.6 MiB  100 GiB  0.01                                  
MIN/MAX VAR: 1.00/1.00  STDDEV: 0


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:10:10,307374336-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:10:34,694522971-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:10:43,819096308-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:10:53,109204089-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:10:53,116798120-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:11:02,339541887-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:11:02,346883422-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:11:11,730121134-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:11:11,737763266-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:11:21,069742534-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:11:21,076801887-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:11:30,534899270-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:11:30,542695041-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:11:30,548126737-07:00] INFO: > The cluster is idle now.[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:11:30,551807393-07:00] STAGE: Start benchmarking of mode write ...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:11:30,558610643-07:00] INFO: > Start system activity collection services[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:11:30,563805082-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:341 - rados_bench() > rm --force /mnt/sda8/ceph-research/scripts/output/sys_activity_64KB_write.dat.rados_bench_host.3
# ./benchmarks/bench-rados:342 - rados_bench() > sar_pid_rados_bench_host=431079
# ./benchmarks/bench-rados:342 - rados_bench() > S_TIME_FORMAT=ISO
# ./benchmarks/bench-rados:342 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/output/sys_activity_64KB_write.dat.rados_bench_host.3 2

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:11:30,570914259-07:00] INFO: >> for OSD host[0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:11:30,579656594-07:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.5 bash
10.10.2.5: b'130873\n'
[1] 08:11:31 [SUCCESS] ljishen@10.10.2.5
130873

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:11:31,692601311-07:00] INFO: > Run rados bench[0m
# ./benchmarks/bench-rados:370 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/output/rados_bench_64KB_write.log.3
## ./benchmarks/bench-rados:361 - rados_bench() > count_cpus 32-63,96-127
# ./benchmarks/bench-rados:361 - rados_bench() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 60 write --pool bench_rados -b 65536 -O 65536 --concurrent-ios 128 --show-time --write-object --write-omap --write-xattr --no-cleanup
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:11:31,712549052-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:11:31,715421414-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '93237fa8-3800-11ec-b51d-53e6e728d2d3', '--', 'rados', 'bench', '60', 'write', '--pool', 'bench_rados', '-b', '65536', '-O', '65536', '--concurrent-ios', '128', '--show-time', '--write-object', '--write-omap', '--write-xattr', '--no-cleanup'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 93237fa8-3800-11ec-b51d-53e6e728d2d3 -- rados bench 60 write --pool bench_rados -b 65536 -O 65536 --concurrent-ios 128 --show-time --write-object --write-omap --write-xattr --no-cleanup
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
hints = 1
2021-10-28T15:11:35.023266+0000 Maintaining 128 concurrent writes of 65536 bytes to objects of size 65536 for up to 60 seconds or 0 objects
2021-10-28T15:11:35.023281+0000 Object prefix: benchmark_data_node-2.bluefield2.ucsc-cmps10_7
2021-10-28T15:11:35.025924+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-28T15:11:35.025924+0000     0       0         0         0         0         0           -           0
2021-10-28T15:11:36.026057+0000     1     128      1030       902   56.3716    56.375    0.124935    0.136747
2021-10-28T15:11:37.026157+0000     2     128      2009      1881   58.7765   61.1875    0.125586    0.131078
2021-10-28T15:11:38.026244+0000     3     128      3013      2885   60.0992     62.75    0.128359    0.129547
2021-10-28T15:11:39.026334+0000     4     128      4037      3909    61.073        64     0.14623    0.129145
2021-10-28T15:11:40.026415+0000     5     128      5061      4933   61.6573        64    0.136954    0.128339
2021-10-28T15:11:41.026496+0000     6     128      6022      5894   61.3907   60.0625    0.133218    0.128577
2021-10-28T15:11:42.026581+0000     7     128      7046      6918   61.7627        64     0.13964    0.128504
2021-10-28T15:11:43.026671+0000     8     128      8070      7942   62.0416        64    0.128341    0.128219
2021-10-28T15:11:44.026762+0000     9     128      8601      8473   58.8353   33.1875    0.254849      0.1336
2021-10-28T15:11:45.026849+0000    10     128      9094      8966   56.0327   30.8125     0.27196    0.140549
2021-10-28T15:11:46.026935+0000    11     128      9606      9478   53.8477        32    0.272098    0.147152
2021-10-28T15:11:47.027027+0000    12     128     10053      9925   51.6883   27.9375    0.292574    0.152324
2021-10-28T15:11:48.027111+0000    13     128     10565     10437   50.1736        32    0.270488    0.157655
2021-10-28T15:11:49.027201+0000    14     128     11077     10949   48.8753        32    0.276724    0.162358
2021-10-28T15:11:50.027281+0000    15     127     11541     11414   47.5543   29.0625    0.257364    0.166449
2021-10-28T15:11:51.027367+0000    16     128     12038     11910   46.5195        31    0.252039    0.170403
2021-10-28T15:11:52.027450+0000    17     128     12806     12678   46.6063        48    0.115015    0.171082
2021-10-28T15:11:53.027537+0000    18     128     13765     13637   47.3466   59.9375    0.119957    0.167947
2021-10-28T15:11:54.027619+0000    19     128     14789     14661   48.2229        64    0.131426    0.165416
2021-10-28T15:11:55.027710+0000 min lat: 0.0735051 max lat: 0.32633 avg lat: 0.162607
2021-10-28T15:11:55.027710+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-28T15:11:55.027710+0000    20     128     15813     15685   49.0114        64     0.11976    0.162607
2021-10-28T15:11:56.027816+0000    21     128     16518     16390   48.7755   44.0625    0.253879    0.162866
2021-10-28T15:11:57.027899+0000    22     128     16965     16837   47.8283   27.9375    0.311066    0.165644
2021-10-28T15:11:58.027979+0000    23     128     17221     17093   46.4444        16    0.503985    0.170376
2021-10-28T15:11:59.028057+0000    24     128     17477     17349   45.1758        16    0.471056    0.174999
2021-10-28T15:12:00.028138+0000    25     128     17733     17605   44.0087        16    0.432826    0.179592
2021-10-28T15:12:01.028235+0000    26     128     17989     17861   42.9314        16    0.509276    0.184189
2021-10-28T15:12:02.028310+0000    27     128     18245     18117   41.9339        16    0.512324    0.188794
2021-10-28T15:12:03.028399+0000    28     128     18501     18373   41.0076        16    0.510313    0.193303
2021-10-28T15:12:04.028482+0000    29     128     18757     18629   40.1453        16    0.504372     0.19771
2021-10-28T15:12:05.028566+0000    30     128     19013     18885   39.3404        16    0.565017    0.202079
2021-10-28T15:12:06.028655+0000    31     128     19269     19141   38.5874        16    0.477852     0.20613
2021-10-28T15:12:07.028746+0000    32     128     19846     19718   38.5084   36.0625    0.345084    0.207203
2021-10-28T15:12:08.028826+0000    33     128     20440     20312   38.4664    37.125     0.21629    0.207152
2021-10-28T15:12:09.028906+0000    34     128     21061     20933   38.4765   38.8125    0.234463    0.207138
2021-10-28T15:12:10.028985+0000    35     128     21701     21573   38.5199        40    0.229362    0.207267
2021-10-28T15:12:11.029066+0000    36     128     22278     22150   38.4516   36.0625     0.21302     0.20756
2021-10-28T15:12:12.029141+0000    37     128     22853     22725   38.3836   35.9375    0.179606    0.207634
2021-10-28T15:12:13.029227+0000    38     128     23493     23365    38.426        40    0.209537    0.207598
2021-10-28T15:12:14.029306+0000    39     128     24070     23942   38.3653   36.0625    0.212007    0.207722
2021-10-28T15:12:15.029390+0000 min lat: 0.0735051 max lat: 0.565347 avg lat: 0.209232
2021-10-28T15:12:15.029390+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-28T15:12:15.029390+0000    40     128     24512     24384   38.0968    27.625    0.450507    0.209232
2021-10-28T15:12:16.029479+0000    41     128     24773     24645   37.5654   16.3125    0.449453    0.212157
2021-10-28T15:12:17.029561+0000    42     128     25029     24901   37.0519        16     0.47976    0.214893
2021-10-28T15:12:18.029642+0000    43     128     25285     25157   36.5623        16     0.51305    0.217811
2021-10-28T15:12:19.029723+0000    44     128     25541     25413    36.095        16     0.46323    0.220441
2021-10-28T15:12:20.029810+0000    45     128     25862     25734   35.7386   20.0625    0.395074    0.223373
2021-10-28T15:12:21.029892+0000    46     128     26309     26181    35.569   27.9375    0.258773    0.224051
2021-10-28T15:12:22.029980+0000    47     128     26821     26693    35.493        32     0.26192    0.224673
2021-10-28T15:12:23.030057+0000    48     128     27333     27205   35.4202        32    0.273738    0.225399
2021-10-28T15:12:24.030136+0000    49     128     27717     27589   35.1871        24    0.273268    0.225976
2021-10-28T15:12:25.030220+0000    50     128     28741     28613   35.7632        64    0.144674    0.223499
2021-10-28T15:12:26.030304+0000    51     128     29765     29637   36.3168        64    0.122992    0.220056
2021-10-28T15:12:27.030387+0000    52     128     30854     30726   36.9272   68.0625    0.123032    0.216494
2021-10-28T15:12:28.030472+0000    53     128     31853     31725   37.4084   62.4375    0.110778    0.213588
2021-10-28T15:12:29.030553+0000    54     128     32581     32453   37.5582      45.5    0.279007    0.212306
2021-10-28T15:12:30.030632+0000    55     128     33093     32965   37.4571        32    0.316056    0.213177
2021-10-28T15:12:31.030710+0000    56     128     33349     33221   37.0739        16    0.505606    0.215031
2021-10-28T15:12:32.030787+0000    57     128     33605     33477   36.7042        16    0.506776    0.217306
2021-10-28T15:12:33.030875+0000    58     128     33861     33733   36.3472        16    0.472826    0.219398
2021-10-28T15:12:34.030950+0000    59     128     34080     33952   35.9631   13.6875     0.53989    0.221289
2021-10-28T15:12:35.031028+0000 min lat: 0.0735051 max lat: 0.569302 avg lat: 0.223428
2021-10-28T15:12:35.031028+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-28T15:12:35.031028+0000    60     128     34310     34182   35.6033    14.375    0.569195    0.223428
2021-10-28T15:12:36.031150+0000 Total time run:         60.3422
Total writes made:      34310
Write size:             65536
Object size:            65536
Bandwidth (MB/sec):     35.5369
Stddev Bandwidth:       18.1647
Max bandwidth (MB/sec): 68.0625
Min bandwidth (MB/sec): 13.6875
Average IOPS:           568
Stddev IOPS:            290.635
Max IOPS:               1089
Min IOPS:               219
Average Latency(s):     0.224621
Stddev Latency(s):      0.127896
Max latency(s):         0.569302
Min latency(s):         0.0735051

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:12:36,780816003-07:00] INFO: > Stop system activity collection services[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:12:36,786620461-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:391 - rados_bench() > kill -INT 431079

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:12:36,792185278-07:00] INFO: >> for OSD host[0m
# ./benchmarks/bench-rados:396 - rados_bench() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.5 kill -INT 130873
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:12:36,799796110-07:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.5 kill -INT 130873
[1] 08:12:38 [SUCCESS] ljishen@10.10.2.5

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:12:38,493714078-07:00] INFO: >>> Copy system activity file from OSD host[0m
## ./benchmarks/bench-rados:403 - rados_bench() > basename -- /tmp/bench-rados/sys_activity_64KB_write.dat.osd_host.3
# ./benchmarks/bench-rados:403 - rados_bench() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.5:/tmp/bench-rados/sys_activity_64KB_write.dat.osd_host.3 /mnt/sda8/ceph-research/scripts/output/sys_activity_64KB_write.dat.osd_host.3


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:12:39,816463654-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:13:04,117841228-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 34.31k objects, 2.1 GiB
    usage:   6.3 GiB used, 94 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:13:04,124853683-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:13:13,524903406-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 34.31k objects, 2.1 GiB
    usage:   6.3 GiB used, 94 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:13:13,532020899-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:13:22,947811967-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 34.31k objects, 2.1 GiB
    usage:   6.3 GiB used, 94 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:13:22,955494905-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:13:32,319815029-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 34.31k objects, 2.1 GiB
    usage:   6.3 GiB used, 94 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:13:32,327449386-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:13:41,662281501-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 34.31k objects, 2.1 GiB
    usage:   6.3 GiB used, 94 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:13:41,669923412-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:13:41,675440378-07:00] INFO: > The cluster is idle now.[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:13:41,678863860-07:00] STAGE: Start benchmarking of mode seq ...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:13:41,685260805-07:00] INFO: > Start system activity collection services[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:13:41,690769847-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:341 - rados_bench() > rm --force /mnt/sda8/ceph-research/scripts/output/sys_activity_64KB_seq.dat.rados_bench_host.3
# ./benchmarks/bench-rados:342 - rados_bench() > sar_pid_rados_bench_host=432428
# ./benchmarks/bench-rados:342 - rados_bench() > S_TIME_FORMAT=ISO
# ./benchmarks/bench-rados:342 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/output/sys_activity_64KB_seq.dat.rados_bench_host.3 2

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:13:41,697603134-07:00] INFO: >> for OSD host[0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:13:41,706376817-07:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.5 bash
10.10.2.5: b'131424\n'
[1] 08:13:43 [SUCCESS] ljishen@10.10.2.5
131424

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:13:43,020935679-07:00] INFO: > Run rados bench[0m
# ./benchmarks/bench-rados:379 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/output/rados_bench_64KB_seq.log.3
## ./benchmarks/bench-rados:376 - rados_bench() > count_cpus 32-63,96-127
# ./benchmarks/bench-rados:376 - rados_bench() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 128 --show-time
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:13:43,041168677-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:13:43,044227942-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '93237fa8-3800-11ec-b51d-53e6e728d2d3', '--', 'rados', 'bench', '99999999', 'seq', '--pool', 'bench_rados', '--concurrent-ios', '128', '--show-time'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 93237fa8-3800-11ec-b51d-53e6e728d2d3 -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 128 --show-time
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
hints = 1
2021-10-28T15:13:46.264684+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-28T15:13:46.264684+0000     0       0         0         0         0         0           -           0
2021-10-28T15:13:47.264837+0000     1     128      8082      7954   497.014   497.125   0.0189186   0.0159553
2021-10-28T15:13:48.264959+0000     2     128     16048     15920   497.414   497.875   0.0114682   0.0160044
2021-10-28T15:13:49.265042+0000     3     128     24221     24093   501.866   510.812   0.0149182   0.0158856
2021-10-28T15:13:50.265164+0000     4     128     32545     32417   506.446    520.25   0.0165027   0.0157526
2021-10-28T15:13:51.265309+0000 Total time run:       4.23503
Total reads made:     34310
Read size:            65536
Object size:          65536
Bandwidth (MB/sec):   506.343
Average IOPS:         8101
Stddev IOPS:          177.675
Max IOPS:             8324
Min IOPS:             7954
Average Latency(s):   0.0157654
Max latency(s):       0.0480752
Min latency(s):       0.00358274

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:13:51,991096337-07:00] INFO: > Stop system activity collection services[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:13:51,996855841-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:391 - rados_bench() > kill -INT 432428

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:13:52,002650641-07:00] INFO: >> for OSD host[0m
# ./benchmarks/bench-rados:396 - rados_bench() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.5 kill -INT 131424
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:13:52,010511885-07:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.5 kill -INT 131424
[1] 08:13:53 [SUCCESS] ljishen@10.10.2.5

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:13:53,124009526-07:00] INFO: >>> Copy system activity file from OSD host[0m
## ./benchmarks/bench-rados:403 - rados_bench() > basename -- /tmp/bench-rados/sys_activity_64KB_seq.dat.osd_host.3
# ./benchmarks/bench-rados:403 - rados_bench() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.5:/tmp/bench-rados/sys_activity_64KB_seq.dat.osd_host.3 /mnt/sda8/ceph-research/scripts/output/sys_activity_64KB_seq.dat.osd_host.3


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:13:54,220685266-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:14:18,476645238-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 34.31k objects, 2.1 GiB
    usage:   6.3 GiB used, 94 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:14:18,484265808-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:14:27,782679208-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 34.31k objects, 2.1 GiB
    usage:   6.3 GiB used, 94 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:14:27,790335376-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:14:37,213207484-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 34.31k objects, 2.1 GiB
    usage:   6.3 GiB used, 94 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:14:37,220756409-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:14:46,927881451-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 34.31k objects, 2.1 GiB
    usage:   6.3 GiB used, 94 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:14:46,935189002-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:14:55,077899809-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 34.31k objects, 2.1 GiB
    usage:   6.3 GiB used, 94 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:14:55,085064551-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:14:55,091097730-07:00] INFO: > The cluster is idle now.[0m


[1;7;39;49m[2021-10-28T08:14:55,094807231-07:00][RUNNING][ROUND 1/4/21] object_size=256KB[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:14:55,098093283-07:00] STAGE: Launch a Ceph cluster (CEPH_HOST: 10.10.2.1, OSD_HOST: 10.10.2.5) ...[0m
# ./benchmarks/bench-rados:183 - launch_ceph_cluster() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.1 bash -euo pipefail
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:14:55,107440578-07:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.1 bash -euo pipefail
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T08:14:55,543001196-07:00] INFO: > Remove existing clusters\x1b[0m\n'
10.10.2.1: b'## bash:17 -  > basename -- /var/lib/ceph/93237fa8-3800-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'# bash:17 -  > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm --host 10.10.2.1 --host 10.10.2.5 rm-cluster --force --fsid 93237fa8-3800-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T08:14:55,553279738-07:00] DEBUG: HOST_PARAMS: ['--host', '10.10.2.1', '--host', '10.10.2.5']\x1b[0m\n"
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T08:14:55,556829421-07:00] DEBUG: CEPHADM_PARAMS: ['rm-cluster', '--force', '--fsid', '93237fa8-3800-11ec-b51d-53e6e728d2d3']\x1b[0m\n"
10.10.2.1: b"# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:63 -  > /tmp/bench-rados/ceph-research/scripts/parallel-ssh --host 10.10.2.1 --host 10.10.2.5 'sudo python3 -u - rm-cluster --force --fsid 93237fa8-3800-11ec-b51d-53e6e728d2d3'\n"
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T08:14:55,565399463-07:00] DEBUG: command from stdin\x1b[0m\n'
10.10.2.1: b"# /tmp/bench-rados/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host 10.10.2.1 --host 10.10.2.5 'sudo python3 -u - rm-cluster --force --fsid 93237fa8-3800-11ec-b51d-53e6e728d2d3'\n"
10.10.2.1: b'[1] 08:15:00 [SUCCESS] 10.10.2.1\n[2] 08:15:06 [SUCCESS] 10.10.2.5\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T08:15:06,815672732-07:00] INFO: > Deploy a new cluster\x1b[0m\n'
10.10.2.1: b'# bash:24 -  > /tmp/bench-rados/ceph-research/scripts/ceph-deploy -m 10.10.2.1 -o sm1/10.10.2.5:/dev/nvme0n1\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T08:15:06,827688911-07:00] STAGE: Verify passwordless SSH to hosts, passwordless sudo via ljishen, and the existence of devices on hosts...\x1b[0m\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T08:15:06,832421748-07:00] INFO: > Check the host for the monitor\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:181 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@10.10.2.1 sudo --non-interactive --validate\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T08:15:06,983923004-07:00] INFO: > Check hosts for OSDs\x1b[0m\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T08:15:06,988585038-07:00] INFO: >> Check host 'sm1'\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:200 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 sudo --non-interactive --validate\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T08:15:08,122792815-07:00] INFO: >>> Check device '/dev/nvme0n1'\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:215 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 test -b /dev/nvme0n1\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:226 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 lsblk --nodeps --noheadings --output TYPE /dev/nvme0n1\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:227 - verify_parameters() > grep --word-regexp disk\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T08:15:10,259887999-07:00] STAGE: Reset devices on hosts...\x1b[0m\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T08:15:10,264652145-07:00] INFO: > Resetting device '/dev/nvme0n1' on host 'sm1'\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:288 - reset_host_devices() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 sudo sh\n'
10.10.2.1: b'  Removing ceph--f2269d3d--d31d--4476--a7a6--2a78e04fe5b6-osd--block--11954751--a6fc--4411--92f6--5bbec2ee77b3 (252:0)\n'
10.10.2.1: b'  Archiving volume group "ceph-f2269d3d-d31d-4476-a7a6-2a78e04fe5b6" metadata (seqno 5).\n'
10.10.2.1: b'  Releasing logical volume "osd-block-11954751-a6fc-4411-92f6-5bbec2ee77b3"\n'
10.10.2.1: b'  Creating volume group backup "/etc/lvm/backup/ceph-f2269d3d-d31d-4476-a7a6-2a78e04fe5b6" (seqno 6).\n'
10.10.2.1: b'  Logical volume "osd-block-11954751-a6fc-4411-92f6-5bbec2ee77b3" successfully removed\n'
10.10.2.1: b'  Removing physical volume "/dev/nvme0n1" from volume group "ceph-f2269d3d-d31d-4476-a7a6-2a78e04fe5b6"\n'
10.10.2.1: b'  Volume group "ceph-f2269d3d-d31d-4476-a7a6-2a78e04fe5b6" successfully removed\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T08:15:12,606216199-07:00] STAGE: Bootstrap a new cluster...\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:317 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.1 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T08:15:12,616601071-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T08:15:12,620347344-07:00] DEBUG: CEPHADM_PARAMS: ['bootstrap', '--config', '/tmp/bench-rados/ceph-research/scripts/ceph.conf', '--mon-ip', '10.10.2.1', '--output-dir', '/tmp/bench-rados/deployment_data_root/etc/ceph', '--ssh-user', 'ljishen', '--skip-dashboard', '--no-minimize-config', '--allow-overwrite', '--allow-fqdn-hostname', '--orphan-initial-daemons', '--skip-monitoring-stack', '--single-host-defaults']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.1 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults\n'
10.10.2.1: b'Verifying podman|docker is present...\nVerifying lvm2 is present...\n'
10.10.2.1: b'Verifying time synchronization is in place...\n'
10.10.2.1: b'Unit ntp.service is enabled and running\n'
10.10.2.1: b'Repeating the final host check...\n'
10.10.2.1: b'podman|docker (/usr/bin/docker) is present\n'
10.10.2.1: b'systemctl is present\n'
10.10.2.1: b'lvcreate is present\n'
10.10.2.1: b'Unit ntp.service is enabled and running\n'
10.10.2.1: b'Host looks OK\n'
10.10.2.1: b'Cluster fsid: d8ff4ba0-3801-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Verifying IP 10.10.2.1 port 3300 ...\n'
10.10.2.1: b'Verifying IP 10.10.2.1 port 6789 ...\n'
10.10.2.1: b'Mon IP `10.10.2.1` is in CIDR network `10.10.2.0/24`\n'
10.10.2.1: b'- internal network (--cluster-network) has not been provided, OSD replication will default to the public_network\n'
10.10.2.1: b'Adjusting default settings to suit single-host cluster...\n'
10.10.2.1: b'Pulling container image quay.io/ceph/ceph:v16...\n'
10.10.2.1: b'Ceph version: ceph version 16.2.6 (ee28fb57e47e9f88813e24bbf4c14496ca299d31) pacific (stable)\n'
10.10.2.1: b'Extracting ceph user uid/gid from container image...\n'
10.10.2.1: b'Creating initial keys...\n'
10.10.2.1: b'Creating initial monmap...\n'
10.10.2.1: b'Creating mon...\n'
10.10.2.1: b'Waiting for mon to start...\nWaiting for mon...\n'
10.10.2.1: b'mon is available\n'
10.10.2.1: b'Setting mon public_network to 10.10.2.0/24\n'
10.10.2.1: b'Wrote config to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf\n'
10.10.2.1: b'Wrote keyring to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring\n'
10.10.2.1: b'Creating mgr...\n'
10.10.2.1: b'Verifying port 9283 ...\n'
10.10.2.1: b'Waiting for mgr to start...\nWaiting for mgr...\n'
10.10.2.1: b'mgr not available, waiting (1/15)...\n'
10.10.2.1: b'mgr not available, waiting (2/15)...\n'
10.10.2.1: b'mgr is available\n'
10.10.2.1: b'Enabling cephadm module...\n'
10.10.2.1: b'Waiting for the mgr to restart...\nWaiting for mgr epoch 5...\n'
10.10.2.1: b'mgr epoch 5 is available\nSetting orchestrator backend to cephadm...\n'
10.10.2.1: b'Generating ssh key...\n'
10.10.2.1: b'Wrote public SSH key to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub\nAdding key to ljishen@localhost authorized_keys...\n'
10.10.2.1: b'Adding host node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us...\n'
10.10.2.1: b'Deploying unmanaged mon service...\n'
10.10.2.1: b'Deploying unmanaged mgr service...\n'
10.10.2.1: b'You can access the Ceph CLI with:\n\n\tsudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --fsid d8ff4ba0-3801-11ec-b51d-53e6e728d2d3 -c /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf -k /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring\n\nPlease consider enabling telemetry to help improve Ceph:\n\n\tceph telemetry on\n\nFor more information see:\n\n\thttps://docs.ceph.com/docs/pacific/mgr/telemetry/\n\n'
10.10.2.1: b'Bootstrap complete.\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T08:16:13,509304812-07:00] INFO: Sleep for 20 seconds\x1b[0m\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T08:16:33,516743102-07:00] INFO: > Set the default number of monitor daemons to 1\x1b[0m\n# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:337 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T08:16:33,527668971-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T08:16:33,531637312-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mon', '1']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1\n'
10.10.2.1: b'Inferring fsid d8ff4ba0-3801-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/d8ff4ba0-3801-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b'Scheduled mon update...\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T08:16:42,741940628-07:00] INFO: > Set the default number of manager daemons to 1\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:342 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T08:16:42,751791065-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T08:16:42,755690195-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mgr', '1']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1\n'
10.10.2.1: b'Inferring fsid d8ff4ba0-3801-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/d8ff4ba0-3801-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b'Scheduled mgr update...\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T08:16:52,573706065-07:00] STAGE: Add hosts to the cluster...\x1b[0m\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T08:16:52,580172321-07:00] INFO: > Adding host 'sm1'\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:361 - add_hosts() > ssh-copy-id -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -f -i /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub ljishen@sm1\n'
10.10.2.1: b'/usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: "/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub"\n'
10.10.2.1: b'\nNumber of key(s) added: 1\n\nNow try logging into the machine, with:   "ssh -o \'GlobalKnownHostsFile=/dev/null\' -o \'LogLevel=ERROR\' -o \'PasswordAuthentication=no\' -o \'StrictHostKeyChecking=no\' -o \'UserKnownHostsFile=/dev/null\' \'ljishen@sm1\'"\nand check to make sure that only the key(s) you wanted were added.\n\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:367 - add_hosts() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add sm1 10.10.2.5\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T08:16:53,744629243-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T08:16:53,748766140-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'host', 'add', 'sm1', '10.10.2.5']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add sm1 10.10.2.5\n'
10.10.2.1: b'Inferring fsid d8ff4ba0-3801-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/d8ff4ba0-3801-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b"Added host 'sm1' with addr '10.10.2.5'\n"
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T08:17:04,787091019-07:00] INFO: Sleep for 20 seconds\x1b[0m\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T08:17:24,792054424-07:00] STAGE: Add OSDs to the cluster...\x1b[0m\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T08:17:24,799009419-07:00] INFO: > Adding devices ['/dev/nvme0n1'] on host 'sm1' as OSDs\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:388 - add_host_devices_as_osds() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd sm1:/dev/nvme0n1\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T08:17:24,808872981-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T08:17:24,812643590-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'daemon', 'add', 'osd', 'sm1:/dev/nvme0n1']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd sm1:/dev/nvme0n1\n'
10.10.2.1: b'Inferring fsid d8ff4ba0-3801-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/d8ff4ba0-3801-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b"Created osd(s) 0 on host 'sm1'\n"
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T08:17:48,830769046-07:00] INFO: > Sleep for 20 seconds\x1b[0m\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T08:18:08,836386276-07:00] STAGE: Show Cluster Status\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:408 - show_status() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T08:18:08,846020367-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T08:18:08,849904769-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', '--status']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status\n'
10.10.2.1: b'Inferring fsid d8ff4ba0-3801-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/d8ff4ba0-3801-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b'  cluster:\n    id:     d8ff4ba0-3801-11ec-b51d-53e6e728d2d3\n    health: HEALTH_WARN\n            1 pool(s) have no replicas configured\n \n  services:\n    mon: 1 daemons, quorum node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us (age 2m)\n    mgr: node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us.adrgzz(active, since 2m)\n    osd: 1 osds: 1 up (since 19s), 1 in (since 40s)\n \n  data:\n    pools:   1 pools, 1 pgs\n    objects: 0 objects, 0 B\n    usage:   4.7 MiB used, 100 GiB / 100 GiB avail\n    pgs:     1 active+clean\n \n'
[1] 08:18:17 [SUCCESS] ljishen@10.10.2.1

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:14:55,543001196-07:00] INFO: > Remove existing clusters[0m
## bash:17 -  > basename -- /var/lib/ceph/93237fa8-3800-11ec-b51d-53e6e728d2d3
# bash:17 -  > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm --host 10.10.2.1 --host 10.10.2.5 rm-cluster --force --fsid 93237fa8-3800-11ec-b51d-53e6e728d2d3
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:14:55,553279738-07:00] DEBUG: HOST_PARAMS: ['--host', '10.10.2.1', '--host', '10.10.2.5'][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:14:55,556829421-07:00] DEBUG: CEPHADM_PARAMS: ['rm-cluster', '--force', '--fsid', '93237fa8-3800-11ec-b51d-53e6e728d2d3'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:63 -  > /tmp/bench-rados/ceph-research/scripts/parallel-ssh --host 10.10.2.1 --host 10.10.2.5 'sudo python3 -u - rm-cluster --force --fsid 93237fa8-3800-11ec-b51d-53e6e728d2d3'
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:14:55,565399463-07:00] DEBUG: command from stdin[0m
# /tmp/bench-rados/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host 10.10.2.1 --host 10.10.2.5 'sudo python3 -u - rm-cluster --force --fsid 93237fa8-3800-11ec-b51d-53e6e728d2d3'
[1] 08:15:00 [SUCCESS] 10.10.2.1
[2] 08:15:06 [SUCCESS] 10.10.2.5

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:15:06,815672732-07:00] INFO: > Deploy a new cluster[0m
# bash:24 -  > /tmp/bench-rados/ceph-research/scripts/ceph-deploy -m 10.10.2.1 -o sm1/10.10.2.5:/dev/nvme0n1


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:15:06,827688911-07:00] STAGE: Verify passwordless SSH to hosts, passwordless sudo via ljishen, and the existence of devices on hosts...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:15:06,832421748-07:00] INFO: > Check the host for the monitor[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:181 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@10.10.2.1 sudo --non-interactive --validate

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:15:06,983923004-07:00] INFO: > Check hosts for OSDs[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:15:06,988585038-07:00] INFO: >> Check host 'sm1'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:200 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 sudo --non-interactive --validate

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:15:08,122792815-07:00] INFO: >>> Check device '/dev/nvme0n1'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:215 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 test -b /dev/nvme0n1
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:226 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 lsblk --nodeps --noheadings --output TYPE /dev/nvme0n1
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:227 - verify_parameters() > grep --word-regexp disk


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:15:10,259887999-07:00] STAGE: Reset devices on hosts...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:15:10,264652145-07:00] INFO: > Resetting device '/dev/nvme0n1' on host 'sm1'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:288 - reset_host_devices() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 sudo sh
  Removing ceph--f2269d3d--d31d--4476--a7a6--2a78e04fe5b6-osd--block--11954751--a6fc--4411--92f6--5bbec2ee77b3 (252:0)
  Archiving volume group "ceph-f2269d3d-d31d-4476-a7a6-2a78e04fe5b6" metadata (seqno 5).
  Releasing logical volume "osd-block-11954751-a6fc-4411-92f6-5bbec2ee77b3"
  Creating volume group backup "/etc/lvm/backup/ceph-f2269d3d-d31d-4476-a7a6-2a78e04fe5b6" (seqno 6).
  Logical volume "osd-block-11954751-a6fc-4411-92f6-5bbec2ee77b3" successfully removed
  Removing physical volume "/dev/nvme0n1" from volume group "ceph-f2269d3d-d31d-4476-a7a6-2a78e04fe5b6"
  Volume group "ceph-f2269d3d-d31d-4476-a7a6-2a78e04fe5b6" successfully removed


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:15:12,606216199-07:00] STAGE: Bootstrap a new cluster...[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:317 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.1 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:15:12,616601071-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:15:12,620347344-07:00] DEBUG: CEPHADM_PARAMS: ['bootstrap', '--config', '/tmp/bench-rados/ceph-research/scripts/ceph.conf', '--mon-ip', '10.10.2.1', '--output-dir', '/tmp/bench-rados/deployment_data_root/etc/ceph', '--ssh-user', 'ljishen', '--skip-dashboard', '--no-minimize-config', '--allow-overwrite', '--allow-fqdn-hostname', '--orphan-initial-daemons', '--skip-monitoring-stack', '--single-host-defaults'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.1 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults
Verifying podman|docker is present...
Verifying lvm2 is present...
Verifying time synchronization is in place...
Unit ntp.service is enabled and running
Repeating the final host check...
podman|docker (/usr/bin/docker) is present
systemctl is present
lvcreate is present
Unit ntp.service is enabled and running
Host looks OK
Cluster fsid: d8ff4ba0-3801-11ec-b51d-53e6e728d2d3
Verifying IP 10.10.2.1 port 3300 ...
Verifying IP 10.10.2.1 port 6789 ...
Mon IP `10.10.2.1` is in CIDR network `10.10.2.0/24`
- internal network (--cluster-network) has not been provided, OSD replication will default to the public_network
Adjusting default settings to suit single-host cluster...
Pulling container image quay.io/ceph/ceph:v16...
Ceph version: ceph version 16.2.6 (ee28fb57e47e9f88813e24bbf4c14496ca299d31) pacific (stable)
Extracting ceph user uid/gid from container image...
Creating initial keys...
Creating initial monmap...
Creating mon...
Waiting for mon to start...
Waiting for mon...
mon is available
Setting mon public_network to 10.10.2.0/24
Wrote config to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf
Wrote keyring to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring
Creating mgr...
Verifying port 9283 ...
Waiting for mgr to start...
Waiting for mgr...
mgr not available, waiting (1/15)...
mgr not available, waiting (2/15)...
mgr is available
Enabling cephadm module...
Waiting for the mgr to restart...
Waiting for mgr epoch 5...
mgr epoch 5 is available
Setting orchestrator backend to cephadm...
Generating ssh key...
Wrote public SSH key to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub
Adding key to ljishen@localhost authorized_keys...
Adding host node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us...
Deploying unmanaged mon service...
Deploying unmanaged mgr service...
You can access the Ceph CLI with:

	sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --fsid d8ff4ba0-3801-11ec-b51d-53e6e728d2d3 -c /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf -k /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring

Please consider enabling telemetry to help improve Ceph:

	ceph telemetry on

For more information see:

	https://docs.ceph.com/docs/pacific/mgr/telemetry/

Bootstrap complete.

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:16:13,509304812-07:00] INFO: Sleep for 20 seconds[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:16:33,516743102-07:00] INFO: > Set the default number of monitor daemons to 1[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:337 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:16:33,527668971-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:16:33,531637312-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mon', '1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1
Inferring fsid d8ff4ba0-3801-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/d8ff4ba0-3801-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Scheduled mon update...

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:16:42,741940628-07:00] INFO: > Set the default number of manager daemons to 1[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:342 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:16:42,751791065-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:16:42,755690195-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mgr', '1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1
Inferring fsid d8ff4ba0-3801-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/d8ff4ba0-3801-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Scheduled mgr update...


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:16:52,573706065-07:00] STAGE: Add hosts to the cluster...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:16:52,580172321-07:00] INFO: > Adding host 'sm1'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:361 - add_hosts() > ssh-copy-id -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -f -i /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub ljishen@sm1
/usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: "/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub"

Number of key(s) added: 1

Now try logging into the machine, with:   "ssh -o 'GlobalKnownHostsFile=/dev/null' -o 'LogLevel=ERROR' -o 'PasswordAuthentication=no' -o 'StrictHostKeyChecking=no' -o 'UserKnownHostsFile=/dev/null' 'ljishen@sm1'"
and check to make sure that only the key(s) you wanted were added.

# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:367 - add_hosts() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add sm1 10.10.2.5
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:16:53,744629243-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:16:53,748766140-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'host', 'add', 'sm1', '10.10.2.5'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add sm1 10.10.2.5
Inferring fsid d8ff4ba0-3801-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/d8ff4ba0-3801-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Added host 'sm1' with addr '10.10.2.5'

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:17:04,787091019-07:00] INFO: Sleep for 20 seconds[0m


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:17:24,792054424-07:00] STAGE: Add OSDs to the cluster...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:17:24,799009419-07:00] INFO: > Adding devices ['/dev/nvme0n1'] on host 'sm1' as OSDs[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:388 - add_host_devices_as_osds() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd sm1:/dev/nvme0n1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:17:24,808872981-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:17:24,812643590-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'daemon', 'add', 'osd', 'sm1:/dev/nvme0n1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd sm1:/dev/nvme0n1
Inferring fsid d8ff4ba0-3801-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/d8ff4ba0-3801-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Created osd(s) 0 on host 'sm1'

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:17:48,830769046-07:00] INFO: > Sleep for 20 seconds[0m


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:18:08,836386276-07:00] STAGE: Show Cluster Status[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:408 - show_status() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:18:08,846020367-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:18:08,849904769-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', '--status'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status
Inferring fsid d8ff4ba0-3801-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/d8ff4ba0-3801-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
  cluster:
    id:     d8ff4ba0-3801-11ec-b51d-53e6e728d2d3
    health: HEALTH_WARN
            1 pool(s) have no replicas configured
 
  services:
    mon: 1 daemons, quorum node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us (age 2m)
    mgr: node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us.adrgzz(active, since 2m)
    osd: 1 osds: 1 up (since 19s), 1 in (since 40s)
 
  data:
    pools:   1 pools, 1 pgs
    objects: 0 objects, 0 B
    usage:   4.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     1 active+clean
 

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:18:17,946445546-07:00] INFO: > Copy configuration files to local[0m
# ./benchmarks/bench-rados:220 - launch_ceph_cluster() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.1 sudo chown --changes --recursive ljishen /tmp/bench-rados/deployment_data_root/etc/ceph
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:18:17,953919891-07:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.1 sudo chown --changes --recursive ljishen /tmp/bench-rados/deployment_data_root/etc/ceph
[1] 08:18:18 [SUCCESS] ljishen@10.10.2.1
# ./benchmarks/bench-rados:223 - launch_ceph_cluster() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.1:/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf
# ./benchmarks/bench-rados:226 - launch_ceph_cluster() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.1:/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:18:18,425136165-07:00] STAGE: Update local ceph.conf as a client...[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:18:18,428484174-07:00] STAGE: Configure Ceph pool...[0m
# ./benchmarks/bench-rados:257 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_max_backfills 32
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:18:18,450150232-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:18:18,452943005-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'd8ff4ba0-3801-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'config', 'set', 'osd', 'osd_max_backfills', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid d8ff4ba0-3801-11ec-b51d-53e6e728d2d3 -- ceph config set osd osd_max_backfills 32
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:258 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_active 32
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:18:21,927501723-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:18:21,930442694-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'd8ff4ba0-3801-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_active', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid d8ff4ba0-3801-11ec-b51d-53e6e728d2d3 -- ceph config set osd osd_recovery_max_active 32
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:259 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_single_start 8
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:18:25,603940779-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:18:25,606830595-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'd8ff4ba0-3801-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_single_start', '8'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid d8ff4ba0-3801-11ec-b51d-53e6e728d2d3 -- ceph config set osd osd_recovery_max_single_start 8
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:260 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_op_priority 63
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:18:29,311136395-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:18:29,314374347-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'd8ff4ba0-3801-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_op_priority', '63'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid d8ff4ba0-3801-11ec-b51d-53e6e728d2d3 -- ceph config set osd osd_recovery_op_priority 63
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:262 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config show-with-defaults osd.0
# ./benchmarks/bench-rados:263 - configure_ceph_pool() > grep 'osd_max_backfills\|osd_recovery'
# ./benchmarks/bench-rados:263 - configure_ceph_pool() > column -t -s ' '
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
osd_max_backfills                        32         mon
osd_recovery_cost                        20971520   default
osd_recovery_delay_start                 0.000000   default
osd_recovery_max_active                  32         mon
osd_recovery_max_active_hdd              3          default
osd_recovery_max_active_ssd              10         default
osd_recovery_max_chunk                   8388608    default
osd_recovery_max_omap_entries_per_chunk  8096       default
osd_recovery_max_single_start            8          mon
osd_recovery_op_priority                 63         mon
osd_recovery_op_warn_multiple            16         default
osd_recovery_priority                    5          default
osd_recovery_retry_interval              30.000000  default
osd_recovery_sleep                       0.000000   default
osd_recovery_sleep_hdd                   0.100000   default
osd_recovery_sleep_hybrid                0.025000   default
osd_recovery_sleep_ssd                   0.000000   default
# ./benchmarks/bench-rados:265 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool create bench_rados 128 128 replicated --size 1 --pg-num-min 128
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:18:36,464439496-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:18:36,467539367-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'd8ff4ba0-3801-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'create', 'bench_rados', '128', '128', 'replicated', '--size', '1', '--pg-num-min', '128'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid d8ff4ba0-3801-11ec-b51d-53e6e728d2d3 -- ceph osd pool create bench_rados 128 128 replicated --size 1 --pg-num-min 128
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
pool 'bench_rados' created
# ./benchmarks/bench-rados:267 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados min_size 1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:18:40,820262945-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:18:40,823211762-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'd8ff4ba0-3801-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'min_size', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid d8ff4ba0-3801-11ec-b51d-53e6e728d2d3 -- ceph osd pool set bench_rados min_size 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 min_size to 1
# ./benchmarks/bench-rados:268 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados pg_autoscale_mode off
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:18:45,136596609-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:18:45,139908350-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'd8ff4ba0-3801-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'pg_autoscale_mode', 'off'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid d8ff4ba0-3801-11ec-b51d-53e6e728d2d3 -- ceph osd pool set bench_rados pg_autoscale_mode off
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 pg_autoscale_mode to off
# ./benchmarks/bench-rados:269 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool application enable bench_rados benchmark
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:18:49,751328163-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:18:49,754297849-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'd8ff4ba0-3801-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'application', 'enable', 'bench_rados', 'benchmark'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid d8ff4ba0-3801-11ec-b51d-53e6e728d2d3 -- ceph osd pool application enable bench_rados benchmark
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
enabled application 'benchmark' on pool 'bench_rados'
# ./benchmarks/bench-rados:270 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados noscrub 1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:18:53,876546174-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:18:53,879571175-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'd8ff4ba0-3801-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'noscrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid d8ff4ba0-3801-11ec-b51d-53e6e728d2d3 -- ceph osd pool set bench_rados noscrub 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 noscrub to 1
# ./benchmarks/bench-rados:271 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados nodeep-scrub 1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:18:58,376650421-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:18:58,379648220-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'd8ff4ba0-3801-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'nodeep-scrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid d8ff4ba0-3801-11ec-b51d-53e6e728d2d3 -- ceph osd pool set bench_rados nodeep-scrub 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 nodeep-scrub to 1
# ./benchmarks/bench-rados:272 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool ls detail
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:19:02,676402149-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:19:02,679329085-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'd8ff4ba0-3801-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'ls', 'detail'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid d8ff4ba0-3801-11ec-b51d-53e6e728d2d3 -- ceph osd pool ls detail
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
pool 1 'device_health_metrics' replicated size 1 min_size 1 crush_rule 0 object_hash rjenkins pg_num 64 pgp_num 64 autoscale_mode on last_change 19 lfor 0/0/16 flags hashpspool stripe_width 0 pg_num_min 1 application mgr_devicehealth
pool 2 'bench_rados' replicated size 1 min_size 1 crush_rule 0 object_hash rjenkins pg_num 128 pgp_num 128 autoscale_mode off last_change 20 flags hashpspool,noscrub,nodeep-scrub stripe_width 0 pg_num_min 128 application benchmark

# ./benchmarks/bench-rados:273 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd df tree
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:19:06,252035207-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:19:06,255014281-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'd8ff4ba0-3801-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'df', 'tree'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid d8ff4ba0-3801-11ec-b51d-53e6e728d2d3 -- ceph osd df tree
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
ID  CLASS  WEIGHT   REWEIGHT  SIZE     RAW USE  DATA     OMAP  META     AVAIL    %USE  VAR   PGS  STATUS  TYPE NAME    
-1         0.09769         -  100 GiB  5.7 MiB  168 KiB   0 B  5.6 MiB  100 GiB  0.01  1.00    -          root default 
-3         0.09769         -  100 GiB  5.7 MiB  168 KiB   0 B  5.6 MiB  100 GiB  0.01  1.00    -              host sm1 
 0    ssd  0.09769   1.00000  100 GiB  5.7 MiB  168 KiB   0 B  5.6 MiB  100 GiB  0.01  1.00  192      up          osd.0
                       TOTAL  100 GiB  5.7 MiB  168 KiB   0 B  5.6 MiB  100 GiB  0.01                                  
MIN/MAX VAR: 1.00/1.00  STDDEV: 0


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:19:09,687003883-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:19:33,184202277-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:19:41,817193382-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:19:50,429812449-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:19:59,080587350-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:19:59,088425650-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:20:07,711040165-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:20:07,718775812-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:20:16,353026967-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:20:16,360645363-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:20:25,026754463-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:20:25,034334918-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:20:33,723382545-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:20:33,730898498-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:20:33,736671126-07:00] INFO: > The cluster is idle now.[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:20:33,740262093-07:00] STAGE: Start benchmarking of mode write ...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:20:33,746929878-07:00] INFO: > Start system activity collection services[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:20:33,752264861-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:341 - rados_bench() > rm --force /mnt/sda8/ceph-research/scripts/output/sys_activity_256KB_write.dat.rados_bench_host.1
# ./benchmarks/bench-rados:342 - rados_bench() > sar_pid_rados_bench_host=439446
# ./benchmarks/bench-rados:342 - rados_bench() > S_TIME_FORMAT=ISO
# ./benchmarks/bench-rados:342 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/output/sys_activity_256KB_write.dat.rados_bench_host.1 2

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:20:33,759029619-07:00] INFO: >> for OSD host[0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:20:33,768048224-07:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.5 bash
10.10.2.5: b'136316\n'
[1] 08:20:34 [SUCCESS] ljishen@10.10.2.5
136316

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:20:34,895596941-07:00] INFO: > Run rados bench[0m
# ./benchmarks/bench-rados:370 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/output/rados_bench_256KB_write.log.1
## ./benchmarks/bench-rados:361 - rados_bench() > count_cpus 32-63,96-127
# ./benchmarks/bench-rados:361 - rados_bench() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 60 write --pool bench_rados -b 262144 -O 262144 --concurrent-ios 128 --show-time --write-object --write-omap --write-xattr --no-cleanup
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:20:34,915724230-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:20:34,918549965-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'd8ff4ba0-3801-11ec-b51d-53e6e728d2d3', '--', 'rados', 'bench', '60', 'write', '--pool', 'bench_rados', '-b', '262144', '-O', '262144', '--concurrent-ios', '128', '--show-time', '--write-object', '--write-omap', '--write-xattr', '--no-cleanup'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid d8ff4ba0-3801-11ec-b51d-53e6e728d2d3 -- rados bench 60 write --pool bench_rados -b 262144 -O 262144 --concurrent-ios 128 --show-time --write-object --write-omap --write-xattr --no-cleanup
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
hints = 1
2021-10-28T15:20:37.744561+0000 Maintaining 128 concurrent writes of 262144 bytes to objects of size 262144 for up to 60 seconds or 0 objects
2021-10-28T15:20:37.744578+0000 Object prefix: benchmark_data_node-2.bluefield2.ucsc-cmps10_7
2021-10-28T15:20:37.753233+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-28T15:20:37.753233+0000     0       0         0         0         0         0           -           0
2021-10-28T15:20:38.753373+0000     1     128       341       213   53.2446     53.25    0.591264    0.465691
2021-10-28T15:20:39.753493+0000     2     128       550       422   52.7442     52.25    0.513809    0.495585
2021-10-28T15:20:40.753603+0000     3     128       832       704   58.6602      70.5    0.471628    0.494638
2021-10-28T15:20:41.753677+0000     4     128      1105       977   61.0563     68.25    0.506628    0.494066
2021-10-28T15:20:42.753743+0000     5     128      1355      1227   61.3442      62.5    0.327875    0.501779
2021-10-28T15:20:43.753816+0000     6     128      1614      1486    61.911     64.75    0.305056    0.497599
2021-10-28T15:20:44.753929+0000     7     128      1860      1732   61.8513      61.5    0.361268    0.498025
2021-10-28T15:20:45.754024+0000     8     128      2142      2014   62.9316      70.5    0.497846    0.495526
2021-10-28T15:20:46.754140+0000     9     128      2269      2141   59.4665     31.75    0.783285    0.512811
2021-10-28T15:20:47.754210+0000    10     128      2364      2236   55.8948     23.75    0.778868     0.53158
2021-10-28T15:20:48.754321+0000    11     128      2540      2412   54.8129        44    0.785851    0.567918
2021-10-28T15:20:49.754402+0000    12     128      2631      2503   52.1409     22.75    0.709999    0.581965
2021-10-28T15:20:50.754518+0000    13     128      2798      2670   51.3412     41.75    0.556531    0.600077
2021-10-28T15:20:51.754635+0000    14     128      2923      2795   49.9059     31.25     1.06691    0.616685
2021-10-28T15:20:52.754700+0000    15     128      3012      2884   48.0621     22.25     0.77511    0.627384
2021-10-28T15:20:53.754796+0000    16     128      3217      3089    48.261     51.25    0.868995    0.653585
2021-10-28T15:20:54.754909+0000    17     128      3387      3259   47.9219      42.5    0.533144    0.655575
2021-10-28T15:20:55.754997+0000    18     128      3651      3523   48.9259        66     0.42016     0.64418
2021-10-28T15:20:56.755120+0000    19     128      3929      3801   50.0083      69.5    0.375812    0.632209
2021-10-28T15:20:57.755189+0000 min lat: 0.182911 max lat: 1.60252 avg lat: 0.623196
2021-10-28T15:20:57.755189+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-28T15:20:57.755189+0000    20     128      4197      4069   50.8576        67    0.364911    0.623196
2021-10-28T15:20:58.755306+0000    21     128      4331      4203   50.0309      33.5    0.985601    0.627853
2021-10-28T15:20:59.755416+0000    22     128      4416      4288   48.7225     21.25    0.915943     0.63629
2021-10-28T15:21:00.755514+0000    23     128      4486      4358   47.3649      17.5     1.16624    0.649218
2021-10-28T15:21:01.755591+0000    24     128      4614      4486   46.7247        32     1.37282    0.671625
2021-10-28T15:21:02.755658+0000    25     128      4632      4504   45.0357       4.5     1.02882    0.673031
2021-10-28T15:21:03.755776+0000    26     128      4725      4597   44.1977     23.25     1.35389    0.692646
2021-10-28T15:21:04.755884+0000    27     128      4849      4721   43.7087        31    0.407747      0.7133
2021-10-28T15:21:05.755977+0000    28     128      4941      4813   42.9691        23     1.27996    0.726184
2021-10-28T15:21:06.756088+0000    29     128      5065      4937   42.5562        31     1.38503    0.743612
2021-10-28T15:21:07.756154+0000    30     128      5096      4968    41.396      7.75    0.557133    0.747498
2021-10-28T15:21:08.756280+0000    31     128      5220      5092   41.0605        31     1.23899    0.762697
2021-10-28T15:21:09.756374+0000    32     128      5330      5202   40.6367      27.5     1.26682    0.776271
2021-10-28T15:21:10.756486+0000    33     128      5490      5362   40.6173        40    0.754318    0.780776
2021-10-28T15:21:11.756605+0000    34     128      5623      5495   40.4005     33.25     0.58237    0.780938
2021-10-28T15:21:12.756670+0000    35     128      5791      5663   40.4461        42    0.802214    0.780848
2021-10-28T15:21:13.756747+0000    36     128      5980      5852    40.635     47.25    0.614046    0.781763
2021-10-28T15:21:14.756865+0000    37     128      6124      5996   40.5096        36      0.6004    0.781663
2021-10-28T15:21:15.756977+0000    38     128      6263      6135   40.3579     34.75    0.701387    0.782693
2021-10-28T15:21:16.757091+0000    39     128      6332      6204   39.7653     17.25     1.07886    0.787528
2021-10-28T15:21:17.757155+0000 min lat: 0.182911 max lat: 2.48775 avg lat: 0.80064
2021-10-28T15:21:17.757155+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-28T15:21:17.757155+0000    40     128      6454      6326   39.5337      30.5     1.28314     0.80064
2021-10-28T15:21:18.757270+0000    41     128      6478      6350   38.7157         6     1.31941    0.802733
2021-10-28T15:21:19.757340+0000    42     128      6580      6452    38.401      25.5        1.05    0.814524
2021-10-28T15:21:20.757460+0000    43     128      6716      6588   38.2986        34     1.62453     0.82652
2021-10-28T15:21:21.757553+0000    44     128      6784      6656   37.8145        17     1.25221    0.830834
2021-10-28T15:21:22.757620+0000    45     128      6863      6735   37.4131     19.75     1.11713    0.838306
2021-10-28T15:21:23.757742+0000    46     128      6962      6834   37.1377     24.75     1.19831    0.850443
2021-10-28T15:21:24.757866+0000    47     128      7052      6924   36.8262      22.5      1.0837    0.857744
2021-10-28T15:21:25.757956+0000    48     128      7172      7044   36.6839        30    0.886275     0.86584
2021-10-28T15:21:26.758070+0000    49     128      7284      7156   36.5066        28     1.05048    0.867291
2021-10-28T15:21:27.758137+0000    50     128      7554      7426   37.1264      67.5    0.386156     0.85897
2021-10-28T15:21:28.758250+0000    51     128      7744      7616   37.3297      47.5     0.42817    0.851207
2021-10-28T15:21:29.758349+0000    52     128      7998      7870   37.8328      63.5    0.623295    0.839695
2021-10-28T15:21:30.758459+0000    53     128      8273      8145   38.4161     68.75    0.379281    0.827748
2021-10-28T15:21:31.758528+0000    54     128      8400      8272   38.2926     31.75    0.912648    0.827369
2021-10-28T15:21:32.758596+0000    55     128      8483      8355   37.9736     20.75    0.816304    0.828224
2021-10-28T15:21:33.758691+0000    56     128      8611      8483   37.8669        32     1.35679    0.836567
2021-10-28T15:21:34.758804+0000    57     128      8690      8562    37.549     19.75     1.50008    0.843057
2021-10-28T15:21:35.758914+0000    58     128      8731      8603   37.0783     10.25     1.47301    0.846103
2021-10-28T15:21:36.759025+0000    59     128      8827      8699   36.8566        24     1.18562    0.854534
2021-10-28T15:21:37.759092+0000 min lat: 0.182911 max lat: 2.66061 avg lat: 0.862615
2021-10-28T15:21:37.759092+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-28T15:21:37.759092+0000    60     128      8944      8816   36.7298     29.25       1.386    0.862615
2021-10-28T15:21:38.759217+0000    61      59      8944      8885   36.4104     17.25     1.11903    0.865929
2021-10-28T15:21:39.759358+0000 Total time run:         61.1718
Total writes made:      8944
Write size:             262144
Object size:            262144
Bandwidth (MB/sec):     36.5528
Stddev Bandwidth:       18.3568
Max bandwidth (MB/sec): 70.5
Min bandwidth (MB/sec): 4.5
Average IOPS:           146
Stddev IOPS:            73.427
Max IOPS:               282
Min IOPS:               18
Average Latency(s):     0.870319
Stddev Latency(s):      0.452864
Max latency(s):         2.66061
Min latency(s):         0.182911

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:21:40,335370868-07:00] INFO: > Stop system activity collection services[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:21:40,340923502-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:391 - rados_bench() > kill -INT 439446

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:21:40,346496644-07:00] INFO: >> for OSD host[0m
# ./benchmarks/bench-rados:396 - rados_bench() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.5 kill -INT 136316
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:21:40,354807335-07:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.5 kill -INT 136316
[1] 08:21:42 [SUCCESS] ljishen@10.10.2.5

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:21:42,177377083-07:00] INFO: >>> Copy system activity file from OSD host[0m
## ./benchmarks/bench-rados:403 - rados_bench() > basename -- /tmp/bench-rados/sys_activity_256KB_write.dat.osd_host.1
# ./benchmarks/bench-rados:403 - rados_bench() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.5:/tmp/bench-rados/sys_activity_256KB_write.dat.osd_host.1 /mnt/sda8/ceph-research/scripts/output/sys_activity_256KB_write.dat.osd_host.1


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:21:43,448295572-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:22:07,113910901-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 8.95k objects, 2.2 GiB
    usage:   6.6 GiB used, 93 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:22:07,121290377-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:22:15,812094577-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 8.95k objects, 2.2 GiB
    usage:   6.6 GiB used, 93 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:22:15,819753240-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:22:24,560448036-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 8.95k objects, 2.2 GiB
    usage:   6.6 GiB used, 93 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:22:24,567904558-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:22:33,243692473-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 8.95k objects, 2.2 GiB
    usage:   6.6 GiB used, 93 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:22:33,251408864-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:22:42,008676093-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 8.95k objects, 2.2 GiB
    usage:   6.6 GiB used, 93 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:22:42,016155527-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:22:42,022046829-07:00] INFO: > The cluster is idle now.[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:22:42,025812135-07:00] STAGE: Start benchmarking of mode seq ...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:22:42,032345887-07:00] INFO: > Start system activity collection services[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:22:42,037826304-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:341 - rados_bench() > rm --force /mnt/sda8/ceph-research/scripts/output/sys_activity_256KB_seq.dat.rados_bench_host.1
# ./benchmarks/bench-rados:342 - rados_bench() > sar_pid_rados_bench_host=440790
# ./benchmarks/bench-rados:342 - rados_bench() > S_TIME_FORMAT=ISO
# ./benchmarks/bench-rados:342 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/output/sys_activity_256KB_seq.dat.rados_bench_host.1 2

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:22:42,044791249-07:00] INFO: >> for OSD host[0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:22:42,054050658-07:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.5 bash
10.10.2.5: b'136868\n'
[1] 08:22:43 [SUCCESS] ljishen@10.10.2.5
136868

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:22:43,369889357-07:00] INFO: > Run rados bench[0m
# ./benchmarks/bench-rados:379 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/output/rados_bench_256KB_seq.log.1
## ./benchmarks/bench-rados:376 - rados_bench() > count_cpus 32-63,96-127
# ./benchmarks/bench-rados:376 - rados_bench() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 128 --show-time
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:22:43,389452683-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:22:43,392521126-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'd8ff4ba0-3801-11ec-b51d-53e6e728d2d3', '--', 'rados', 'bench', '99999999', 'seq', '--pool', 'bench_rados', '--concurrent-ios', '128', '--show-time'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid d8ff4ba0-3801-11ec-b51d-53e6e728d2d3 -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 128 --show-time
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
hints = 1
2021-10-28T15:22:46.189777+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-28T15:22:46.189777+0000     0       0         0         0         0         0           -           0
2021-10-28T15:22:47.189936+0000     1     128      3234      3106   776.314     776.5   0.0431028   0.0404002
2021-10-28T15:22:48.190055+0000     2     128      6280      6152   768.862     761.5   0.0266769   0.0411106
2021-10-28T15:22:49.190198+0000 Total time run:       2.89991
Total reads made:     8944
Read size:            262144
Object size:          262144
Bandwidth (MB/sec):   771.058
Average IOPS:         3084
Stddev IOPS:          42.4264
Max IOPS:             3106
Min IOPS:             3046
Average Latency(s):   0.0412191
Max latency(s):       0.0724381
Min latency(s):       0.0051864

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:22:49,913871133-07:00] INFO: > Stop system activity collection services[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:22:49,919633892-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:391 - rados_bench() > kill -INT 440790

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:22:49,925540363-07:00] INFO: >> for OSD host[0m
# ./benchmarks/bench-rados:396 - rados_bench() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.5 kill -INT 136868
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:22:49,933490224-07:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.5 kill -INT 136868
[1] 08:22:51 [SUCCESS] ljishen@10.10.2.5

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:22:51,052150363-07:00] INFO: >>> Copy system activity file from OSD host[0m
## ./benchmarks/bench-rados:403 - rados_bench() > basename -- /tmp/bench-rados/sys_activity_256KB_seq.dat.osd_host.1
# ./benchmarks/bench-rados:403 - rados_bench() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.5:/tmp/bench-rados/sys_activity_256KB_seq.dat.osd_host.1 /mnt/sda8/ceph-research/scripts/output/sys_activity_256KB_seq.dat.osd_host.1


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:22:52,160527105-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:23:15,734079241-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 8.95k objects, 2.2 GiB
    usage:   6.6 GiB used, 93 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:23:15,742243265-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:23:24,424646173-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 8.95k objects, 2.2 GiB
    usage:   6.6 GiB used, 93 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:23:24,432245794-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:23:33,198069312-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 8.95k objects, 2.2 GiB
    usage:   6.6 GiB used, 93 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:23:33,205856667-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:23:41,880209527-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 8.95k objects, 2.2 GiB
    usage:   6.6 GiB used, 93 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:23:41,888068446-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:23:50,580895431-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 8.95k objects, 2.2 GiB
    usage:   6.6 GiB used, 93 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:23:50,588729082-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:23:50,594664517-07:00] INFO: > The cluster is idle now.[0m


[1;7;39;49m[2021-10-28T08:23:50,596758674-07:00][RUNNING][ROUND 2/4/21] object_size=256KB[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:23:50,600155826-07:00] STAGE: Launch a Ceph cluster (CEPH_HOST: 10.10.2.1, OSD_HOST: 10.10.2.5) ...[0m
# ./benchmarks/bench-rados:183 - launch_ceph_cluster() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.1 bash -euo pipefail
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:23:50,609461922-07:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.1 bash -euo pipefail
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T08:23:51,061038052-07:00] INFO: > Remove existing clusters\x1b[0m\n'
10.10.2.1: b'## bash:17 -  > basename -- /var/lib/ceph/d8ff4ba0-3801-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'# bash:17 -  > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm --host 10.10.2.1 --host 10.10.2.5 rm-cluster --force --fsid d8ff4ba0-3801-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T08:23:51,072483348-07:00] DEBUG: HOST_PARAMS: ['--host', '10.10.2.1', '--host', '10.10.2.5']\x1b[0m\n"
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T08:23:51,075997695-07:00] DEBUG: CEPHADM_PARAMS: ['rm-cluster', '--force', '--fsid', 'd8ff4ba0-3801-11ec-b51d-53e6e728d2d3']\x1b[0m\n"
10.10.2.1: b"# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:63 -  > /tmp/bench-rados/ceph-research/scripts/parallel-ssh --host 10.10.2.1 --host 10.10.2.5 'sudo python3 -u - rm-cluster --force --fsid d8ff4ba0-3801-11ec-b51d-53e6e728d2d3'\n"
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T08:23:51,083730492-07:00] DEBUG: command from stdin\x1b[0m\n'
10.10.2.1: b"# /tmp/bench-rados/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host 10.10.2.1 --host 10.10.2.5 'sudo python3 -u - rm-cluster --force --fsid d8ff4ba0-3801-11ec-b51d-53e6e728d2d3'\n"
10.10.2.1: b'[1] 08:23:56 [SUCCESS] 10.10.2.1\n[2] 08:24:02 [SUCCESS] 10.10.2.5\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T08:24:02,849726417-07:00] INFO: > Deploy a new cluster\x1b[0m\n'
10.10.2.1: b'# bash:24 -  > /tmp/bench-rados/ceph-research/scripts/ceph-deploy -m 10.10.2.1 -o sm1/10.10.2.5:/dev/nvme0n1\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T08:24:02,861860838-07:00] STAGE: Verify passwordless SSH to hosts, passwordless sudo via ljishen, and the existence of devices on hosts...\x1b[0m\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T08:24:02,866467859-07:00] INFO: > Check the host for the monitor\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:181 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@10.10.2.1 sudo --non-interactive --validate\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T08:24:03,014760453-07:00] INFO: > Check hosts for OSDs\x1b[0m\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T08:24:03,019539918-07:00] INFO: >> Check host 'sm1'\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:200 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 sudo --non-interactive --validate\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T08:24:04,111000473-07:00] INFO: >>> Check device '/dev/nvme0n1'\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:215 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 test -b /dev/nvme0n1\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:226 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 lsblk --nodeps --noheadings --output TYPE /dev/nvme0n1\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:227 - verify_parameters() > grep --word-regexp disk\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T08:24:06,255431767-07:00] STAGE: Reset devices on hosts...\x1b[0m\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T08:24:06,260507439-07:00] INFO: > Resetting device '/dev/nvme0n1' on host 'sm1'\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:288 - reset_host_devices() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 sudo sh\n'
10.10.2.1: b'  Removing ceph--74b956ae--1b24--400b--9bda--3f04a77af8c2-osd--block--5772b653--2ae0--4229--bc87--497a1a016f5e (252:0)\n'
10.10.2.1: b'  Archiving volume group "ceph-74b956ae-1b24-400b-9bda-3f04a77af8c2" metadata (seqno 5).\n'
10.10.2.1: b'  Releasing logical volume "osd-block-5772b653-2ae0-4229-bc87-497a1a016f5e"\n'
10.10.2.1: b'  Creating volume group backup "/etc/lvm/backup/ceph-74b956ae-1b24-400b-9bda-3f04a77af8c2" (seqno 6).\n'
10.10.2.1: b'  Logical volume "osd-block-5772b653-2ae0-4229-bc87-497a1a016f5e" successfully removed\n'
10.10.2.1: b'  Removing physical volume "/dev/nvme0n1" from volume group "ceph-74b956ae-1b24-400b-9bda-3f04a77af8c2"\n'
10.10.2.1: b'  Volume group "ceph-74b956ae-1b24-400b-9bda-3f04a77af8c2" successfully removed\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T08:24:08,622190433-07:00] STAGE: Bootstrap a new cluster...\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:317 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.1 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T08:24:08,632513269-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T08:24:08,636624428-07:00] DEBUG: CEPHADM_PARAMS: ['bootstrap', '--config', '/tmp/bench-rados/ceph-research/scripts/ceph.conf', '--mon-ip', '10.10.2.1', '--output-dir', '/tmp/bench-rados/deployment_data_root/etc/ceph', '--ssh-user', 'ljishen', '--skip-dashboard', '--no-minimize-config', '--allow-overwrite', '--allow-fqdn-hostname', '--orphan-initial-daemons', '--skip-monitoring-stack', '--single-host-defaults']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.1 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults\n'
10.10.2.1: b'Verifying podman|docker is present...\nVerifying lvm2 is present...\n'
10.10.2.1: b'Verifying time synchronization is in place...\n'
10.10.2.1: b'Unit ntp.service is enabled and running\n'
10.10.2.1: b'Repeating the final host check...\n'
10.10.2.1: b'podman|docker (/usr/bin/docker) is present\n'
10.10.2.1: b'systemctl is present\n'
10.10.2.1: b'lvcreate is present\n'
10.10.2.1: b'Unit ntp.service is enabled and running\n'
10.10.2.1: b'Host looks OK\n'
10.10.2.1: b'Cluster fsid: 187cba0a-3803-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Verifying IP 10.10.2.1 port 3300 ...\n'
10.10.2.1: b'Verifying IP 10.10.2.1 port 6789 ...\n'
10.10.2.1: b'Mon IP `10.10.2.1` is in CIDR network `10.10.2.0/24`\n'
10.10.2.1: b'- internal network (--cluster-network) has not been provided, OSD replication will default to the public_network\n'
10.10.2.1: b'Adjusting default settings to suit single-host cluster...\n'
10.10.2.1: b'Pulling container image quay.io/ceph/ceph:v16...\n'
10.10.2.1: b'Ceph version: ceph version 16.2.6 (ee28fb57e47e9f88813e24bbf4c14496ca299d31) pacific (stable)\n'
10.10.2.1: b'Extracting ceph user uid/gid from container image...\n'
10.10.2.1: b'Creating initial keys...\n'
10.10.2.1: b'Creating initial monmap...\n'
10.10.2.1: b'Creating mon...\n'
10.10.2.1: b'Waiting for mon to start...\n'
10.10.2.1: b'Waiting for mon...\n'
10.10.2.1: b'mon is available\n'
10.10.2.1: b'Setting mon public_network to 10.10.2.0/24\n'
10.10.2.1: b'Wrote config to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf\n'
10.10.2.1: b'Wrote keyring to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring\n'
10.10.2.1: b'Creating mgr...\nVerifying port 9283 ...\n'
10.10.2.1: b'Waiting for mgr to start...\n'
10.10.2.1: b'Waiting for mgr...\n'
10.10.2.1: b'mgr not available, waiting (1/15)...\n'
10.10.2.1: b'mgr not available, waiting (2/15)...\n'
10.10.2.1: b'mgr is available\n'
10.10.2.1: b'Enabling cephadm module...\n'
10.10.2.1: b'Waiting for the mgr to restart...\n'
10.10.2.1: b'Waiting for mgr epoch 5...\n'
10.10.2.1: b'mgr epoch 5 is available\n'
10.10.2.1: b'Setting orchestrator backend to cephadm...\n'
10.10.2.1: b'Generating ssh key...\n'
10.10.2.1: b'Wrote public SSH key to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub\n'
10.10.2.1: b'Adding key to ljishen@localhost authorized_keys...\n'
10.10.2.1: b'Adding host node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us...\n'
10.10.2.1: b'Deploying unmanaged mon service...\n'
10.10.2.1: b'Deploying unmanaged mgr service...\n'
10.10.2.1: b'You can access the Ceph CLI with:\n\n\tsudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --fsid 187cba0a-3803-11ec-b51d-53e6e728d2d3 -c /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf -k /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring\n\nPlease consider enabling telemetry to help improve Ceph:\n\n\tceph telemetry on\n\nFor more information see:\n\n\thttps://docs.ceph.com/docs/pacific/mgr/telemetry/\n\n'
10.10.2.1: b'Bootstrap complete.\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T08:25:09,730501099-07:00] INFO: Sleep for 20 seconds\x1b[0m\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T08:25:29,737490084-07:00] INFO: > Set the default number of monitor daemons to 1\x1b[0m\n# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:337 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T08:25:29,747822267-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T08:25:29,751331454-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mon', '1']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1\n'
10.10.2.1: b'Inferring fsid 187cba0a-3803-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/187cba0a-3803-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b'Scheduled mon update...\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T08:25:38,690684739-07:00] INFO: > Set the default number of manager daemons to 1\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:342 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T08:25:38,701535357-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T08:25:38,704948664-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mgr', '1']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1\n'
10.10.2.1: b'Inferring fsid 187cba0a-3803-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/187cba0a-3803-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b'Scheduled mgr update...\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T08:25:47,485727887-07:00] STAGE: Add hosts to the cluster...\x1b[0m\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T08:25:47,492110155-07:00] INFO: > Adding host 'sm1'\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:361 - add_hosts() > ssh-copy-id -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -f -i /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub ljishen@sm1\n'
10.10.2.1: b'/usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: "/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub"\n'
10.10.2.1: b'\nNumber of key(s) added: 1\n\nNow try logging into the machine, with:   "ssh -o \'GlobalKnownHostsFile=/dev/null\' -o \'LogLevel=ERROR\' -o \'PasswordAuthentication=no\' -o \'StrictHostKeyChecking=no\' -o \'UserKnownHostsFile=/dev/null\' \'ljishen@sm1\'"\nand check to make sure that only the key(s) you wanted were added.\n\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:367 - add_hosts() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add sm1 10.10.2.5\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T08:25:48,648740974-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T08:25:48,652121069-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'host', 'add', 'sm1', '10.10.2.5']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add sm1 10.10.2.5\n'
10.10.2.1: b'Inferring fsid 187cba0a-3803-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/187cba0a-3803-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b"Added host 'sm1' with addr '10.10.2.5'\n"
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T08:25:59,220010514-07:00] INFO: Sleep for 20 seconds\x1b[0m\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T08:26:19,224895060-07:00] STAGE: Add OSDs to the cluster...\x1b[0m\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T08:26:19,231660799-07:00] INFO: > Adding devices ['/dev/nvme0n1'] on host 'sm1' as OSDs\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:388 - add_host_devices_as_osds() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd sm1:/dev/nvme0n1\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T08:26:19,241143095-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T08:26:19,245101175-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'daemon', 'add', 'osd', 'sm1:/dev/nvme0n1']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd sm1:/dev/nvme0n1\n'
10.10.2.1: b'Inferring fsid 187cba0a-3803-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/187cba0a-3803-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b"Created osd(s) 0 on host 'sm1'\n"
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T08:26:44,003841454-07:00] INFO: > Sleep for 20 seconds\x1b[0m\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T08:27:04,009489801-07:00] STAGE: Show Cluster Status\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:408 - show_status() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T08:27:04,020035747-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T08:27:04,023246523-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', '--status']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status\n'
10.10.2.1: b'Inferring fsid 187cba0a-3803-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/187cba0a-3803-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b'  cluster:\n    id:     187cba0a-3803-11ec-b51d-53e6e728d2d3\n    health: HEALTH_WARN\n            1 pool(s) have no replicas configured\n \n  services:\n    mon: 1 daemons, quorum node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us (age 2m)\n    mgr: node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us.cfcdji(active, since 2m)\n    osd: 1 osds: 1 up (since 20s), 1 in (since 40s)\n \n  data:\n    pools:   1 pools, 1 pgs\n    objects: 0 objects, 0 B\n    usage:   4.7 MiB used, 100 GiB / 100 GiB avail\n    pgs:     1 active+clean\n \n'
[1] 08:27:12 [SUCCESS] ljishen@10.10.2.1

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:23:51,061038052-07:00] INFO: > Remove existing clusters[0m
## bash:17 -  > basename -- /var/lib/ceph/d8ff4ba0-3801-11ec-b51d-53e6e728d2d3
# bash:17 -  > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm --host 10.10.2.1 --host 10.10.2.5 rm-cluster --force --fsid d8ff4ba0-3801-11ec-b51d-53e6e728d2d3
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:23:51,072483348-07:00] DEBUG: HOST_PARAMS: ['--host', '10.10.2.1', '--host', '10.10.2.5'][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:23:51,075997695-07:00] DEBUG: CEPHADM_PARAMS: ['rm-cluster', '--force', '--fsid', 'd8ff4ba0-3801-11ec-b51d-53e6e728d2d3'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:63 -  > /tmp/bench-rados/ceph-research/scripts/parallel-ssh --host 10.10.2.1 --host 10.10.2.5 'sudo python3 -u - rm-cluster --force --fsid d8ff4ba0-3801-11ec-b51d-53e6e728d2d3'
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:23:51,083730492-07:00] DEBUG: command from stdin[0m
# /tmp/bench-rados/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host 10.10.2.1 --host 10.10.2.5 'sudo python3 -u - rm-cluster --force --fsid d8ff4ba0-3801-11ec-b51d-53e6e728d2d3'
[1] 08:23:56 [SUCCESS] 10.10.2.1
[2] 08:24:02 [SUCCESS] 10.10.2.5

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:24:02,849726417-07:00] INFO: > Deploy a new cluster[0m
# bash:24 -  > /tmp/bench-rados/ceph-research/scripts/ceph-deploy -m 10.10.2.1 -o sm1/10.10.2.5:/dev/nvme0n1


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:24:02,861860838-07:00] STAGE: Verify passwordless SSH to hosts, passwordless sudo via ljishen, and the existence of devices on hosts...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:24:02,866467859-07:00] INFO: > Check the host for the monitor[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:181 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@10.10.2.1 sudo --non-interactive --validate

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:24:03,014760453-07:00] INFO: > Check hosts for OSDs[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:24:03,019539918-07:00] INFO: >> Check host 'sm1'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:200 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 sudo --non-interactive --validate

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:24:04,111000473-07:00] INFO: >>> Check device '/dev/nvme0n1'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:215 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 test -b /dev/nvme0n1
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:226 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 lsblk --nodeps --noheadings --output TYPE /dev/nvme0n1
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:227 - verify_parameters() > grep --word-regexp disk


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:24:06,255431767-07:00] STAGE: Reset devices on hosts...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:24:06,260507439-07:00] INFO: > Resetting device '/dev/nvme0n1' on host 'sm1'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:288 - reset_host_devices() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 sudo sh
  Removing ceph--74b956ae--1b24--400b--9bda--3f04a77af8c2-osd--block--5772b653--2ae0--4229--bc87--497a1a016f5e (252:0)
  Archiving volume group "ceph-74b956ae-1b24-400b-9bda-3f04a77af8c2" metadata (seqno 5).
  Releasing logical volume "osd-block-5772b653-2ae0-4229-bc87-497a1a016f5e"
  Creating volume group backup "/etc/lvm/backup/ceph-74b956ae-1b24-400b-9bda-3f04a77af8c2" (seqno 6).
  Logical volume "osd-block-5772b653-2ae0-4229-bc87-497a1a016f5e" successfully removed
  Removing physical volume "/dev/nvme0n1" from volume group "ceph-74b956ae-1b24-400b-9bda-3f04a77af8c2"
  Volume group "ceph-74b956ae-1b24-400b-9bda-3f04a77af8c2" successfully removed


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:24:08,622190433-07:00] STAGE: Bootstrap a new cluster...[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:317 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.1 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:24:08,632513269-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:24:08,636624428-07:00] DEBUG: CEPHADM_PARAMS: ['bootstrap', '--config', '/tmp/bench-rados/ceph-research/scripts/ceph.conf', '--mon-ip', '10.10.2.1', '--output-dir', '/tmp/bench-rados/deployment_data_root/etc/ceph', '--ssh-user', 'ljishen', '--skip-dashboard', '--no-minimize-config', '--allow-overwrite', '--allow-fqdn-hostname', '--orphan-initial-daemons', '--skip-monitoring-stack', '--single-host-defaults'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.1 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults
Verifying podman|docker is present...
Verifying lvm2 is present...
Verifying time synchronization is in place...
Unit ntp.service is enabled and running
Repeating the final host check...
podman|docker (/usr/bin/docker) is present
systemctl is present
lvcreate is present
Unit ntp.service is enabled and running
Host looks OK
Cluster fsid: 187cba0a-3803-11ec-b51d-53e6e728d2d3
Verifying IP 10.10.2.1 port 3300 ...
Verifying IP 10.10.2.1 port 6789 ...
Mon IP `10.10.2.1` is in CIDR network `10.10.2.0/24`
- internal network (--cluster-network) has not been provided, OSD replication will default to the public_network
Adjusting default settings to suit single-host cluster...
Pulling container image quay.io/ceph/ceph:v16...
Ceph version: ceph version 16.2.6 (ee28fb57e47e9f88813e24bbf4c14496ca299d31) pacific (stable)
Extracting ceph user uid/gid from container image...
Creating initial keys...
Creating initial monmap...
Creating mon...
Waiting for mon to start...
Waiting for mon...
mon is available
Setting mon public_network to 10.10.2.0/24
Wrote config to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf
Wrote keyring to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring
Creating mgr...
Verifying port 9283 ...
Waiting for mgr to start...
Waiting for mgr...
mgr not available, waiting (1/15)...
mgr not available, waiting (2/15)...
mgr is available
Enabling cephadm module...
Waiting for the mgr to restart...
Waiting for mgr epoch 5...
mgr epoch 5 is available
Setting orchestrator backend to cephadm...
Generating ssh key...
Wrote public SSH key to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub
Adding key to ljishen@localhost authorized_keys...
Adding host node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us...
Deploying unmanaged mon service...
Deploying unmanaged mgr service...
You can access the Ceph CLI with:

	sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --fsid 187cba0a-3803-11ec-b51d-53e6e728d2d3 -c /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf -k /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring

Please consider enabling telemetry to help improve Ceph:

	ceph telemetry on

For more information see:

	https://docs.ceph.com/docs/pacific/mgr/telemetry/

Bootstrap complete.

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:25:09,730501099-07:00] INFO: Sleep for 20 seconds[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:25:29,737490084-07:00] INFO: > Set the default number of monitor daemons to 1[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:337 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:25:29,747822267-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:25:29,751331454-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mon', '1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1
Inferring fsid 187cba0a-3803-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/187cba0a-3803-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Scheduled mon update...

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:25:38,690684739-07:00] INFO: > Set the default number of manager daemons to 1[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:342 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:25:38,701535357-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:25:38,704948664-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mgr', '1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1
Inferring fsid 187cba0a-3803-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/187cba0a-3803-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Scheduled mgr update...


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:25:47,485727887-07:00] STAGE: Add hosts to the cluster...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:25:47,492110155-07:00] INFO: > Adding host 'sm1'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:361 - add_hosts() > ssh-copy-id -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -f -i /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub ljishen@sm1
/usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: "/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub"

Number of key(s) added: 1

Now try logging into the machine, with:   "ssh -o 'GlobalKnownHostsFile=/dev/null' -o 'LogLevel=ERROR' -o 'PasswordAuthentication=no' -o 'StrictHostKeyChecking=no' -o 'UserKnownHostsFile=/dev/null' 'ljishen@sm1'"
and check to make sure that only the key(s) you wanted were added.

# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:367 - add_hosts() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add sm1 10.10.2.5
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:25:48,648740974-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:25:48,652121069-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'host', 'add', 'sm1', '10.10.2.5'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add sm1 10.10.2.5
Inferring fsid 187cba0a-3803-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/187cba0a-3803-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Added host 'sm1' with addr '10.10.2.5'

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:25:59,220010514-07:00] INFO: Sleep for 20 seconds[0m


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:26:19,224895060-07:00] STAGE: Add OSDs to the cluster...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:26:19,231660799-07:00] INFO: > Adding devices ['/dev/nvme0n1'] on host 'sm1' as OSDs[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:388 - add_host_devices_as_osds() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd sm1:/dev/nvme0n1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:26:19,241143095-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:26:19,245101175-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'daemon', 'add', 'osd', 'sm1:/dev/nvme0n1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd sm1:/dev/nvme0n1
Inferring fsid 187cba0a-3803-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/187cba0a-3803-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Created osd(s) 0 on host 'sm1'

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:26:44,003841454-07:00] INFO: > Sleep for 20 seconds[0m


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:27:04,009489801-07:00] STAGE: Show Cluster Status[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:408 - show_status() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:27:04,020035747-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:27:04,023246523-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', '--status'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status
Inferring fsid 187cba0a-3803-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/187cba0a-3803-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
  cluster:
    id:     187cba0a-3803-11ec-b51d-53e6e728d2d3
    health: HEALTH_WARN
            1 pool(s) have no replicas configured
 
  services:
    mon: 1 daemons, quorum node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us (age 2m)
    mgr: node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us.cfcdji(active, since 2m)
    osd: 1 osds: 1 up (since 20s), 1 in (since 40s)
 
  data:
    pools:   1 pools, 1 pgs
    objects: 0 objects, 0 B
    usage:   4.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     1 active+clean
 

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:27:12,673139425-07:00] INFO: > Copy configuration files to local[0m
# ./benchmarks/bench-rados:220 - launch_ceph_cluster() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.1 sudo chown --changes --recursive ljishen /tmp/bench-rados/deployment_data_root/etc/ceph
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:27:12,680842311-07:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.1 sudo chown --changes --recursive ljishen /tmp/bench-rados/deployment_data_root/etc/ceph
[1] 08:27:12 [SUCCESS] ljishen@10.10.2.1
# ./benchmarks/bench-rados:223 - launch_ceph_cluster() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.1:/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf
# ./benchmarks/bench-rados:226 - launch_ceph_cluster() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.1:/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:27:13,156861248-07:00] STAGE: Update local ceph.conf as a client...[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:27:13,160408663-07:00] STAGE: Configure Ceph pool...[0m
# ./benchmarks/bench-rados:257 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_max_backfills 32
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:27:13,182842799-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:27:13,185559969-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '187cba0a-3803-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'config', 'set', 'osd', 'osd_max_backfills', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 187cba0a-3803-11ec-b51d-53e6e728d2d3 -- ceph config set osd osd_max_backfills 32
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:258 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_active 32
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:27:16,890654754-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:27:16,893813777-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '187cba0a-3803-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_active', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 187cba0a-3803-11ec-b51d-53e6e728d2d3 -- ceph config set osd osd_recovery_max_active 32
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:259 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_single_start 8
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:27:20,606617871-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:27:20,609749082-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '187cba0a-3803-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_single_start', '8'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 187cba0a-3803-11ec-b51d-53e6e728d2d3 -- ceph config set osd osd_recovery_max_single_start 8
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:260 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_op_priority 63
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:27:24,283124033-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:27:24,286087948-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '187cba0a-3803-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_op_priority', '63'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 187cba0a-3803-11ec-b51d-53e6e728d2d3 -- ceph config set osd osd_recovery_op_priority 63
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:262 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config show-with-defaults osd.0
# ./benchmarks/bench-rados:263 - configure_ceph_pool() > grep 'osd_max_backfills\|osd_recovery'
# ./benchmarks/bench-rados:263 - configure_ceph_pool() > column -t -s ' '
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
osd_max_backfills                        32         mon
osd_recovery_cost                        20971520   default
osd_recovery_delay_start                 0.000000   default
osd_recovery_max_active                  32         mon
osd_recovery_max_active_hdd              3          default
osd_recovery_max_active_ssd              10         default
osd_recovery_max_chunk                   8388608    default
osd_recovery_max_omap_entries_per_chunk  8096       default
osd_recovery_max_single_start            8          mon
osd_recovery_op_priority                 63         mon
osd_recovery_op_warn_multiple            16         default
osd_recovery_priority                    5          default
osd_recovery_retry_interval              30.000000  default
osd_recovery_sleep                       0.000000   default
osd_recovery_sleep_hdd                   0.100000   default
osd_recovery_sleep_hybrid                0.025000   default
osd_recovery_sleep_ssd                   0.000000   default
# ./benchmarks/bench-rados:265 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool create bench_rados 128 128 replicated --size 1 --pg-num-min 128
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:27:31,480331184-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:27:31,483398253-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '187cba0a-3803-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'create', 'bench_rados', '128', '128', 'replicated', '--size', '1', '--pg-num-min', '128'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 187cba0a-3803-11ec-b51d-53e6e728d2d3 -- ceph osd pool create bench_rados 128 128 replicated --size 1 --pg-num-min 128
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
pool 'bench_rados' created
# ./benchmarks/bench-rados:267 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados min_size 1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:27:36,006659372-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:27:36,009551442-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '187cba0a-3803-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'min_size', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 187cba0a-3803-11ec-b51d-53e6e728d2d3 -- ceph osd pool set bench_rados min_size 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 min_size to 1
# ./benchmarks/bench-rados:268 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados pg_autoscale_mode off
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:27:40,422426866-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:27:40,425319928-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '187cba0a-3803-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'pg_autoscale_mode', 'off'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 187cba0a-3803-11ec-b51d-53e6e728d2d3 -- ceph osd pool set bench_rados pg_autoscale_mode off
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 pg_autoscale_mode to off
# ./benchmarks/bench-rados:269 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool application enable bench_rados benchmark
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:27:45,087534867-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:27:45,090543437-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '187cba0a-3803-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'application', 'enable', 'bench_rados', 'benchmark'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 187cba0a-3803-11ec-b51d-53e6e728d2d3 -- ceph osd pool application enable bench_rados benchmark
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
enabled application 'benchmark' on pool 'bench_rados'
# ./benchmarks/bench-rados:270 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados noscrub 1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:27:48,819882741-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:27:48,822963366-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '187cba0a-3803-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'noscrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 187cba0a-3803-11ec-b51d-53e6e728d2d3 -- ceph osd pool set bench_rados noscrub 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 noscrub to 1
# ./benchmarks/bench-rados:271 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados nodeep-scrub 1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:27:52,870505574-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:27:52,873747353-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '187cba0a-3803-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'nodeep-scrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 187cba0a-3803-11ec-b51d-53e6e728d2d3 -- ceph osd pool set bench_rados nodeep-scrub 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 nodeep-scrub to 1
# ./benchmarks/bench-rados:272 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool ls detail
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:27:57,660907354-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:27:57,663832336-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '187cba0a-3803-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'ls', 'detail'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 187cba0a-3803-11ec-b51d-53e6e728d2d3 -- ceph osd pool ls detail
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
pool 1 'device_health_metrics' replicated size 1 min_size 1 crush_rule 0 object_hash rjenkins pg_num 64 pgp_num 64 autoscale_mode on last_change 19 lfor 0/0/17 flags hashpspool stripe_width 0 pg_num_min 1 application mgr_devicehealth
pool 2 'bench_rados' replicated size 1 min_size 1 crush_rule 0 object_hash rjenkins pg_num 128 pgp_num 128 autoscale_mode off last_change 20 flags hashpspool,noscrub,nodeep-scrub stripe_width 0 pg_num_min 128 application benchmark

# ./benchmarks/bench-rados:273 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd df tree
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:28:01,428246928-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:28:01,431338764-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '187cba0a-3803-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'df', 'tree'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 187cba0a-3803-11ec-b51d-53e6e728d2d3 -- ceph osd df tree
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
ID  CLASS  WEIGHT   REWEIGHT  SIZE     RAW USE  DATA     OMAP  META     AVAIL    %USE  VAR   PGS  STATUS  TYPE NAME    
-1         0.09769         -  100 GiB  5.7 MiB  168 KiB   0 B  5.6 MiB  100 GiB  0.01  1.00    -          root default 
-3         0.09769         -  100 GiB  5.7 MiB  168 KiB   0 B  5.6 MiB  100 GiB  0.01  1.00    -              host sm1 
 0    ssd  0.09769   1.00000  100 GiB  5.7 MiB  168 KiB   0 B  5.6 MiB  100 GiB  0.01  1.00  192      up          osd.0
                       TOTAL  100 GiB  5.7 MiB  168 KiB   0 B  5.6 MiB  100 GiB  0.01                                  
MIN/MAX VAR: 1.00/1.00  STDDEV: 0


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:28:05,062469692-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:28:28,776339590-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:28:37,434488420-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:28:46,208267619-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:28:54,796709201-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:28:54,804749603-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:29:03,561037803-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:29:03,568977655-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:29:12,234504743-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:29:12,242300153-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:29:20,893503980-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:29:20,901111476-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:29:29,698883021-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:29:29,706307382-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:29:29,712392490-07:00] INFO: > The cluster is idle now.[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:29:29,716387608-07:00] STAGE: Start benchmarking of mode write ...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:29:29,723140504-07:00] INFO: > Start system activity collection services[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:29:29,728674452-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:341 - rados_bench() > rm --force /mnt/sda8/ceph-research/scripts/output/sys_activity_256KB_write.dat.rados_bench_host.2
# ./benchmarks/bench-rados:342 - rados_bench() > sar_pid_rados_bench_host=446593
# ./benchmarks/bench-rados:342 - rados_bench() > S_TIME_FORMAT=ISO
# ./benchmarks/bench-rados:342 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/output/sys_activity_256KB_write.dat.rados_bench_host.2 2

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:29:29,735393483-07:00] INFO: >> for OSD host[0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:29:29,744847639-07:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.5 bash
10.10.2.5: b'141605\n'
[1] 08:29:30 [SUCCESS] ljishen@10.10.2.5
141605

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:29:30,844480449-07:00] INFO: > Run rados bench[0m
# ./benchmarks/bench-rados:370 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/output/rados_bench_256KB_write.log.2
## ./benchmarks/bench-rados:361 - rados_bench() > count_cpus 32-63,96-127
# ./benchmarks/bench-rados:361 - rados_bench() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 60 write --pool bench_rados -b 262144 -O 262144 --concurrent-ios 128 --show-time --write-object --write-omap --write-xattr --no-cleanup
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:29:30,864793909-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:29:30,867659980-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '187cba0a-3803-11ec-b51d-53e6e728d2d3', '--', 'rados', 'bench', '60', 'write', '--pool', 'bench_rados', '-b', '262144', '-O', '262144', '--concurrent-ios', '128', '--show-time', '--write-object', '--write-omap', '--write-xattr', '--no-cleanup'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 187cba0a-3803-11ec-b51d-53e6e728d2d3 -- rados bench 60 write --pool bench_rados -b 262144 -O 262144 --concurrent-ios 128 --show-time --write-object --write-omap --write-xattr --no-cleanup
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
hints = 1
2021-10-28T15:29:33.718953+0000 Maintaining 128 concurrent writes of 262144 bytes to objects of size 262144 for up to 60 seconds or 0 objects
2021-10-28T15:29:33.718966+0000 Object prefix: benchmark_data_node-2.bluefield2.ucsc-cmps10_7
2021-10-28T15:29:33.727574+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-28T15:29:33.727574+0000     0       0         0         0         0         0           -           0
2021-10-28T15:29:34.727714+0000     1     128       328       200   49.9963        50    0.478186    0.481442
2021-10-28T15:29:35.727827+0000     2     128       602       474   59.2445      68.5    0.379713    0.482325
2021-10-28T15:29:36.727939+0000     3     128       841       713   59.4108     59.75    0.368236    0.488007
2021-10-28T15:29:37.728023+0000     4     128      1124       996   62.2441     70.75    0.309648    0.493734
2021-10-28T15:29:38.728131+0000     5     128      1379      1251   62.5439     63.75    0.344722    0.490822
2021-10-28T15:29:39.728244+0000     6     128      1602      1474   61.4105     55.75    0.355274    0.491087
2021-10-28T15:29:40.728359+0000     7     128      1872      1744   62.2793      67.5    0.411553    0.490596
2021-10-28T15:29:41.728443+0000     8     128      2122      1994   62.3063      62.5    0.474859    0.490498
2021-10-28T15:29:42.728555+0000     9     128      2224      2096   58.2163      25.5     1.22339    0.503008
2021-10-28T15:29:43.728666+0000    10     128      2403      2275   56.8692     44.75    0.753649    0.536499
2021-10-28T15:29:44.728782+0000    11     128      2533      2405   54.6534      32.5    0.784586    0.563612
2021-10-28T15:29:45.728865+0000    12     128      2628      2500    52.078     23.75    0.969699    0.574114
2021-10-28T15:29:46.728979+0000    13     128      2814      2686   51.6485      46.5    0.745608    0.596565
2021-10-28T15:29:47.729095+0000    14     128      2938      2810   50.1734        31     1.07295    0.614547
2021-10-28T15:29:48.729205+0000    15     128      3049      2921   48.6783     27.75    0.809334    0.631793
2021-10-28T15:29:49.729295+0000    16     128      3230      3102   48.4637     45.25    0.813258    0.648916
2021-10-28T15:29:50.729411+0000    17     128      3400      3272   48.1126      42.5     0.41272    0.646176
2021-10-28T15:29:51.729522+0000    18     128      3761      3633   50.4531     90.25    0.603026    0.628755
2021-10-28T15:29:52.729638+0000    19     128      3997      3869   50.9025        59    0.355807    0.619663
2021-10-28T15:29:53.729726+0000 min lat: 0.0240071 max lat: 1.66076 avg lat: 0.614015
2021-10-28T15:29:53.729726+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-28T15:29:53.729726+0000    20     128      4211      4083   51.0322      53.5    0.639501    0.614015
2021-10-28T15:29:54.729846+0000    21     128      4351      4223   50.2685        35    0.865416    0.623365
2021-10-28T15:29:55.729958+0000    22     128      4439      4311   48.9835        22    0.866256    0.631271
2021-10-28T15:29:56.730072+0000    23     128      4516      4388   47.6906     19.25     1.43865    0.647769
2021-10-28T15:29:57.730158+0000    24     128      4628      4500   46.8701        28     1.29595    0.666121
2021-10-28T15:29:58.730271+0000    25     128      4675      4547   45.4652     11.75     1.28969    0.674283
2021-10-28T15:29:59.730382+0000    26     128      4780      4652   44.7261     26.25     1.32481    0.693065
2021-10-28T15:30:00.730492+0000    27     128      4890      4762   44.0879      27.5     1.25525    0.712863
2021-10-28T15:30:01.730576+0000    28     128      4978      4850    43.299        22     1.16268    0.724622
2021-10-28T15:30:02.730690+0000    29     128      5023      4895   42.1938     11.25     1.70701    0.732041
2021-10-28T15:30:03.730800+0000    30     128      5131      5003   41.6873        27     1.83011    0.746497
2021-10-28T15:30:04.730911+0000    31     128      5188      5060   40.8021     14.25      1.2955    0.755276
2021-10-28T15:30:05.730994+0000    32     128      5316      5188    40.527        32     1.34811    0.774041
2021-10-28T15:30:06.731107+0000    33     128      5475      5347   40.5033     39.75    0.723963    0.779959
2021-10-28T15:30:07.731219+0000    34     128      5651      5523    40.606        44    0.611229    0.779752
2021-10-28T15:30:08.731330+0000    35     128      5793      5665     40.46      35.5    0.658706    0.780743
2021-10-28T15:30:09.731420+0000    36     128      5976      5848   40.6068     45.75    0.590902     0.78068
2021-10-28T15:30:10.731537+0000    37     128      6126      5998   40.5228      37.5     0.94085    0.780903
2021-10-28T15:30:11.731651+0000    38     128      6239      6111   40.1997     28.25    0.874659    0.781407
2021-10-28T15:30:12.731767+0000    39     128      6356      6228   39.9189     29.25     1.23852    0.791469
2021-10-28T15:30:13.731858+0000 min lat: 0.0240071 max lat: 2.533 avg lat: 0.797711
2021-10-28T15:30:13.731858+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-28T15:30:13.731858+0000    40     128      6432      6304   39.3958        19      1.1598    0.797711
2021-10-28T15:30:14.731987+0000    41     128      6487      6359   38.7703     13.75     1.37776       0.804
2021-10-28T15:30:15.732103+0000    42     128      6598      6470   38.5078     27.75      1.3927    0.815511
2021-10-28T15:30:16.732223+0000    43     128      6656      6528   37.9494      14.5     1.25141    0.821434
2021-10-28T15:30:17.732318+0000    44     128      6781      6653   37.7971     31.25     1.40168    0.835614
2021-10-28T15:30:18.732427+0000    45     128      6855      6727   37.3682      18.5     1.33644    0.841724
2021-10-28T15:30:19.732542+0000    46     128      6909      6781   36.8493      13.5     1.41936    0.846523
2021-10-28T15:30:20.732661+0000    47     128      7019      6891   36.6503      27.5     1.38804    0.855794
2021-10-28T15:30:21.732748+0000    48     128      7188      7060   36.7669     42.25    0.845345    0.863225
2021-10-28T15:30:22.732861+0000    49     128      7297      7169   36.5726     27.25    0.886441    0.865445
2021-10-28T15:30:23.732968+0000    50     128      7540      7412   37.0561     60.75     0.34716    0.860339
2021-10-28T15:30:24.733080+0000    51     128      7810      7682   37.6528      67.5    0.412409    0.846537
2021-10-28T15:30:25.733166+0000    52     128      8062      7934   38.1402        63    0.326054    0.835245
2021-10-28T15:30:26.733281+0000    53     128      8298      8170   38.5336        59    0.453323     0.82538
2021-10-28T15:30:27.733396+0000    54     128      8459      8331   38.5653     40.25    0.943928    0.824678
2021-10-28T15:30:28.733509+0000    55     128      8493      8365   38.0187       8.5      1.0204    0.825342
2021-10-28T15:30:29.733593+0000    56     128      8641      8513   38.0004        37     1.60391    0.834948
2021-10-28T15:30:30.733707+0000    57     128      8706      8578   37.6188     16.25     1.34315     0.84057
2021-10-28T15:30:31.733823+0000    58     128      8804      8676   37.3926      24.5     1.20267     0.84882
2021-10-28T15:30:32.733938+0000    59     128      8900      8772   37.1655        24     1.06252    0.856781
2021-10-28T15:30:33.734029+0000 min lat: 0.0240071 max lat: 2.533 avg lat: 0.857944
2021-10-28T15:30:33.734029+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-28T15:30:33.734029+0000    60     128      8916      8788   36.6128         4      1.4979    0.857944
2021-10-28T15:30:34.734198+0000 Total time run:         60.6802
Total writes made:      8916
Write size:             262144
Object size:            262144
Bandwidth (MB/sec):     36.7336
Stddev Bandwidth:       18.8378
Max bandwidth (MB/sec): 90.25
Min bandwidth (MB/sec): 4
Average IOPS:           146
Stddev IOPS:            75.351
Max IOPS:               361
Min IOPS:               16
Average Latency(s):     0.866875
Stddev Latency(s):      0.450548
Max latency(s):         2.533
Min latency(s):         0.0240071

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:30:35,329157064-07:00] INFO: > Stop system activity collection services[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:30:35,335016125-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:391 - rados_bench() > kill -INT 446593

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:30:35,340503134-07:00] INFO: >> for OSD host[0m
# ./benchmarks/bench-rados:396 - rados_bench() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.5 kill -INT 141605
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:30:35,348718356-07:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.5 kill -INT 141605
[1] 08:30:37 [SUCCESS] ljishen@10.10.2.5

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:30:37,153669977-07:00] INFO: >>> Copy system activity file from OSD host[0m
## ./benchmarks/bench-rados:403 - rados_bench() > basename -- /tmp/bench-rados/sys_activity_256KB_write.dat.osd_host.2
# ./benchmarks/bench-rados:403 - rados_bench() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.5:/tmp/bench-rados/sys_activity_256KB_write.dat.osd_host.2 /mnt/sda8/ceph-research/scripts/output/sys_activity_256KB_write.dat.osd_host.2


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:30:38,508685010-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:31:02,024585495-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 8.92k objects, 2.2 GiB
    usage:   6.6 GiB used, 93 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:31:02,032521731-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:31:10,756090107-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 8.92k objects, 2.2 GiB
    usage:   6.6 GiB used, 93 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:31:10,763670863-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:31:19,530545508-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 8.92k objects, 2.2 GiB
    usage:   6.6 GiB used, 93 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:31:19,538467396-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:31:28,279851978-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 8.92k objects, 2.2 GiB
    usage:   6.6 GiB used, 93 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:31:28,287344968-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:31:37,043825929-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 8.92k objects, 2.2 GiB
    usage:   6.6 GiB used, 93 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:31:37,051508747-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:31:37,057740369-07:00] INFO: > The cluster is idle now.[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:31:37,061618418-07:00] STAGE: Start benchmarking of mode seq ...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:31:37,068779733-07:00] INFO: > Start system activity collection services[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:31:37,074337226-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:341 - rados_bench() > rm --force /mnt/sda8/ceph-research/scripts/output/sys_activity_256KB_seq.dat.rados_bench_host.2
# ./benchmarks/bench-rados:342 - rados_bench() > sar_pid_rados_bench_host=447947
# ./benchmarks/bench-rados:342 - rados_bench() > S_TIME_FORMAT=ISO
# ./benchmarks/bench-rados:342 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/output/sys_activity_256KB_seq.dat.rados_bench_host.2 2

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:31:37,081338018-07:00] INFO: >> for OSD host[0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:31:37,090194778-07:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.5 bash
10.10.2.5: b'142285\n'
[1] 08:31:38 [SUCCESS] ljishen@10.10.2.5
142285

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:31:38,381956245-07:00] INFO: > Run rados bench[0m
# ./benchmarks/bench-rados:379 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/output/rados_bench_256KB_seq.log.2
## ./benchmarks/bench-rados:376 - rados_bench() > count_cpus 32-63,96-127
# ./benchmarks/bench-rados:376 - rados_bench() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 128 --show-time
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:31:38,402365565-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:31:38,405527814-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '187cba0a-3803-11ec-b51d-53e6e728d2d3', '--', 'rados', 'bench', '99999999', 'seq', '--pool', 'bench_rados', '--concurrent-ios', '128', '--show-time'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 187cba0a-3803-11ec-b51d-53e6e728d2d3 -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 128 --show-time
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
hints = 1
2021-10-28T15:31:41.267567+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-28T15:31:41.267567+0000     0       0         0         0         0         0           -           0
2021-10-28T15:31:42.267693+0000     1     128      2873      2745   686.109    686.25   0.0428991   0.0454423
2021-10-28T15:31:43.267810+0000     2     128      5664      5536   691.889    697.75   0.0426854   0.0456825
2021-10-28T15:31:44.267893+0000     3     128      8374      8246   687.074     677.5   0.0351326   0.0461126
2021-10-28T15:31:45.268003+0000 Total time run:       3.23132
Total reads made:     8916
Read size:            262144
Object size:          262144
Bandwidth (MB/sec):   689.81
Average IOPS:         2759
Stddev IOPS:          40.6243
Max IOPS:             2791
Min IOPS:             2710
Average Latency(s):   0.0460522
Max latency(s):       0.0782887
Min latency(s):       0.004582

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:31:45,901112322-07:00] INFO: > Stop system activity collection services[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:31:45,906979629-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:391 - rados_bench() > kill -INT 447947

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:31:45,912428076-07:00] INFO: >> for OSD host[0m
# ./benchmarks/bench-rados:396 - rados_bench() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.5 kill -INT 142285
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:31:45,920576391-07:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.5 kill -INT 142285
[1] 08:31:47 [SUCCESS] ljishen@10.10.2.5

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:31:47,020929591-07:00] INFO: >>> Copy system activity file from OSD host[0m
## ./benchmarks/bench-rados:403 - rados_bench() > basename -- /tmp/bench-rados/sys_activity_256KB_seq.dat.osd_host.2
# ./benchmarks/bench-rados:403 - rados_bench() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.5:/tmp/bench-rados/sys_activity_256KB_seq.dat.osd_host.2 /mnt/sda8/ceph-research/scripts/output/sys_activity_256KB_seq.dat.osd_host.2


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:31:48,120080344-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:32:11,726958974-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 8.92k objects, 2.2 GiB
    usage:   6.6 GiB used, 93 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:32:11,734552613-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:32:20,442529316-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 8.92k objects, 2.2 GiB
    usage:   6.6 GiB used, 93 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:32:20,450278398-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:32:29,108890497-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 8.92k objects, 2.2 GiB
    usage:   6.6 GiB used, 93 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:32:29,117168807-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:32:37,740674110-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 8.92k objects, 2.2 GiB
    usage:   6.6 GiB used, 93 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:32:37,748511088-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:32:46,448339138-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 8.92k objects, 2.2 GiB
    usage:   6.6 GiB used, 93 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:32:46,456329576-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:32:46,462259610-07:00] INFO: > The cluster is idle now.[0m


[1;7;39;49m[2021-10-28T08:32:46,464616532-07:00][RUNNING][ROUND 3/4/21] object_size=256KB[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:32:46,468249878-07:00] STAGE: Launch a Ceph cluster (CEPH_HOST: 10.10.2.1, OSD_HOST: 10.10.2.5) ...[0m
# ./benchmarks/bench-rados:183 - launch_ceph_cluster() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.1 bash -euo pipefail
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:32:46,477551577-07:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.1 bash -euo pipefail
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T08:32:46,895000017-07:00] INFO: > Remove existing clusters\x1b[0m\n'
10.10.2.1: b'## bash:17 -  > basename -- /var/lib/ceph/187cba0a-3803-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'# bash:17 -  > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm --host 10.10.2.1 --host 10.10.2.5 rm-cluster --force --fsid 187cba0a-3803-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T08:32:46,906488424-07:00] DEBUG: HOST_PARAMS: ['--host', '10.10.2.1', '--host', '10.10.2.5']\x1b[0m\n"
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T08:32:46,910561160-07:00] DEBUG: CEPHADM_PARAMS: ['rm-cluster', '--force', '--fsid', '187cba0a-3803-11ec-b51d-53e6e728d2d3']\x1b[0m\n"
10.10.2.1: b"# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:63 -  > /tmp/bench-rados/ceph-research/scripts/parallel-ssh --host 10.10.2.1 --host 10.10.2.5 'sudo python3 -u - rm-cluster --force --fsid 187cba0a-3803-11ec-b51d-53e6e728d2d3'\n"
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T08:32:46,918787424-07:00] DEBUG: command from stdin\x1b[0m\n'
10.10.2.1: b"# /tmp/bench-rados/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host 10.10.2.1 --host 10.10.2.5 'sudo python3 -u - rm-cluster --force --fsid 187cba0a-3803-11ec-b51d-53e6e728d2d3'\n"
10.10.2.1: b'[1] 08:32:52 [SUCCESS] 10.10.2.1\n[2] 08:32:58 [SUCCESS] 10.10.2.5\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T08:32:58,083344810-07:00] INFO: > Deploy a new cluster\x1b[0m\n'
10.10.2.1: b'# bash:24 -  > /tmp/bench-rados/ceph-research/scripts/ceph-deploy -m 10.10.2.1 -o sm1/10.10.2.5:/dev/nvme0n1\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T08:32:58,095329971-07:00] STAGE: Verify passwordless SSH to hosts, passwordless sudo via ljishen, and the existence of devices on hosts...\x1b[0m\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T08:32:58,100013706-07:00] INFO: > Check the host for the monitor\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:181 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@10.10.2.1 sudo --non-interactive --validate\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T08:32:58,250949930-07:00] INFO: > Check hosts for OSDs\x1b[0m\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T08:32:58,255285350-07:00] INFO: >> Check host 'sm1'\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:200 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 sudo --non-interactive --validate\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T08:32:59,311009008-07:00] INFO: >>> Check device '/dev/nvme0n1'\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:215 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 test -b /dev/nvme0n1\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:226 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 lsblk --nodeps --noheadings --output TYPE /dev/nvme0n1\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:227 - verify_parameters() > grep --word-regexp disk\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T08:33:01,453330864-07:00] STAGE: Reset devices on hosts...\x1b[0m\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T08:33:01,458271763-07:00] INFO: > Resetting device '/dev/nvme0n1' on host 'sm1'\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:288 - reset_host_devices() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 sudo sh\n'
10.10.2.1: b'  Removing ceph--6a522060--65db--4185--8da1--6ea3e11a4ff1-osd--block--16e89b79--d609--41a0--8d78--3a8e9774c78a (252:0)\n'
10.10.2.1: b'  Archiving volume group "ceph-6a522060-65db-4185-8da1-6ea3e11a4ff1" metadata (seqno 5).\n'
10.10.2.1: b'  Releasing logical volume "osd-block-16e89b79-d609-41a0-8d78-3a8e9774c78a"\n'
10.10.2.1: b'  Creating volume group backup "/etc/lvm/backup/ceph-6a522060-65db-4185-8da1-6ea3e11a4ff1" (seqno 6).\n'
10.10.2.1: b'  Logical volume "osd-block-16e89b79-d609-41a0-8d78-3a8e9774c78a" successfully removed\n'
10.10.2.1: b'  Removing physical volume "/dev/nvme0n1" from volume group "ceph-6a522060-65db-4185-8da1-6ea3e11a4ff1"\n'
10.10.2.1: b'  Volume group "ceph-6a522060-65db-4185-8da1-6ea3e11a4ff1" successfully removed\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T08:33:03,753047637-07:00] STAGE: Bootstrap a new cluster...\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:317 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.1 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T08:33:03,762674564-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T08:33:03,766492912-07:00] DEBUG: CEPHADM_PARAMS: ['bootstrap', '--config', '/tmp/bench-rados/ceph-research/scripts/ceph.conf', '--mon-ip', '10.10.2.1', '--output-dir', '/tmp/bench-rados/deployment_data_root/etc/ceph', '--ssh-user', 'ljishen', '--skip-dashboard', '--no-minimize-config', '--allow-overwrite', '--allow-fqdn-hostname', '--orphan-initial-daemons', '--skip-monitoring-stack', '--single-host-defaults']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.1 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults\n'
10.10.2.1: b'Verifying podman|docker is present...\nVerifying lvm2 is present...\n'
10.10.2.1: b'Verifying time synchronization is in place...\n'
10.10.2.1: b'Unit ntp.service is enabled and running\n'
10.10.2.1: b'Repeating the final host check...\n'
10.10.2.1: b'podman|docker (/usr/bin/docker) is present\n'
10.10.2.1: b'systemctl is present\nlvcreate is present\n'
10.10.2.1: b'Unit ntp.service is enabled and running\n'
10.10.2.1: b'Host looks OK\nCluster fsid: 5773228e-3804-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Verifying IP 10.10.2.1 port 3300 ...\n'
10.10.2.1: b'Verifying IP 10.10.2.1 port 6789 ...\n'
10.10.2.1: b'Mon IP `10.10.2.1` is in CIDR network `10.10.2.0/24`\n'
10.10.2.1: b'- internal network (--cluster-network) has not been provided, OSD replication will default to the public_network\n'
10.10.2.1: b'Adjusting default settings to suit single-host cluster...\n'
10.10.2.1: b'Pulling container image quay.io/ceph/ceph:v16...\n'
10.10.2.1: b'Ceph version: ceph version 16.2.6 (ee28fb57e47e9f88813e24bbf4c14496ca299d31) pacific (stable)\n'
10.10.2.1: b'Extracting ceph user uid/gid from container image...\n'
10.10.2.1: b'Creating initial keys...\n'
10.10.2.1: b'Creating initial monmap...\n'
10.10.2.1: b'Creating mon...\n'
10.10.2.1: b'Waiting for mon to start...\n'
10.10.2.1: b'Waiting for mon...\n'
10.10.2.1: b'mon is available\nSetting mon public_network to 10.10.2.0/24\n'
10.10.2.1: b'Wrote config to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf\n'
10.10.2.1: b'Wrote keyring to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring\n'
10.10.2.1: b'Creating mgr...\n'
10.10.2.1: b'Verifying port 9283 ...\n'
10.10.2.1: b'Waiting for mgr to start...\nWaiting for mgr...\n'
10.10.2.1: b'mgr not available, waiting (1/15)...\n'
10.10.2.1: b'mgr not available, waiting (2/15)...\n'
10.10.2.1: b'mgr is available\n'
10.10.2.1: b'Enabling cephadm module...\n'
10.10.2.1: b'Waiting for the mgr to restart...\n'
10.10.2.1: b'Waiting for mgr epoch 5...\n'
10.10.2.1: b'mgr epoch 5 is available\n'
10.10.2.1: b'Setting orchestrator backend to cephadm...\n'
10.10.2.1: b'Generating ssh key...\n'
10.10.2.1: b'Wrote public SSH key to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub\n'
10.10.2.1: b'Adding key to ljishen@localhost authorized_keys...\n'
10.10.2.1: b'Adding host node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us...\n'
10.10.2.1: b'Deploying unmanaged mon service...\n'
10.10.2.1: b'Deploying unmanaged mgr service...\n'
10.10.2.1: b'You can access the Ceph CLI with:\n\n\tsudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --fsid 5773228e-3804-11ec-b51d-53e6e728d2d3 -c /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf -k /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring\n\n'
10.10.2.1: b'Please consider enabling telemetry to help improve Ceph:\n\n\tceph telemetry on\n\nFor more information see:\n\n\thttps://docs.ceph.com/docs/pacific/mgr/telemetry/\n\nBootstrap complete.\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T08:34:05,111593278-07:00] INFO: Sleep for 20 seconds\x1b[0m\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T08:34:25,118858921-07:00] INFO: > Set the default number of monitor daemons to 1\x1b[0m\n# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:337 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T08:34:25,129351717-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T08:34:25,133285221-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mon', '1']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1\n'
10.10.2.1: b'Inferring fsid 5773228e-3804-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/5773228e-3804-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b'Scheduled mon update...\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T08:34:34,506485022-07:00] INFO: > Set the default number of manager daemons to 1\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:342 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T08:34:34,516656673-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T08:34:34,520509526-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mgr', '1']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1\n'
10.10.2.1: b'Inferring fsid 5773228e-3804-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/5773228e-3804-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b'Scheduled mgr update...\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T08:34:44,028333326-07:00] STAGE: Add hosts to the cluster...\x1b[0m\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T08:34:44,034099977-07:00] INFO: > Adding host 'sm1'\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:361 - add_hosts() > ssh-copy-id -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -f -i /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub ljishen@sm1\n'
10.10.2.1: b'/usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: "/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub"\n'
10.10.2.1: b'\nNumber of key(s) added: 1\n\nNow try logging into the machine, with:   "ssh -o \'GlobalKnownHostsFile=/dev/null\' -o \'LogLevel=ERROR\' -o \'PasswordAuthentication=no\' -o \'StrictHostKeyChecking=no\' -o \'UserKnownHostsFile=/dev/null\' \'ljishen@sm1\'"\nand check to make sure that only the key(s) you wanted were added.\n\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:367 - add_hosts() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add sm1 10.10.2.5\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T08:34:45,196548827-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T08:34:45,200587389-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'host', 'add', 'sm1', '10.10.2.5']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add sm1 10.10.2.5\n'
10.10.2.1: b'Inferring fsid 5773228e-3804-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/5773228e-3804-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b"Added host 'sm1' with addr '10.10.2.5'\n"
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T08:34:55,953424903-07:00] INFO: Sleep for 20 seconds\x1b[0m\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T08:35:15,958074615-07:00] STAGE: Add OSDs to the cluster...\x1b[0m\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T08:35:15,964962283-07:00] INFO: > Adding devices ['/dev/nvme0n1'] on host 'sm1' as OSDs\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:388 - add_host_devices_as_osds() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd sm1:/dev/nvme0n1\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T08:35:15,974701232-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T08:35:15,979248911-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'daemon', 'add', 'osd', 'sm1:/dev/nvme0n1']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd sm1:/dev/nvme0n1\n'
10.10.2.1: b'Inferring fsid 5773228e-3804-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/5773228e-3804-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b"Created osd(s) 0 on host 'sm1'\n"
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T08:35:40,692679805-07:00] INFO: > Sleep for 20 seconds\x1b[0m\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T08:36:00,698385558-07:00] STAGE: Show Cluster Status\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:408 - show_status() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T08:36:00,708721829-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T08:36:00,712478802-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', '--status']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status\n'
10.10.2.1: b'Inferring fsid 5773228e-3804-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/5773228e-3804-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b'  cluster:\n    id:     5773228e-3804-11ec-b51d-53e6e728d2d3\n    health: HEALTH_WARN\n            1 pool(s) have no replicas configured\n \n  services:\n    mon: 1 daemons, quorum node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us (age 2m)\n    mgr: node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us.flxmte(active, since 2m)\n    osd: 1 osds: 1 up (since 20s), 1 in (since 40s)\n \n  data:\n    pools:   1 pools, 1 pgs\n    objects: 0 objects, 0 B\n    usage:   4.7 MiB used, 100 GiB / 100 GiB avail\n    pgs:     1 active+clean\n \n'
[1] 08:36:09 [SUCCESS] ljishen@10.10.2.1

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:32:46,895000017-07:00] INFO: > Remove existing clusters[0m
## bash:17 -  > basename -- /var/lib/ceph/187cba0a-3803-11ec-b51d-53e6e728d2d3
# bash:17 -  > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm --host 10.10.2.1 --host 10.10.2.5 rm-cluster --force --fsid 187cba0a-3803-11ec-b51d-53e6e728d2d3
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:32:46,906488424-07:00] DEBUG: HOST_PARAMS: ['--host', '10.10.2.1', '--host', '10.10.2.5'][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:32:46,910561160-07:00] DEBUG: CEPHADM_PARAMS: ['rm-cluster', '--force', '--fsid', '187cba0a-3803-11ec-b51d-53e6e728d2d3'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:63 -  > /tmp/bench-rados/ceph-research/scripts/parallel-ssh --host 10.10.2.1 --host 10.10.2.5 'sudo python3 -u - rm-cluster --force --fsid 187cba0a-3803-11ec-b51d-53e6e728d2d3'
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:32:46,918787424-07:00] DEBUG: command from stdin[0m
# /tmp/bench-rados/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host 10.10.2.1 --host 10.10.2.5 'sudo python3 -u - rm-cluster --force --fsid 187cba0a-3803-11ec-b51d-53e6e728d2d3'
[1] 08:32:52 [SUCCESS] 10.10.2.1
[2] 08:32:58 [SUCCESS] 10.10.2.5

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:32:58,083344810-07:00] INFO: > Deploy a new cluster[0m
# bash:24 -  > /tmp/bench-rados/ceph-research/scripts/ceph-deploy -m 10.10.2.1 -o sm1/10.10.2.5:/dev/nvme0n1


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:32:58,095329971-07:00] STAGE: Verify passwordless SSH to hosts, passwordless sudo via ljishen, and the existence of devices on hosts...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:32:58,100013706-07:00] INFO: > Check the host for the monitor[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:181 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@10.10.2.1 sudo --non-interactive --validate

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:32:58,250949930-07:00] INFO: > Check hosts for OSDs[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:32:58,255285350-07:00] INFO: >> Check host 'sm1'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:200 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 sudo --non-interactive --validate

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:32:59,311009008-07:00] INFO: >>> Check device '/dev/nvme0n1'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:215 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 test -b /dev/nvme0n1
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:226 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 lsblk --nodeps --noheadings --output TYPE /dev/nvme0n1
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:227 - verify_parameters() > grep --word-regexp disk


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:33:01,453330864-07:00] STAGE: Reset devices on hosts...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:33:01,458271763-07:00] INFO: > Resetting device '/dev/nvme0n1' on host 'sm1'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:288 - reset_host_devices() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 sudo sh
  Removing ceph--6a522060--65db--4185--8da1--6ea3e11a4ff1-osd--block--16e89b79--d609--41a0--8d78--3a8e9774c78a (252:0)
  Archiving volume group "ceph-6a522060-65db-4185-8da1-6ea3e11a4ff1" metadata (seqno 5).
  Releasing logical volume "osd-block-16e89b79-d609-41a0-8d78-3a8e9774c78a"
  Creating volume group backup "/etc/lvm/backup/ceph-6a522060-65db-4185-8da1-6ea3e11a4ff1" (seqno 6).
  Logical volume "osd-block-16e89b79-d609-41a0-8d78-3a8e9774c78a" successfully removed
  Removing physical volume "/dev/nvme0n1" from volume group "ceph-6a522060-65db-4185-8da1-6ea3e11a4ff1"
  Volume group "ceph-6a522060-65db-4185-8da1-6ea3e11a4ff1" successfully removed


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:33:03,753047637-07:00] STAGE: Bootstrap a new cluster...[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:317 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.1 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:33:03,762674564-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:33:03,766492912-07:00] DEBUG: CEPHADM_PARAMS: ['bootstrap', '--config', '/tmp/bench-rados/ceph-research/scripts/ceph.conf', '--mon-ip', '10.10.2.1', '--output-dir', '/tmp/bench-rados/deployment_data_root/etc/ceph', '--ssh-user', 'ljishen', '--skip-dashboard', '--no-minimize-config', '--allow-overwrite', '--allow-fqdn-hostname', '--orphan-initial-daemons', '--skip-monitoring-stack', '--single-host-defaults'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.1 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults
Verifying podman|docker is present...
Verifying lvm2 is present...
Verifying time synchronization is in place...
Unit ntp.service is enabled and running
Repeating the final host check...
podman|docker (/usr/bin/docker) is present
systemctl is present
lvcreate is present
Unit ntp.service is enabled and running
Host looks OK
Cluster fsid: 5773228e-3804-11ec-b51d-53e6e728d2d3
Verifying IP 10.10.2.1 port 3300 ...
Verifying IP 10.10.2.1 port 6789 ...
Mon IP `10.10.2.1` is in CIDR network `10.10.2.0/24`
- internal network (--cluster-network) has not been provided, OSD replication will default to the public_network
Adjusting default settings to suit single-host cluster...
Pulling container image quay.io/ceph/ceph:v16...
Ceph version: ceph version 16.2.6 (ee28fb57e47e9f88813e24bbf4c14496ca299d31) pacific (stable)
Extracting ceph user uid/gid from container image...
Creating initial keys...
Creating initial monmap...
Creating mon...
Waiting for mon to start...
Waiting for mon...
mon is available
Setting mon public_network to 10.10.2.0/24
Wrote config to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf
Wrote keyring to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring
Creating mgr...
Verifying port 9283 ...
Waiting for mgr to start...
Waiting for mgr...
mgr not available, waiting (1/15)...
mgr not available, waiting (2/15)...
mgr is available
Enabling cephadm module...
Waiting for the mgr to restart...
Waiting for mgr epoch 5...
mgr epoch 5 is available
Setting orchestrator backend to cephadm...
Generating ssh key...
Wrote public SSH key to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub
Adding key to ljishen@localhost authorized_keys...
Adding host node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us...
Deploying unmanaged mon service...
Deploying unmanaged mgr service...
You can access the Ceph CLI with:

	sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --fsid 5773228e-3804-11ec-b51d-53e6e728d2d3 -c /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf -k /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring

Please consider enabling telemetry to help improve Ceph:

	ceph telemetry on

For more information see:

	https://docs.ceph.com/docs/pacific/mgr/telemetry/

Bootstrap complete.

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:34:05,111593278-07:00] INFO: Sleep for 20 seconds[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:34:25,118858921-07:00] INFO: > Set the default number of monitor daemons to 1[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:337 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:34:25,129351717-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:34:25,133285221-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mon', '1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1
Inferring fsid 5773228e-3804-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/5773228e-3804-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Scheduled mon update...

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:34:34,506485022-07:00] INFO: > Set the default number of manager daemons to 1[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:342 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:34:34,516656673-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:34:34,520509526-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mgr', '1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1
Inferring fsid 5773228e-3804-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/5773228e-3804-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Scheduled mgr update...


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:34:44,028333326-07:00] STAGE: Add hosts to the cluster...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:34:44,034099977-07:00] INFO: > Adding host 'sm1'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:361 - add_hosts() > ssh-copy-id -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -f -i /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub ljishen@sm1
/usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: "/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub"

Number of key(s) added: 1

Now try logging into the machine, with:   "ssh -o 'GlobalKnownHostsFile=/dev/null' -o 'LogLevel=ERROR' -o 'PasswordAuthentication=no' -o 'StrictHostKeyChecking=no' -o 'UserKnownHostsFile=/dev/null' 'ljishen@sm1'"
and check to make sure that only the key(s) you wanted were added.

# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:367 - add_hosts() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add sm1 10.10.2.5
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:34:45,196548827-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:34:45,200587389-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'host', 'add', 'sm1', '10.10.2.5'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add sm1 10.10.2.5
Inferring fsid 5773228e-3804-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/5773228e-3804-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Added host 'sm1' with addr '10.10.2.5'

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:34:55,953424903-07:00] INFO: Sleep for 20 seconds[0m


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:35:15,958074615-07:00] STAGE: Add OSDs to the cluster...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:35:15,964962283-07:00] INFO: > Adding devices ['/dev/nvme0n1'] on host 'sm1' as OSDs[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:388 - add_host_devices_as_osds() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd sm1:/dev/nvme0n1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:35:15,974701232-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:35:15,979248911-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'daemon', 'add', 'osd', 'sm1:/dev/nvme0n1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd sm1:/dev/nvme0n1
Inferring fsid 5773228e-3804-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/5773228e-3804-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Created osd(s) 0 on host 'sm1'

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:35:40,692679805-07:00] INFO: > Sleep for 20 seconds[0m


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:36:00,698385558-07:00] STAGE: Show Cluster Status[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:408 - show_status() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:36:00,708721829-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:36:00,712478802-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', '--status'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status
Inferring fsid 5773228e-3804-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/5773228e-3804-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
  cluster:
    id:     5773228e-3804-11ec-b51d-53e6e728d2d3
    health: HEALTH_WARN
            1 pool(s) have no replicas configured
 
  services:
    mon: 1 daemons, quorum node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us (age 2m)
    mgr: node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us.flxmte(active, since 2m)
    osd: 1 osds: 1 up (since 20s), 1 in (since 40s)
 
  data:
    pools:   1 pools, 1 pgs
    objects: 0 objects, 0 B
    usage:   4.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     1 active+clean
 

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:36:09,524135862-07:00] INFO: > Copy configuration files to local[0m
# ./benchmarks/bench-rados:220 - launch_ceph_cluster() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.1 sudo chown --changes --recursive ljishen /tmp/bench-rados/deployment_data_root/etc/ceph
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:36:09,531919660-07:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.1 sudo chown --changes --recursive ljishen /tmp/bench-rados/deployment_data_root/etc/ceph
[1] 08:36:09 [SUCCESS] ljishen@10.10.2.1
# ./benchmarks/bench-rados:223 - launch_ceph_cluster() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.1:/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf
# ./benchmarks/bench-rados:226 - launch_ceph_cluster() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.1:/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:36:10,008690497-07:00] STAGE: Update local ceph.conf as a client...[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:36:10,012182267-07:00] STAGE: Configure Ceph pool...[0m
# ./benchmarks/bench-rados:257 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_max_backfills 32
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:36:10,034310066-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:36:10,037160597-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '5773228e-3804-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'config', 'set', 'osd', 'osd_max_backfills', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 5773228e-3804-11ec-b51d-53e6e728d2d3 -- ceph config set osd osd_max_backfills 32
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:258 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_active 32
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:36:13,647090973-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:36:13,650259774-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '5773228e-3804-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_active', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 5773228e-3804-11ec-b51d-53e6e728d2d3 -- ceph config set osd osd_recovery_max_active 32
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:259 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_single_start 8
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:36:17,379687566-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:36:17,382693881-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '5773228e-3804-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_single_start', '8'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 5773228e-3804-11ec-b51d-53e6e728d2d3 -- ceph config set osd osd_recovery_max_single_start 8
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:260 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_op_priority 63
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:36:21,131746606-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:36:21,134641641-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '5773228e-3804-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_op_priority', '63'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 5773228e-3804-11ec-b51d-53e6e728d2d3 -- ceph config set osd osd_recovery_op_priority 63
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:262 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config show-with-defaults osd.0
# ./benchmarks/bench-rados:263 - configure_ceph_pool() > grep 'osd_max_backfills\|osd_recovery'
# ./benchmarks/bench-rados:263 - configure_ceph_pool() > column -t -s ' '
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
osd_max_backfills                        32         mon
osd_recovery_cost                        20971520   default
osd_recovery_delay_start                 0.000000   default
osd_recovery_max_active                  32         mon
osd_recovery_max_active_hdd              3          default
osd_recovery_max_active_ssd              10         default
osd_recovery_max_chunk                   8388608    default
osd_recovery_max_omap_entries_per_chunk  8096       default
osd_recovery_max_single_start            8          mon
osd_recovery_op_priority                 63         mon
osd_recovery_op_warn_multiple            16         default
osd_recovery_priority                    5          default
osd_recovery_retry_interval              30.000000  default
osd_recovery_sleep                       0.000000   default
osd_recovery_sleep_hdd                   0.100000   default
osd_recovery_sleep_hybrid                0.025000   default
osd_recovery_sleep_ssd                   0.000000   default
# ./benchmarks/bench-rados:265 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool create bench_rados 128 128 replicated --size 1 --pg-num-min 128
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:36:28,415753427-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:36:28,418670935-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '5773228e-3804-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'create', 'bench_rados', '128', '128', 'replicated', '--size', '1', '--pg-num-min', '128'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 5773228e-3804-11ec-b51d-53e6e728d2d3 -- ceph osd pool create bench_rados 128 128 replicated --size 1 --pg-num-min 128
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
pool 'bench_rados' created
# ./benchmarks/bench-rados:267 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados min_size 1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:36:33,079202730-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:36:33,082153089-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '5773228e-3804-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'min_size', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 5773228e-3804-11ec-b51d-53e6e728d2d3 -- ceph osd pool set bench_rados min_size 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 min_size to 1
# ./benchmarks/bench-rados:268 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados pg_autoscale_mode off
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:36:37,494869831-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:36:37,497895112-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '5773228e-3804-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'pg_autoscale_mode', 'off'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 5773228e-3804-11ec-b51d-53e6e728d2d3 -- ceph osd pool set bench_rados pg_autoscale_mode off
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 pg_autoscale_mode to off
# ./benchmarks/bench-rados:269 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool application enable bench_rados benchmark
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:36:42,238311004-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:36:42,241415865-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '5773228e-3804-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'application', 'enable', 'bench_rados', 'benchmark'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 5773228e-3804-11ec-b51d-53e6e728d2d3 -- ceph osd pool application enable bench_rados benchmark
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
enabled application 'benchmark' on pool 'bench_rados'
# ./benchmarks/bench-rados:270 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados noscrub 1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:36:46,512891206-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:36:46,516196424-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '5773228e-3804-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'noscrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 5773228e-3804-11ec-b51d-53e6e728d2d3 -- ceph osd pool set bench_rados noscrub 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 noscrub to 1
# ./benchmarks/bench-rados:271 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados nodeep-scrub 1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:36:51,086847294-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:36:51,089921427-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '5773228e-3804-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'nodeep-scrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 5773228e-3804-11ec-b51d-53e6e728d2d3 -- ceph osd pool set bench_rados nodeep-scrub 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 nodeep-scrub to 1
# ./benchmarks/bench-rados:272 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool ls detail
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:36:55,453103414-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:36:55,456169963-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '5773228e-3804-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'ls', 'detail'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 5773228e-3804-11ec-b51d-53e6e728d2d3 -- ceph osd pool ls detail
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
pool 1 'device_health_metrics' replicated size 1 min_size 1 crush_rule 0 object_hash rjenkins pg_num 64 pgp_num 64 autoscale_mode on last_change 19 lfor 0/0/16 flags hashpspool stripe_width 0 pg_num_min 1 application mgr_devicehealth
pool 2 'bench_rados' replicated size 1 min_size 1 crush_rule 0 object_hash rjenkins pg_num 128 pgp_num 128 autoscale_mode off last_change 20 flags hashpspool,noscrub,nodeep-scrub stripe_width 0 pg_num_min 128 application benchmark

# ./benchmarks/bench-rados:273 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd df tree
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:36:59,211592322-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:36:59,214797121-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '5773228e-3804-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'df', 'tree'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 5773228e-3804-11ec-b51d-53e6e728d2d3 -- ceph osd df tree
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
ID  CLASS  WEIGHT   REWEIGHT  SIZE     RAW USE  DATA     OMAP  META     AVAIL    %USE  VAR   PGS  STATUS  TYPE NAME    
-1         0.09769         -  100 GiB  5.7 MiB  168 KiB   0 B  5.6 MiB  100 GiB  0.01  1.00    -          root default 
-3         0.09769         -  100 GiB  5.7 MiB  168 KiB   0 B  5.6 MiB  100 GiB  0.01  1.00    -              host sm1 
 0    ssd  0.09769   1.00000  100 GiB  5.7 MiB  168 KiB   0 B  5.6 MiB  100 GiB  0.01  1.00  192      up          osd.0
                       TOTAL  100 GiB  5.7 MiB  168 KiB   0 B  5.6 MiB  100 GiB  0.01                                  
MIN/MAX VAR: 1.00/1.00  STDDEV: 0


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:37:02,818678530-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:37:26,537692769-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:37:35,293334168-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:37:44,109660025-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:37:52,756320308-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:37:52,763860727-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:38:01,539575286-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:38:01,547197199-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:38:10,362918279-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:38:10,370529492-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:38:19,110478294-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:38:19,118157896-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:38:27,943588034-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:38:27,951371191-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:38:27,957146554-07:00] INFO: > The cluster is idle now.[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:38:27,960799117-07:00] STAGE: Start benchmarking of mode write ...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:38:27,967520603-07:00] INFO: > Start system activity collection services[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:38:27,973234471-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:341 - rados_bench() > rm --force /mnt/sda8/ceph-research/scripts/output/sys_activity_256KB_write.dat.rados_bench_host.3
# ./benchmarks/bench-rados:342 - rados_bench() > sar_pid_rados_bench_host=453712
# ./benchmarks/bench-rados:342 - rados_bench() > S_TIME_FORMAT=ISO
# ./benchmarks/bench-rados:342 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/output/sys_activity_256KB_write.dat.rados_bench_host.3 2

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:38:27,980269958-07:00] INFO: >> for OSD host[0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:38:27,989285197-07:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.5 bash
10.10.2.5: b'146969\n'
[1] 08:38:29 [SUCCESS] ljishen@10.10.2.5
146969

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:38:29,104159065-07:00] INFO: > Run rados bench[0m
# ./benchmarks/bench-rados:370 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/output/rados_bench_256KB_write.log.3
## ./benchmarks/bench-rados:361 - rados_bench() > count_cpus 32-63,96-127
# ./benchmarks/bench-rados:361 - rados_bench() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 60 write --pool bench_rados -b 262144 -O 262144 --concurrent-ios 128 --show-time --write-object --write-omap --write-xattr --no-cleanup
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:38:29,124742353-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:38:29,127586973-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '5773228e-3804-11ec-b51d-53e6e728d2d3', '--', 'rados', 'bench', '60', 'write', '--pool', 'bench_rados', '-b', '262144', '-O', '262144', '--concurrent-ios', '128', '--show-time', '--write-object', '--write-omap', '--write-xattr', '--no-cleanup'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 5773228e-3804-11ec-b51d-53e6e728d2d3 -- rados bench 60 write --pool bench_rados -b 262144 -O 262144 --concurrent-ios 128 --show-time --write-object --write-omap --write-xattr --no-cleanup
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
hints = 1
2021-10-28T15:38:32.083541+0000 Maintaining 128 concurrent writes of 262144 bytes to objects of size 262144 for up to 60 seconds or 0 objects
2021-10-28T15:38:32.083554+0000 Object prefix: benchmark_data_node-2.bluefield2.ucsc-cmps10_7
2021-10-28T15:38:32.092413+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-28T15:38:32.092413+0000     0       0         0         0         0         0           -           0
2021-10-28T15:38:33.092569+0000     1     128       331       203   50.7451     50.75     0.44981    0.494963
2021-10-28T15:38:34.092650+0000     2     128       586       458   57.2449     63.75    0.362416    0.494967
2021-10-28T15:38:35.092770+0000     3     128       859       731   60.9106     68.25    0.408767    0.498771
2021-10-28T15:38:36.092850+0000     4     128      1099       971   60.6818        60    0.351759    0.494242
2021-10-28T15:38:37.092968+0000     5     127      1358      1231   61.5439        65    0.530966    0.496962
2021-10-28T15:38:38.093047+0000     6     128      1604      1476   61.4941     61.25    0.592234    0.499481
2021-10-28T15:38:39.093169+0000     7     128      1852      1724   61.5653        62    0.340841    0.499948
2021-10-28T15:38:40.093249+0000     8     128      2097      1969   61.5253     61.25    0.528951      0.4998
2021-10-28T15:38:41.093373+0000     9     128      2237      2109   58.5775        35    0.653014    0.507739
2021-10-28T15:38:42.093462+0000    10     128      2404      2276   56.8944     41.75    0.796027    0.550218
2021-10-28T15:38:43.093581+0000    11     128      2506      2378     54.04      25.5    0.965821    0.568503
2021-10-28T15:38:44.093659+0000    12     128      2615      2487   51.8074     27.25    0.867576    0.582327
2021-10-28T15:38:45.093785+0000    13     128      2791      2663   51.2064        44    0.850977     0.60755
2021-10-28T15:38:46.093875+0000    14     128      2887      2759   49.2629        24    0.896501    0.620571
2021-10-28T15:38:47.093998+0000    15     128      2989      2861   47.6785      25.5    0.981519    0.632839
2021-10-28T15:38:48.094083+0000    16     128      3177      3049   47.6358        47    0.801597    0.656462
2021-10-28T15:38:49.094204+0000    17     128      3378      3250   47.7892     50.25    0.401561    0.662193
2021-10-28T15:38:50.094284+0000    18     128      3626      3498   48.5784        62    0.370553    0.652959
2021-10-28T15:38:51.094384+0000    19     128      3897      3769   49.5871     67.75     0.33727    0.639138
2021-10-28T15:38:52.094463+0000 min lat: 0.0358156 max lat: 1.66026 avg lat: 0.632833
2021-10-28T15:38:52.094463+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-28T15:38:52.094463+0000    20     128      4103      3975   49.6826      51.5     0.69112    0.632833
2021-10-28T15:38:53.094591+0000    21     128      4266      4138   49.2569     40.75     0.70034      0.6308
2021-10-28T15:38:54.094669+0000    22     128      4375      4247   48.2565     27.25    0.907939    0.640097
2021-10-28T15:38:55.094763+0000    23     128      4467      4339   47.1583        23     1.26713    0.648484
2021-10-28T15:38:56.094841+0000    24     128      4558      4430   46.1413     22.75     1.15077      0.6688
2021-10-28T15:38:57.094960+0000    25     128      4660      4532   45.3155      25.5     1.25804     0.69084
2021-10-28T15:38:58.095041+0000    26     128      4678      4550   43.7457       4.5    0.470629    0.693223
2021-10-28T15:38:59.095172+0000    27     128      4796      4668   43.2179      29.5    0.491068    0.714556
2021-10-28T15:39:00.095259+0000    28     128      4900      4772   42.6029        26    0.341264     0.73094
2021-10-28T15:39:01.095379+0000    29     128      5001      4873   42.0044     25.25     1.19267    0.746008
2021-10-28T15:39:02.095460+0000    30     128      5103      4975   41.4542      25.5     1.30897    0.757927
2021-10-28T15:39:03.095582+0000    31     128      5157      5029   40.5524      13.5     1.44378    0.767123
2021-10-28T15:39:04.095712+0000    32     128      5281      5153   40.2537        31     1.37548    0.783426
2021-10-28T15:39:05.095847+0000    33     128      5385      5257   39.8217        26    0.946037    0.791919
2021-10-28T15:39:06.095965+0000    34     128      5547      5419   39.8415      40.5    0.696525    0.794268
2021-10-28T15:39:07.096083+0000    35     128      5658      5530   39.4959     27.75     1.16141     0.79548
2021-10-28T15:39:08.096204+0000    36     128      5837      5709   39.6417     44.75    0.827366    0.796188
2021-10-28T15:39:09.096327+0000    37     128      6006      5878   39.7121     42.25    0.636472    0.795351
2021-10-28T15:39:10.096440+0000    38     128      6172      6044    39.759      41.5    0.835255    0.796445
2021-10-28T15:39:11.096562+0000    39     128      6276      6148   39.4061        26     1.05908    0.798053
2021-10-28T15:39:12.096660+0000 min lat: 0.0358156 max lat: 2.71736 avg lat: 0.80844
2021-10-28T15:39:12.096660+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-28T15:39:12.096660+0000    40     128      6367      6239   38.9897     22.75     1.44717     0.80844
2021-10-28T15:39:13.096786+0000    41     128      6459      6331   38.5996        23     1.11522    0.814512
2021-10-28T15:39:14.096897+0000    42     128      6538      6410   38.1507     19.75     1.51808    0.826024
2021-10-28T15:39:15.097020+0000    43     128      6576      6448   37.4844       9.5      1.5414    0.830202
2021-10-28T15:39:16.097145+0000    44     128      6709      6581   37.3881     33.25     1.45324     0.84416
2021-10-28T15:39:17.097271+0000    45     128      6783      6655   36.9683      18.5     1.63167    0.851959
2021-10-28T15:39:18.097404+0000    46     128      6827      6699   36.4037        11     1.44232    0.855778
2021-10-28T15:39:19.097533+0000    47     128      6954      6826   36.3046     31.75     1.48898    0.868639
2021-10-28T15:39:20.097664+0000    48     128      7016      6888   35.8711      15.5     1.29561    0.872792
2021-10-28T15:39:21.097789+0000    49     128      7185      7057   36.0012     42.25    0.767081    0.883911
2021-10-28T15:39:22.097900+0000    50     128      7286      7158   35.7861     25.25    0.885307    0.882956
2021-10-28T15:39:23.098029+0000    51     128      7539      7411   36.3245     63.25    0.435074    0.877083
2021-10-28T15:39:24.098157+0000    52     128      7811      7683   36.9335        68    0.322083    0.863042
2021-10-28T15:39:25.098277+0000    53     128      8039      7911    37.312        57    0.556023    0.852836
2021-10-28T15:39:26.098385+0000    54     128      8267      8139   37.6764        57     0.47717     0.84329
2021-10-28T15:39:27.098508+0000    55     128      8388      8260   37.5413     30.25    0.911412    0.843028
2021-10-28T15:39:28.098638+0000    56     128      8460      8332   37.1923        18     1.01087    0.842888
2021-10-28T15:39:29.098757+0000    57     128      8588      8460   37.1012        32    0.529259    0.852411
2021-10-28T15:39:30.098865+0000    58     128      8679      8551   36.8537     22.75     1.47787    0.859444
2021-10-28T15:39:31.098987+0000    59     128      8774      8646   36.6315     23.75      1.1409    0.866613
2021-10-28T15:39:32.099110+0000 min lat: 0.0358156 max lat: 2.71736 avg lat: 0.874247
2021-10-28T15:39:32.099110+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-28T15:39:32.099110+0000    60     128      8872      8744   36.4293      24.5     1.14164    0.874247
2021-10-28T15:39:33.099243+0000    61     105      8872      8767   35.9263      5.75     1.59971    0.875774
2021-10-28T15:39:34.099385+0000 Total time run:         61.4513
Total writes made:      8872
Write size:             262144
Object size:            262144
Bandwidth (MB/sec):     36.0936
Stddev Bandwidth:       17.4642
Max bandwidth (MB/sec): 68.25
Min bandwidth (MB/sec): 4.5
Average IOPS:           144
Stddev IOPS:            69.8568
Max IOPS:               273
Min IOPS:               18
Average Latency(s):     0.881555
Stddev Latency(s):      0.466577
Max latency(s):         2.71736
Min latency(s):         0.0358156

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:39:34,705724914-07:00] INFO: > Stop system activity collection services[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:39:34,711796195-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:391 - rados_bench() > kill -INT 453712

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:39:34,717422006-07:00] INFO: >> for OSD host[0m
# ./benchmarks/bench-rados:396 - rados_bench() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.5 kill -INT 146969
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:39:34,725524184-07:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.5 kill -INT 146969
[1] 08:39:36 [SUCCESS] ljishen@10.10.2.5

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:39:36,589886261-07:00] INFO: >>> Copy system activity file from OSD host[0m
## ./benchmarks/bench-rados:403 - rados_bench() > basename -- /tmp/bench-rados/sys_activity_256KB_write.dat.osd_host.3
# ./benchmarks/bench-rados:403 - rados_bench() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.5:/tmp/bench-rados/sys_activity_256KB_write.dat.osd_host.3 /mnt/sda8/ceph-research/scripts/output/sys_activity_256KB_write.dat.osd_host.3


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:39:38,112207221-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:40:02,035662955-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 8.87k objects, 2.2 GiB
    usage:   6.5 GiB used, 93 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:40:02,043359048-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:40:10,976431843-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 8.87k objects, 2.2 GiB
    usage:   6.5 GiB used, 93 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:40:10,984172469-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:40:19,770046337-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 8.87k objects, 2.2 GiB
    usage:   6.5 GiB used, 93 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:40:19,777883606-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:40:28,733685905-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 8.87k objects, 2.2 GiB
    usage:   6.5 GiB used, 93 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:40:28,741642899-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:40:37,534367899-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 8.87k objects, 2.2 GiB
    usage:   6.5 GiB used, 93 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:40:37,542262927-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:40:37,548357482-07:00] INFO: > The cluster is idle now.[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:40:37,552133257-07:00] STAGE: Start benchmarking of mode seq ...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:40:37,558833403-07:00] INFO: > Start system activity collection services[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:40:37,564433216-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:341 - rados_bench() > rm --force /mnt/sda8/ceph-research/scripts/output/sys_activity_256KB_seq.dat.rados_bench_host.3
# ./benchmarks/bench-rados:342 - rados_bench() > sar_pid_rados_bench_host=455061
# ./benchmarks/bench-rados:342 - rados_bench() > S_TIME_FORMAT=ISO
# ./benchmarks/bench-rados:342 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/output/sys_activity_256KB_seq.dat.rados_bench_host.3 2

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:40:37,571269679-07:00] INFO: >> for OSD host[0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:40:37,580184969-07:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.5 bash
10.10.2.5: b'147653\n'
[1] 08:40:38 [SUCCESS] ljishen@10.10.2.5
147653

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:40:38,937965032-07:00] INFO: > Run rados bench[0m
# ./benchmarks/bench-rados:379 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/output/rados_bench_256KB_seq.log.3
## ./benchmarks/bench-rados:376 - rados_bench() > count_cpus 32-63,96-127
# ./benchmarks/bench-rados:376 - rados_bench() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 128 --show-time
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:40:38,958069177-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:40:38,960924738-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '5773228e-3804-11ec-b51d-53e6e728d2d3', '--', 'rados', 'bench', '99999999', 'seq', '--pool', 'bench_rados', '--concurrent-ios', '128', '--show-time'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 5773228e-3804-11ec-b51d-53e6e728d2d3 -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 128 --show-time
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
hints = 1
2021-10-28T15:40:41.790642+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-28T15:40:41.790642+0000     0       0         0         0         0         0           -           0
2021-10-28T15:40:42.790764+0000     1     128      3152      3024    755.86       756   0.0357207   0.0415386
2021-10-28T15:40:43.790884+0000     2     128      6241      6113   764.008    772.25   0.0426035   0.0414143
2021-10-28T15:40:44.790983+0000 Total time run:       2.88829
Total reads made:     8872
Read size:            262144
Object size:          262144
Bandwidth (MB/sec):   767.928
Average IOPS:         3071
Stddev IOPS:          45.9619
Max IOPS:             3089
Min IOPS:             3024
Average Latency(s):   0.0413773
Max latency(s):       0.0861899
Min latency(s):       0.00795864

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:40:45,507592506-07:00] INFO: > Stop system activity collection services[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:40:45,513674217-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:391 - rados_bench() > kill -INT 455061

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:40:45,519891934-07:00] INFO: >> for OSD host[0m
# ./benchmarks/bench-rados:396 - rados_bench() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.5 kill -INT 147653
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:40:45,528389377-07:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.5 kill -INT 147653
[1] 08:40:46 [SUCCESS] ljishen@10.10.2.5

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:40:46,652252065-07:00] INFO: >>> Copy system activity file from OSD host[0m
## ./benchmarks/bench-rados:403 - rados_bench() > basename -- /tmp/bench-rados/sys_activity_256KB_seq.dat.osd_host.3
# ./benchmarks/bench-rados:403 - rados_bench() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.5:/tmp/bench-rados/sys_activity_256KB_seq.dat.osd_host.3 /mnt/sda8/ceph-research/scripts/output/sys_activity_256KB_seq.dat.osd_host.3


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:40:47,732128343-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:41:11,443200781-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 8.87k objects, 2.2 GiB
    usage:   6.5 GiB used, 93 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:41:11,451594107-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:41:20,395858820-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 8.87k objects, 2.2 GiB
    usage:   6.5 GiB used, 93 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:41:20,403655803-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:41:29,226940706-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 8.87k objects, 2.2 GiB
    usage:   6.5 GiB used, 93 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:41:29,234885147-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:41:38,017938076-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 8.87k objects, 2.2 GiB
    usage:   6.5 GiB used, 93 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:41:38,025704922-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:41:46,768943985-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 8.87k objects, 2.2 GiB
    usage:   6.5 GiB used, 93 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:41:46,776749715-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:41:46,782963074-07:00] INFO: > The cluster is idle now.[0m


[1;7;39;49m[2021-10-28T08:41:46,786880135-07:00][RUNNING][ROUND 1/5/21] object_size=1MB[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:41:46,790396261-07:00] STAGE: Launch a Ceph cluster (CEPH_HOST: 10.10.2.1, OSD_HOST: 10.10.2.5) ...[0m
# ./benchmarks/bench-rados:183 - launch_ceph_cluster() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.1 bash -euo pipefail
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:41:46,800228148-07:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.1 bash -euo pipefail
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T08:41:47,246932335-07:00] INFO: > Remove existing clusters\x1b[0m\n'
10.10.2.1: b'## bash:17 -  > basename -- /var/lib/ceph/5773228e-3804-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'# bash:17 -  > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm --host 10.10.2.1 --host 10.10.2.5 rm-cluster --force --fsid 5773228e-3804-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T08:41:47,257477629-07:00] DEBUG: HOST_PARAMS: ['--host', '10.10.2.1', '--host', '10.10.2.5']\x1b[0m\n"
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T08:41:47,260824481-07:00] DEBUG: CEPHADM_PARAMS: ['rm-cluster', '--force', '--fsid', '5773228e-3804-11ec-b51d-53e6e728d2d3']\x1b[0m\n"
10.10.2.1: b"# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:63 -  > /tmp/bench-rados/ceph-research/scripts/parallel-ssh --host 10.10.2.1 --host 10.10.2.5 'sudo python3 -u - rm-cluster --force --fsid 5773228e-3804-11ec-b51d-53e6e728d2d3'\n"
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T08:41:47,268443815-07:00] DEBUG: command from stdin\x1b[0m\n'
10.10.2.1: b"# /tmp/bench-rados/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host 10.10.2.1 --host 10.10.2.5 'sudo python3 -u - rm-cluster --force --fsid 5773228e-3804-11ec-b51d-53e6e728d2d3'\n"
10.10.2.1: b'[1] 08:41:53 [SUCCESS] 10.10.2.1\n[2] 08:41:59 [SUCCESS] 10.10.2.5\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T08:41:59,139334548-07:00] INFO: > Deploy a new cluster\x1b[0m\n'
10.10.2.1: b'# bash:24 -  > /tmp/bench-rados/ceph-research/scripts/ceph-deploy -m 10.10.2.1 -o sm1/10.10.2.5:/dev/nvme0n1\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T08:41:59,152486473-07:00] STAGE: Verify passwordless SSH to hosts, passwordless sudo via ljishen, and the existence of devices on hosts...\x1b[0m\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T08:41:59,156956205-07:00] INFO: > Check the host for the monitor\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:181 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@10.10.2.1 sudo --non-interactive --validate\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T08:41:59,307200760-07:00] INFO: > Check hosts for OSDs\x1b[0m\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T08:41:59,311664150-07:00] INFO: >> Check host 'sm1'\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:200 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 sudo --non-interactive --validate\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T08:42:00,443078091-07:00] INFO: >>> Check device '/dev/nvme0n1'\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:215 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 test -b /dev/nvme0n1\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:226 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 lsblk --nodeps --noheadings --output TYPE /dev/nvme0n1\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:227 - verify_parameters() > grep --word-regexp disk\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T08:42:02,593570534-07:00] STAGE: Reset devices on hosts...\x1b[0m\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T08:42:02,598835993-07:00] INFO: > Resetting device '/dev/nvme0n1' on host 'sm1'\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:288 - reset_host_devices() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 sudo sh\n'
10.10.2.1: b'  Removing ceph--ecc15429--09d1--4452--8ab8--7b4052f53d6c-osd--block--85dbbfab--d842--4fd2--b3f0--5ad10f4eee60 (252:0)\n'
10.10.2.1: b'  Archiving volume group "ceph-ecc15429-09d1-4452-8ab8-7b4052f53d6c" metadata (seqno 5).\n'
10.10.2.1: b'  Releasing logical volume "osd-block-85dbbfab-d842-4fd2-b3f0-5ad10f4eee60"\n'
10.10.2.1: b'  Creating volume group backup "/etc/lvm/backup/ceph-ecc15429-09d1-4452-8ab8-7b4052f53d6c" (seqno 6).\n'
10.10.2.1: b'  Logical volume "osd-block-85dbbfab-d842-4fd2-b3f0-5ad10f4eee60" successfully removed\n'
10.10.2.1: b'  Removing physical volume "/dev/nvme0n1" from volume group "ceph-ecc15429-09d1-4452-8ab8-7b4052f53d6c"\n'
10.10.2.1: b'  Volume group "ceph-ecc15429-09d1-4452-8ab8-7b4052f53d6c" successfully removed\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T08:42:04,872342178-07:00] STAGE: Bootstrap a new cluster...\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:317 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.1 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T08:42:04,882359981-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T08:42:04,886245766-07:00] DEBUG: CEPHADM_PARAMS: ['bootstrap', '--config', '/tmp/bench-rados/ceph-research/scripts/ceph.conf', '--mon-ip', '10.10.2.1', '--output-dir', '/tmp/bench-rados/deployment_data_root/etc/ceph', '--ssh-user', 'ljishen', '--skip-dashboard', '--no-minimize-config', '--allow-overwrite', '--allow-fqdn-hostname', '--orphan-initial-daemons', '--skip-monitoring-stack', '--single-host-defaults']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.1 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults\n'
10.10.2.1: b'Verifying podman|docker is present...\nVerifying lvm2 is present...\n'
10.10.2.1: b'Verifying time synchronization is in place...\n'
10.10.2.1: b'Unit ntp.service is enabled and running\n'
10.10.2.1: b'Repeating the final host check...\n'
10.10.2.1: b'podman|docker (/usr/bin/docker) is present\n'
10.10.2.1: b'systemctl is present\n'
10.10.2.1: b'lvcreate is present\n'
10.10.2.1: b'Unit ntp.service is enabled and running\n'
10.10.2.1: b'Host looks OK\n'
10.10.2.1: b'Cluster fsid: 99fb4504-3805-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Verifying IP 10.10.2.1 port 3300 ...\n'
10.10.2.1: b'Verifying IP 10.10.2.1 port 6789 ...\n'
10.10.2.1: b'Mon IP `10.10.2.1` is in CIDR network `10.10.2.0/24`\n'
10.10.2.1: b'- internal network (--cluster-network) has not been provided, OSD replication will default to the public_network\n'
10.10.2.1: b'Adjusting default settings to suit single-host cluster...\n'
10.10.2.1: b'Pulling container image quay.io/ceph/ceph:v16...\n'
10.10.2.1: b'Ceph version: ceph version 16.2.6 (ee28fb57e47e9f88813e24bbf4c14496ca299d31) pacific (stable)\n'
10.10.2.1: b'Extracting ceph user uid/gid from container image...\n'
10.10.2.1: b'Creating initial keys...\n'
10.10.2.1: b'Creating initial monmap...\n'
10.10.2.1: b'Creating mon...\n'
10.10.2.1: b'Waiting for mon to start...\n'
10.10.2.1: b'Waiting for mon...\n'
10.10.2.1: b'mon is available\nSetting mon public_network to 10.10.2.0/24\n'
10.10.2.1: b'Wrote config to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf\n'
10.10.2.1: b'Wrote keyring to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring\n'
10.10.2.1: b'Creating mgr...\n'
10.10.2.1: b'Verifying port 9283 ...\n'
10.10.2.1: b'Waiting for mgr to start...\nWaiting for mgr...\n'
10.10.2.1: b'mgr not available, waiting (1/15)...\n'
10.10.2.1: b'mgr not available, waiting (2/15)...\n'
10.10.2.1: b'mgr is available\n'
10.10.2.1: b'Enabling cephadm module...\n'
10.10.2.1: b'Waiting for the mgr to restart...\n'
10.10.2.1: b'Waiting for mgr epoch 5...\n'
10.10.2.1: b'mgr epoch 5 is available\nSetting orchestrator backend to cephadm...\n'
10.10.2.1: b'Generating ssh key...\n'
10.10.2.1: b'Wrote public SSH key to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub\n'
10.10.2.1: b'Adding key to ljishen@localhost authorized_keys...\n'
10.10.2.1: b'Adding host node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us...\n'
10.10.2.1: b'Deploying unmanaged mon service...\n'
10.10.2.1: b'Deploying unmanaged mgr service...\n'
10.10.2.1: b'You can access the Ceph CLI with:\n\n\tsudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --fsid 99fb4504-3805-11ec-b51d-53e6e728d2d3 -c /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf -k /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring\n\n'
10.10.2.1: b'Please consider enabling telemetry to help improve Ceph:\n\n\tceph telemetry on\n\nFor more information see:\n\n\thttps://docs.ceph.com/docs/pacific/mgr/telemetry/\n\n'
10.10.2.1: b'Bootstrap complete.\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T08:43:07,288982446-07:00] INFO: Sleep for 20 seconds\x1b[0m\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T08:43:27,296915435-07:00] INFO: > Set the default number of monitor daemons to 1\x1b[0m\n# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:337 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T08:43:27,307013178-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T08:43:27,310783335-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mon', '1']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1\n'
10.10.2.1: b'Inferring fsid 99fb4504-3805-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/99fb4504-3805-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b'Scheduled mon update...\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T08:43:36,516584171-07:00] INFO: > Set the default number of manager daemons to 1\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:342 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T08:43:36,526894563-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T08:43:36,531039946-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mgr', '1']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1\n'
10.10.2.1: b'Inferring fsid 99fb4504-3805-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/99fb4504-3805-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b'Scheduled mgr update...\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T08:43:45,409824176-07:00] STAGE: Add hosts to the cluster...\x1b[0m\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T08:43:45,416134599-07:00] INFO: > Adding host 'sm1'\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:361 - add_hosts() > ssh-copy-id -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -f -i /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub ljishen@sm1\n'
10.10.2.1: b'/usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: "/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub"\n'
10.10.2.1: b'\nNumber of key(s) added: 1\n\nNow try logging into the machine, with:   "ssh -o \'GlobalKnownHostsFile=/dev/null\' -o \'LogLevel=ERROR\' -o \'PasswordAuthentication=no\' -o \'StrictHostKeyChecking=no\' -o \'UserKnownHostsFile=/dev/null\' \'ljishen@sm1\'"\nand check to make sure that only the key(s) you wanted were added.\n\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:367 - add_hosts() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add sm1 10.10.2.5\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T08:43:46,557334166-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T08:43:46,560945916-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'host', 'add', 'sm1', '10.10.2.5']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add sm1 10.10.2.5\n'
10.10.2.1: b'Inferring fsid 99fb4504-3805-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/99fb4504-3805-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b"Added host 'sm1' with addr '10.10.2.5'\n"
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T08:43:57,102380537-07:00] INFO: Sleep for 20 seconds\x1b[0m\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T08:44:17,107315144-07:00] STAGE: Add OSDs to the cluster...\x1b[0m\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T08:44:17,113777302-07:00] INFO: > Adding devices ['/dev/nvme0n1'] on host 'sm1' as OSDs\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:388 - add_host_devices_as_osds() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd sm1:/dev/nvme0n1\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T08:44:17,124268665-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T08:44:17,128251743-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'daemon', 'add', 'osd', 'sm1:/dev/nvme0n1']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd sm1:/dev/nvme0n1\n'
10.10.2.1: b'Inferring fsid 99fb4504-3805-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/99fb4504-3805-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b"Created osd(s) 0 on host 'sm1'\n"
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T08:44:41,630817456-07:00] INFO: > Sleep for 20 seconds\x1b[0m\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T08:45:01,637198566-07:00] STAGE: Show Cluster Status\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:408 - show_status() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T08:45:01,647413479-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T08:45:01,651043634-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', '--status']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status\n'
10.10.2.1: b'Inferring fsid 99fb4504-3805-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/99fb4504-3805-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b'  cluster:\n    id:     99fb4504-3805-11ec-b51d-53e6e728d2d3\n    health: HEALTH_WARN\n            1 pool(s) have no replicas configured\n \n  services:\n    mon: 1 daemons, quorum node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us (age 2m)\n    mgr: node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us.qqdurd(active, since 2m)\n    osd: 1 osds: 1 up (since 19s), 1 in (since 40s)\n \n  data:\n    pools:   1 pools, 1 pgs\n    objects: 0 objects, 0 B\n    usage:   4.7 MiB used, 100 GiB / 100 GiB avail\n    pgs:     1 active+clean\n \n'
[1] 08:45:10 [SUCCESS] ljishen@10.10.2.1

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:41:47,246932335-07:00] INFO: > Remove existing clusters[0m
## bash:17 -  > basename -- /var/lib/ceph/5773228e-3804-11ec-b51d-53e6e728d2d3
# bash:17 -  > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm --host 10.10.2.1 --host 10.10.2.5 rm-cluster --force --fsid 5773228e-3804-11ec-b51d-53e6e728d2d3
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:41:47,257477629-07:00] DEBUG: HOST_PARAMS: ['--host', '10.10.2.1', '--host', '10.10.2.5'][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:41:47,260824481-07:00] DEBUG: CEPHADM_PARAMS: ['rm-cluster', '--force', '--fsid', '5773228e-3804-11ec-b51d-53e6e728d2d3'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:63 -  > /tmp/bench-rados/ceph-research/scripts/parallel-ssh --host 10.10.2.1 --host 10.10.2.5 'sudo python3 -u - rm-cluster --force --fsid 5773228e-3804-11ec-b51d-53e6e728d2d3'
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:41:47,268443815-07:00] DEBUG: command from stdin[0m
# /tmp/bench-rados/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host 10.10.2.1 --host 10.10.2.5 'sudo python3 -u - rm-cluster --force --fsid 5773228e-3804-11ec-b51d-53e6e728d2d3'
[1] 08:41:53 [SUCCESS] 10.10.2.1
[2] 08:41:59 [SUCCESS] 10.10.2.5

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:41:59,139334548-07:00] INFO: > Deploy a new cluster[0m
# bash:24 -  > /tmp/bench-rados/ceph-research/scripts/ceph-deploy -m 10.10.2.1 -o sm1/10.10.2.5:/dev/nvme0n1


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:41:59,152486473-07:00] STAGE: Verify passwordless SSH to hosts, passwordless sudo via ljishen, and the existence of devices on hosts...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:41:59,156956205-07:00] INFO: > Check the host for the monitor[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:181 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@10.10.2.1 sudo --non-interactive --validate

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:41:59,307200760-07:00] INFO: > Check hosts for OSDs[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:41:59,311664150-07:00] INFO: >> Check host 'sm1'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:200 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 sudo --non-interactive --validate

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:42:00,443078091-07:00] INFO: >>> Check device '/dev/nvme0n1'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:215 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 test -b /dev/nvme0n1
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:226 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 lsblk --nodeps --noheadings --output TYPE /dev/nvme0n1
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:227 - verify_parameters() > grep --word-regexp disk


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:42:02,593570534-07:00] STAGE: Reset devices on hosts...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:42:02,598835993-07:00] INFO: > Resetting device '/dev/nvme0n1' on host 'sm1'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:288 - reset_host_devices() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 sudo sh
  Removing ceph--ecc15429--09d1--4452--8ab8--7b4052f53d6c-osd--block--85dbbfab--d842--4fd2--b3f0--5ad10f4eee60 (252:0)
  Archiving volume group "ceph-ecc15429-09d1-4452-8ab8-7b4052f53d6c" metadata (seqno 5).
  Releasing logical volume "osd-block-85dbbfab-d842-4fd2-b3f0-5ad10f4eee60"
  Creating volume group backup "/etc/lvm/backup/ceph-ecc15429-09d1-4452-8ab8-7b4052f53d6c" (seqno 6).
  Logical volume "osd-block-85dbbfab-d842-4fd2-b3f0-5ad10f4eee60" successfully removed
  Removing physical volume "/dev/nvme0n1" from volume group "ceph-ecc15429-09d1-4452-8ab8-7b4052f53d6c"
  Volume group "ceph-ecc15429-09d1-4452-8ab8-7b4052f53d6c" successfully removed


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:42:04,872342178-07:00] STAGE: Bootstrap a new cluster...[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:317 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.1 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:42:04,882359981-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:42:04,886245766-07:00] DEBUG: CEPHADM_PARAMS: ['bootstrap', '--config', '/tmp/bench-rados/ceph-research/scripts/ceph.conf', '--mon-ip', '10.10.2.1', '--output-dir', '/tmp/bench-rados/deployment_data_root/etc/ceph', '--ssh-user', 'ljishen', '--skip-dashboard', '--no-minimize-config', '--allow-overwrite', '--allow-fqdn-hostname', '--orphan-initial-daemons', '--skip-monitoring-stack', '--single-host-defaults'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.1 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults
Verifying podman|docker is present...
Verifying lvm2 is present...
Verifying time synchronization is in place...
Unit ntp.service is enabled and running
Repeating the final host check...
podman|docker (/usr/bin/docker) is present
systemctl is present
lvcreate is present
Unit ntp.service is enabled and running
Host looks OK
Cluster fsid: 99fb4504-3805-11ec-b51d-53e6e728d2d3
Verifying IP 10.10.2.1 port 3300 ...
Verifying IP 10.10.2.1 port 6789 ...
Mon IP `10.10.2.1` is in CIDR network `10.10.2.0/24`
- internal network (--cluster-network) has not been provided, OSD replication will default to the public_network
Adjusting default settings to suit single-host cluster...
Pulling container image quay.io/ceph/ceph:v16...
Ceph version: ceph version 16.2.6 (ee28fb57e47e9f88813e24bbf4c14496ca299d31) pacific (stable)
Extracting ceph user uid/gid from container image...
Creating initial keys...
Creating initial monmap...
Creating mon...
Waiting for mon to start...
Waiting for mon...
mon is available
Setting mon public_network to 10.10.2.0/24
Wrote config to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf
Wrote keyring to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring
Creating mgr...
Verifying port 9283 ...
Waiting for mgr to start...
Waiting for mgr...
mgr not available, waiting (1/15)...
mgr not available, waiting (2/15)...
mgr is available
Enabling cephadm module...
Waiting for the mgr to restart...
Waiting for mgr epoch 5...
mgr epoch 5 is available
Setting orchestrator backend to cephadm...
Generating ssh key...
Wrote public SSH key to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub
Adding key to ljishen@localhost authorized_keys...
Adding host node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us...
Deploying unmanaged mon service...
Deploying unmanaged mgr service...
You can access the Ceph CLI with:

	sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --fsid 99fb4504-3805-11ec-b51d-53e6e728d2d3 -c /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf -k /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring

Please consider enabling telemetry to help improve Ceph:

	ceph telemetry on

For more information see:

	https://docs.ceph.com/docs/pacific/mgr/telemetry/

Bootstrap complete.

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:43:07,288982446-07:00] INFO: Sleep for 20 seconds[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:43:27,296915435-07:00] INFO: > Set the default number of monitor daemons to 1[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:337 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:43:27,307013178-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:43:27,310783335-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mon', '1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1
Inferring fsid 99fb4504-3805-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/99fb4504-3805-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Scheduled mon update...

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:43:36,516584171-07:00] INFO: > Set the default number of manager daemons to 1[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:342 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:43:36,526894563-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:43:36,531039946-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mgr', '1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1
Inferring fsid 99fb4504-3805-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/99fb4504-3805-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Scheduled mgr update...


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:43:45,409824176-07:00] STAGE: Add hosts to the cluster...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:43:45,416134599-07:00] INFO: > Adding host 'sm1'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:361 - add_hosts() > ssh-copy-id -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -f -i /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub ljishen@sm1
/usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: "/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub"

Number of key(s) added: 1

Now try logging into the machine, with:   "ssh -o 'GlobalKnownHostsFile=/dev/null' -o 'LogLevel=ERROR' -o 'PasswordAuthentication=no' -o 'StrictHostKeyChecking=no' -o 'UserKnownHostsFile=/dev/null' 'ljishen@sm1'"
and check to make sure that only the key(s) you wanted were added.

# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:367 - add_hosts() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add sm1 10.10.2.5
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:43:46,557334166-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:43:46,560945916-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'host', 'add', 'sm1', '10.10.2.5'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add sm1 10.10.2.5
Inferring fsid 99fb4504-3805-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/99fb4504-3805-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Added host 'sm1' with addr '10.10.2.5'

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:43:57,102380537-07:00] INFO: Sleep for 20 seconds[0m


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:44:17,107315144-07:00] STAGE: Add OSDs to the cluster...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:44:17,113777302-07:00] INFO: > Adding devices ['/dev/nvme0n1'] on host 'sm1' as OSDs[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:388 - add_host_devices_as_osds() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd sm1:/dev/nvme0n1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:44:17,124268665-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:44:17,128251743-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'daemon', 'add', 'osd', 'sm1:/dev/nvme0n1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd sm1:/dev/nvme0n1
Inferring fsid 99fb4504-3805-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/99fb4504-3805-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Created osd(s) 0 on host 'sm1'

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:44:41,630817456-07:00] INFO: > Sleep for 20 seconds[0m


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:45:01,637198566-07:00] STAGE: Show Cluster Status[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:408 - show_status() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:45:01,647413479-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:45:01,651043634-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', '--status'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status
Inferring fsid 99fb4504-3805-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/99fb4504-3805-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
  cluster:
    id:     99fb4504-3805-11ec-b51d-53e6e728d2d3
    health: HEALTH_WARN
            1 pool(s) have no replicas configured
 
  services:
    mon: 1 daemons, quorum node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us (age 2m)
    mgr: node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us.qqdurd(active, since 2m)
    osd: 1 osds: 1 up (since 19s), 1 in (since 40s)
 
  data:
    pools:   1 pools, 1 pgs
    objects: 0 objects, 0 B
    usage:   4.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     1 active+clean
 

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:45:10,211657593-07:00] INFO: > Copy configuration files to local[0m
# ./benchmarks/bench-rados:220 - launch_ceph_cluster() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.1 sudo chown --changes --recursive ljishen /tmp/bench-rados/deployment_data_root/etc/ceph
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:45:10,219560396-07:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.1 sudo chown --changes --recursive ljishen /tmp/bench-rados/deployment_data_root/etc/ceph
[1] 08:45:10 [SUCCESS] ljishen@10.10.2.1
# ./benchmarks/bench-rados:223 - launch_ceph_cluster() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.1:/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf
# ./benchmarks/bench-rados:226 - launch_ceph_cluster() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.1:/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:45:10,697004715-07:00] STAGE: Update local ceph.conf as a client...[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:45:10,700581976-07:00] STAGE: Configure Ceph pool...[0m
# ./benchmarks/bench-rados:257 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_max_backfills 32
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:45:10,722755090-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:45:10,725620931-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '99fb4504-3805-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'config', 'set', 'osd', 'osd_max_backfills', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 99fb4504-3805-11ec-b51d-53e6e728d2d3 -- ceph config set osd osd_max_backfills 32
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:258 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_active 32
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:45:14,518992764-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:45:14,521975305-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '99fb4504-3805-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_active', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 99fb4504-3805-11ec-b51d-53e6e728d2d3 -- ceph config set osd osd_recovery_max_active 32
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:259 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_single_start 8
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:45:18,253944087-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:45:18,256950032-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '99fb4504-3805-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_single_start', '8'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 99fb4504-3805-11ec-b51d-53e6e728d2d3 -- ceph config set osd osd_recovery_max_single_start 8
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:260 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_op_priority 63
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:45:22,020432371-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:45:22,023590061-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '99fb4504-3805-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_op_priority', '63'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 99fb4504-3805-11ec-b51d-53e6e728d2d3 -- ceph config set osd osd_recovery_op_priority 63
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:262 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config show-with-defaults osd.0
# ./benchmarks/bench-rados:263 - configure_ceph_pool() > grep 'osd_max_backfills\|osd_recovery'
# ./benchmarks/bench-rados:263 - configure_ceph_pool() > column -t -s ' '
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
osd_max_backfills                        32         mon
osd_recovery_cost                        20971520   default
osd_recovery_delay_start                 0.000000   default
osd_recovery_max_active                  32         mon
osd_recovery_max_active_hdd              3          default
osd_recovery_max_active_ssd              10         default
osd_recovery_max_chunk                   8388608    default
osd_recovery_max_omap_entries_per_chunk  8096       default
osd_recovery_max_single_start            8          mon
osd_recovery_op_priority                 63         mon
osd_recovery_op_warn_multiple            16         default
osd_recovery_priority                    5          default
osd_recovery_retry_interval              30.000000  default
osd_recovery_sleep                       0.000000   default
osd_recovery_sleep_hdd                   0.100000   default
osd_recovery_sleep_hybrid                0.025000   default
osd_recovery_sleep_ssd                   0.000000   default
# ./benchmarks/bench-rados:265 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool create bench_rados 128 128 replicated --size 1 --pg-num-min 128
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:45:29,699964614-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:45:29,702961311-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '99fb4504-3805-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'create', 'bench_rados', '128', '128', 'replicated', '--size', '1', '--pg-num-min', '128'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 99fb4504-3805-11ec-b51d-53e6e728d2d3 -- ceph osd pool create bench_rados 128 128 replicated --size 1 --pg-num-min 128
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
pool 'bench_rados' created
# ./benchmarks/bench-rados:267 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados min_size 1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:45:33,732590880-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:45:33,735717652-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '99fb4504-3805-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'min_size', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 99fb4504-3805-11ec-b51d-53e6e728d2d3 -- ceph osd pool set bench_rados min_size 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 min_size to 1
# ./benchmarks/bench-rados:268 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados pg_autoscale_mode off
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:45:38,216086047-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:45:38,219190687-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '99fb4504-3805-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'pg_autoscale_mode', 'off'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 99fb4504-3805-11ec-b51d-53e6e728d2d3 -- ceph osd pool set bench_rados pg_autoscale_mode off
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 pg_autoscale_mode to off
# ./benchmarks/bench-rados:269 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool application enable bench_rados benchmark
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:45:42,032845324-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:45:42,036007683-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '99fb4504-3805-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'application', 'enable', 'bench_rados', 'benchmark'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 99fb4504-3805-11ec-b51d-53e6e728d2d3 -- ceph osd pool application enable bench_rados benchmark
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
enabled application 'benchmark' on pool 'bench_rados'
# ./benchmarks/bench-rados:270 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados noscrub 1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:45:45,918008695-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:45:45,921075223-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '99fb4504-3805-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'noscrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 99fb4504-3805-11ec-b51d-53e6e728d2d3 -- ceph osd pool set bench_rados noscrub 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 noscrub to 1
# ./benchmarks/bench-rados:271 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados nodeep-scrub 1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:45:49,845985315-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:45:49,849001038-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '99fb4504-3805-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'nodeep-scrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 99fb4504-3805-11ec-b51d-53e6e728d2d3 -- ceph osd pool set bench_rados nodeep-scrub 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 nodeep-scrub to 1
# ./benchmarks/bench-rados:272 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool ls detail
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:45:54,267746839-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:45:54,270763072-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '99fb4504-3805-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'ls', 'detail'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 99fb4504-3805-11ec-b51d-53e6e728d2d3 -- ceph osd pool ls detail
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
pool 1 'device_health_metrics' replicated size 1 min_size 1 crush_rule 0 object_hash rjenkins pg_num 64 pgp_num 64 autoscale_mode on last_change 19 lfor 0/0/17 flags hashpspool stripe_width 0 pg_num_min 1 application mgr_devicehealth
pool 2 'bench_rados' replicated size 1 min_size 1 crush_rule 0 object_hash rjenkins pg_num 128 pgp_num 128 autoscale_mode off last_change 20 flags hashpspool,noscrub,nodeep-scrub stripe_width 0 pg_num_min 128 application benchmark

# ./benchmarks/bench-rados:273 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd df tree
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:45:58,028532608-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:45:58,031592293-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '99fb4504-3805-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'df', 'tree'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 99fb4504-3805-11ec-b51d-53e6e728d2d3 -- ceph osd df tree
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
ID  CLASS  WEIGHT   REWEIGHT  SIZE     RAW USE  DATA     OMAP  META     AVAIL    %USE  VAR   PGS  STATUS  TYPE NAME    
-1         0.09769         -  100 GiB  5.7 MiB  168 KiB   0 B  5.6 MiB  100 GiB  0.01  1.00    -          root default 
-3         0.09769         -  100 GiB  5.7 MiB  168 KiB   0 B  5.6 MiB  100 GiB  0.01  1.00    -              host sm1 
 0    ssd  0.09769   1.00000  100 GiB  5.7 MiB  168 KiB   0 B  5.6 MiB  100 GiB  0.01  1.00  192      up          osd.0
                       TOTAL  100 GiB  5.7 MiB  168 KiB   0 B  5.6 MiB  100 GiB  0.01                                  
MIN/MAX VAR: 1.00/1.00  STDDEV: 0


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:46:01,729839802-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:46:25,652079012-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:46:34,543192474-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:46:43,475983366-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:46:52,144656912-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:46:52,152311015-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:47:00,934846350-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:47:00,942328399-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:47:09,832874696-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:47:09,840918143-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:47:18,690952227-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:47:18,698874025-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:47:27,664912504-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:47:27,672449477-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:47:27,678677984-07:00] INFO: > The cluster is idle now.[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:47:27,682430095-07:00] STAGE: Start benchmarking of mode write ...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:47:27,689523652-07:00] INFO: > Start system activity collection services[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:47:27,695190321-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:341 - rados_bench() > rm --force /mnt/sda8/ceph-research/scripts/output/sys_activity_1MB_write.dat.rados_bench_host.1
# ./benchmarks/bench-rados:342 - rados_bench() > sar_pid_rados_bench_host=460837
# ./benchmarks/bench-rados:342 - rados_bench() > S_TIME_FORMAT=ISO
# ./benchmarks/bench-rados:342 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/output/sys_activity_1MB_write.dat.rados_bench_host.1 2

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:47:27,702230407-07:00] INFO: >> for OSD host[0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:47:27,711455362-07:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.5 bash
10.10.2.5: b'152364\n'
[1] 08:47:28 [SUCCESS] ljishen@10.10.2.5
152364

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:47:28,831760250-07:00] INFO: > Run rados bench[0m
# ./benchmarks/bench-rados:370 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/output/rados_bench_1MB_write.log.1
## ./benchmarks/bench-rados:361 - rados_bench() > count_cpus 32-63,96-127
# ./benchmarks/bench-rados:361 - rados_bench() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 60 write --pool bench_rados -b 1048576 -O 1048576 --concurrent-ios 128 --show-time --write-object --write-omap --write-xattr --no-cleanup
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:47:28,851893650-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:47:28,854745684-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '99fb4504-3805-11ec-b51d-53e6e728d2d3', '--', 'rados', 'bench', '60', 'write', '--pool', 'bench_rados', '-b', '1048576', '-O', '1048576', '--concurrent-ios', '128', '--show-time', '--write-object', '--write-omap', '--write-xattr', '--no-cleanup'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 99fb4504-3805-11ec-b51d-53e6e728d2d3 -- rados bench 60 write --pool bench_rados -b 1048576 -O 1048576 --concurrent-ios 128 --show-time --write-object --write-omap --write-xattr --no-cleanup
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
hints = 1
2021-10-28T15:47:31.789082+0000 Maintaining 128 concurrent writes of 1048576 bytes to objects of size 1048576 for up to 60 seconds or 0 objects
2021-10-28T15:47:31.789094+0000 Object prefix: benchmark_data_node-2.bluefield2.ucsc-cmps10_7
2021-10-28T15:47:31.821599+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-28T15:47:31.821599+0000     0       0         0         0         0         0           -           0
2021-10-28T15:47:32.821710+0000     1      85        85         0         0         0           -           0
2021-10-28T15:47:33.821788+0000     2     127       144        17   8.49945       8.5     1.83864     1.74049
2021-10-28T15:47:34.821861+0000     3     127       218        91   30.3313        74     1.81789     1.93786
2021-10-28T15:47:35.821931+0000     4     127       286       159   39.7473        68     1.81729     1.92539
2021-10-28T15:47:36.821999+0000     5     127       345       218    43.597        59     1.86983     1.93048
2021-10-28T15:47:37.822072+0000     6     127       417       290     48.33        72     1.73549     1.93548
2021-10-28T15:47:38.822149+0000     7     127       472       345   49.2823        55     1.87554     1.93361
2021-10-28T15:47:39.822226+0000     8     127       543       416   51.9963        71     1.94426     1.93136
2021-10-28T15:47:40.822309+0000     9     127       573       446    49.552        30     2.41921      1.9548
2021-10-28T15:47:41.822378+0000    10     127       605       478   47.7966        32     2.80223     2.01839
2021-10-28T15:47:42.822455+0000    11     127       632       505   45.9058        27     3.45683     2.09427
2021-10-28T15:47:43.822530+0000    12     127       664       537   44.7467        32     4.03674     2.20623
2021-10-28T15:47:44.822606+0000    13     127       693       566   43.5353        29     3.98186     2.31451
2021-10-28T15:47:45.822679+0000    14     127       726       599   42.7826        33     4.15069     2.42319
2021-10-28T15:47:46.822755+0000    15     127       753       626   41.7303        27     4.18015     2.50595
2021-10-28T15:47:47.822832+0000    16     127       808       681   42.5594        55     3.13466     2.61379
2021-10-28T15:47:48.822907+0000    17     127       882       755   44.4085        74     1.62904     2.62054
2021-10-28T15:47:49.822985+0000    18     127       940       813   45.1633        58     1.95958     2.57135
2021-10-28T15:47:50.823052+0000    19     127      1014       887   46.6808        74     1.81471     2.51864
2021-10-28T15:47:51.823121+0000 min lat: 1.62904 max lat: 4.92616 avg lat: 2.49154
2021-10-28T15:47:51.823121+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-28T15:47:51.823121+0000    20     127      1062       935   46.7466        48     2.03605     2.49154
2021-10-28T15:47:52.823195+0000    21     127      1092       965    45.949        30     2.61294     2.49448
2021-10-28T15:47:53.823270+0000    22     127      1099       972   44.1786         7     2.94719     2.49798
2021-10-28T15:47:54.823353+0000    23     127      1121       994   43.2142        22     3.80612     2.52868
2021-10-28T15:47:55.823432+0000    24     127      1133      1006   41.9136        12       4.201     2.54965
2021-10-28T15:47:56.823504+0000    25     127      1153      1026    41.037        20     5.03852     2.60157
2021-10-28T15:47:57.823582+0000    26     127      1166      1039   39.9586        13     5.60102     2.64043
2021-10-28T15:47:58.823652+0000    27     127      1199      1072   39.7008        33     6.06935     2.76197
2021-10-28T15:47:59.823729+0000    28     127      1199      1072   38.2829         0           -     2.76197
2021-10-28T15:48:00.823807+0000    29     127      1232      1105   38.1006      16.5     6.28755     2.89182
2021-10-28T15:48:01.823880+0000    30     127      1252      1125   37.4972        20     6.72036     2.97023
2021-10-28T15:48:02.823956+0000    31     127      1283      1156   37.2876        31     5.95255     3.07176
2021-10-28T15:48:03.824025+0000    32     127      1315      1188   37.1223        32     5.09456     3.14799
2021-10-28T15:48:04.824107+0000    33     127      1340      1213   36.7548        25     4.61948     3.18556
2021-10-28T15:48:05.824185+0000    34     127      1381      1254   36.8796        41     3.10785     3.21435
2021-10-28T15:48:06.824263+0000    35     127      1425      1298    37.083        44     2.87894     3.22056
2021-10-28T15:48:07.824343+0000    36     127      1465      1338   37.1639        40      3.1179     3.22302
2021-10-28T15:48:08.824420+0000    37     127      1499      1372   37.0783        34     3.18635     3.22566
2021-10-28T15:48:09.824490+0000    38     127      1546      1419   37.3393        47     3.22058     3.22872
2021-10-28T15:48:10.824560+0000    39     127      1563      1436   36.8178        17     3.37942     3.23066
2021-10-28T15:48:11.824629+0000 min lat: 1.62904 max lat: 7.92128 avg lat: 3.24074
2021-10-28T15:48:11.824629+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-28T15:48:11.824629+0000    40     127      1586      1459   36.4723        23     3.67991     3.24074
2021-10-28T15:48:12.824732+0000    41     127      1596      1469   35.8266        10     4.00576     3.24721
2021-10-28T15:48:13.824798+0000    42     127      1630      1503    35.783        34     4.89061     3.28498
2021-10-28T15:48:14.824875+0000    43     127      1630      1503   34.9509         0           -     3.28498
2021-10-28T15:48:15.824959+0000    44     127      1663      1536   34.9065      16.5     5.86093     3.34186
2021-10-28T15:48:16.825042+0000    45     127      1693      1566   34.7974        30     5.48746     3.40026
2021-10-28T15:48:17.825116+0000    46     127      1723      1596    34.693        30     5.86574     3.44993
2021-10-28T15:48:18.825188+0000    47     127      1753      1626   34.5931        30      4.9774     3.48511
2021-10-28T15:48:19.825271+0000    48     127      1784      1657   34.5182        31     4.09608     3.50773
2021-10-28T15:48:20.825346+0000    49     127      1842      1715   34.9974        58     2.93167     3.50957
2021-10-28T15:48:21.825423+0000    50     127      1902      1775   35.4973        60     1.88168      3.4742
2021-10-28T15:48:22.825501+0000    51     127      1972      1845   36.1737        70     1.76214     3.41924
2021-10-28T15:48:23.825575+0000    52     127      2046      1919   36.9011        74     1.97666     3.36343
2021-10-28T15:48:24.825655+0000    53     127      2088      1961   36.9972        42     1.92543     3.33474
2021-10-28T15:48:25.825727+0000    54     127      2119      1992   36.8861        31     2.56841      3.3246
2021-10-28T15:48:26.825802+0000    55     127      2129      2002   36.3973        10     3.16637     3.32385
2021-10-28T15:48:27.825895+0000    56     127      2149      2022   36.1044        20      3.8352     3.33095
2021-10-28T15:48:28.825991+0000    57     127      2162      2035    35.699        13     4.47962     3.33837
2021-10-28T15:48:29.826092+0000    58     127      2182      2055   35.4283        20     5.19316     3.35814
2021-10-28T15:48:30.826208+0000    59     127      2195      2068   35.0481        13     5.88983     3.37439
2021-10-28T15:48:31.826278+0000 min lat: 1.62904 max lat: 7.92128 avg lat: 3.40156
2021-10-28T15:48:31.826278+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-28T15:48:31.826278+0000    60     127      2213      2086    34.764        18     6.50188     3.40156
2021-10-28T15:48:32.826401+0000    61      20      2214      2194   35.9644       108      1.8235     3.45456
2021-10-28T15:48:33.826541+0000 Total time run:         61.0875
Total writes made:      2214
Write size:             1048576
Object size:            1048576
Bandwidth (MB/sec):     36.2431
Stddev Bandwidth:       22.8498
Max bandwidth (MB/sec): 108
Min bandwidth (MB/sec): 0
Average IOPS:           36
Stddev IOPS:            22.8735
Max IOPS:               108
Min IOPS:               0
Average Latency(s):     3.44056
Stddev Latency(s):      1.60494
Max latency(s):         7.92128
Min latency(s):         1.62904

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:48:34,425899965-07:00] INFO: > Stop system activity collection services[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:48:34,431915982-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:391 - rados_bench() > kill -INT 460837

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:48:34,437618327-07:00] INFO: >> for OSD host[0m
# ./benchmarks/bench-rados:396 - rados_bench() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.5 kill -INT 152364
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:48:34,445175509-07:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.5 kill -INT 152364
[1] 08:48:36 [SUCCESS] ljishen@10.10.2.5

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:48:36,213368340-07:00] INFO: >>> Copy system activity file from OSD host[0m
## ./benchmarks/bench-rados:403 - rados_bench() > basename -- /tmp/bench-rados/sys_activity_1MB_write.dat.osd_host.1
# ./benchmarks/bench-rados:403 - rados_bench() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.5:/tmp/bench-rados/sys_activity_1MB_write.dat.osd_host.1 /mnt/sda8/ceph-research/scripts/output/sys_activity_1MB_write.dat.osd_host.1


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:48:37,520659286-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:49:01,262513056-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 2.21k objects, 2.2 GiB
    usage:   6.5 GiB used, 93 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:49:01,270200503-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:49:10,113644598-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 2.21k objects, 2.2 GiB
    usage:   6.5 GiB used, 93 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:49:10,121381819-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:49:18,852527273-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 2.21k objects, 2.2 GiB
    usage:   6.5 GiB used, 93 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:49:18,860593162-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:49:27,740305174-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 2.21k objects, 2.2 GiB
    usage:   6.5 GiB used, 93 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:49:27,748020523-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:49:36,559339603-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 2.21k objects, 2.2 GiB
    usage:   6.5 GiB used, 93 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:49:36,567286218-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:49:36,573401251-07:00] INFO: > The cluster is idle now.[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:49:36,577010242-07:00] STAGE: Start benchmarking of mode seq ...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:49:36,584145338-07:00] INFO: > Start system activity collection services[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:49:36,589837534-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:341 - rados_bench() > rm --force /mnt/sda8/ceph-research/scripts/output/sys_activity_1MB_seq.dat.rados_bench_host.1
# ./benchmarks/bench-rados:342 - rados_bench() > sar_pid_rados_bench_host=462175
# ./benchmarks/bench-rados:342 - rados_bench() > S_TIME_FORMAT=ISO
# ./benchmarks/bench-rados:342 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/output/sys_activity_1MB_seq.dat.rados_bench_host.1 2

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:49:36,596805726-07:00] INFO: >> for OSD host[0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:49:36,605916474-07:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.5 bash
10.10.2.5: b'153044\n'
[1] 08:49:37 [SUCCESS] ljishen@10.10.2.5
153044

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:49:37,909823473-07:00] INFO: > Run rados bench[0m
# ./benchmarks/bench-rados:379 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/output/rados_bench_1MB_seq.log.1
## ./benchmarks/bench-rados:376 - rados_bench() > count_cpus 32-63,96-127
# ./benchmarks/bench-rados:376 - rados_bench() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 128 --show-time
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:49:37,929712323-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:49:37,932626164-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '99fb4504-3805-11ec-b51d-53e6e728d2d3', '--', 'rados', 'bench', '99999999', 'seq', '--pool', 'bench_rados', '--concurrent-ios', '128', '--show-time'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 99fb4504-3805-11ec-b51d-53e6e728d2d3 -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 128 --show-time
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
hints = 1
2021-10-28T15:49:40.883520+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-28T15:49:40.883520+0000     0       0         0         0         0         0           -           0
2021-10-28T15:49:41.883658+0000     1     127       659       532   531.891       532    0.228446    0.209107
2021-10-28T15:49:42.883772+0000     2     127      1243      1116   557.911       584     0.22541    0.214029
2021-10-28T15:49:43.883885+0000     3     127      1850      1723   574.251       607    0.229835    0.213896
2021-10-28T15:49:44.884014+0000 Total time run:       3.78443
Total reads made:     2214
Read size:            1048576
Object size:          1048576
Bandwidth (MB/sec):   585.028
Average IOPS:         585
Stddev IOPS:          38.4231
Max IOPS:             607
Min IOPS:             532
Average Latency(s):   0.212418
Max latency(s):       0.387902
Min latency(s):       0.0919281

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:49:45,570526093-07:00] INFO: > Stop system activity collection services[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:49:45,576902249-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:391 - rados_bench() > kill -INT 462175

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:49:45,582925399-07:00] INFO: >> for OSD host[0m
# ./benchmarks/bench-rados:396 - rados_bench() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.5 kill -INT 153044
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:49:45,590942567-07:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.5 kill -INT 153044
[1] 08:49:46 [SUCCESS] ljishen@10.10.2.5

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:49:46,704289705-07:00] INFO: >>> Copy system activity file from OSD host[0m
## ./benchmarks/bench-rados:403 - rados_bench() > basename -- /tmp/bench-rados/sys_activity_1MB_seq.dat.osd_host.1
# ./benchmarks/bench-rados:403 - rados_bench() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.5:/tmp/bench-rados/sys_activity_1MB_seq.dat.osd_host.1 /mnt/sda8/ceph-research/scripts/output/sys_activity_1MB_seq.dat.osd_host.1


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:49:47,764159127-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:50:11,641581838-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 2.21k objects, 2.2 GiB
    usage:   6.5 GiB used, 93 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:50:11,649051384-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:50:20,562985861-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 2.21k objects, 2.2 GiB
    usage:   6.5 GiB used, 93 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:50:20,570933037-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:50:29,505606155-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 2.21k objects, 2.2 GiB
    usage:   6.5 GiB used, 93 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:50:29,513619246-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:50:38,334881297-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 2.21k objects, 2.2 GiB
    usage:   6.5 GiB used, 93 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:50:38,342675034-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:50:47,083458288-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 2.21k objects, 2.2 GiB
    usage:   6.5 GiB used, 93 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:50:47,091120307-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:50:47,097291627-07:00] INFO: > The cluster is idle now.[0m


[1;7;39;49m[2021-10-28T08:50:47,099666351-07:00][RUNNING][ROUND 2/5/21] object_size=1MB[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:50:47,103269552-07:00] STAGE: Launch a Ceph cluster (CEPH_HOST: 10.10.2.1, OSD_HOST: 10.10.2.5) ...[0m
# ./benchmarks/bench-rados:183 - launch_ceph_cluster() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.1 bash -euo pipefail
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:50:47,112861848-07:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.1 bash -euo pipefail
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T08:50:47,519326666-07:00] INFO: > Remove existing clusters\x1b[0m\n'
10.10.2.1: b'## bash:17 -  > basename -- /var/lib/ceph/99fb4504-3805-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'# bash:17 -  > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm --host 10.10.2.1 --host 10.10.2.5 rm-cluster --force --fsid 99fb4504-3805-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T08:50:47,530263105-07:00] DEBUG: HOST_PARAMS: ['--host', '10.10.2.1', '--host', '10.10.2.5']\x1b[0m\n"
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T08:50:47,534001763-07:00] DEBUG: CEPHADM_PARAMS: ['rm-cluster', '--force', '--fsid', '99fb4504-3805-11ec-b51d-53e6e728d2d3']\x1b[0m\n"
10.10.2.1: b"# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:63 -  > /tmp/bench-rados/ceph-research/scripts/parallel-ssh --host 10.10.2.1 --host 10.10.2.5 'sudo python3 -u - rm-cluster --force --fsid 99fb4504-3805-11ec-b51d-53e6e728d2d3'\n"
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T08:50:47,542929046-07:00] DEBUG: command from stdin\x1b[0m\n'
10.10.2.1: b"# /tmp/bench-rados/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host 10.10.2.1 --host 10.10.2.5 'sudo python3 -u - rm-cluster --force --fsid 99fb4504-3805-11ec-b51d-53e6e728d2d3'\n"
10.10.2.1: b'[1] 08:50:53 [SUCCESS] 10.10.2.1\n[2] 08:50:59 [SUCCESS] 10.10.2.5\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T08:50:59,144397505-07:00] INFO: > Deploy a new cluster\x1b[0m\n'
10.10.2.1: b'# bash:24 -  > /tmp/bench-rados/ceph-research/scripts/ceph-deploy -m 10.10.2.1 -o sm1/10.10.2.5:/dev/nvme0n1\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T08:50:59,156194442-07:00] STAGE: Verify passwordless SSH to hosts, passwordless sudo via ljishen, and the existence of devices on hosts...\x1b[0m\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T08:50:59,160689202-07:00] INFO: > Check the host for the monitor\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:181 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@10.10.2.1 sudo --non-interactive --validate\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T08:50:59,311227198-07:00] INFO: > Check hosts for OSDs\x1b[0m\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T08:50:59,315952682-07:00] INFO: >> Check host 'sm1'\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:200 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 sudo --non-interactive --validate\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T08:51:00,386841266-07:00] INFO: >>> Check device '/dev/nvme0n1'\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:215 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 test -b /dev/nvme0n1\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:226 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 lsblk --nodeps --noheadings --output TYPE /dev/nvme0n1\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:227 - verify_parameters() > grep --word-regexp disk\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T08:51:02,545627829-07:00] STAGE: Reset devices on hosts...\x1b[0m\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T08:51:02,550502844-07:00] INFO: > Resetting device '/dev/nvme0n1' on host 'sm1'\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:288 - reset_host_devices() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 sudo sh\n'
10.10.2.1: b'  Removing ceph--08bbfda3--6dbf--4d35--855c--10060893aea7-osd--block--a4f48825--480d--4afa--bb32--7128b54a00b8 (252:0)\n'
10.10.2.1: b'  Archiving volume group "ceph-08bbfda3-6dbf-4d35-855c-10060893aea7" metadata (seqno 5).\n'
10.10.2.1: b'  Releasing logical volume "osd-block-a4f48825-480d-4afa-bb32-7128b54a00b8"\n'
10.10.2.1: b'  Creating volume group backup "/etc/lvm/backup/ceph-08bbfda3-6dbf-4d35-855c-10060893aea7" (seqno 6).\n'
10.10.2.1: b'  Logical volume "osd-block-a4f48825-480d-4afa-bb32-7128b54a00b8" successfully removed\n  Volume group "ceph-08bbfda3-6dbf-4d35-855c-10060893aea7" successfully removed\n  Removing physical volume "/dev/nvme0n1" from volume group "ceph-08bbfda3-6dbf-4d35-855c-10060893aea7"\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T08:51:04,944745083-07:00] STAGE: Bootstrap a new cluster...\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:317 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.1 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T08:51:04,955253758-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T08:51:04,958939727-07:00] DEBUG: CEPHADM_PARAMS: ['bootstrap', '--config', '/tmp/bench-rados/ceph-research/scripts/ceph.conf', '--mon-ip', '10.10.2.1', '--output-dir', '/tmp/bench-rados/deployment_data_root/etc/ceph', '--ssh-user', 'ljishen', '--skip-dashboard', '--no-minimize-config', '--allow-overwrite', '--allow-fqdn-hostname', '--orphan-initial-daemons', '--skip-monitoring-stack', '--single-host-defaults']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.1 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults\n'
10.10.2.1: b'Verifying podman|docker is present...\nVerifying lvm2 is present...\n'
10.10.2.1: b'Verifying time synchronization is in place...\n'
10.10.2.1: b'Unit ntp.service is enabled and running\n'
10.10.2.1: b'Repeating the final host check...\n'
10.10.2.1: b'podman|docker (/usr/bin/docker) is present\n'
10.10.2.1: b'systemctl is present\n'
10.10.2.1: b'lvcreate is present\n'
10.10.2.1: b'Unit ntp.service is enabled and running\n'
10.10.2.1: b'Host looks OK\n'
10.10.2.1: b'Cluster fsid: dbe3f41a-3806-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Verifying IP 10.10.2.1 port 3300 ...\n'
10.10.2.1: b'Verifying IP 10.10.2.1 port 6789 ...\n'
10.10.2.1: b'Mon IP `10.10.2.1` is in CIDR network `10.10.2.0/24`\n'
10.10.2.1: b'- internal network (--cluster-network) has not been provided, OSD replication will default to the public_network\n'
10.10.2.1: b'Adjusting default settings to suit single-host cluster...\n'
10.10.2.1: b'Pulling container image quay.io/ceph/ceph:v16...\n'
10.10.2.1: b'Ceph version: ceph version 16.2.6 (ee28fb57e47e9f88813e24bbf4c14496ca299d31) pacific (stable)\n'
10.10.2.1: b'Extracting ceph user uid/gid from container image...\n'
10.10.2.1: b'Creating initial keys...\n'
10.10.2.1: b'Creating initial monmap...\n'
10.10.2.1: b'Creating mon...\n'
10.10.2.1: b'Waiting for mon to start...\n'
10.10.2.1: b'Waiting for mon...\n'
10.10.2.1: b'mon is available\n'
10.10.2.1: b'Setting mon public_network to 10.10.2.0/24\n'
10.10.2.1: b'Wrote config to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf\n'
10.10.2.1: b'Wrote keyring to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring\n'
10.10.2.1: b'Creating mgr...\n'
10.10.2.1: b'Verifying port 9283 ...\n'
10.10.2.1: b'Waiting for mgr to start...\nWaiting for mgr...\n'
10.10.2.1: b'mgr not available, waiting (1/15)...\n'
10.10.2.1: b'mgr not available, waiting (2/15)...\n'
10.10.2.1: b'mgr is available\n'
10.10.2.1: b'Enabling cephadm module...\n'
10.10.2.1: b'Waiting for the mgr to restart...\nWaiting for mgr epoch 5...\n'
10.10.2.1: b'mgr epoch 5 is available\nSetting orchestrator backend to cephadm...\n'
10.10.2.1: b'Generating ssh key...\n'
10.10.2.1: b'Wrote public SSH key to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub\nAdding key to ljishen@localhost authorized_keys...\n'
10.10.2.1: b'Adding host node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us...\n'
10.10.2.1: b'Deploying unmanaged mon service...\n'
10.10.2.1: b'Deploying unmanaged mgr service...\n'
10.10.2.1: b'You can access the Ceph CLI with:\n\n\tsudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --fsid dbe3f41a-3806-11ec-b51d-53e6e728d2d3 -c /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf -k /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring\n\nPlease consider enabling telemetry to help improve Ceph:\n\n\tceph telemetry on\n\nFor more information see:\n\n\thttps://docs.ceph.com/docs/pacific/mgr/telemetry/\n\nBootstrap complete.\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T08:52:05,732314528-07:00] INFO: Sleep for 20 seconds\x1b[0m\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T08:52:25,739808974-07:00] INFO: > Set the default number of monitor daemons to 1\x1b[0m\n# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:337 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T08:52:25,749608336-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T08:52:25,753180591-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mon', '1']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1\n'
10.10.2.1: b'Inferring fsid dbe3f41a-3806-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/dbe3f41a-3806-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b'Scheduled mon update...\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T08:52:34,823470805-07:00] INFO: > Set the default number of manager daemons to 1\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:342 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T08:52:34,834180178-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T08:52:34,837842062-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mgr', '1']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1\n'
10.10.2.1: b'Inferring fsid dbe3f41a-3806-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/dbe3f41a-3806-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b'Scheduled mgr update...\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T08:52:44,040690943-07:00] STAGE: Add hosts to the cluster...\x1b[0m\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T08:52:44,046990335-07:00] INFO: > Adding host 'sm1'\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:361 - add_hosts() > ssh-copy-id -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -f -i /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub ljishen@sm1\n'
10.10.2.1: b'/usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: "/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub"\n'
10.10.2.1: b'\nNumber of key(s) added: 1\n\nNow try logging into the machine, with:   "ssh -o \'GlobalKnownHostsFile=/dev/null\' -o \'LogLevel=ERROR\' -o \'PasswordAuthentication=no\' -o \'StrictHostKeyChecking=no\' -o \'UserKnownHostsFile=/dev/null\' \'ljishen@sm1\'"\nand check to make sure that only the key(s) you wanted were added.\n\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:367 - add_hosts() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add sm1 10.10.2.5\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T08:52:45,198093995-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T08:52:45,201371887-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'host', 'add', 'sm1', '10.10.2.5']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add sm1 10.10.2.5\n'
10.10.2.1: b'Inferring fsid dbe3f41a-3806-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/dbe3f41a-3806-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b"Added host 'sm1' with addr '10.10.2.5'\n"
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T08:52:56,549995470-07:00] INFO: Sleep for 20 seconds\x1b[0m\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T08:53:16,555154952-07:00] STAGE: Add OSDs to the cluster...\x1b[0m\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T08:53:16,561774987-07:00] INFO: > Adding devices ['/dev/nvme0n1'] on host 'sm1' as OSDs\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:388 - add_host_devices_as_osds() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd sm1:/dev/nvme0n1\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T08:53:16,571852131-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T08:53:16,575492484-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'daemon', 'add', 'osd', 'sm1:/dev/nvme0n1']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd sm1:/dev/nvme0n1\n'
10.10.2.1: b'Inferring fsid dbe3f41a-3806-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/dbe3f41a-3806-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b"Created osd(s) 0 on host 'sm1'\n"
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T08:53:41,153055919-07:00] INFO: > Sleep for 20 seconds\x1b[0m\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T08:54:01,158646545-07:00] STAGE: Show Cluster Status\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:408 - show_status() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T08:54:01,168865787-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T08:54:01,172485932-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', '--status']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status\n'
10.10.2.1: b'Inferring fsid dbe3f41a-3806-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/dbe3f41a-3806-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b'  cluster:\n    id:     dbe3f41a-3806-11ec-b51d-53e6e728d2d3\n    health: HEALTH_WARN\n            1 pool(s) have no replicas configured\n \n  services:\n    mon: 1 daemons, quorum node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us (age 2m)\n    mgr: node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us.lmbcob(active, since 2m)\n    osd: 1 osds: 1 up (since 19s), 1 in (since 40s)\n \n  data:\n    pools:   1 pools, 1 pgs\n    objects: 0 objects, 0 B\n    usage:   4.7 MiB used, 100 GiB / 100 GiB avail\n    pgs:     1 active+clean\n \n'
[1] 08:54:09 [SUCCESS] ljishen@10.10.2.1

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:50:47,519326666-07:00] INFO: > Remove existing clusters[0m
## bash:17 -  > basename -- /var/lib/ceph/99fb4504-3805-11ec-b51d-53e6e728d2d3
# bash:17 -  > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm --host 10.10.2.1 --host 10.10.2.5 rm-cluster --force --fsid 99fb4504-3805-11ec-b51d-53e6e728d2d3
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:50:47,530263105-07:00] DEBUG: HOST_PARAMS: ['--host', '10.10.2.1', '--host', '10.10.2.5'][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:50:47,534001763-07:00] DEBUG: CEPHADM_PARAMS: ['rm-cluster', '--force', '--fsid', '99fb4504-3805-11ec-b51d-53e6e728d2d3'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:63 -  > /tmp/bench-rados/ceph-research/scripts/parallel-ssh --host 10.10.2.1 --host 10.10.2.5 'sudo python3 -u - rm-cluster --force --fsid 99fb4504-3805-11ec-b51d-53e6e728d2d3'
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:50:47,542929046-07:00] DEBUG: command from stdin[0m
# /tmp/bench-rados/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host 10.10.2.1 --host 10.10.2.5 'sudo python3 -u - rm-cluster --force --fsid 99fb4504-3805-11ec-b51d-53e6e728d2d3'
[1] 08:50:53 [SUCCESS] 10.10.2.1
[2] 08:50:59 [SUCCESS] 10.10.2.5

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:50:59,144397505-07:00] INFO: > Deploy a new cluster[0m
# bash:24 -  > /tmp/bench-rados/ceph-research/scripts/ceph-deploy -m 10.10.2.1 -o sm1/10.10.2.5:/dev/nvme0n1


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:50:59,156194442-07:00] STAGE: Verify passwordless SSH to hosts, passwordless sudo via ljishen, and the existence of devices on hosts...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:50:59,160689202-07:00] INFO: > Check the host for the monitor[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:181 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@10.10.2.1 sudo --non-interactive --validate

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:50:59,311227198-07:00] INFO: > Check hosts for OSDs[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:50:59,315952682-07:00] INFO: >> Check host 'sm1'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:200 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 sudo --non-interactive --validate

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:51:00,386841266-07:00] INFO: >>> Check device '/dev/nvme0n1'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:215 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 test -b /dev/nvme0n1
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:226 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 lsblk --nodeps --noheadings --output TYPE /dev/nvme0n1
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:227 - verify_parameters() > grep --word-regexp disk


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:51:02,545627829-07:00] STAGE: Reset devices on hosts...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:51:02,550502844-07:00] INFO: > Resetting device '/dev/nvme0n1' on host 'sm1'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:288 - reset_host_devices() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 sudo sh
  Removing ceph--08bbfda3--6dbf--4d35--855c--10060893aea7-osd--block--a4f48825--480d--4afa--bb32--7128b54a00b8 (252:0)
  Archiving volume group "ceph-08bbfda3-6dbf-4d35-855c-10060893aea7" metadata (seqno 5).
  Releasing logical volume "osd-block-a4f48825-480d-4afa-bb32-7128b54a00b8"
  Creating volume group backup "/etc/lvm/backup/ceph-08bbfda3-6dbf-4d35-855c-10060893aea7" (seqno 6).
  Logical volume "osd-block-a4f48825-480d-4afa-bb32-7128b54a00b8" successfully removed
  Volume group "ceph-08bbfda3-6dbf-4d35-855c-10060893aea7" successfully removed
  Removing physical volume "/dev/nvme0n1" from volume group "ceph-08bbfda3-6dbf-4d35-855c-10060893aea7"


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:51:04,944745083-07:00] STAGE: Bootstrap a new cluster...[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:317 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.1 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:51:04,955253758-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:51:04,958939727-07:00] DEBUG: CEPHADM_PARAMS: ['bootstrap', '--config', '/tmp/bench-rados/ceph-research/scripts/ceph.conf', '--mon-ip', '10.10.2.1', '--output-dir', '/tmp/bench-rados/deployment_data_root/etc/ceph', '--ssh-user', 'ljishen', '--skip-dashboard', '--no-minimize-config', '--allow-overwrite', '--allow-fqdn-hostname', '--orphan-initial-daemons', '--skip-monitoring-stack', '--single-host-defaults'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.1 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults
Verifying podman|docker is present...
Verifying lvm2 is present...
Verifying time synchronization is in place...
Unit ntp.service is enabled and running
Repeating the final host check...
podman|docker (/usr/bin/docker) is present
systemctl is present
lvcreate is present
Unit ntp.service is enabled and running
Host looks OK
Cluster fsid: dbe3f41a-3806-11ec-b51d-53e6e728d2d3
Verifying IP 10.10.2.1 port 3300 ...
Verifying IP 10.10.2.1 port 6789 ...
Mon IP `10.10.2.1` is in CIDR network `10.10.2.0/24`
- internal network (--cluster-network) has not been provided, OSD replication will default to the public_network
Adjusting default settings to suit single-host cluster...
Pulling container image quay.io/ceph/ceph:v16...
Ceph version: ceph version 16.2.6 (ee28fb57e47e9f88813e24bbf4c14496ca299d31) pacific (stable)
Extracting ceph user uid/gid from container image...
Creating initial keys...
Creating initial monmap...
Creating mon...
Waiting for mon to start...
Waiting for mon...
mon is available
Setting mon public_network to 10.10.2.0/24
Wrote config to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf
Wrote keyring to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring
Creating mgr...
Verifying port 9283 ...
Waiting for mgr to start...
Waiting for mgr...
mgr not available, waiting (1/15)...
mgr not available, waiting (2/15)...
mgr is available
Enabling cephadm module...
Waiting for the mgr to restart...
Waiting for mgr epoch 5...
mgr epoch 5 is available
Setting orchestrator backend to cephadm...
Generating ssh key...
Wrote public SSH key to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub
Adding key to ljishen@localhost authorized_keys...
Adding host node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us...
Deploying unmanaged mon service...
Deploying unmanaged mgr service...
You can access the Ceph CLI with:

	sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --fsid dbe3f41a-3806-11ec-b51d-53e6e728d2d3 -c /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf -k /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring

Please consider enabling telemetry to help improve Ceph:

	ceph telemetry on

For more information see:

	https://docs.ceph.com/docs/pacific/mgr/telemetry/

Bootstrap complete.

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:52:05,732314528-07:00] INFO: Sleep for 20 seconds[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:52:25,739808974-07:00] INFO: > Set the default number of monitor daemons to 1[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:337 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:52:25,749608336-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:52:25,753180591-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mon', '1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1
Inferring fsid dbe3f41a-3806-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/dbe3f41a-3806-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Scheduled mon update...

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:52:34,823470805-07:00] INFO: > Set the default number of manager daemons to 1[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:342 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:52:34,834180178-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:52:34,837842062-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mgr', '1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1
Inferring fsid dbe3f41a-3806-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/dbe3f41a-3806-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Scheduled mgr update...


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:52:44,040690943-07:00] STAGE: Add hosts to the cluster...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:52:44,046990335-07:00] INFO: > Adding host 'sm1'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:361 - add_hosts() > ssh-copy-id -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -f -i /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub ljishen@sm1
/usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: "/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub"

Number of key(s) added: 1

Now try logging into the machine, with:   "ssh -o 'GlobalKnownHostsFile=/dev/null' -o 'LogLevel=ERROR' -o 'PasswordAuthentication=no' -o 'StrictHostKeyChecking=no' -o 'UserKnownHostsFile=/dev/null' 'ljishen@sm1'"
and check to make sure that only the key(s) you wanted were added.

# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:367 - add_hosts() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add sm1 10.10.2.5
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:52:45,198093995-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:52:45,201371887-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'host', 'add', 'sm1', '10.10.2.5'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add sm1 10.10.2.5
Inferring fsid dbe3f41a-3806-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/dbe3f41a-3806-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Added host 'sm1' with addr '10.10.2.5'

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:52:56,549995470-07:00] INFO: Sleep for 20 seconds[0m


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:53:16,555154952-07:00] STAGE: Add OSDs to the cluster...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:53:16,561774987-07:00] INFO: > Adding devices ['/dev/nvme0n1'] on host 'sm1' as OSDs[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:388 - add_host_devices_as_osds() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd sm1:/dev/nvme0n1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:53:16,571852131-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:53:16,575492484-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'daemon', 'add', 'osd', 'sm1:/dev/nvme0n1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd sm1:/dev/nvme0n1
Inferring fsid dbe3f41a-3806-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/dbe3f41a-3806-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Created osd(s) 0 on host 'sm1'

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:53:41,153055919-07:00] INFO: > Sleep for 20 seconds[0m


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:54:01,158646545-07:00] STAGE: Show Cluster Status[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:408 - show_status() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:54:01,168865787-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:54:01,172485932-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', '--status'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status
Inferring fsid dbe3f41a-3806-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/dbe3f41a-3806-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
  cluster:
    id:     dbe3f41a-3806-11ec-b51d-53e6e728d2d3
    health: HEALTH_WARN
            1 pool(s) have no replicas configured
 
  services:
    mon: 1 daemons, quorum node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us (age 2m)
    mgr: node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us.lmbcob(active, since 2m)
    osd: 1 osds: 1 up (since 19s), 1 in (since 40s)
 
  data:
    pools:   1 pools, 1 pgs
    objects: 0 objects, 0 B
    usage:   4.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     1 active+clean
 

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:54:09,914551975-07:00] INFO: > Copy configuration files to local[0m
# ./benchmarks/bench-rados:220 - launch_ceph_cluster() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.1 sudo chown --changes --recursive ljishen /tmp/bench-rados/deployment_data_root/etc/ceph
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:54:09,922490064-07:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.1 sudo chown --changes --recursive ljishen /tmp/bench-rados/deployment_data_root/etc/ceph
[1] 08:54:10 [SUCCESS] ljishen@10.10.2.1
# ./benchmarks/bench-rados:223 - launch_ceph_cluster() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.1:/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf
# ./benchmarks/bench-rados:226 - launch_ceph_cluster() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.1:/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:54:10,396962425-07:00] STAGE: Update local ceph.conf as a client...[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:54:10,400789838-07:00] STAGE: Configure Ceph pool...[0m
# ./benchmarks/bench-rados:257 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_max_backfills 32
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:54:10,422612702-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:54:10,425348036-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'dbe3f41a-3806-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'config', 'set', 'osd', 'osd_max_backfills', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid dbe3f41a-3806-11ec-b51d-53e6e728d2d3 -- ceph config set osd osd_max_backfills 32
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:258 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_active 32
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:54:14,259139068-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:54:14,262212369-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'dbe3f41a-3806-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_active', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid dbe3f41a-3806-11ec-b51d-53e6e728d2d3 -- ceph config set osd osd_recovery_max_active 32
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:259 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_single_start 8
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:54:18,174622195-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:54:18,177741543-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'dbe3f41a-3806-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_single_start', '8'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid dbe3f41a-3806-11ec-b51d-53e6e728d2d3 -- ceph config set osd osd_recovery_max_single_start 8
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:260 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_op_priority 63
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:54:21,999064289-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:54:22,002070995-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'dbe3f41a-3806-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_op_priority', '63'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid dbe3f41a-3806-11ec-b51d-53e6e728d2d3 -- ceph config set osd osd_recovery_op_priority 63
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:262 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config show-with-defaults osd.0
# ./benchmarks/bench-rados:263 - configure_ceph_pool() > grep 'osd_max_backfills\|osd_recovery'
# ./benchmarks/bench-rados:263 - configure_ceph_pool() > column -t -s ' '
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
osd_max_backfills                        32         mon
osd_recovery_cost                        20971520   default
osd_recovery_delay_start                 0.000000   default
osd_recovery_max_active                  32         mon
osd_recovery_max_active_hdd              3          default
osd_recovery_max_active_ssd              10         default
osd_recovery_max_chunk                   8388608    default
osd_recovery_max_omap_entries_per_chunk  8096       default
osd_recovery_max_single_start            8          mon
osd_recovery_op_priority                 63         mon
osd_recovery_op_warn_multiple            16         default
osd_recovery_priority                    5          default
osd_recovery_retry_interval              30.000000  default
osd_recovery_sleep                       0.000000   default
osd_recovery_sleep_hdd                   0.100000   default
osd_recovery_sleep_hybrid                0.025000   default
osd_recovery_sleep_ssd                   0.000000   default
# ./benchmarks/bench-rados:265 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool create bench_rados 128 128 replicated --size 1 --pg-num-min 128
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:54:29,557894386-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:54:29,560814659-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'dbe3f41a-3806-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'create', 'bench_rados', '128', '128', 'replicated', '--size', '1', '--pg-num-min', '128'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid dbe3f41a-3806-11ec-b51d-53e6e728d2d3 -- ceph osd pool create bench_rados 128 128 replicated --size 1 --pg-num-min 128
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
pool 'bench_rados' created
# ./benchmarks/bench-rados:267 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados min_size 1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:54:34,141809572-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:54:34,145024090-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'dbe3f41a-3806-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'min_size', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid dbe3f41a-3806-11ec-b51d-53e6e728d2d3 -- ceph osd pool set bench_rados min_size 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 min_size to 1
# ./benchmarks/bench-rados:268 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados pg_autoscale_mode off
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:54:38,490589077-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:54:38,493804957-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'dbe3f41a-3806-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'pg_autoscale_mode', 'off'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid dbe3f41a-3806-11ec-b51d-53e6e728d2d3 -- ceph osd pool set bench_rados pg_autoscale_mode off
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 pg_autoscale_mode to off
# ./benchmarks/bench-rados:269 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool application enable bench_rados benchmark
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:54:43,015135497-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:54:43,018150889-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'dbe3f41a-3806-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'application', 'enable', 'bench_rados', 'benchmark'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid dbe3f41a-3806-11ec-b51d-53e6e728d2d3 -- ceph osd pool application enable bench_rados benchmark
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
enabled application 'benchmark' on pool 'bench_rados'
# ./benchmarks/bench-rados:270 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados noscrub 1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:54:47,256779259-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:54:47,259723878-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'dbe3f41a-3806-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'noscrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid dbe3f41a-3806-11ec-b51d-53e6e728d2d3 -- ceph osd pool set bench_rados noscrub 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 noscrub to 1
# ./benchmarks/bench-rados:271 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados nodeep-scrub 1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:54:52,040097372-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:54:52,043180161-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'dbe3f41a-3806-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'nodeep-scrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid dbe3f41a-3806-11ec-b51d-53e6e728d2d3 -- ceph osd pool set bench_rados nodeep-scrub 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 nodeep-scrub to 1
# ./benchmarks/bench-rados:272 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool ls detail
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:54:56,282252147-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:54:56,285177741-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'dbe3f41a-3806-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'ls', 'detail'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid dbe3f41a-3806-11ec-b51d-53e6e728d2d3 -- ceph osd pool ls detail
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
pool 1 'device_health_metrics' replicated size 1 min_size 1 crush_rule 0 object_hash rjenkins pg_num 64 pgp_num 64 autoscale_mode on last_change 19 lfor 0/0/16 flags hashpspool stripe_width 0 pg_num_min 1 application mgr_devicehealth
pool 2 'bench_rados' replicated size 1 min_size 1 crush_rule 0 object_hash rjenkins pg_num 128 pgp_num 128 autoscale_mode off last_change 20 flags hashpspool,noscrub,nodeep-scrub stripe_width 0 pg_num_min 128 application benchmark

# ./benchmarks/bench-rados:273 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd df tree
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:55:00,100729302-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:55:00,103754684-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'dbe3f41a-3806-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'df', 'tree'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid dbe3f41a-3806-11ec-b51d-53e6e728d2d3 -- ceph osd df tree
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
ID  CLASS  WEIGHT   REWEIGHT  SIZE     RAW USE  DATA     OMAP  META     AVAIL    %USE  VAR   PGS  STATUS  TYPE NAME    
-1         0.09769         -  100 GiB  5.7 MiB  168 KiB   0 B  5.6 MiB  100 GiB  0.01  1.00    -          root default 
-3         0.09769         -  100 GiB  5.7 MiB  168 KiB   0 B  5.6 MiB  100 GiB  0.01  1.00    -              host sm1 
 0    ssd  0.09769   1.00000  100 GiB  5.7 MiB  168 KiB   0 B  5.6 MiB  100 GiB  0.01  1.00  192      up          osd.0
                       TOTAL  100 GiB  5.7 MiB  168 KiB   0 B  5.6 MiB  100 GiB  0.01                                  
MIN/MAX VAR: 1.00/1.00  STDDEV: 0


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:55:04,125434886-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:55:27,875139168-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:55:36,879450261-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:55:45,836368517-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:55:54,644117630-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:55:54,652012237-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:56:03,524520777-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:56:03,532304194-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:56:12,507722053-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:56:12,515195837-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:56:21,421755767-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:56:21,429319030-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:56:30,269738732-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:56:30,277457167-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:56:30,283630941-07:00] INFO: > The cluster is idle now.[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:56:30,287542102-07:00] STAGE: Start benchmarking of mode write ...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:56:30,294832590-07:00] INFO: > Start system activity collection services[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:56:30,300679749-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:341 - rados_bench() > rm --force /mnt/sda8/ceph-research/scripts/output/sys_activity_1MB_write.dat.rados_bench_host.2
# ./benchmarks/bench-rados:342 - rados_bench() > sar_pid_rados_bench_host=467908
# ./benchmarks/bench-rados:342 - rados_bench() > S_TIME_FORMAT=ISO
# ./benchmarks/bench-rados:342 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/output/sys_activity_1MB_write.dat.rados_bench_host.2 2

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:56:30,307832007-07:00] INFO: >> for OSD host[0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:56:30,316992519-07:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.5 bash
10.10.2.5: b'157939\n'
[1] 08:56:31 [SUCCESS] ljishen@10.10.2.5
157939

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:56:31,471756791-07:00] INFO: > Run rados bench[0m
# ./benchmarks/bench-rados:370 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/output/rados_bench_1MB_write.log.2
## ./benchmarks/bench-rados:361 - rados_bench() > count_cpus 32-63,96-127
# ./benchmarks/bench-rados:361 - rados_bench() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 60 write --pool bench_rados -b 1048576 -O 1048576 --concurrent-ios 128 --show-time --write-object --write-omap --write-xattr --no-cleanup
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:56:31,492493508-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:56:31,495432276-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'dbe3f41a-3806-11ec-b51d-53e6e728d2d3', '--', 'rados', 'bench', '60', 'write', '--pool', 'bench_rados', '-b', '1048576', '-O', '1048576', '--concurrent-ios', '128', '--show-time', '--write-object', '--write-omap', '--write-xattr', '--no-cleanup'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid dbe3f41a-3806-11ec-b51d-53e6e728d2d3 -- rados bench 60 write --pool bench_rados -b 1048576 -O 1048576 --concurrent-ios 128 --show-time --write-object --write-omap --write-xattr --no-cleanup
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
hints = 1
2021-10-28T15:56:34.397438+0000 Maintaining 128 concurrent writes of 1048576 bytes to objects of size 1048576 for up to 60 seconds or 0 objects
2021-10-28T15:56:34.397451+0000 Object prefix: benchmark_data_node-2.bluefield2.ucsc-cmps10_7
2021-10-28T15:56:34.429672+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-28T15:56:34.429672+0000     0       0         0         0         0         0           -           0
2021-10-28T15:56:35.429782+0000     1      83        83         0         0         0           -           0
2021-10-28T15:56:36.429856+0000     2     127       156        29   14.4991      14.5     1.93905     1.84103
2021-10-28T15:56:37.429932+0000     3     127       220        93   30.9979        64      1.9093     1.92987
2021-10-28T15:56:38.430006+0000     4     127       292       165   41.2471        72     1.80796     1.92003
2021-10-28T15:56:39.430088+0000     5     127       355       228   45.5967        63     1.66263     1.89553
2021-10-28T15:56:40.430180+0000     6     127       417       290   48.3297        62     1.80137     1.88847
2021-10-28T15:56:41.430258+0000     7     127       492       365   52.1389        75     1.80528     1.88864
2021-10-28T15:56:42.430338+0000     8     127       555       428   53.4959        63     1.90798     1.89336
2021-10-28T15:56:43.430419+0000     9     127       585       458    50.885        30     2.31438     1.93014
2021-10-28T15:56:44.430498+0000    10     127       614       487   48.6962        29     3.05431     1.99228
2021-10-28T15:56:45.430589+0000    11     127       641       514   46.7236        27     3.58047     2.07334
2021-10-28T15:56:46.430673+0000    12     127       673       546   45.4964        32     3.89162     2.18995
2021-10-28T15:56:47.430749+0000    13     127       703       576   44.3042        30     3.94943     2.29692
2021-10-28T15:56:48.430824+0000    14     127       731       604   43.1395        28     3.87327     2.38633
2021-10-28T15:56:49.430905+0000    15     127       790       663   44.1965        59     3.42303     2.51527
2021-10-28T15:56:50.431013+0000    16     127       850       723   45.1839        60     2.52133     2.54887
2021-10-28T15:56:51.431097+0000    17     127       901       774   45.5257        51     1.99495     2.51724
2021-10-28T15:56:52.431182+0000    18     127       975       848   47.1073        74     1.90683     2.46829
2021-10-28T15:56:53.431261+0000    19     127      1044       917   48.2593        69     1.87573     2.42815
2021-10-28T15:56:54.431336+0000 min lat: 1.57567 max lat: 4.52024 avg lat: 2.41894
2021-10-28T15:56:54.431336+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-28T15:56:54.431336+0000    20     127      1083       956   47.7962        39     2.38387     2.41894
2021-10-28T15:56:55.431455+0000    21     127      1089       962   45.8058         6      2.4115     2.41889
2021-10-28T15:56:56.431568+0000    22     127      1111       984   44.7235        22     3.17772     2.43968
2021-10-28T15:56:57.431661+0000    23     127      1124       997   43.3442        13     3.54851      2.4579
2021-10-28T15:56:58.431734+0000    24     127      1157      1030   42.9131        33     5.05194     2.53155
2021-10-28T15:56:59.431812+0000    25     127      1177      1050   41.9965        20     5.75282     2.59497
2021-10-28T15:57:00.431905+0000    26     127      1190      1063   40.8812        13     6.21197      2.6397
2021-10-28T15:57:01.431991+0000    27     127      1210      1083   40.1077        20     6.57013     2.71336
2021-10-28T15:57:02.432094+0000    28     127      1223      1096   39.1395        13     5.98808     2.75926
2021-10-28T15:57:03.432170+0000    29     127      1256      1129   38.9278        33      5.9686     2.87727
2021-10-28T15:57:04.432255+0000    30     127      1273      1146   38.1968        17     5.93537     2.93264
2021-10-28T15:57:05.432332+0000    31     127      1314      1187   38.2871        41     5.34432      3.0317
2021-10-28T15:57:06.432420+0000    32     127      1348      1221    38.153        34     4.72671     3.08253
2021-10-28T15:57:07.432501+0000    33     127      1390      1263   38.2695        42     3.10492     3.10528
2021-10-28T15:57:08.432584+0000    34     127      1427      1300   38.2321        37     3.21533     3.11218
2021-10-28T15:57:09.432666+0000    35     127      1473      1346   38.4539        46     2.80973      3.1158
2021-10-28T15:57:10.432753+0000    36     127      1519      1392   38.6634        46     2.81581      3.1165
2021-10-28T15:57:11.432844+0000    37     127      1551      1424   38.4832        32     3.03124     3.11663
2021-10-28T15:57:12.432918+0000    38     127      1582      1455   38.2863        31     3.49336     3.12081
2021-10-28T15:57:13.432990+0000    39     127      1603      1476    37.843        21     3.72994     3.13467
2021-10-28T15:57:14.433082+0000 min lat: 1.57567 max lat: 7.3249 avg lat: 3.14503
2021-10-28T15:57:14.433082+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-28T15:57:14.433082+0000    40     127      1616      1489   37.2219        13     3.98548     3.14503
2021-10-28T15:57:15.433192+0000    41     127      1636      1509   36.8018        20     4.59717     3.16823
2021-10-28T15:57:16.433306+0000    42     127      1649      1522    36.235        13     5.16048     3.18619
2021-10-28T15:57:17.433392+0000    43     127      1682      1555   36.1597        33     5.46227     3.24143
2021-10-28T15:57:18.433474+0000    44     127      1710      1583   35.9742        28     5.77525     3.28858
2021-10-28T15:57:19.433561+0000    45     127      1743      1616   35.9081        33     5.13476     3.32912
2021-10-28T15:57:20.433637+0000    46     127      1786      1659   36.0622        43     3.67434     3.35913
2021-10-28T15:57:21.433713+0000    47     127      1857      1730   36.8054        71      2.7568     3.35247
2021-10-28T15:57:22.433800+0000    48     127      1920      1793    37.351        63     1.79154     3.30943
2021-10-28T15:57:23.433880+0000    49     127      1988      1861   37.9764        68     1.82125     3.26014
2021-10-28T15:57:24.433955+0000    50     127      2058      1931   38.6167        70     1.80342     3.21095
2021-10-28T15:57:25.434029+0000    51     127      2103      1976   38.7418        45     2.10365     3.18393
2021-10-28T15:57:26.434124+0000    52     127      2119      1992   38.3045        16     2.46384      3.1793
2021-10-28T15:57:27.434229+0000    53     127      2136      2009   37.9024        17     3.04029     3.17962
2021-10-28T15:57:28.434304+0000    54     127      2152      2025   37.4968        16     3.67696     3.18465
2021-10-28T15:57:29.434390+0000    55     127      2169      2042   37.1241        17      4.2745     3.19576
2021-10-28T15:57:30.434501+0000    56     127      2185      2058   36.7469        16     5.16512     3.21111
2021-10-28T15:57:31.434576+0000    57     127      2219      2092   36.6986        34      5.9761     3.25947
2021-10-28T15:57:32.434671+0000    58     127      2239      2112   36.4107        20     6.47852      3.2929
2021-10-28T15:57:33.434743+0000    59     127      2247      2120   35.9291         8     6.97816     3.30679
2021-10-28T15:57:34.434832+0000 min lat: 1.57567 max lat: 7.3249 avg lat: 3.34421
2021-10-28T15:57:34.434832+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-28T15:57:34.434832+0000    60     127      2269      2142    35.697        22     6.28124     3.34421
2021-10-28T15:57:35.434919+0000    61       4      2270      2266   37.1444       124      1.2603     3.36284
2021-10-28T15:57:36.435067+0000 Total time run:         61.0594
Total writes made:      2270
Write size:             1048576
Object size:            1048576
Bandwidth (MB/sec):     37.1769
Stddev Bandwidth:       23.1908
Max bandwidth (MB/sec): 124
Min bandwidth (MB/sec): 0
Average IOPS:           37
Stddev IOPS:            23.1989
Max IOPS:               124
Min IOPS:               0
Average Latency(s):     3.35961
Stddev Latency(s):      1.58183
Max latency(s):         7.3249
Min latency(s):         1.25929

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:57:37,073995694-07:00] INFO: > Stop system activity collection services[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:57:37,080219703-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:391 - rados_bench() > kill -INT 467908

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:57:37,086463629-07:00] INFO: >> for OSD host[0m
# ./benchmarks/bench-rados:396 - rados_bench() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.5 kill -INT 157939
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:57:37,094205889-07:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.5 kill -INT 157939
[1] 08:57:38 [SUCCESS] ljishen@10.10.2.5

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:57:38,990089167-07:00] INFO: >>> Copy system activity file from OSD host[0m
## ./benchmarks/bench-rados:403 - rados_bench() > basename -- /tmp/bench-rados/sys_activity_1MB_write.dat.osd_host.2
# ./benchmarks/bench-rados:403 - rados_bench() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.5:/tmp/bench-rados/sys_activity_1MB_write.dat.osd_host.2 /mnt/sda8/ceph-research/scripts/output/sys_activity_1MB_write.dat.osd_host.2


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:57:40,355444017-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:58:04,209954184-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 2.27k objects, 2.2 GiB
    usage:   6.7 GiB used, 93 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:58:04,218126284-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:58:13,161299378-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 2.27k objects, 2.2 GiB
    usage:   6.7 GiB used, 93 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:58:13,169482349-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:58:22,068429583-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 2.27k objects, 2.2 GiB
    usage:   6.7 GiB used, 93 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:58:22,076473090-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:58:30,850812296-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 2.27k objects, 2.2 GiB
    usage:   6.7 GiB used, 93 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:58:30,858453696-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:58:39,798986386-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 2.27k objects, 2.2 GiB
    usage:   6.7 GiB used, 93 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:58:39,806691577-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:58:39,812870120-07:00] INFO: > The cluster is idle now.[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:58:39,816986156-07:00] STAGE: Start benchmarking of mode seq ...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:58:39,824227061-07:00] INFO: > Start system activity collection services[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:58:39,830100650-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:341 - rados_bench() > rm --force /mnt/sda8/ceph-research/scripts/output/sys_activity_1MB_seq.dat.rados_bench_host.2
# ./benchmarks/bench-rados:342 - rados_bench() > sar_pid_rados_bench_host=469241
# ./benchmarks/bench-rados:342 - rados_bench() > S_TIME_FORMAT=ISO
# ./benchmarks/bench-rados:342 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/output/sys_activity_1MB_seq.dat.rados_bench_host.2 2

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:58:39,837322900-07:00] INFO: >> for OSD host[0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:58:39,846529700-07:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.5 bash
10.10.2.5: b'158489\n'
[1] 08:58:41 [SUCCESS] ljishen@10.10.2.5
158489

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:58:41,158000268-07:00] INFO: > Run rados bench[0m
# ./benchmarks/bench-rados:379 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/output/rados_bench_1MB_seq.log.2
## ./benchmarks/bench-rados:376 - rados_bench() > count_cpus 32-63,96-127
# ./benchmarks/bench-rados:376 - rados_bench() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 128 --show-time
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:58:41,178459373-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:58:41,181342196-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'dbe3f41a-3806-11ec-b51d-53e6e728d2d3', '--', 'rados', 'bench', '99999999', 'seq', '--pool', 'bench_rados', '--concurrent-ios', '128', '--show-time'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid dbe3f41a-3806-11ec-b51d-53e6e728d2d3 -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 128 --show-time
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
hints = 1
2021-10-28T15:58:44.053994+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-28T15:58:44.053994+0000     0       0         0         0         0         0           -           0
2021-10-28T15:58:45.054158+0000     1     127       703       576   575.868       576    0.198718    0.195827
2021-10-28T15:58:46.054276+0000     2     127      1305      1178   588.898       602    0.203406    0.203964
2021-10-28T15:58:47.054404+0000     3     127      1928      1801   600.238       623    0.212586    0.204648
2021-10-28T15:58:48.054550+0000 Total time run:       3.71904
Total reads made:     2270
Read size:            1048576
Object size:          1048576
Bandwidth (MB/sec):   610.373
Average IOPS:         610
Stddev IOPS:          23.5443
Max IOPS:             623
Min IOPS:             576
Average Latency(s):   0.203935
Max latency(s):       0.429628
Min latency(s):       0.0876136

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:58:48,838489423-07:00] INFO: > Stop system activity collection services[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:58:48,844679679-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:391 - rados_bench() > kill -INT 469241

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:58:48,850844917-07:00] INFO: >> for OSD host[0m
# ./benchmarks/bench-rados:396 - rados_bench() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.5 kill -INT 158489
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:58:48,858691093-07:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.5 kill -INT 158489
[1] 08:58:49 [SUCCESS] ljishen@10.10.2.5

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:58:50,005509141-07:00] INFO: >>> Copy system activity file from OSD host[0m
## ./benchmarks/bench-rados:403 - rados_bench() > basename -- /tmp/bench-rados/sys_activity_1MB_seq.dat.osd_host.2
# ./benchmarks/bench-rados:403 - rados_bench() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.5:/tmp/bench-rados/sys_activity_1MB_seq.dat.osd_host.2 /mnt/sda8/ceph-research/scripts/output/sys_activity_1MB_seq.dat.osd_host.2


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:58:51,080411998-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:59:14,841382567-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 2.27k objects, 2.2 GiB
    usage:   6.7 GiB used, 93 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:59:14,849354380-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:59:23,771826479-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 2.27k objects, 2.2 GiB
    usage:   6.7 GiB used, 93 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:59:23,779747977-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:59:32,678308287-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 2.27k objects, 2.2 GiB
    usage:   6.7 GiB used, 93 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:59:32,686498170-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:59:41,601713567-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 2.27k objects, 2.2 GiB
    usage:   6.7 GiB used, 93 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:59:41,609634533-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:59:50,540153228-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 2.27k objects, 2.2 GiB
    usage:   6.7 GiB used, 93 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:59:50,548890864-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:59:50,555114392-07:00] INFO: > The cluster is idle now.[0m


[1;7;39;49m[2021-10-28T08:59:50,557487333-07:00][RUNNING][ROUND 3/5/21] object_size=1MB[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:59:50,561222943-07:00] STAGE: Launch a Ceph cluster (CEPH_HOST: 10.10.2.1, OSD_HOST: 10.10.2.5) ...[0m
# ./benchmarks/bench-rados:183 - launch_ceph_cluster() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.1 bash -euo pipefail
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:59:50,571093053-07:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.1 bash -euo pipefail
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T08:59:50,972733796-07:00] INFO: > Remove existing clusters\x1b[0m\n'
10.10.2.1: b'## bash:17 -  > basename -- /var/lib/ceph/dbe3f41a-3806-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'# bash:17 -  > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm --host 10.10.2.1 --host 10.10.2.5 rm-cluster --force --fsid dbe3f41a-3806-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T08:59:50,983183981-07:00] DEBUG: HOST_PARAMS: ['--host', '10.10.2.1', '--host', '10.10.2.5']\x1b[0m\n"
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T08:59:50,986883616-07:00] DEBUG: CEPHADM_PARAMS: ['rm-cluster', '--force', '--fsid', 'dbe3f41a-3806-11ec-b51d-53e6e728d2d3']\x1b[0m\n"
10.10.2.1: b"# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:63 -  > /tmp/bench-rados/ceph-research/scripts/parallel-ssh --host 10.10.2.1 --host 10.10.2.5 'sudo python3 -u - rm-cluster --force --fsid dbe3f41a-3806-11ec-b51d-53e6e728d2d3'\n"
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T08:59:50,995514821-07:00] DEBUG: command from stdin\x1b[0m\n'
10.10.2.1: b"# /tmp/bench-rados/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host 10.10.2.1 --host 10.10.2.5 'sudo python3 -u - rm-cluster --force --fsid dbe3f41a-3806-11ec-b51d-53e6e728d2d3'\n"
10.10.2.1: b'[1] 08:59:56 [SUCCESS] 10.10.2.1\n[2] 09:00:02 [SUCCESS] 10.10.2.5\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T09:00:02,956512192-07:00] INFO: > Deploy a new cluster\x1b[0m\n'
10.10.2.1: b'# bash:24 -  > /tmp/bench-rados/ceph-research/scripts/ceph-deploy -m 10.10.2.1 -o sm1/10.10.2.5:/dev/nvme0n1\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T09:00:02,968512721-07:00] STAGE: Verify passwordless SSH to hosts, passwordless sudo via ljishen, and the existence of devices on hosts...\x1b[0m\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T09:00:02,973158325-07:00] INFO: > Check the host for the monitor\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:181 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@10.10.2.1 sudo --non-interactive --validate\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T09:00:03,123376090-07:00] INFO: > Check hosts for OSDs\x1b[0m\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T09:00:03,127717932-07:00] INFO: >> Check host 'sm1'\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:200 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 sudo --non-interactive --validate\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T09:00:04,210937452-07:00] INFO: >>> Check device '/dev/nvme0n1'\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:215 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 test -b /dev/nvme0n1\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:226 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 lsblk --nodeps --noheadings --output TYPE /dev/nvme0n1\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:227 - verify_parameters() > grep --word-regexp disk\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T09:00:06,361481550-07:00] STAGE: Reset devices on hosts...\x1b[0m\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T09:00:06,366606835-07:00] INFO: > Resetting device '/dev/nvme0n1' on host 'sm1'\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:288 - reset_host_devices() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 sudo sh\n'
10.10.2.1: b'  Removing ceph--d2b1c451--0aa1--4955--ac88--26f47ee6cf0a-osd--block--9388b2e4--d20b--4716--a794--b410af453481 (252:0)\n'
10.10.2.1: b'  Archiving volume group "ceph-d2b1c451-0aa1-4955-ac88-26f47ee6cf0a" metadata (seqno 5).\n'
10.10.2.1: b'  Releasing logical volume "osd-block-9388b2e4-d20b-4716-a794-b410af453481"\n'
10.10.2.1: b'  Creating volume group backup "/etc/lvm/backup/ceph-d2b1c451-0aa1-4955-ac88-26f47ee6cf0a" (seqno 6).\n'
10.10.2.1: b'  Logical volume "osd-block-9388b2e4-d20b-4716-a794-b410af453481" successfully removed\n'
10.10.2.1: b'  Removing physical volume "/dev/nvme0n1" from volume group "ceph-d2b1c451-0aa1-4955-ac88-26f47ee6cf0a"\n'
10.10.2.1: b'  Volume group "ceph-d2b1c451-0aa1-4955-ac88-26f47ee6cf0a" successfully removed\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T09:00:08,676372375-07:00] STAGE: Bootstrap a new cluster...\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:317 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.1 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T09:00:08,686600814-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T09:00:08,690531562-07:00] DEBUG: CEPHADM_PARAMS: ['bootstrap', '--config', '/tmp/bench-rados/ceph-research/scripts/ceph.conf', '--mon-ip', '10.10.2.1', '--output-dir', '/tmp/bench-rados/deployment_data_root/etc/ceph', '--ssh-user', 'ljishen', '--skip-dashboard', '--no-minimize-config', '--allow-overwrite', '--allow-fqdn-hostname', '--orphan-initial-daemons', '--skip-monitoring-stack', '--single-host-defaults']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.1 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults\n'
10.10.2.1: b'Verifying podman|docker is present...\nVerifying lvm2 is present...\n'
10.10.2.1: b'Verifying time synchronization is in place...\n'
10.10.2.1: b'Unit ntp.service is enabled and running\n'
10.10.2.1: b'Repeating the final host check...\n'
10.10.2.1: b'podman|docker (/usr/bin/docker) is present\n'
10.10.2.1: b'systemctl is present\n'
10.10.2.1: b'lvcreate is present\n'
10.10.2.1: b'Unit ntp.service is enabled and running\n'
10.10.2.1: b'Host looks OK\n'
10.10.2.1: b'Cluster fsid: 1ffa2fc4-3808-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Verifying IP 10.10.2.1 port 3300 ...\n'
10.10.2.1: b'Verifying IP 10.10.2.1 port 6789 ...\n'
10.10.2.1: b'Mon IP `10.10.2.1` is in CIDR network `10.10.2.0/24`\n'
10.10.2.1: b'- internal network (--cluster-network) has not been provided, OSD replication will default to the public_network\n'
10.10.2.1: b'Adjusting default settings to suit single-host cluster...\n'
10.10.2.1: b'Pulling container image quay.io/ceph/ceph:v16...\n'
10.10.2.1: b'Ceph version: ceph version 16.2.6 (ee28fb57e47e9f88813e24bbf4c14496ca299d31) pacific (stable)\n'
10.10.2.1: b'Extracting ceph user uid/gid from container image...\n'
10.10.2.1: b'Creating initial keys...\n'
10.10.2.1: b'Creating initial monmap...\n'
10.10.2.1: b'Creating mon...\n'
10.10.2.1: b'Waiting for mon to start...\n'
10.10.2.1: b'Waiting for mon...\n'
10.10.2.1: b'mon is available\nSetting mon public_network to 10.10.2.0/24\n'
10.10.2.1: b'Wrote config to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf\n'
10.10.2.1: b'Wrote keyring to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring\n'
10.10.2.1: b'Creating mgr...\n'
10.10.2.1: b'Verifying port 9283 ...\n'
10.10.2.1: b'Waiting for mgr to start...\nWaiting for mgr...\n'
10.10.2.1: b'mgr not available, waiting (1/15)...\n'
10.10.2.1: b'mgr not available, waiting (2/15)...\n'
10.10.2.1: b'mgr is available\n'
10.10.2.1: b'Enabling cephadm module...\n'
10.10.2.1: b'Waiting for the mgr to restart...\n'
10.10.2.1: b'Waiting for mgr epoch 5...\n'
10.10.2.1: b'mgr epoch 5 is available\n'
10.10.2.1: b'Setting orchestrator backend to cephadm...\n'
10.10.2.1: b'Generating ssh key...\n'
10.10.2.1: b'Wrote public SSH key to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub\n'
10.10.2.1: b'Adding key to ljishen@localhost authorized_keys...\n'
10.10.2.1: b'Adding host node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us...\n'
10.10.2.1: b'Deploying unmanaged mon service...\n'
10.10.2.1: b'Deploying unmanaged mgr service...\n'
10.10.2.1: b'You can access the Ceph CLI with:\n\n\tsudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --fsid 1ffa2fc4-3808-11ec-b51d-53e6e728d2d3 -c /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf -k /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring\n\n'
10.10.2.1: b'Please consider enabling telemetry to help improve Ceph:\n\n\tceph telemetry on\n\nFor more information see:\n\n\thttps://docs.ceph.com/docs/pacific/mgr/telemetry/\n\nBootstrap complete.\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T09:01:10,778650917-07:00] INFO: Sleep for 20 seconds\x1b[0m\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T09:01:30,786365752-07:00] INFO: > Set the default number of monitor daemons to 1\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:337 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T09:01:30,797008789-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T09:01:30,800873133-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mon', '1']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1\n'
10.10.2.1: b'Inferring fsid 1ffa2fc4-3808-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/1ffa2fc4-3808-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b'Scheduled mon update...\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T09:01:39,806788695-07:00] INFO: > Set the default number of manager daemons to 1\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:342 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T09:01:39,816892237-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T09:01:39,820152627-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mgr', '1']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1\n'
10.10.2.1: b'Inferring fsid 1ffa2fc4-3808-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/1ffa2fc4-3808-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b'Scheduled mgr update...\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T09:01:49,599385903-07:00] STAGE: Add hosts to the cluster...\x1b[0m\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T09:01:49,605825639-07:00] INFO: > Adding host 'sm1'\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:361 - add_hosts() > ssh-copy-id -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -f -i /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub ljishen@sm1\n'
10.10.2.1: b'/usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: "/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub"\n'
10.10.2.1: b'\nNumber of key(s) added: 1\n\nNow try logging into the machine, with:   "ssh -o \'GlobalKnownHostsFile=/dev/null\' -o \'LogLevel=ERROR\' -o \'PasswordAuthentication=no\' -o \'StrictHostKeyChecking=no\' -o \'UserKnownHostsFile=/dev/null\' \'ljishen@sm1\'"\nand check to make sure that only the key(s) you wanted were added.\n\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:367 - add_hosts() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add sm1 10.10.2.5\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T09:01:50,798390572-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T09:01:50,802137155-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'host', 'add', 'sm1', '10.10.2.5']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add sm1 10.10.2.5\n'
10.10.2.1: b'Inferring fsid 1ffa2fc4-3808-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/1ffa2fc4-3808-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b"Added host 'sm1' with addr '10.10.2.5'\n"
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T09:02:01,409246619-07:00] INFO: Sleep for 20 seconds\x1b[0m\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T09:02:21,413966686-07:00] STAGE: Add OSDs to the cluster...\x1b[0m\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T09:02:21,420699603-07:00] INFO: > Adding devices ['/dev/nvme0n1'] on host 'sm1' as OSDs\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:388 - add_host_devices_as_osds() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd sm1:/dev/nvme0n1\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T09:02:21,430616225-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T09:02:21,434744355-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'daemon', 'add', 'osd', 'sm1:/dev/nvme0n1']\x1b[0m\n# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd sm1:/dev/nvme0n1\n"
10.10.2.1: b'Inferring fsid 1ffa2fc4-3808-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/1ffa2fc4-3808-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b"Created osd(s) 0 on host 'sm1'\n"
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T09:02:46,043027818-07:00] INFO: > Sleep for 20 seconds\x1b[0m\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T09:03:06,048505098-07:00] STAGE: Show Cluster Status\x1b[0m\n# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:408 - show_status() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T09:03:06,058251380-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T09:03:06,062052114-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', '--status']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status\n'
10.10.2.1: b'Inferring fsid 1ffa2fc4-3808-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/1ffa2fc4-3808-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b'  cluster:\n    id:     1ffa2fc4-3808-11ec-b51d-53e6e728d2d3\n    health: HEALTH_WARN\n            1 pool(s) have no replicas configured\n \n  services:\n    mon: 1 daemons, quorum node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us (age 2m)\n    mgr: node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us.ohkkci(active, since 2m)\n    osd: 1 osds: 1 up (since 20s), 1 in (since 40s)\n \n  data:\n    pools:   1 pools, 1 pgs\n    objects: 0 objects, 0 B\n    usage:   4.7 MiB used, 100 GiB / 100 GiB avail\n    pgs:     1 active+clean\n \n'
[1] 09:03:14 [SUCCESS] ljishen@10.10.2.1

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:59:50,972733796-07:00] INFO: > Remove existing clusters[0m
## bash:17 -  > basename -- /var/lib/ceph/dbe3f41a-3806-11ec-b51d-53e6e728d2d3
# bash:17 -  > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm --host 10.10.2.1 --host 10.10.2.5 rm-cluster --force --fsid dbe3f41a-3806-11ec-b51d-53e6e728d2d3
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:59:50,983183981-07:00] DEBUG: HOST_PARAMS: ['--host', '10.10.2.1', '--host', '10.10.2.5'][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:59:50,986883616-07:00] DEBUG: CEPHADM_PARAMS: ['rm-cluster', '--force', '--fsid', 'dbe3f41a-3806-11ec-b51d-53e6e728d2d3'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:63 -  > /tmp/bench-rados/ceph-research/scripts/parallel-ssh --host 10.10.2.1 --host 10.10.2.5 'sudo python3 -u - rm-cluster --force --fsid dbe3f41a-3806-11ec-b51d-53e6e728d2d3'
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T08:59:50,995514821-07:00] DEBUG: command from stdin[0m
# /tmp/bench-rados/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host 10.10.2.1 --host 10.10.2.5 'sudo python3 -u - rm-cluster --force --fsid dbe3f41a-3806-11ec-b51d-53e6e728d2d3'
[1] 08:59:56 [SUCCESS] 10.10.2.1
[2] 09:00:02 [SUCCESS] 10.10.2.5

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:00:02,956512192-07:00] INFO: > Deploy a new cluster[0m
# bash:24 -  > /tmp/bench-rados/ceph-research/scripts/ceph-deploy -m 10.10.2.1 -o sm1/10.10.2.5:/dev/nvme0n1


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:00:02,968512721-07:00] STAGE: Verify passwordless SSH to hosts, passwordless sudo via ljishen, and the existence of devices on hosts...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:00:02,973158325-07:00] INFO: > Check the host for the monitor[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:181 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@10.10.2.1 sudo --non-interactive --validate

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:00:03,123376090-07:00] INFO: > Check hosts for OSDs[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:00:03,127717932-07:00] INFO: >> Check host 'sm1'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:200 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 sudo --non-interactive --validate

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:00:04,210937452-07:00] INFO: >>> Check device '/dev/nvme0n1'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:215 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 test -b /dev/nvme0n1
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:226 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 lsblk --nodeps --noheadings --output TYPE /dev/nvme0n1
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:227 - verify_parameters() > grep --word-regexp disk


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:00:06,361481550-07:00] STAGE: Reset devices on hosts...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:00:06,366606835-07:00] INFO: > Resetting device '/dev/nvme0n1' on host 'sm1'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:288 - reset_host_devices() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 sudo sh
  Removing ceph--d2b1c451--0aa1--4955--ac88--26f47ee6cf0a-osd--block--9388b2e4--d20b--4716--a794--b410af453481 (252:0)
  Archiving volume group "ceph-d2b1c451-0aa1-4955-ac88-26f47ee6cf0a" metadata (seqno 5).
  Releasing logical volume "osd-block-9388b2e4-d20b-4716-a794-b410af453481"
  Creating volume group backup "/etc/lvm/backup/ceph-d2b1c451-0aa1-4955-ac88-26f47ee6cf0a" (seqno 6).
  Logical volume "osd-block-9388b2e4-d20b-4716-a794-b410af453481" successfully removed
  Removing physical volume "/dev/nvme0n1" from volume group "ceph-d2b1c451-0aa1-4955-ac88-26f47ee6cf0a"
  Volume group "ceph-d2b1c451-0aa1-4955-ac88-26f47ee6cf0a" successfully removed


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:00:08,676372375-07:00] STAGE: Bootstrap a new cluster...[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:317 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.1 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:00:08,686600814-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:00:08,690531562-07:00] DEBUG: CEPHADM_PARAMS: ['bootstrap', '--config', '/tmp/bench-rados/ceph-research/scripts/ceph.conf', '--mon-ip', '10.10.2.1', '--output-dir', '/tmp/bench-rados/deployment_data_root/etc/ceph', '--ssh-user', 'ljishen', '--skip-dashboard', '--no-minimize-config', '--allow-overwrite', '--allow-fqdn-hostname', '--orphan-initial-daemons', '--skip-monitoring-stack', '--single-host-defaults'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.1 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults
Verifying podman|docker is present...
Verifying lvm2 is present...
Verifying time synchronization is in place...
Unit ntp.service is enabled and running
Repeating the final host check...
podman|docker (/usr/bin/docker) is present
systemctl is present
lvcreate is present
Unit ntp.service is enabled and running
Host looks OK
Cluster fsid: 1ffa2fc4-3808-11ec-b51d-53e6e728d2d3
Verifying IP 10.10.2.1 port 3300 ...
Verifying IP 10.10.2.1 port 6789 ...
Mon IP `10.10.2.1` is in CIDR network `10.10.2.0/24`
- internal network (--cluster-network) has not been provided, OSD replication will default to the public_network
Adjusting default settings to suit single-host cluster...
Pulling container image quay.io/ceph/ceph:v16...
Ceph version: ceph version 16.2.6 (ee28fb57e47e9f88813e24bbf4c14496ca299d31) pacific (stable)
Extracting ceph user uid/gid from container image...
Creating initial keys...
Creating initial monmap...
Creating mon...
Waiting for mon to start...
Waiting for mon...
mon is available
Setting mon public_network to 10.10.2.0/24
Wrote config to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf
Wrote keyring to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring
Creating mgr...
Verifying port 9283 ...
Waiting for mgr to start...
Waiting for mgr...
mgr not available, waiting (1/15)...
mgr not available, waiting (2/15)...
mgr is available
Enabling cephadm module...
Waiting for the mgr to restart...
Waiting for mgr epoch 5...
mgr epoch 5 is available
Setting orchestrator backend to cephadm...
Generating ssh key...
Wrote public SSH key to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub
Adding key to ljishen@localhost authorized_keys...
Adding host node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us...
Deploying unmanaged mon service...
Deploying unmanaged mgr service...
You can access the Ceph CLI with:

	sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --fsid 1ffa2fc4-3808-11ec-b51d-53e6e728d2d3 -c /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf -k /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring

Please consider enabling telemetry to help improve Ceph:

	ceph telemetry on

For more information see:

	https://docs.ceph.com/docs/pacific/mgr/telemetry/

Bootstrap complete.

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:01:10,778650917-07:00] INFO: Sleep for 20 seconds[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:01:30,786365752-07:00] INFO: > Set the default number of monitor daemons to 1[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:337 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:01:30,797008789-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:01:30,800873133-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mon', '1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1
Inferring fsid 1ffa2fc4-3808-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/1ffa2fc4-3808-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Scheduled mon update...

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:01:39,806788695-07:00] INFO: > Set the default number of manager daemons to 1[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:342 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:01:39,816892237-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:01:39,820152627-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mgr', '1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1
Inferring fsid 1ffa2fc4-3808-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/1ffa2fc4-3808-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Scheduled mgr update...


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:01:49,599385903-07:00] STAGE: Add hosts to the cluster...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:01:49,605825639-07:00] INFO: > Adding host 'sm1'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:361 - add_hosts() > ssh-copy-id -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -f -i /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub ljishen@sm1
/usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: "/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub"

Number of key(s) added: 1

Now try logging into the machine, with:   "ssh -o 'GlobalKnownHostsFile=/dev/null' -o 'LogLevel=ERROR' -o 'PasswordAuthentication=no' -o 'StrictHostKeyChecking=no' -o 'UserKnownHostsFile=/dev/null' 'ljishen@sm1'"
and check to make sure that only the key(s) you wanted were added.

# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:367 - add_hosts() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add sm1 10.10.2.5
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:01:50,798390572-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:01:50,802137155-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'host', 'add', 'sm1', '10.10.2.5'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add sm1 10.10.2.5
Inferring fsid 1ffa2fc4-3808-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/1ffa2fc4-3808-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Added host 'sm1' with addr '10.10.2.5'

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:02:01,409246619-07:00] INFO: Sleep for 20 seconds[0m


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:02:21,413966686-07:00] STAGE: Add OSDs to the cluster...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:02:21,420699603-07:00] INFO: > Adding devices ['/dev/nvme0n1'] on host 'sm1' as OSDs[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:388 - add_host_devices_as_osds() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd sm1:/dev/nvme0n1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:02:21,430616225-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:02:21,434744355-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'daemon', 'add', 'osd', 'sm1:/dev/nvme0n1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd sm1:/dev/nvme0n1
Inferring fsid 1ffa2fc4-3808-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/1ffa2fc4-3808-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Created osd(s) 0 on host 'sm1'

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:02:46,043027818-07:00] INFO: > Sleep for 20 seconds[0m


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:03:06,048505098-07:00] STAGE: Show Cluster Status[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:408 - show_status() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:03:06,058251380-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:03:06,062052114-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', '--status'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status
Inferring fsid 1ffa2fc4-3808-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/1ffa2fc4-3808-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
  cluster:
    id:     1ffa2fc4-3808-11ec-b51d-53e6e728d2d3
    health: HEALTH_WARN
            1 pool(s) have no replicas configured
 
  services:
    mon: 1 daemons, quorum node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us (age 2m)
    mgr: node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us.ohkkci(active, since 2m)
    osd: 1 osds: 1 up (since 20s), 1 in (since 40s)
 
  data:
    pools:   1 pools, 1 pgs
    objects: 0 objects, 0 B
    usage:   4.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     1 active+clean
 

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:03:14,780632065-07:00] INFO: > Copy configuration files to local[0m
# ./benchmarks/bench-rados:220 - launch_ceph_cluster() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.1 sudo chown --changes --recursive ljishen /tmp/bench-rados/deployment_data_root/etc/ceph
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:03:14,788999573-07:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.1 sudo chown --changes --recursive ljishen /tmp/bench-rados/deployment_data_root/etc/ceph
[1] 09:03:14 [SUCCESS] ljishen@10.10.2.1
# ./benchmarks/bench-rados:223 - launch_ceph_cluster() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.1:/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf
# ./benchmarks/bench-rados:226 - launch_ceph_cluster() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.1:/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:03:15,270134832-07:00] STAGE: Update local ceph.conf as a client...[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:03:15,273870252-07:00] STAGE: Configure Ceph pool...[0m
# ./benchmarks/bench-rados:257 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_max_backfills 32
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:03:15,295672267-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:03:15,298468426-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '1ffa2fc4-3808-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'config', 'set', 'osd', 'osd_max_backfills', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 1ffa2fc4-3808-11ec-b51d-53e6e728d2d3 -- ceph config set osd osd_max_backfills 32
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:258 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_active 32
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:03:19,264147133-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:03:19,267077765-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '1ffa2fc4-3808-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_active', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 1ffa2fc4-3808-11ec-b51d-53e6e728d2d3 -- ceph config set osd osd_recovery_max_active 32
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:259 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_single_start 8
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:03:22,981964454-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:03:22,984929312-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '1ffa2fc4-3808-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_single_start', '8'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 1ffa2fc4-3808-11ec-b51d-53e6e728d2d3 -- ceph config set osd osd_recovery_max_single_start 8
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:260 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_op_priority 63
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:03:26,822242088-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:03:26,825483356-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '1ffa2fc4-3808-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_op_priority', '63'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 1ffa2fc4-3808-11ec-b51d-53e6e728d2d3 -- ceph config set osd osd_recovery_op_priority 63
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:262 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config show-with-defaults osd.0
# ./benchmarks/bench-rados:263 - configure_ceph_pool() > grep 'osd_max_backfills\|osd_recovery'
# ./benchmarks/bench-rados:263 - configure_ceph_pool() > column -t -s ' '
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
osd_max_backfills                        32         mon
osd_recovery_cost                        20971520   default
osd_recovery_delay_start                 0.000000   default
osd_recovery_max_active                  32         mon
osd_recovery_max_active_hdd              3          default
osd_recovery_max_active_ssd              10         default
osd_recovery_max_chunk                   8388608    default
osd_recovery_max_omap_entries_per_chunk  8096       default
osd_recovery_max_single_start            8          mon
osd_recovery_op_priority                 3          default
osd_recovery_op_warn_multiple            16         default
osd_recovery_priority                    5          default
osd_recovery_retry_interval              30.000000  default
osd_recovery_sleep                       0.000000   default
osd_recovery_sleep_hdd                   0.100000   default
osd_recovery_sleep_hybrid                0.025000   default
osd_recovery_sleep_ssd                   0.000000   default
# ./benchmarks/bench-rados:265 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool create bench_rados 128 128 replicated --size 1 --pg-num-min 128
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:03:34,515248830-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:03:34,518214849-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '1ffa2fc4-3808-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'create', 'bench_rados', '128', '128', 'replicated', '--size', '1', '--pg-num-min', '128'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 1ffa2fc4-3808-11ec-b51d-53e6e728d2d3 -- ceph osd pool create bench_rados 128 128 replicated --size 1 --pg-num-min 128
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
pool 'bench_rados' created
# ./benchmarks/bench-rados:267 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados min_size 1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:03:38,713596622-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:03:38,716759883-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '1ffa2fc4-3808-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'min_size', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 1ffa2fc4-3808-11ec-b51d-53e6e728d2d3 -- ceph osd pool set bench_rados min_size 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 min_size to 1
# ./benchmarks/bench-rados:268 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados pg_autoscale_mode off
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:03:42,992077442-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:03:42,995020688-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '1ffa2fc4-3808-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'pg_autoscale_mode', 'off'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 1ffa2fc4-3808-11ec-b51d-53e6e728d2d3 -- ceph osd pool set bench_rados pg_autoscale_mode off
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 pg_autoscale_mode to off
# ./benchmarks/bench-rados:269 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool application enable bench_rados benchmark
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:03:47,183315156-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:03:47,186099473-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '1ffa2fc4-3808-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'application', 'enable', 'bench_rados', 'benchmark'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 1ffa2fc4-3808-11ec-b51d-53e6e728d2d3 -- ceph osd pool application enable bench_rados benchmark
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
enabled application 'benchmark' on pool 'bench_rados'
# ./benchmarks/bench-rados:270 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados noscrub 1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:03:51,594675653-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:03:51,597804769-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '1ffa2fc4-3808-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'noscrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 1ffa2fc4-3808-11ec-b51d-53e6e728d2d3 -- ceph osd pool set bench_rados noscrub 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 noscrub to 1
# ./benchmarks/bench-rados:271 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados nodeep-scrub 1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:03:56,051808315-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:03:56,054824088-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '1ffa2fc4-3808-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'nodeep-scrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 1ffa2fc4-3808-11ec-b51d-53e6e728d2d3 -- ceph osd pool set bench_rados nodeep-scrub 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 nodeep-scrub to 1
# ./benchmarks/bench-rados:272 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool ls detail
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:04:00,310076906-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:04:00,312883285-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '1ffa2fc4-3808-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'ls', 'detail'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 1ffa2fc4-3808-11ec-b51d-53e6e728d2d3 -- ceph osd pool ls detail
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
pool 1 'device_health_metrics' replicated size 1 min_size 1 crush_rule 0 object_hash rjenkins pg_num 64 pgp_num 64 autoscale_mode on last_change 19 lfor 0/0/16 flags hashpspool stripe_width 0 pg_num_min 1 application mgr_devicehealth
pool 2 'bench_rados' replicated size 1 min_size 1 crush_rule 0 object_hash rjenkins pg_num 128 pgp_num 128 autoscale_mode off last_change 20 flags hashpspool,noscrub,nodeep-scrub stripe_width 0 pg_num_min 128 application benchmark

# ./benchmarks/bench-rados:273 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd df tree
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:04:04,202626215-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:04:04,205993931-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '1ffa2fc4-3808-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'df', 'tree'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 1ffa2fc4-3808-11ec-b51d-53e6e728d2d3 -- ceph osd df tree
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
ID  CLASS  WEIGHT   REWEIGHT  SIZE     RAW USE  DATA     OMAP  META     AVAIL    %USE  VAR   PGS  STATUS  TYPE NAME    
-1         0.09769         -  100 GiB  5.7 MiB  168 KiB   0 B  5.6 MiB  100 GiB  0.01  1.00    -          root default 
-3         0.09769         -  100 GiB  5.7 MiB  168 KiB   0 B  5.6 MiB  100 GiB  0.01  1.00    -              host sm1 
 0    ssd  0.09769   1.00000  100 GiB  5.7 MiB  168 KiB   0 B  5.6 MiB  100 GiB  0.01  1.00  192      up          osd.0
                       TOTAL  100 GiB  5.7 MiB  168 KiB   0 B  5.6 MiB  100 GiB  0.01                                  
MIN/MAX VAR: 1.00/1.00  STDDEV: 0


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:04:07,946515210-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:04:31,760930756-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:04:40,627620140-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:04:49,529004574-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:04:58,419318788-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:04:58,427323863-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:05:07,327547141-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:05:07,335874093-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:05:16,127085323-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:05:16,135125634-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:05:24,933761686-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:05:24,941956349-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:05:33,866706519-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:05:33,875065821-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:05:33,881006056-07:00] INFO: > The cluster is idle now.[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:05:33,884802830-07:00] STAGE: Start benchmarking of mode write ...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:05:33,892016003-07:00] INFO: > Start system activity collection services[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:05:33,898145775-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:341 - rados_bench() > rm --force /mnt/sda8/ceph-research/scripts/output/sys_activity_1MB_write.dat.rados_bench_host.3
# ./benchmarks/bench-rados:342 - rados_bench() > sar_pid_rados_bench_host=475014
# ./benchmarks/bench-rados:342 - rados_bench() > S_TIME_FORMAT=ISO
# ./benchmarks/bench-rados:342 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/output/sys_activity_1MB_write.dat.rados_bench_host.3 2

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:05:33,905241827-07:00] INFO: >> for OSD host[0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:05:33,914277634-07:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.5 bash
10.10.2.5: b'163350\n'
[1] 09:05:35 [SUCCESS] ljishen@10.10.2.5
163350

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:05:35,044540229-07:00] INFO: > Run rados bench[0m
# ./benchmarks/bench-rados:370 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/output/rados_bench_1MB_write.log.3
## ./benchmarks/bench-rados:361 - rados_bench() > count_cpus 32-63,96-127
# ./benchmarks/bench-rados:361 - rados_bench() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 60 write --pool bench_rados -b 1048576 -O 1048576 --concurrent-ios 128 --show-time --write-object --write-omap --write-xattr --no-cleanup
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:05:35,064805668-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:05:35,067572041-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '1ffa2fc4-3808-11ec-b51d-53e6e728d2d3', '--', 'rados', 'bench', '60', 'write', '--pool', 'bench_rados', '-b', '1048576', '-O', '1048576', '--concurrent-ios', '128', '--show-time', '--write-object', '--write-omap', '--write-xattr', '--no-cleanup'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 1ffa2fc4-3808-11ec-b51d-53e6e728d2d3 -- rados bench 60 write --pool bench_rados -b 1048576 -O 1048576 --concurrent-ios 128 --show-time --write-object --write-omap --write-xattr --no-cleanup
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
hints = 1
2021-10-28T16:05:37.925083+0000 Maintaining 128 concurrent writes of 1048576 bytes to objects of size 1048576 for up to 60 seconds or 0 objects
2021-10-28T16:05:37.925096+0000 Object prefix: benchmark_data_node-2.bluefield2.ucsc-cmps10_7
2021-10-28T16:05:37.958073+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-28T16:05:37.958073+0000     0       0         0         0         0         0           -           0
2021-10-28T16:05:38.958197+0000     1      80        80         0         0         0           -           0
2021-10-28T16:05:39.958273+0000     2     127       154        27   13.4991      13.5      1.9474     1.82587
2021-10-28T16:05:40.958349+0000     3     127       221        94   31.3311        67     1.94932     1.93878
2021-10-28T16:05:41.958426+0000     4     127       290       163   40.7471        69     1.74994     1.94284
2021-10-28T16:05:42.958505+0000     5     127       355       228   45.5967        65      1.7081     1.93562
2021-10-28T16:05:43.958586+0000     6     127       413       286   47.6631        58     1.84296     1.92819
2021-10-28T16:05:44.958659+0000     7     127       488       361   51.5676        75     1.70117     1.91463
2021-10-28T16:05:45.958736+0000     8     127       541       414   51.7461        53     1.95375     1.91229
2021-10-28T16:05:46.958803+0000     9     127       577       450   49.9963        36     2.61563     1.94666
2021-10-28T16:05:47.958880+0000    10     127       605       478   47.7965        28     3.04362     2.00549
2021-10-28T16:05:48.958956+0000    11     127       635       508   46.1784        30     3.55434     2.09459
2021-10-28T16:05:49.959031+0000    12     127       664       537   44.7467        29     4.03489     2.19577
2021-10-28T16:05:50.959105+0000    13     127       694       567   43.6121        30     4.10381     2.30301
2021-10-28T16:05:51.959179+0000    14     127       722       595   42.4968        28      4.2534     2.39637
2021-10-28T16:05:52.959250+0000    15     127       775       648   43.1968        53     3.55747     2.53667
2021-10-28T16:05:53.959322+0000    16     127       823       696   43.4968        48     2.46173     2.59452
2021-10-28T16:05:54.959393+0000    17     127       891       764   44.9379        68     2.00099     2.57225
2021-10-28T16:05:55.959467+0000    18     127       953       826   45.8855        62     1.73709     2.53368
2021-10-28T16:05:56.959543+0000    19     127      1017       890   46.8386        64     1.97801     2.49438
2021-10-28T16:05:57.959624+0000 min lat: 1.64065 max lat: 4.89314 avg lat: 2.4716
2021-10-28T16:05:57.959624+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-28T16:05:57.959624+0000    20     127      1064       937   46.8465        47     2.11611      2.4716
2021-10-28T16:05:58.959709+0000    21     127      1095       968   46.0918        31     2.83068     2.47685
2021-10-28T16:05:59.959784+0000    22     127      1095       968   43.9967         0           -     2.47685
2021-10-28T16:06:00.959861+0000    23     127      1130      1003   43.6054      17.5     4.05545      2.5255
2021-10-28T16:06:01.959942+0000    24     127      1150      1023   42.6218        20     4.75528     2.57329
2021-10-28T16:06:02.960018+0000    25     127      1163      1036   41.4369        13     5.38407     2.60946
2021-10-28T16:06:03.960092+0000    26     127      1183      1056   40.6123        20     6.20004      2.6774
2021-10-28T16:06:04.960168+0000    27     127      1196      1069   39.5896        13     5.98281     2.72346
2021-10-28T16:06:05.960243+0000    28     127      1215      1088   38.8542        19     6.52068     2.79782
2021-10-28T16:06:06.960318+0000    29     127      1229      1102   37.9971        14     6.06678     2.84777
2021-10-28T16:06:07.960400+0000    30     127      1262      1135   37.8305        33     6.08353     2.96666
2021-10-28T16:06:08.960480+0000    31     127      1281      1154    37.223        19     5.92657     3.02525
2021-10-28T16:06:09.960557+0000    32     127      1323      1196   37.3722        42     5.39402       3.123
2021-10-28T16:06:10.960637+0000    33     127      1367      1240   37.5729        44     3.67695     3.17055
2021-10-28T16:06:11.960713+0000    34     127      1398      1271   37.3795        31     3.20675     3.18112
2021-10-28T16:06:12.960792+0000    35     127      1438      1311   37.4543        40     3.22815     3.18527
2021-10-28T16:06:13.960866+0000    36     127      1484      1357   37.6916        46     2.97859     3.18722
2021-10-28T16:06:14.960943+0000    37     127      1524      1397   37.7539        40     3.10996     3.18842
2021-10-28T16:06:15.961018+0000    38     127      1566      1439   37.8656        42     3.15166     3.18765
2021-10-28T16:06:16.961090+0000    39     127      1582      1455   37.3049        16     3.10636      3.1907
2021-10-28T16:06:17.961177+0000 min lat: 1.64065 max lat: 7.21861 avg lat: 3.19974
2021-10-28T16:06:17.961177+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-28T16:06:17.961177+0000    40     127      1599      1472   36.7972        17     3.65058     3.19974
2021-10-28T16:06:18.961277+0000    41     127      1614      1487   36.2655        15     4.07117     3.21204
2021-10-28T16:06:19.961359+0000    42     127      1646      1519   36.1639        32     5.11663     3.24961
2021-10-28T16:06:20.961442+0000    43     127      1665      1538   35.7647        19     5.47241     3.28109
2021-10-28T16:06:21.961515+0000    44     127      1679      1552     35.27        14     6.05333     3.30618
2021-10-28T16:06:22.961592+0000    45     127      1707      1580   35.1084        28     5.89798     3.35557
2021-10-28T16:06:23.961673+0000    46     127      1737      1610   34.9973        30     5.16755     3.39645
2021-10-28T16:06:24.961744+0000    47     127      1785      1658   35.2739        48     3.99082     3.43648
2021-10-28T16:06:25.961817+0000    48     127      1853      1726   35.9556        68     2.48397     3.43283
2021-10-28T16:06:26.961886+0000    49     127      1914      1787   36.4666        61     1.80772     3.39115
2021-10-28T16:06:27.961958+0000    50     127      1983      1856   37.1172        69     1.86361     3.33677
2021-10-28T16:06:28.962038+0000    51     127      2053      1926   37.7618        70     1.78658     3.28419
2021-10-28T16:06:29.962113+0000    52     127      2081      1954   37.5741        28     1.76736     3.26532
2021-10-28T16:06:30.962191+0000    53     127      2110      1983   37.4122        29      2.5184     3.25285
2021-10-28T16:06:31.962271+0000    54     127      2132      2005   37.1268        22     3.26653     3.25302
2021-10-28T16:06:32.962345+0000    55     127      2145      2018   36.6881        13     3.69824     3.25623
2021-10-28T16:06:33.962421+0000    56     127      2178      2051   36.6222        33     5.04286     3.28023
2021-10-28T16:06:34.962497+0000    57     127      2178      2051   35.9797         0           -     3.28023
2021-10-28T16:06:35.962566+0000    58     127      2208      2081   35.8766        15     6.30996     3.32016
2021-10-28T16:06:36.962635+0000    59     127      2230      2103   35.6414        22     6.72669     3.35594
2021-10-28T16:06:37.962701+0000 min lat: 1.64065 max lat: 7.28978 avg lat: 3.36954
2021-10-28T16:06:37.962701+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-28T16:06:37.962701+0000    60     127      2238      2111   35.1807         8     6.94262     3.36954
2021-10-28T16:06:38.962825+0000 Total time run:         60.9989
Total writes made:      2239
Write size:             1048576
Object size:            1048576
Bandwidth (MB/sec):     36.7056
Stddev Bandwidth:       20.342
Max bandwidth (MB/sec): 75
Min bandwidth (MB/sec): 0
Average IOPS:           36
Stddev IOPS:            20.358
Max IOPS:               75
Min IOPS:               0
Average Latency(s):     3.4125
Stddev Latency(s):      1.58924
Max latency(s):         7.28978
Min latency(s):         1.5095

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:06:39,589971288-07:00] INFO: > Stop system activity collection services[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:06:39,596118973-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:391 - rados_bench() > kill -INT 475014

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:06:39,602407734-07:00] INFO: >> for OSD host[0m
# ./benchmarks/bench-rados:396 - rados_bench() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.5 kill -INT 163350
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:06:39,610062239-07:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.5 kill -INT 163350
[1] 09:06:41 [SUCCESS] ljishen@10.10.2.5

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:06:41,485282326-07:00] INFO: >>> Copy system activity file from OSD host[0m
## ./benchmarks/bench-rados:403 - rados_bench() > basename -- /tmp/bench-rados/sys_activity_1MB_write.dat.osd_host.3
# ./benchmarks/bench-rados:403 - rados_bench() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.5:/tmp/bench-rados/sys_activity_1MB_write.dat.osd_host.3 /mnt/sda8/ceph-research/scripts/output/sys_activity_1MB_write.dat.osd_host.3


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:06:42,742405735-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:07:06,446980720-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 2.24k objects, 2.2 GiB
    usage:   6.6 GiB used, 93 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:07:06,454801889-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:07:15,193807678-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 2.24k objects, 2.2 GiB
    usage:   6.6 GiB used, 93 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:07:15,202008221-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:07:24,069375142-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 2.24k objects, 2.2 GiB
    usage:   6.6 GiB used, 93 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:07:24,077773699-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:07:32,917833431-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 2.24k objects, 2.2 GiB
    usage:   6.6 GiB used, 93 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:07:32,926156154-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:07:41,742808036-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 2.24k objects, 2.2 GiB
    usage:   6.6 GiB used, 93 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:07:41,750555797-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:07:41,756939937-07:00] INFO: > The cluster is idle now.[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:07:41,760967487-07:00] STAGE: Start benchmarking of mode seq ...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:07:41,768014938-07:00] INFO: > Start system activity collection services[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:07:41,773927700-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:341 - rados_bench() > rm --force /mnt/sda8/ceph-research/scripts/output/sys_activity_1MB_seq.dat.rados_bench_host.3
# ./benchmarks/bench-rados:342 - rados_bench() > sar_pid_rados_bench_host=476371
# ./benchmarks/bench-rados:342 - rados_bench() > S_TIME_FORMAT=ISO
# ./benchmarks/bench-rados:342 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/output/sys_activity_1MB_seq.dat.rados_bench_host.3 2

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:07:41,781144840-07:00] INFO: >> for OSD host[0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:07:41,790304612-07:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.5 bash
10.10.2.5: b'163896\n'
[1] 09:07:43 [SUCCESS] ljishen@10.10.2.5
163896

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:07:43,101856830-07:00] INFO: > Run rados bench[0m
# ./benchmarks/bench-rados:379 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/output/rados_bench_1MB_seq.log.3
## ./benchmarks/bench-rados:376 - rados_bench() > count_cpus 32-63,96-127
# ./benchmarks/bench-rados:376 - rados_bench() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 128 --show-time
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:07:43,122121347-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:07:43,124946391-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '1ffa2fc4-3808-11ec-b51d-53e6e728d2d3', '--', 'rados', 'bench', '99999999', 'seq', '--pool', 'bench_rados', '--concurrent-ios', '128', '--show-time'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 1ffa2fc4-3808-11ec-b51d-53e6e728d2d3 -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 128 --show-time
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
hints = 1
2021-10-28T16:07:46.074324+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-28T16:07:46.074324+0000     0       0         0         0         0         0           -           0
2021-10-28T16:07:47.074424+0000     1     127       780       653   652.889       653     0.19536    0.174667
2021-10-28T16:07:48.074570+0000     2     127      1486      1359   679.393       706    0.170702    0.178989
2021-10-28T16:07:49.074694+0000     3     127      2218      2091   696.898       732    0.166762     0.17745
2021-10-28T16:07:50.074828+0000 Total time run:       3.14834
Total reads made:     2239
Read size:            1048576
Object size:          1048576
Bandwidth (MB/sec):   711.168
Average IOPS:         711
Stddev IOPS:          40.2616
Max IOPS:             732
Min IOPS:             653
Average Latency(s):   0.174861
Max latency(s):       0.287221
Min latency(s):       0.0974243

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:07:50,774811439-07:00] INFO: > Stop system activity collection services[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:07:50,781157258-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:391 - rados_bench() > kill -INT 476371

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:07:50,787277621-07:00] INFO: >> for OSD host[0m
# ./benchmarks/bench-rados:396 - rados_bench() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.5 kill -INT 163896
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:07:50,795483795-07:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.5 kill -INT 163896
[1] 09:07:51 [SUCCESS] ljishen@10.10.2.5

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:07:51,940967274-07:00] INFO: >>> Copy system activity file from OSD host[0m
## ./benchmarks/bench-rados:403 - rados_bench() > basename -- /tmp/bench-rados/sys_activity_1MB_seq.dat.osd_host.3
# ./benchmarks/bench-rados:403 - rados_bench() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.5:/tmp/bench-rados/sys_activity_1MB_seq.dat.osd_host.3 /mnt/sda8/ceph-research/scripts/output/sys_activity_1MB_seq.dat.osd_host.3


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:07:53,020281649-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:08:16,805128646-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 2.24k objects, 2.2 GiB
    usage:   6.6 GiB used, 93 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:08:16,813003647-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:08:25,646473574-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 2.24k objects, 2.2 GiB
    usage:   6.6 GiB used, 93 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:08:25,654460876-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:08:34,543849480-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 2.24k objects, 2.2 GiB
    usage:   6.6 GiB used, 93 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:08:34,551552145-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:08:43,444106486-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 2.24k objects, 2.2 GiB
    usage:   6.6 GiB used, 93 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:08:43,452180011-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:08:52,250765952-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 2.24k objects, 2.2 GiB
    usage:   6.6 GiB used, 93 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:08:52,258663835-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:08:52,264911619-07:00] INFO: > The cluster is idle now.[0m


[1;7;39;49m[2021-10-28T09:08:52,268812069-07:00][RUNNING][ROUND 1/6/21] object_size=4MB[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:08:52,272621358-07:00] STAGE: Launch a Ceph cluster (CEPH_HOST: 10.10.2.1, OSD_HOST: 10.10.2.5) ...[0m
# ./benchmarks/bench-rados:183 - launch_ceph_cluster() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.1 bash -euo pipefail
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:08:52,281989782-07:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.1 bash -euo pipefail
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T09:08:52,690510359-07:00] INFO: > Remove existing clusters\x1b[0m\n'
10.10.2.1: b'## bash:17 -  > basename -- /var/lib/ceph/1ffa2fc4-3808-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'# bash:17 -  > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm --host 10.10.2.1 --host 10.10.2.5 rm-cluster --force --fsid 1ffa2fc4-3808-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T09:08:52,701496873-07:00] DEBUG: HOST_PARAMS: ['--host', '10.10.2.1', '--host', '10.10.2.5']\x1b[0m\n"
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T09:08:52,705208049-07:00] DEBUG: CEPHADM_PARAMS: ['rm-cluster', '--force', '--fsid', '1ffa2fc4-3808-11ec-b51d-53e6e728d2d3']\x1b[0m\n"
10.10.2.1: b"# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:63 -  > /tmp/bench-rados/ceph-research/scripts/parallel-ssh --host 10.10.2.1 --host 10.10.2.5 'sudo python3 -u - rm-cluster --force --fsid 1ffa2fc4-3808-11ec-b51d-53e6e728d2d3'\n"
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T09:08:52,713286826-07:00] DEBUG: command from stdin\x1b[0m\n'
10.10.2.1: b"# /tmp/bench-rados/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host 10.10.2.1 --host 10.10.2.5 'sudo python3 -u - rm-cluster --force --fsid 1ffa2fc4-3808-11ec-b51d-53e6e728d2d3'\n"
10.10.2.1: b'[1] 09:08:58 [SUCCESS] 10.10.2.1\n[2] 09:09:04 [SUCCESS] 10.10.2.5\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T09:09:04,281683464-07:00] INFO: > Deploy a new cluster\x1b[0m\n'
10.10.2.1: b'# bash:24 -  > /tmp/bench-rados/ceph-research/scripts/ceph-deploy -m 10.10.2.1 -o sm1/10.10.2.5:/dev/nvme0n1\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T09:09:04,293998935-07:00] STAGE: Verify passwordless SSH to hosts, passwordless sudo via ljishen, and the existence of devices on hosts...\x1b[0m\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T09:09:04,299083133-07:00] INFO: > Check the host for the monitor\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:181 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@10.10.2.1 sudo --non-interactive --validate\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T09:09:04,447630499-07:00] INFO: > Check hosts for OSDs\x1b[0m\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T09:09:04,452683808-07:00] INFO: >> Check host 'sm1'\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:200 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 sudo --non-interactive --validate\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T09:09:05,535027568-07:00] INFO: >>> Check device '/dev/nvme0n1'\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:215 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 test -b /dev/nvme0n1\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:226 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 lsblk --nodeps --noheadings --output TYPE /dev/nvme0n1\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:227 - verify_parameters() > grep --word-regexp disk\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T09:09:07,661372069-07:00] STAGE: Reset devices on hosts...\x1b[0m\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T09:09:07,666566634-07:00] INFO: > Resetting device '/dev/nvme0n1' on host 'sm1'\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:288 - reset_host_devices() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 sudo sh\n'
10.10.2.1: b'  Removing ceph--16c3dcec--8954--4160--9906--72b01207bdfc-osd--block--8e95be1d--f627--49d8--8cb8--617a63bbd3a1 (252:0)\n'
10.10.2.1: b'  Archiving volume group "ceph-16c3dcec-8954-4160-9906-72b01207bdfc" metadata (seqno 5).\n'
10.10.2.1: b'  Releasing logical volume "osd-block-8e95be1d-f627-49d8-8cb8-617a63bbd3a1"\n'
10.10.2.1: b'  Creating volume group backup "/etc/lvm/backup/ceph-16c3dcec-8954-4160-9906-72b01207bdfc" (seqno 6).\n'
10.10.2.1: b'  Logical volume "osd-block-8e95be1d-f627-49d8-8cb8-617a63bbd3a1" successfully removed\n'
10.10.2.1: b'  Removing physical volume "/dev/nvme0n1" from volume group "ceph-16c3dcec-8954-4160-9906-72b01207bdfc"\n'
10.10.2.1: b'  Volume group "ceph-16c3dcec-8954-4160-9906-72b01207bdfc" successfully removed\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T09:09:09,996452441-07:00] STAGE: Bootstrap a new cluster...\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:317 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.1 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T09:09:10,006792829-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T09:09:10,010539142-07:00] DEBUG: CEPHADM_PARAMS: ['bootstrap', '--config', '/tmp/bench-rados/ceph-research/scripts/ceph.conf', '--mon-ip', '10.10.2.1', '--output-dir', '/tmp/bench-rados/deployment_data_root/etc/ceph', '--ssh-user', 'ljishen', '--skip-dashboard', '--no-minimize-config', '--allow-overwrite', '--allow-fqdn-hostname', '--orphan-initial-daemons', '--skip-monitoring-stack', '--single-host-defaults']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.1 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults\n'
10.10.2.1: b'Verifying podman|docker is present...\nVerifying lvm2 is present...\n'
10.10.2.1: b'Verifying time synchronization is in place...\n'
10.10.2.1: b'Unit ntp.service is enabled and running\n'
10.10.2.1: b'Repeating the final host check...\n'
10.10.2.1: b'podman|docker (/usr/bin/docker) is present\n'
10.10.2.1: b'systemctl is present\nlvcreate is present\n'
10.10.2.1: b'Unit ntp.service is enabled and running\n'
10.10.2.1: b'Host looks OK\n'
10.10.2.1: b'Cluster fsid: 62a19fb4-3809-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Verifying IP 10.10.2.1 port 3300 ...\n'
10.10.2.1: b'Verifying IP 10.10.2.1 port 6789 ...\n'
10.10.2.1: b'Mon IP `10.10.2.1` is in CIDR network `10.10.2.0/24`\n'
10.10.2.1: b'- internal network (--cluster-network) has not been provided, OSD replication will default to the public_network\n'
10.10.2.1: b'Adjusting default settings to suit single-host cluster...\n'
10.10.2.1: b'Pulling container image quay.io/ceph/ceph:v16...\n'
10.10.2.1: b'Ceph version: ceph version 16.2.6 (ee28fb57e47e9f88813e24bbf4c14496ca299d31) pacific (stable)\n'
10.10.2.1: b'Extracting ceph user uid/gid from container image...\n'
10.10.2.1: b'Creating initial keys...\n'
10.10.2.1: b'Creating initial monmap...\n'
10.10.2.1: b'Creating mon...\n'
10.10.2.1: b'Waiting for mon to start...\n'
10.10.2.1: b'Waiting for mon...\n'
10.10.2.1: b'mon is available\nSetting mon public_network to 10.10.2.0/24\n'
10.10.2.1: b'Wrote config to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf\n'
10.10.2.1: b'Wrote keyring to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring\n'
10.10.2.1: b'Creating mgr...\n'
10.10.2.1: b'Verifying port 9283 ...\n'
10.10.2.1: b'Waiting for mgr to start...\n'
10.10.2.1: b'Waiting for mgr...\n'
10.10.2.1: b'mgr not available, waiting (1/15)...\n'
10.10.2.1: b'mgr not available, waiting (2/15)...\n'
10.10.2.1: b'mgr is available\n'
10.10.2.1: b'Enabling cephadm module...\n'
10.10.2.1: b'Waiting for the mgr to restart...\n'
10.10.2.1: b'Waiting for mgr epoch 5...\n'
10.10.2.1: b'mgr epoch 5 is available\nSetting orchestrator backend to cephadm...\n'
10.10.2.1: b'Generating ssh key...\n'
10.10.2.1: b'Wrote public SSH key to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub\n'
10.10.2.1: b'Adding key to ljishen@localhost authorized_keys...\n'
10.10.2.1: b'Adding host node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us...\n'
10.10.2.1: b'Deploying unmanaged mon service...\n'
10.10.2.1: b'Deploying unmanaged mgr service...\n'
10.10.2.1: b'You can access the Ceph CLI with:\n\n\tsudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --fsid 62a19fb4-3809-11ec-b51d-53e6e728d2d3 -c /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf -k /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring\n\n'
10.10.2.1: b'Please consider enabling telemetry to help improve Ceph:\n\n\tceph telemetry on\n\nFor more information see:\n\n\thttps://docs.ceph.com/docs/pacific/mgr/telemetry/\n\n'
10.10.2.1: b'Bootstrap complete.\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T09:10:11,741529356-07:00] INFO: Sleep for 20 seconds\x1b[0m\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T09:10:31,749505834-07:00] INFO: > Set the default number of monitor daemons to 1\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:337 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T09:10:31,759964485-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T09:10:31,763631448-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mon', '1']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1\n'
10.10.2.1: b'Inferring fsid 62a19fb4-3809-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/62a19fb4-3809-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b'Scheduled mon update...\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T09:10:40,929645936-07:00] INFO: > Set the default number of manager daemons to 1\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:342 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T09:10:40,939784675-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T09:10:40,943411433-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mgr', '1']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1\n'
10.10.2.1: b'Inferring fsid 62a19fb4-3809-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/62a19fb4-3809-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b'Scheduled mgr update...\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T09:10:50,103791931-07:00] STAGE: Add hosts to the cluster...\x1b[0m\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T09:10:50,109442323-07:00] INFO: > Adding host 'sm1'\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:361 - add_hosts() > ssh-copy-id -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -f -i /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub ljishen@sm1\n'
10.10.2.1: b'/usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: "/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub"\n'
10.10.2.1: b'\nNumber of key(s) added: 1\n\nNow try logging into the machine, with:   "ssh -o \'GlobalKnownHostsFile=/dev/null\' -o \'LogLevel=ERROR\' -o \'PasswordAuthentication=no\' -o \'StrictHostKeyChecking=no\' -o \'UserKnownHostsFile=/dev/null\' \'ljishen@sm1\'"\nand check to make sure that only the key(s) you wanted were added.\n\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:367 - add_hosts() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add sm1 10.10.2.5\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T09:10:51,261603811-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T09:10:51,265302434-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'host', 'add', 'sm1', '10.10.2.5']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add sm1 10.10.2.5\n'
10.10.2.1: b'Inferring fsid 62a19fb4-3809-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/62a19fb4-3809-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b"Added host 'sm1' with addr '10.10.2.5'\n"
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T09:11:02,306828129-07:00] INFO: Sleep for 20 seconds\x1b[0m\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T09:11:22,311670748-07:00] STAGE: Add OSDs to the cluster...\x1b[0m\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T09:11:22,318707526-07:00] INFO: > Adding devices ['/dev/nvme0n1'] on host 'sm1' as OSDs\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:388 - add_host_devices_as_osds() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd sm1:/dev/nvme0n1\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T09:11:22,328935303-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T09:11:22,332874357-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'daemon', 'add', 'osd', 'sm1:/dev/nvme0n1']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd sm1:/dev/nvme0n1\n'
10.10.2.1: b'Inferring fsid 62a19fb4-3809-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/62a19fb4-3809-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b"Created osd(s) 0 on host 'sm1'\n"
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T09:11:46,985308405-07:00] INFO: > Sleep for 20 seconds\x1b[0m\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T09:12:06,990807768-07:00] STAGE: Show Cluster Status\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:408 - show_status() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T09:12:07,000833385-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T09:12:07,004432210-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', '--status']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status\n'
10.10.2.1: b'Inferring fsid 62a19fb4-3809-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/62a19fb4-3809-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b'  cluster:\n    id:     62a19fb4-3809-11ec-b51d-53e6e728d2d3\n    health: HEALTH_WARN\n            1 pool(s) have no replicas configured\n \n  services:\n    mon: 1 daemons, quorum node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us (age 2m)\n    mgr: node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us.aaofik(active, since 2m)\n    osd: 1 osds: 1 up (since 19s), 1 in (since 40s)\n \n  data:\n    pools:   1 pools, 1 pgs\n    objects: 0 objects, 0 B\n    usage:   4.7 MiB used, 100 GiB / 100 GiB avail\n    pgs:     1 active+clean\n \n'
[1] 09:12:15 [SUCCESS] ljishen@10.10.2.1

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:08:52,690510359-07:00] INFO: > Remove existing clusters[0m
## bash:17 -  > basename -- /var/lib/ceph/1ffa2fc4-3808-11ec-b51d-53e6e728d2d3
# bash:17 -  > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm --host 10.10.2.1 --host 10.10.2.5 rm-cluster --force --fsid 1ffa2fc4-3808-11ec-b51d-53e6e728d2d3
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:08:52,701496873-07:00] DEBUG: HOST_PARAMS: ['--host', '10.10.2.1', '--host', '10.10.2.5'][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:08:52,705208049-07:00] DEBUG: CEPHADM_PARAMS: ['rm-cluster', '--force', '--fsid', '1ffa2fc4-3808-11ec-b51d-53e6e728d2d3'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:63 -  > /tmp/bench-rados/ceph-research/scripts/parallel-ssh --host 10.10.2.1 --host 10.10.2.5 'sudo python3 -u - rm-cluster --force --fsid 1ffa2fc4-3808-11ec-b51d-53e6e728d2d3'
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:08:52,713286826-07:00] DEBUG: command from stdin[0m
# /tmp/bench-rados/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host 10.10.2.1 --host 10.10.2.5 'sudo python3 -u - rm-cluster --force --fsid 1ffa2fc4-3808-11ec-b51d-53e6e728d2d3'
[1] 09:08:58 [SUCCESS] 10.10.2.1
[2] 09:09:04 [SUCCESS] 10.10.2.5

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:09:04,281683464-07:00] INFO: > Deploy a new cluster[0m
# bash:24 -  > /tmp/bench-rados/ceph-research/scripts/ceph-deploy -m 10.10.2.1 -o sm1/10.10.2.5:/dev/nvme0n1


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:09:04,293998935-07:00] STAGE: Verify passwordless SSH to hosts, passwordless sudo via ljishen, and the existence of devices on hosts...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:09:04,299083133-07:00] INFO: > Check the host for the monitor[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:181 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@10.10.2.1 sudo --non-interactive --validate

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:09:04,447630499-07:00] INFO: > Check hosts for OSDs[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:09:04,452683808-07:00] INFO: >> Check host 'sm1'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:200 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 sudo --non-interactive --validate

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:09:05,535027568-07:00] INFO: >>> Check device '/dev/nvme0n1'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:215 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 test -b /dev/nvme0n1
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:226 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 lsblk --nodeps --noheadings --output TYPE /dev/nvme0n1
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:227 - verify_parameters() > grep --word-regexp disk


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:09:07,661372069-07:00] STAGE: Reset devices on hosts...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:09:07,666566634-07:00] INFO: > Resetting device '/dev/nvme0n1' on host 'sm1'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:288 - reset_host_devices() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 sudo sh
  Removing ceph--16c3dcec--8954--4160--9906--72b01207bdfc-osd--block--8e95be1d--f627--49d8--8cb8--617a63bbd3a1 (252:0)
  Archiving volume group "ceph-16c3dcec-8954-4160-9906-72b01207bdfc" metadata (seqno 5).
  Releasing logical volume "osd-block-8e95be1d-f627-49d8-8cb8-617a63bbd3a1"
  Creating volume group backup "/etc/lvm/backup/ceph-16c3dcec-8954-4160-9906-72b01207bdfc" (seqno 6).
  Logical volume "osd-block-8e95be1d-f627-49d8-8cb8-617a63bbd3a1" successfully removed
  Removing physical volume "/dev/nvme0n1" from volume group "ceph-16c3dcec-8954-4160-9906-72b01207bdfc"
  Volume group "ceph-16c3dcec-8954-4160-9906-72b01207bdfc" successfully removed


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:09:09,996452441-07:00] STAGE: Bootstrap a new cluster...[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:317 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.1 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:09:10,006792829-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:09:10,010539142-07:00] DEBUG: CEPHADM_PARAMS: ['bootstrap', '--config', '/tmp/bench-rados/ceph-research/scripts/ceph.conf', '--mon-ip', '10.10.2.1', '--output-dir', '/tmp/bench-rados/deployment_data_root/etc/ceph', '--ssh-user', 'ljishen', '--skip-dashboard', '--no-minimize-config', '--allow-overwrite', '--allow-fqdn-hostname', '--orphan-initial-daemons', '--skip-monitoring-stack', '--single-host-defaults'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.1 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults
Verifying podman|docker is present...
Verifying lvm2 is present...
Verifying time synchronization is in place...
Unit ntp.service is enabled and running
Repeating the final host check...
podman|docker (/usr/bin/docker) is present
systemctl is present
lvcreate is present
Unit ntp.service is enabled and running
Host looks OK
Cluster fsid: 62a19fb4-3809-11ec-b51d-53e6e728d2d3
Verifying IP 10.10.2.1 port 3300 ...
Verifying IP 10.10.2.1 port 6789 ...
Mon IP `10.10.2.1` is in CIDR network `10.10.2.0/24`
- internal network (--cluster-network) has not been provided, OSD replication will default to the public_network
Adjusting default settings to suit single-host cluster...
Pulling container image quay.io/ceph/ceph:v16...
Ceph version: ceph version 16.2.6 (ee28fb57e47e9f88813e24bbf4c14496ca299d31) pacific (stable)
Extracting ceph user uid/gid from container image...
Creating initial keys...
Creating initial monmap...
Creating mon...
Waiting for mon to start...
Waiting for mon...
mon is available
Setting mon public_network to 10.10.2.0/24
Wrote config to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf
Wrote keyring to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring
Creating mgr...
Verifying port 9283 ...
Waiting for mgr to start...
Waiting for mgr...
mgr not available, waiting (1/15)...
mgr not available, waiting (2/15)...
mgr is available
Enabling cephadm module...
Waiting for the mgr to restart...
Waiting for mgr epoch 5...
mgr epoch 5 is available
Setting orchestrator backend to cephadm...
Generating ssh key...
Wrote public SSH key to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub
Adding key to ljishen@localhost authorized_keys...
Adding host node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us...
Deploying unmanaged mon service...
Deploying unmanaged mgr service...
You can access the Ceph CLI with:

	sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --fsid 62a19fb4-3809-11ec-b51d-53e6e728d2d3 -c /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf -k /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring

Please consider enabling telemetry to help improve Ceph:

	ceph telemetry on

For more information see:

	https://docs.ceph.com/docs/pacific/mgr/telemetry/

Bootstrap complete.

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:10:11,741529356-07:00] INFO: Sleep for 20 seconds[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:10:31,749505834-07:00] INFO: > Set the default number of monitor daemons to 1[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:337 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:10:31,759964485-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:10:31,763631448-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mon', '1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1
Inferring fsid 62a19fb4-3809-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/62a19fb4-3809-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Scheduled mon update...

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:10:40,929645936-07:00] INFO: > Set the default number of manager daemons to 1[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:342 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:10:40,939784675-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:10:40,943411433-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mgr', '1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1
Inferring fsid 62a19fb4-3809-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/62a19fb4-3809-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Scheduled mgr update...


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:10:50,103791931-07:00] STAGE: Add hosts to the cluster...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:10:50,109442323-07:00] INFO: > Adding host 'sm1'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:361 - add_hosts() > ssh-copy-id -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -f -i /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub ljishen@sm1
/usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: "/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub"

Number of key(s) added: 1

Now try logging into the machine, with:   "ssh -o 'GlobalKnownHostsFile=/dev/null' -o 'LogLevel=ERROR' -o 'PasswordAuthentication=no' -o 'StrictHostKeyChecking=no' -o 'UserKnownHostsFile=/dev/null' 'ljishen@sm1'"
and check to make sure that only the key(s) you wanted were added.

# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:367 - add_hosts() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add sm1 10.10.2.5
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:10:51,261603811-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:10:51,265302434-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'host', 'add', 'sm1', '10.10.2.5'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add sm1 10.10.2.5
Inferring fsid 62a19fb4-3809-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/62a19fb4-3809-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Added host 'sm1' with addr '10.10.2.5'

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:11:02,306828129-07:00] INFO: Sleep for 20 seconds[0m


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:11:22,311670748-07:00] STAGE: Add OSDs to the cluster...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:11:22,318707526-07:00] INFO: > Adding devices ['/dev/nvme0n1'] on host 'sm1' as OSDs[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:388 - add_host_devices_as_osds() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd sm1:/dev/nvme0n1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:11:22,328935303-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:11:22,332874357-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'daemon', 'add', 'osd', 'sm1:/dev/nvme0n1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd sm1:/dev/nvme0n1
Inferring fsid 62a19fb4-3809-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/62a19fb4-3809-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Created osd(s) 0 on host 'sm1'

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:11:46,985308405-07:00] INFO: > Sleep for 20 seconds[0m


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:12:06,990807768-07:00] STAGE: Show Cluster Status[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:408 - show_status() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:12:07,000833385-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:12:07,004432210-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', '--status'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status
Inferring fsid 62a19fb4-3809-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/62a19fb4-3809-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
  cluster:
    id:     62a19fb4-3809-11ec-b51d-53e6e728d2d3
    health: HEALTH_WARN
            1 pool(s) have no replicas configured
 
  services:
    mon: 1 daemons, quorum node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us (age 2m)
    mgr: node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us.aaofik(active, since 2m)
    osd: 1 osds: 1 up (since 19s), 1 in (since 40s)
 
  data:
    pools:   1 pools, 1 pgs
    objects: 0 objects, 0 B
    usage:   4.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     1 active+clean
 

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:12:15,779181384-07:00] INFO: > Copy configuration files to local[0m
# ./benchmarks/bench-rados:220 - launch_ceph_cluster() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.1 sudo chown --changes --recursive ljishen /tmp/bench-rados/deployment_data_root/etc/ceph
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:12:15,787006259-07:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.1 sudo chown --changes --recursive ljishen /tmp/bench-rados/deployment_data_root/etc/ceph
[1] 09:12:15 [SUCCESS] ljishen@10.10.2.1
# ./benchmarks/bench-rados:223 - launch_ceph_cluster() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.1:/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf
# ./benchmarks/bench-rados:226 - launch_ceph_cluster() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.1:/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:12:16,264873680-07:00] STAGE: Update local ceph.conf as a client...[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:12:16,268550218-07:00] STAGE: Configure Ceph pool...[0m
# ./benchmarks/bench-rados:257 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_max_backfills 32
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:12:16,290671313-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:12:16,293713676-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '62a19fb4-3809-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'config', 'set', 'osd', 'osd_max_backfills', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 62a19fb4-3809-11ec-b51d-53e6e728d2d3 -- ceph config set osd osd_max_backfills 32
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:258 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_active 32
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:12:20,195183182-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:12:20,197960946-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '62a19fb4-3809-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_active', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 62a19fb4-3809-11ec-b51d-53e6e728d2d3 -- ceph config set osd osd_recovery_max_active 32
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:259 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_single_start 8
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:12:24,046190909-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:12:24,049346245-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '62a19fb4-3809-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_single_start', '8'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 62a19fb4-3809-11ec-b51d-53e6e728d2d3 -- ceph config set osd osd_recovery_max_single_start 8
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:260 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_op_priority 63
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:12:27,905470664-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:12:27,908776654-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '62a19fb4-3809-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_op_priority', '63'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 62a19fb4-3809-11ec-b51d-53e6e728d2d3 -- ceph config set osd osd_recovery_op_priority 63
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:262 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config show-with-defaults osd.0
# ./benchmarks/bench-rados:263 - configure_ceph_pool() > grep 'osd_max_backfills\|osd_recovery'
# ./benchmarks/bench-rados:263 - configure_ceph_pool() > column -t -s ' '
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
osd_max_backfills                        32         mon
osd_recovery_cost                        20971520   default
osd_recovery_delay_start                 0.000000   default
osd_recovery_max_active                  32         mon
osd_recovery_max_active_hdd              3          default
osd_recovery_max_active_ssd              10         default
osd_recovery_max_chunk                   8388608    default
osd_recovery_max_omap_entries_per_chunk  8096       default
osd_recovery_max_single_start            8          mon
osd_recovery_op_priority                 3          default
osd_recovery_op_warn_multiple            16         default
osd_recovery_priority                    5          default
osd_recovery_retry_interval              30.000000  default
osd_recovery_sleep                       0.000000   default
osd_recovery_sleep_hdd                   0.100000   default
osd_recovery_sleep_hybrid                0.025000   default
osd_recovery_sleep_ssd                   0.000000   default
# ./benchmarks/bench-rados:265 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool create bench_rados 128 128 replicated --size 1 --pg-num-min 128
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:12:35,395346544-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:12:35,398364251-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '62a19fb4-3809-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'create', 'bench_rados', '128', '128', 'replicated', '--size', '1', '--pg-num-min', '128'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 62a19fb4-3809-11ec-b51d-53e6e728d2d3 -- ceph osd pool create bench_rados 128 128 replicated --size 1 --pg-num-min 128
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
pool 'bench_rados' created
# ./benchmarks/bench-rados:267 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados min_size 1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:12:38,760925733-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:12:38,763837560-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '62a19fb4-3809-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'min_size', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 62a19fb4-3809-11ec-b51d-53e6e728d2d3 -- ceph osd pool set bench_rados min_size 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 min_size to 1
# ./benchmarks/bench-rados:268 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados pg_autoscale_mode off
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:12:42,429459408-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:12:42,432453500-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '62a19fb4-3809-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'pg_autoscale_mode', 'off'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 62a19fb4-3809-11ec-b51d-53e6e728d2d3 -- ceph osd pool set bench_rados pg_autoscale_mode off
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 pg_autoscale_mode to off
# ./benchmarks/bench-rados:269 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool application enable bench_rados benchmark
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:12:47,089013827-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:12:47,092076528-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '62a19fb4-3809-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'application', 'enable', 'bench_rados', 'benchmark'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 62a19fb4-3809-11ec-b51d-53e6e728d2d3 -- ceph osd pool application enable bench_rados benchmark
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
enabled application 'benchmark' on pool 'bench_rados'
# ./benchmarks/bench-rados:270 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados noscrub 1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:12:51,387007734-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:12:51,390196182-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '62a19fb4-3809-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'noscrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 62a19fb4-3809-11ec-b51d-53e6e728d2d3 -- ceph osd pool set bench_rados noscrub 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 noscrub to 1
# ./benchmarks/bench-rados:271 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados nodeep-scrub 1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:12:55,926290214-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:12:55,929152808-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '62a19fb4-3809-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'nodeep-scrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 62a19fb4-3809-11ec-b51d-53e6e728d2d3 -- ceph osd pool set bench_rados nodeep-scrub 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 nodeep-scrub to 1
# ./benchmarks/bench-rados:272 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool ls detail
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:13:00,203210841-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:13:00,206069828-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '62a19fb4-3809-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'ls', 'detail'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 62a19fb4-3809-11ec-b51d-53e6e728d2d3 -- ceph osd pool ls detail
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
pool 1 'device_health_metrics' replicated size 1 min_size 1 crush_rule 0 object_hash rjenkins pg_num 64 pgp_num 64 autoscale_mode on last_change 19 lfor 0/0/16 flags hashpspool stripe_width 0 pg_num_min 1 application mgr_devicehealth
pool 2 'bench_rados' replicated size 1 min_size 1 crush_rule 0 object_hash rjenkins pg_num 128 pgp_num 128 autoscale_mode off last_change 20 flags hashpspool,noscrub,nodeep-scrub stripe_width 0 pg_num_min 128 application benchmark

# ./benchmarks/bench-rados:273 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd df tree
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:13:03,993646588-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:13:03,996858300-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '62a19fb4-3809-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'df', 'tree'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 62a19fb4-3809-11ec-b51d-53e6e728d2d3 -- ceph osd df tree
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
ID  CLASS  WEIGHT   REWEIGHT  SIZE     RAW USE  DATA     OMAP  META     AVAIL    %USE  VAR   PGS  STATUS  TYPE NAME    
-1         0.09769         -  100 GiB  5.7 MiB  168 KiB   0 B  5.6 MiB  100 GiB  0.01  1.00    -          root default 
-3         0.09769         -  100 GiB  5.7 MiB  168 KiB   0 B  5.6 MiB  100 GiB  0.01  1.00    -              host sm1 
 0    ssd  0.09769   1.00000  100 GiB  5.7 MiB  168 KiB   0 B  5.6 MiB  100 GiB  0.01  1.00  192      up          osd.0
                       TOTAL  100 GiB  5.7 MiB  168 KiB   0 B  5.6 MiB  100 GiB  0.01                                  
MIN/MAX VAR: 1.00/1.00  STDDEV: 0


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:13:07,871127864-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:13:31,683729032-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:13:40,732655488-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:13:49,624146273-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:13:58,455867030-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:13:58,464057974-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:14:07,270875334-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:14:07,278997390-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:14:16,102008185-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:14:16,110100073-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:14:24,918882408-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:14:24,926769611-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:14:33,749533684-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:14:33,757376202-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:14:33,763542322-07:00] INFO: > The cluster is idle now.[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:14:33,767210464-07:00] STAGE: Start benchmarking of mode write ...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:14:33,773930277-07:00] INFO: > Start system activity collection services[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:14:33,779918360-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:341 - rados_bench() > rm --force /mnt/sda8/ceph-research/scripts/output/sys_activity_4MB_write.dat.rados_bench_host.1
# ./benchmarks/bench-rados:342 - rados_bench() > sar_pid_rados_bench_host=482157
# ./benchmarks/bench-rados:342 - rados_bench() > S_TIME_FORMAT=ISO
# ./benchmarks/bench-rados:342 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/output/sys_activity_4MB_write.dat.rados_bench_host.1 2

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:14:33,787166349-07:00] INFO: >> for OSD host[0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:14:33,796480600-07:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.5 bash
10.10.2.5: b'168744\n'
[1] 09:14:34 [SUCCESS] ljishen@10.10.2.5
168744

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:14:34,928286000-07:00] INFO: > Run rados bench[0m
# ./benchmarks/bench-rados:370 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/output/rados_bench_4MB_write.log.1
## ./benchmarks/bench-rados:361 - rados_bench() > count_cpus 32-63,96-127
# ./benchmarks/bench-rados:361 - rados_bench() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 60 write --pool bench_rados -b 4194304 -O 4194304 --concurrent-ios 128 --show-time --write-object --write-omap --write-xattr --no-cleanup
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:14:34,948465066-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:14:34,951527276-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '62a19fb4-3809-11ec-b51d-53e6e728d2d3', '--', 'rados', 'bench', '60', 'write', '--pool', 'bench_rados', '-b', '4194304', '-O', '4194304', '--concurrent-ios', '128', '--show-time', '--write-object', '--write-omap', '--write-xattr', '--no-cleanup'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 62a19fb4-3809-11ec-b51d-53e6e728d2d3 -- rados bench 60 write --pool bench_rados -b 4194304 -O 4194304 --concurrent-ios 128 --show-time --write-object --write-omap --write-xattr --no-cleanup
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
hints = 1
2021-10-28T16:14:37.821554+0000 Maintaining 128 concurrent writes of 4194304 bytes to objects of size 4194304 for up to 60 seconds or 0 objects
2021-10-28T16:14:37.821572+0000 Object prefix: benchmark_data_node-2.bluefield2.ucsc-cmps10_7
2021-10-28T16:14:37.947952+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-28T16:14:37.947952+0000     0       0         0         0         0         0           -           0
2021-10-28T16:14:38.948101+0000     1      21        21         0         0         0           -           0
2021-10-28T16:14:39.948214+0000     2      38        38         0         0         0           -           0
2021-10-28T16:14:40.948326+0000     3      53        53         0         0         0           -           0
2021-10-28T16:14:41.948398+0000     4      72        72         0         0         0           -           0
2021-10-28T16:14:42.948511+0000     5      87        87         0         0         0           -           0
2021-10-28T16:14:43.948624+0000     6     106       106         0         0         0           -           0
2021-10-28T16:14:44.948742+0000     7     121       121         0         0         0           -           0
2021-10-28T16:14:45.948840+0000     8     127       136         9   4.49955       4.5     7.97289     7.60962
2021-10-28T16:14:46.948909+0000     9     127       146        19   8.44362        40     8.12525     7.83769
2021-10-28T16:14:47.949022+0000    10     127       154        27   10.7989        32     8.77033     8.07996
2021-10-28T16:14:48.949132+0000    11     127       160        33   11.9988        24     9.40241     8.28336
2021-10-28T16:14:49.949227+0000    12     127       170        43   14.3319        40      9.6375     8.58752
2021-10-28T16:14:50.949339+0000    13     127       178        51   15.6907        32     10.1337     8.84112
2021-10-28T16:14:51.949406+0000    14     127       192        65   18.5696        56     10.3185     9.19323
2021-10-28T16:14:52.949519+0000    15     127       207        80   21.3312        60     10.5954      9.4509
2021-10-28T16:14:53.949614+0000    16     127       224        97   24.2476        68     10.5484     9.65085
2021-10-28T16:14:54.949726+0000    17     127       238       111    26.115        56     10.5672     9.77008
2021-10-28T16:14:55.949838+0000    18     127       257       130    28.886        76     10.5233      9.8942
2021-10-28T16:14:56.949910+0000    19     127       266       139   29.2603        36     10.6018     9.94277
2021-10-28T16:14:57.950009+0000 min lat: 7.4235 max lat: 10.9215 avg lat: 9.98235
2021-10-28T16:14:57.950009+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-28T16:14:57.950009+0000    20     127       273       146   29.1971        28     10.9215     9.98235
2021-10-28T16:14:58.950128+0000    21     127       278       151    28.759        20     11.4196     10.0274
2021-10-28T16:14:59.950242+0000    22     127       282       155    28.179        16     11.9182     10.0762
2021-10-28T16:15:00.950356+0000    23     127       286       159   27.6494        16     12.4135      10.135
2021-10-28T16:15:01.950428+0000    24     127       290       163    27.164        16     12.7941      10.201
2021-10-28T16:15:02.950548+0000    25     127       294       167   26.7173        16     13.2895      10.275
2021-10-28T16:15:03.950668+0000    26     127       295       168   25.8435         4     13.1385      10.292
2021-10-28T16:15:04.950785+0000    27     127       307       180   26.6639        48      13.649     10.5303
2021-10-28T16:15:05.950883+0000    28     127       316       189   26.9973        36     13.9083     10.6931
2021-10-28T16:15:06.950958+0000    29     127       329       202   27.8593        52     14.3776     10.9243
2021-10-28T16:15:07.951071+0000    30     127       337       210   27.9972        32     14.5714     11.0654
2021-10-28T16:15:08.951184+0000    31     127       347       220   28.3842        40     15.1498     11.2469
2021-10-28T16:15:09.951282+0000    32     127       356       229   28.6221        36     15.5532     11.4114
2021-10-28T16:15:10.951396+0000    33     127       366       239   28.9667        40     15.8925     11.5958
2021-10-28T16:15:11.951474+0000    34     127       377       250   29.4088        44     16.1606      11.799
2021-10-28T16:15:12.951591+0000    35     127       385       258   29.4827        32      16.699     11.9483
2021-10-28T16:15:13.951685+0000    36     127       393       266   29.5526        32     16.8453     12.0972
2021-10-28T16:15:14.951796+0000    37     127       401       274   29.6186        32     16.5552     12.2366
2021-10-28T16:15:15.951907+0000    38     127       409       282   29.6812        32     15.5737     12.3451
2021-10-28T16:15:16.951979+0000    39     127       417       290   29.7406        32     14.6226     12.4207
2021-10-28T16:15:17.952079+0000 min lat: 7.4235 max lat: 17.4625 avg lat: 12.4699
2021-10-28T16:15:17.952079+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-28T16:15:17.952079+0000    40     127       425       298    29.797        32     13.4798     12.4699
2021-10-28T16:15:18.952198+0000    41     127       433       306   29.8506        32     13.5859     12.5068
2021-10-28T16:15:19.952318+0000    42     127       452       325   30.9492        76     13.3059     12.5775
2021-10-28T16:15:20.952433+0000    43     127       469       342   31.8107        68     12.7686     12.6013
2021-10-28T16:15:21.952506+0000    44     127       483       356   32.3603        56     12.1977      12.592
2021-10-28T16:15:22.952619+0000    45     127       502       375   33.3299        76     11.1567     12.5479
2021-10-28T16:15:23.952730+0000    46     127       519       392   34.0835        68     10.5259     12.4808
2021-10-28T16:15:24.952849+0000    47     127       524       397   33.7838        20     10.4567     12.4572
2021-10-28T16:15:25.952950+0000    48     127       532       405   33.7465        32     10.8235     12.4257
2021-10-28T16:15:26.953025+0000    49     127       536       409   33.3844        16     11.3993     12.4176
2021-10-28T16:15:27.953105+0000    50     127       536       409   32.7167         0           -     12.4176
2021-10-28T16:15:28.953204+0000    51     127       544       417   32.7026        16     12.2333     12.4144
2021-10-28T16:15:29.953300+0000    52     127       549       422   32.4582        20     12.8351     12.4209
2021-10-28T16:15:30.953412+0000    53     127       552       425   32.0722        12     12.9968     12.4258
2021-10-28T16:15:31.953488+0000    54     127       558       431   31.9227        24     13.6746     12.4431
2021-10-28T16:15:32.953602+0000    55     127       566       439    31.924        32     13.5539     12.4641
2021-10-28T16:15:33.953696+0000    56     127       576       449   32.0682        40     13.9518     12.4943
2021-10-28T16:15:34.953809+0000    57     127       589       462   32.4178        52     14.2695     12.5444
2021-10-28T16:15:35.953922+0000    58     127       600       473   32.6174        44     14.7595     12.5941
2021-10-28T16:15:36.953995+0000    59     127       610       483   32.7425        40     15.2189     12.6461
2021-10-28T16:15:37.954091+0000 min lat: 7.4235 max lat: 17.4625 avg lat: 12.6922
2021-10-28T16:15:37.954091+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-28T16:15:37.954091+0000    60     127       618       491     32.73        32     15.4142     12.6922
2021-10-28T16:15:38.954244+0000 Total time run:         60.7208
Total writes made:      619
Write size:             4194304
Object size:            4194304
Bandwidth (MB/sec):     40.7768
Stddev Bandwidth:       21.0842
Max bandwidth (MB/sec): 76
Min bandwidth (MB/sec): 0
Average IOPS:           10
Stddev IOPS:            5.27383
Max IOPS:               19
Min IOPS:               0
Average Latency(s):     11.747
Stddev Latency(s):      3.63312
Max latency(s):         17.4625
Min latency(s):         0.794077

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:15:39,545122327-07:00] INFO: > Stop system activity collection services[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:15:39,551323573-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:391 - rados_bench() > kill -INT 482157

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:15:39,557500172-07:00] INFO: >> for OSD host[0m
# ./benchmarks/bench-rados:396 - rados_bench() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.5 kill -INT 168744
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:15:39,565662373-07:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.5 kill -INT 168744
[1] 09:15:41 [SUCCESS] ljishen@10.10.2.5

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:15:41,390026500-07:00] INFO: >>> Copy system activity file from OSD host[0m
## ./benchmarks/bench-rados:403 - rados_bench() > basename -- /tmp/bench-rados/sys_activity_4MB_write.dat.osd_host.1
# ./benchmarks/bench-rados:403 - rados_bench() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.5:/tmp/bench-rados/sys_activity_4MB_write.dat.osd_host.1 /mnt/sda8/ceph-research/scripts/output/sys_activity_4MB_write.dat.osd_host.1


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:15:42,927521471-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:16:06,692485319-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 620 objects, 2.4 GiB
    usage:   7.3 GiB used, 93 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:16:06,700818823-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:16:15,523026911-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 620 objects, 2.4 GiB
    usage:   7.3 GiB used, 93 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:16:15,531373329-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:16:24,372346550-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 620 objects, 2.4 GiB
    usage:   7.3 GiB used, 93 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:16:24,380396179-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:16:33,186411105-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 620 objects, 2.4 GiB
    usage:   7.3 GiB used, 93 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:16:33,194215762-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:16:42,080375214-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 620 objects, 2.4 GiB
    usage:   7.3 GiB used, 93 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:16:42,088792536-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:16:42,095278428-07:00] INFO: > The cluster is idle now.[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:16:42,099427206-07:00] STAGE: Start benchmarking of mode seq ...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:16:42,106460399-07:00] INFO: > Start system activity collection services[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:16:42,112479141-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:341 - rados_bench() > rm --force /mnt/sda8/ceph-research/scripts/output/sys_activity_4MB_seq.dat.rados_bench_host.1
# ./benchmarks/bench-rados:342 - rados_bench() > sar_pid_rados_bench_host=483519
# ./benchmarks/bench-rados:342 - rados_bench() > S_TIME_FORMAT=ISO
# ./benchmarks/bench-rados:342 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/output/sys_activity_4MB_seq.dat.rados_bench_host.1 2

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:16:42,119949508-07:00] INFO: >> for OSD host[0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:16:42,129146178-07:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.5 bash
10.10.2.5: b'169296\n'
[1] 09:16:43 [SUCCESS] ljishen@10.10.2.5
169296

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:16:43,447031038-07:00] INFO: > Run rados bench[0m
# ./benchmarks/bench-rados:379 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/output/rados_bench_4MB_seq.log.1
## ./benchmarks/bench-rados:376 - rados_bench() > count_cpus 32-63,96-127
# ./benchmarks/bench-rados:376 - rados_bench() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 128 --show-time
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:16:43,467073426-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:16:43,470089068-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '62a19fb4-3809-11ec-b51d-53e6e728d2d3', '--', 'rados', 'bench', '99999999', 'seq', '--pool', 'bench_rados', '--concurrent-ios', '128', '--show-time'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 62a19fb4-3809-11ec-b51d-53e6e728d2d3 -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 128 --show-time
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
hints = 1
2021-10-28T16:16:46.404395+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-28T16:16:46.404395+0000     0       0         0         0         0         0           -           0
2021-10-28T16:16:47.404517+0000     1     127       192        65    259.95       260    0.706327    0.696281
2021-10-28T16:16:48.404590+0000     2     127       377       250   499.933       740    0.693915    0.704251
2021-10-28T16:16:49.404666+0000     3     127       555       428   570.602       712    0.692881    0.710021
2021-10-28T16:16:50.404769+0000 Total time run:       3.49744
Total reads made:     619
Read size:            4194304
Object size:          4194304
Bandwidth (MB/sec):   707.946
Average IOPS:         176
Stddev IOPS:          67.3523
Max IOPS:             185
Min IOPS:             65
Average Latency(s):   0.644666
Max latency(s):       0.765021
Min latency(s):       0.0913584

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:16:51,119427027-07:00] INFO: > Stop system activity collection services[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:16:51,125843058-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:391 - rados_bench() > kill -INT 483519

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:16:51,132459906-07:00] INFO: >> for OSD host[0m
# ./benchmarks/bench-rados:396 - rados_bench() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.5 kill -INT 169296
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:16:51,140804802-07:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.5 kill -INT 169296
[1] 09:16:52 [SUCCESS] ljishen@10.10.2.5

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:16:52,244302541-07:00] INFO: >>> Copy system activity file from OSD host[0m
## ./benchmarks/bench-rados:403 - rados_bench() > basename -- /tmp/bench-rados/sys_activity_4MB_seq.dat.osd_host.1
# ./benchmarks/bench-rados:403 - rados_bench() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.5:/tmp/bench-rados/sys_activity_4MB_seq.dat.osd_host.1 /mnt/sda8/ceph-research/scripts/output/sys_activity_4MB_seq.dat.osd_host.1


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:16:53,312838248-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:17:17,228032794-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 620 objects, 2.4 GiB
    usage:   7.3 GiB used, 93 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:17:17,236457850-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:17:26,185947627-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 620 objects, 2.4 GiB
    usage:   7.3 GiB used, 93 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:17:26,194277605-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:17:34,984371317-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 620 objects, 2.4 GiB
    usage:   7.3 GiB used, 93 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:17:34,992520082-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:17:43,925032009-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 620 objects, 2.4 GiB
    usage:   7.3 GiB used, 93 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:17:43,933441525-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:17:52,859199506-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 620 objects, 2.4 GiB
    usage:   7.3 GiB used, 93 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:17:52,867332793-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:17:52,873429481-07:00] INFO: > The cluster is idle now.[0m


[1;7;39;49m[2021-10-28T09:17:52,876170777-07:00][RUNNING][ROUND 2/6/21] object_size=4MB[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:17:52,880114428-07:00] STAGE: Launch a Ceph cluster (CEPH_HOST: 10.10.2.1, OSD_HOST: 10.10.2.5) ...[0m
# ./benchmarks/bench-rados:183 - launch_ceph_cluster() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.1 bash -euo pipefail
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:17:52,889573193-07:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.1 bash -euo pipefail
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T09:17:53,312045599-07:00] INFO: > Remove existing clusters\x1b[0m\n'
10.10.2.1: b'## bash:17 -  > basename -- /var/lib/ceph/62a19fb4-3809-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'# bash:17 -  > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm --host 10.10.2.1 --host 10.10.2.5 rm-cluster --force --fsid 62a19fb4-3809-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T09:17:53,323560355-07:00] DEBUG: HOST_PARAMS: ['--host', '10.10.2.1', '--host', '10.10.2.5']\x1b[0m\n"
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T09:17:53,327277302-07:00] DEBUG: CEPHADM_PARAMS: ['rm-cluster', '--force', '--fsid', '62a19fb4-3809-11ec-b51d-53e6e728d2d3']\x1b[0m\n"
10.10.2.1: b"# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:63 -  > /tmp/bench-rados/ceph-research/scripts/parallel-ssh --host 10.10.2.1 --host 10.10.2.5 'sudo python3 -u - rm-cluster --force --fsid 62a19fb4-3809-11ec-b51d-53e6e728d2d3'\n"
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T09:17:53,336679928-07:00] DEBUG: command from stdin\x1b[0m\n'
10.10.2.1: b"# /tmp/bench-rados/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host 10.10.2.1 --host 10.10.2.5 'sudo python3 -u - rm-cluster --force --fsid 62a19fb4-3809-11ec-b51d-53e6e728d2d3'\n"
10.10.2.1: b'[1] 09:17:58 [SUCCESS] 10.10.2.1\n[2] 09:18:05 [SUCCESS] 10.10.2.5\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T09:18:05,410358144-07:00] INFO: > Deploy a new cluster\x1b[0m\n'
10.10.2.1: b'# bash:24 -  > /tmp/bench-rados/ceph-research/scripts/ceph-deploy -m 10.10.2.1 -o sm1/10.10.2.5:/dev/nvme0n1\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T09:18:05,422901904-07:00] STAGE: Verify passwordless SSH to hosts, passwordless sudo via ljishen, and the existence of devices on hosts...\x1b[0m\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T09:18:05,428401573-07:00] INFO: > Check the host for the monitor\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:181 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@10.10.2.1 sudo --non-interactive --validate\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T09:18:05,579518420-07:00] INFO: > Check hosts for OSDs\x1b[0m\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T09:18:05,584300920-07:00] INFO: >> Check host 'sm1'\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:200 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 sudo --non-interactive --validate\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T09:18:06,674851724-07:00] INFO: >>> Check device '/dev/nvme0n1'\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:215 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 test -b /dev/nvme0n1\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:226 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 lsblk --nodeps --noheadings --output TYPE /dev/nvme0n1\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:227 - verify_parameters() > grep --word-regexp disk\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T09:18:08,805988356-07:00] STAGE: Reset devices on hosts...\x1b[0m\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T09:18:08,811089455-07:00] INFO: > Resetting device '/dev/nvme0n1' on host 'sm1'\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:288 - reset_host_devices() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 sudo sh\n'
10.10.2.1: b'  Removing ceph--c8b8ded1--5637--46b6--81d2--69236de052d9-osd--block--1c37b5a5--8d35--481e--bb4a--269524fdda81 (252:0)\n'
10.10.2.1: b'  Archiving volume group "ceph-c8b8ded1-5637-46b6-81d2-69236de052d9" metadata (seqno 5).\n'
10.10.2.1: b'  Releasing logical volume "osd-block-1c37b5a5-8d35-481e-bb4a-269524fdda81"\n'
10.10.2.1: b'  Creating volume group backup "/etc/lvm/backup/ceph-c8b8ded1-5637-46b6-81d2-69236de052d9" (seqno 6).\n'
10.10.2.1: b'  Logical volume "osd-block-1c37b5a5-8d35-481e-bb4a-269524fdda81" successfully removed\n'
10.10.2.1: b'  Removing physical volume "/dev/nvme0n1" from volume group "ceph-c8b8ded1-5637-46b6-81d2-69236de052d9"\n'
10.10.2.1: b'  Volume group "ceph-c8b8ded1-5637-46b6-81d2-69236de052d9" successfully removed\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T09:18:11,128504001-07:00] STAGE: Bootstrap a new cluster...\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:317 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.1 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T09:18:11,138427977-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T09:18:11,141904442-07:00] DEBUG: CEPHADM_PARAMS: ['bootstrap', '--config', '/tmp/bench-rados/ceph-research/scripts/ceph.conf', '--mon-ip', '10.10.2.1', '--output-dir', '/tmp/bench-rados/deployment_data_root/etc/ceph', '--ssh-user', 'ljishen', '--skip-dashboard', '--no-minimize-config', '--allow-overwrite', '--allow-fqdn-hostname', '--orphan-initial-daemons', '--skip-monitoring-stack', '--single-host-defaults']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.1 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults\n'
10.10.2.1: b'Verifying podman|docker is present...\nVerifying lvm2 is present...\n'
10.10.2.1: b'Verifying time synchronization is in place...\n'
10.10.2.1: b'Unit ntp.service is enabled and running\n'
10.10.2.1: b'Repeating the final host check...\n'
10.10.2.1: b'podman|docker (/usr/bin/docker) is present\n'
10.10.2.1: b'systemctl is present\n'
10.10.2.1: b'lvcreate is present\n'
10.10.2.1: b'Unit ntp.service is enabled and running\n'
10.10.2.1: b'Host looks OK\n'
10.10.2.1: b'Cluster fsid: a52ba374-380a-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Verifying IP 10.10.2.1 port 3300 ...\n'
10.10.2.1: b'Verifying IP 10.10.2.1 port 6789 ...\n'
10.10.2.1: b'Mon IP `10.10.2.1` is in CIDR network `10.10.2.0/24`\n- internal network (--cluster-network) has not been provided, OSD replication will default to the public_network\n'
10.10.2.1: b'Adjusting default settings to suit single-host cluster...\n'
10.10.2.1: b'Pulling container image quay.io/ceph/ceph:v16...\n'
10.10.2.1: b'Ceph version: ceph version 16.2.6 (ee28fb57e47e9f88813e24bbf4c14496ca299d31) pacific (stable)\n'
10.10.2.1: b'Extracting ceph user uid/gid from container image...\n'
10.10.2.1: b'Creating initial keys...\n'
10.10.2.1: b'Creating initial monmap...\n'
10.10.2.1: b'Creating mon...\n'
10.10.2.1: b'Waiting for mon to start...\n'
10.10.2.1: b'Waiting for mon...\n'
10.10.2.1: b'mon is available\nSetting mon public_network to 10.10.2.0/24\n'
10.10.2.1: b'Wrote config to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf\n'
10.10.2.1: b'Wrote keyring to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring\n'
10.10.2.1: b'Creating mgr...\n'
10.10.2.1: b'Verifying port 9283 ...\n'
10.10.2.1: b'Waiting for mgr to start...\nWaiting for mgr...\n'
10.10.2.1: b'mgr not available, waiting (1/15)...\n'
10.10.2.1: b'mgr not available, waiting (2/15)...\n'
10.10.2.1: b'mgr is available\n'
10.10.2.1: b'Enabling cephadm module...\n'
10.10.2.1: b'Waiting for the mgr to restart...\n'
10.10.2.1: b'Waiting for mgr epoch 5...\n'
10.10.2.1: b'mgr epoch 5 is available\nSetting orchestrator backend to cephadm...\n'
10.10.2.1: b'Generating ssh key...\n'
10.10.2.1: b'Wrote public SSH key to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub\n'
10.10.2.1: b'Adding key to ljishen@localhost authorized_keys...\n'
10.10.2.1: b'Adding host node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us...\n'
10.10.2.1: b'Deploying unmanaged mon service...\n'
10.10.2.1: b'Deploying unmanaged mgr service...\n'
10.10.2.1: b'You can access the Ceph CLI with:\n\n\tsudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --fsid a52ba374-380a-11ec-b51d-53e6e728d2d3 -c /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf -k /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring\n\n'
10.10.2.1: b'Please consider enabling telemetry to help improve Ceph:\n\n\tceph telemetry on\n\nFor more information see:\n\n\thttps://docs.ceph.com/docs/pacific/mgr/telemetry/\n\n'
10.10.2.1: b'Bootstrap complete.\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T09:19:12,728171603-07:00] INFO: Sleep for 20 seconds\x1b[0m\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T09:19:32,735525660-07:00] INFO: > Set the default number of monitor daemons to 1\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:337 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T09:19:32,746018525-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T09:19:32,750194686-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mon', '1']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1\n'
10.10.2.1: b'Inferring fsid a52ba374-380a-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/a52ba374-380a-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b'Scheduled mon update...\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T09:19:42,660931832-07:00] INFO: > Set the default number of manager daemons to 1\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:342 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T09:19:42,671578356-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T09:19:42,675115435-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mgr', '1']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1\n'
10.10.2.1: b'Inferring fsid a52ba374-380a-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/a52ba374-380a-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b'Scheduled mgr update...\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T09:19:52,209153164-07:00] STAGE: Add hosts to the cluster...\x1b[0m\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T09:19:52,215125811-07:00] INFO: > Adding host 'sm1'\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:361 - add_hosts() > ssh-copy-id -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -f -i /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub ljishen@sm1\n'
10.10.2.1: b'/usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: "/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub"\n'
10.10.2.1: b'\nNumber of key(s) added: 1\n\nNow try logging into the machine, with:   "ssh -o \'GlobalKnownHostsFile=/dev/null\' -o \'LogLevel=ERROR\' -o \'PasswordAuthentication=no\' -o \'StrictHostKeyChecking=no\' -o \'UserKnownHostsFile=/dev/null\' \'ljishen@sm1\'"\nand check to make sure that only the key(s) you wanted were added.\n\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:367 - add_hosts() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add sm1 10.10.2.5\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T09:19:53,385137601-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T09:19:53,388920412-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'host', 'add', 'sm1', '10.10.2.5']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add sm1 10.10.2.5\n'
10.10.2.1: b'Inferring fsid a52ba374-380a-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/a52ba374-380a-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b"Added host 'sm1' with addr '10.10.2.5'\n"
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T09:20:04,402929915-07:00] INFO: Sleep for 20 seconds\x1b[0m\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T09:20:24,407516273-07:00] STAGE: Add OSDs to the cluster...\x1b[0m\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T09:20:24,414074782-07:00] INFO: > Adding devices ['/dev/nvme0n1'] on host 'sm1' as OSDs\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:388 - add_host_devices_as_osds() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd sm1:/dev/nvme0n1\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T09:20:24,424417194-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T09:20:24,428220434-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'daemon', 'add', 'osd', 'sm1:/dev/nvme0n1']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd sm1:/dev/nvme0n1\n'
10.10.2.1: b'Inferring fsid a52ba374-380a-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/a52ba374-380a-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b"Created osd(s) 0 on host 'sm1'\n"
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T09:20:48,225009435-07:00] INFO: > Sleep for 20 seconds\x1b[0m\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T09:21:08,230708268-07:00] STAGE: Show Cluster Status\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:408 - show_status() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T09:21:08,240808275-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T09:21:08,244594903-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', '--status']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status\n'
10.10.2.1: b'Inferring fsid a52ba374-380a-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/a52ba374-380a-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b'  cluster:\n    id:     a52ba374-380a-11ec-b51d-53e6e728d2d3\n    health: HEALTH_WARN\n            1 pool(s) have no replicas configured\n \n  services:\n    mon: 1 daemons, quorum node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us (age 2m)\n    mgr: node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us.jfatfm(active, since 2m)\n    osd: 1 osds: 1 up (since 20s), 1 in (since 40s)\n \n  data:\n    pools:   1 pools, 1 pgs\n    objects: 0 objects, 0 B\n    usage:   4.7 MiB used, 100 GiB / 100 GiB avail\n    pgs:     1 active+clean\n \n'
[1] 09:21:17 [SUCCESS] ljishen@10.10.2.1

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:17:53,312045599-07:00] INFO: > Remove existing clusters[0m
## bash:17 -  > basename -- /var/lib/ceph/62a19fb4-3809-11ec-b51d-53e6e728d2d3
# bash:17 -  > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm --host 10.10.2.1 --host 10.10.2.5 rm-cluster --force --fsid 62a19fb4-3809-11ec-b51d-53e6e728d2d3
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:17:53,323560355-07:00] DEBUG: HOST_PARAMS: ['--host', '10.10.2.1', '--host', '10.10.2.5'][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:17:53,327277302-07:00] DEBUG: CEPHADM_PARAMS: ['rm-cluster', '--force', '--fsid', '62a19fb4-3809-11ec-b51d-53e6e728d2d3'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:63 -  > /tmp/bench-rados/ceph-research/scripts/parallel-ssh --host 10.10.2.1 --host 10.10.2.5 'sudo python3 -u - rm-cluster --force --fsid 62a19fb4-3809-11ec-b51d-53e6e728d2d3'
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:17:53,336679928-07:00] DEBUG: command from stdin[0m
# /tmp/bench-rados/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host 10.10.2.1 --host 10.10.2.5 'sudo python3 -u - rm-cluster --force --fsid 62a19fb4-3809-11ec-b51d-53e6e728d2d3'
[1] 09:17:58 [SUCCESS] 10.10.2.1
[2] 09:18:05 [SUCCESS] 10.10.2.5

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:18:05,410358144-07:00] INFO: > Deploy a new cluster[0m
# bash:24 -  > /tmp/bench-rados/ceph-research/scripts/ceph-deploy -m 10.10.2.1 -o sm1/10.10.2.5:/dev/nvme0n1


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:18:05,422901904-07:00] STAGE: Verify passwordless SSH to hosts, passwordless sudo via ljishen, and the existence of devices on hosts...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:18:05,428401573-07:00] INFO: > Check the host for the monitor[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:181 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@10.10.2.1 sudo --non-interactive --validate

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:18:05,579518420-07:00] INFO: > Check hosts for OSDs[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:18:05,584300920-07:00] INFO: >> Check host 'sm1'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:200 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 sudo --non-interactive --validate

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:18:06,674851724-07:00] INFO: >>> Check device '/dev/nvme0n1'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:215 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 test -b /dev/nvme0n1
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:226 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 lsblk --nodeps --noheadings --output TYPE /dev/nvme0n1
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:227 - verify_parameters() > grep --word-regexp disk


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:18:08,805988356-07:00] STAGE: Reset devices on hosts...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:18:08,811089455-07:00] INFO: > Resetting device '/dev/nvme0n1' on host 'sm1'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:288 - reset_host_devices() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 sudo sh
  Removing ceph--c8b8ded1--5637--46b6--81d2--69236de052d9-osd--block--1c37b5a5--8d35--481e--bb4a--269524fdda81 (252:0)
  Archiving volume group "ceph-c8b8ded1-5637-46b6-81d2-69236de052d9" metadata (seqno 5).
  Releasing logical volume "osd-block-1c37b5a5-8d35-481e-bb4a-269524fdda81"
  Creating volume group backup "/etc/lvm/backup/ceph-c8b8ded1-5637-46b6-81d2-69236de052d9" (seqno 6).
  Logical volume "osd-block-1c37b5a5-8d35-481e-bb4a-269524fdda81" successfully removed
  Removing physical volume "/dev/nvme0n1" from volume group "ceph-c8b8ded1-5637-46b6-81d2-69236de052d9"
  Volume group "ceph-c8b8ded1-5637-46b6-81d2-69236de052d9" successfully removed


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:18:11,128504001-07:00] STAGE: Bootstrap a new cluster...[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:317 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.1 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:18:11,138427977-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:18:11,141904442-07:00] DEBUG: CEPHADM_PARAMS: ['bootstrap', '--config', '/tmp/bench-rados/ceph-research/scripts/ceph.conf', '--mon-ip', '10.10.2.1', '--output-dir', '/tmp/bench-rados/deployment_data_root/etc/ceph', '--ssh-user', 'ljishen', '--skip-dashboard', '--no-minimize-config', '--allow-overwrite', '--allow-fqdn-hostname', '--orphan-initial-daemons', '--skip-monitoring-stack', '--single-host-defaults'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.1 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults
Verifying podman|docker is present...
Verifying lvm2 is present...
Verifying time synchronization is in place...
Unit ntp.service is enabled and running
Repeating the final host check...
podman|docker (/usr/bin/docker) is present
systemctl is present
lvcreate is present
Unit ntp.service is enabled and running
Host looks OK
Cluster fsid: a52ba374-380a-11ec-b51d-53e6e728d2d3
Verifying IP 10.10.2.1 port 3300 ...
Verifying IP 10.10.2.1 port 6789 ...
Mon IP `10.10.2.1` is in CIDR network `10.10.2.0/24`
- internal network (--cluster-network) has not been provided, OSD replication will default to the public_network
Adjusting default settings to suit single-host cluster...
Pulling container image quay.io/ceph/ceph:v16...
Ceph version: ceph version 16.2.6 (ee28fb57e47e9f88813e24bbf4c14496ca299d31) pacific (stable)
Extracting ceph user uid/gid from container image...
Creating initial keys...
Creating initial monmap...
Creating mon...
Waiting for mon to start...
Waiting for mon...
mon is available
Setting mon public_network to 10.10.2.0/24
Wrote config to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf
Wrote keyring to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring
Creating mgr...
Verifying port 9283 ...
Waiting for mgr to start...
Waiting for mgr...
mgr not available, waiting (1/15)...
mgr not available, waiting (2/15)...
mgr is available
Enabling cephadm module...
Waiting for the mgr to restart...
Waiting for mgr epoch 5...
mgr epoch 5 is available
Setting orchestrator backend to cephadm...
Generating ssh key...
Wrote public SSH key to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub
Adding key to ljishen@localhost authorized_keys...
Adding host node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us...
Deploying unmanaged mon service...
Deploying unmanaged mgr service...
You can access the Ceph CLI with:

	sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --fsid a52ba374-380a-11ec-b51d-53e6e728d2d3 -c /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf -k /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring

Please consider enabling telemetry to help improve Ceph:

	ceph telemetry on

For more information see:

	https://docs.ceph.com/docs/pacific/mgr/telemetry/

Bootstrap complete.

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:19:12,728171603-07:00] INFO: Sleep for 20 seconds[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:19:32,735525660-07:00] INFO: > Set the default number of monitor daemons to 1[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:337 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:19:32,746018525-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:19:32,750194686-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mon', '1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1
Inferring fsid a52ba374-380a-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/a52ba374-380a-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Scheduled mon update...

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:19:42,660931832-07:00] INFO: > Set the default number of manager daemons to 1[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:342 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:19:42,671578356-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:19:42,675115435-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mgr', '1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1
Inferring fsid a52ba374-380a-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/a52ba374-380a-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Scheduled mgr update...


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:19:52,209153164-07:00] STAGE: Add hosts to the cluster...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:19:52,215125811-07:00] INFO: > Adding host 'sm1'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:361 - add_hosts() > ssh-copy-id -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -f -i /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub ljishen@sm1
/usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: "/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub"

Number of key(s) added: 1

Now try logging into the machine, with:   "ssh -o 'GlobalKnownHostsFile=/dev/null' -o 'LogLevel=ERROR' -o 'PasswordAuthentication=no' -o 'StrictHostKeyChecking=no' -o 'UserKnownHostsFile=/dev/null' 'ljishen@sm1'"
and check to make sure that only the key(s) you wanted were added.

# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:367 - add_hosts() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add sm1 10.10.2.5
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:19:53,385137601-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:19:53,388920412-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'host', 'add', 'sm1', '10.10.2.5'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add sm1 10.10.2.5
Inferring fsid a52ba374-380a-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/a52ba374-380a-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Added host 'sm1' with addr '10.10.2.5'

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:20:04,402929915-07:00] INFO: Sleep for 20 seconds[0m


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:20:24,407516273-07:00] STAGE: Add OSDs to the cluster...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:20:24,414074782-07:00] INFO: > Adding devices ['/dev/nvme0n1'] on host 'sm1' as OSDs[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:388 - add_host_devices_as_osds() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd sm1:/dev/nvme0n1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:20:24,424417194-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:20:24,428220434-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'daemon', 'add', 'osd', 'sm1:/dev/nvme0n1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd sm1:/dev/nvme0n1
Inferring fsid a52ba374-380a-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/a52ba374-380a-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Created osd(s) 0 on host 'sm1'

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:20:48,225009435-07:00] INFO: > Sleep for 20 seconds[0m


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:21:08,230708268-07:00] STAGE: Show Cluster Status[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:408 - show_status() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:21:08,240808275-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:21:08,244594903-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', '--status'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status
Inferring fsid a52ba374-380a-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/a52ba374-380a-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
  cluster:
    id:     a52ba374-380a-11ec-b51d-53e6e728d2d3
    health: HEALTH_WARN
            1 pool(s) have no replicas configured
 
  services:
    mon: 1 daemons, quorum node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us (age 2m)
    mgr: node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us.jfatfm(active, since 2m)
    osd: 1 osds: 1 up (since 20s), 1 in (since 40s)
 
  data:
    pools:   1 pools, 1 pgs
    objects: 0 objects, 0 B
    usage:   4.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     1 active+clean
 

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:21:17,146463751-07:00] INFO: > Copy configuration files to local[0m
# ./benchmarks/bench-rados:220 - launch_ceph_cluster() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.1 sudo chown --changes --recursive ljishen /tmp/bench-rados/deployment_data_root/etc/ceph
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:21:17,154423150-07:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.1 sudo chown --changes --recursive ljishen /tmp/bench-rados/deployment_data_root/etc/ceph
[1] 09:21:17 [SUCCESS] ljishen@10.10.2.1
# ./benchmarks/bench-rados:223 - launch_ceph_cluster() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.1:/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf
# ./benchmarks/bench-rados:226 - launch_ceph_cluster() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.1:/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:21:17,629378547-07:00] STAGE: Update local ceph.conf as a client...[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:21:17,633105309-07:00] STAGE: Configure Ceph pool...[0m
# ./benchmarks/bench-rados:257 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_max_backfills 32
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:21:17,655592714-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:21:17,658420574-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'a52ba374-380a-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'config', 'set', 'osd', 'osd_max_backfills', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid a52ba374-380a-11ec-b51d-53e6e728d2d3 -- ceph config set osd osd_max_backfills 32
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:258 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_active 32
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:21:21,608213482-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:21:21,611351384-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'a52ba374-380a-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_active', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid a52ba374-380a-11ec-b51d-53e6e728d2d3 -- ceph config set osd osd_recovery_max_active 32
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:259 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_single_start 8
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:21:25,555409516-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:21:25,558521610-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'a52ba374-380a-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_single_start', '8'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid a52ba374-380a-11ec-b51d-53e6e728d2d3 -- ceph config set osd osd_recovery_max_single_start 8
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:260 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_op_priority 63
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:21:29,350092334-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:21:29,352991888-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'a52ba374-380a-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_op_priority', '63'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid a52ba374-380a-11ec-b51d-53e6e728d2d3 -- ceph config set osd osd_recovery_op_priority 63
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:262 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config show-with-defaults osd.0
# ./benchmarks/bench-rados:263 - configure_ceph_pool() > grep 'osd_max_backfills\|osd_recovery'
# ./benchmarks/bench-rados:263 - configure_ceph_pool() > column -t -s ' '
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
osd_max_backfills                        32         mon
osd_recovery_cost                        20971520   default
osd_recovery_delay_start                 0.000000   default
osd_recovery_max_active                  32         mon
osd_recovery_max_active_hdd              3          default
osd_recovery_max_active_ssd              10         default
osd_recovery_max_chunk                   8388608    default
osd_recovery_max_omap_entries_per_chunk  8096       default
osd_recovery_max_single_start            8          mon
osd_recovery_op_priority                 3          default
osd_recovery_op_warn_multiple            16         default
osd_recovery_priority                    5          default
osd_recovery_retry_interval              30.000000  default
osd_recovery_sleep                       0.000000   default
osd_recovery_sleep_hdd                   0.100000   default
osd_recovery_sleep_hybrid                0.025000   default
osd_recovery_sleep_ssd                   0.000000   default
# ./benchmarks/bench-rados:265 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool create bench_rados 128 128 replicated --size 1 --pg-num-min 128
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:21:36,940663103-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:21:36,944034847-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'a52ba374-380a-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'create', 'bench_rados', '128', '128', 'replicated', '--size', '1', '--pg-num-min', '128'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid a52ba374-380a-11ec-b51d-53e6e728d2d3 -- ceph osd pool create bench_rados 128 128 replicated --size 1 --pg-num-min 128
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
pool 'bench_rados' created
# ./benchmarks/bench-rados:267 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados min_size 1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:21:41,481397984-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:21:41,484625036-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'a52ba374-380a-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'min_size', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid a52ba374-380a-11ec-b51d-53e6e728d2d3 -- ceph osd pool set bench_rados min_size 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 min_size to 1
# ./benchmarks/bench-rados:268 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados pg_autoscale_mode off
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:21:46,013303027-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:21:46,016419790-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'a52ba374-380a-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'pg_autoscale_mode', 'off'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid a52ba374-380a-11ec-b51d-53e6e728d2d3 -- ceph osd pool set bench_rados pg_autoscale_mode off
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 pg_autoscale_mode to off
# ./benchmarks/bench-rados:269 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool application enable bench_rados benchmark
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:21:50,264675043-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:21:50,267711395-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'a52ba374-380a-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'application', 'enable', 'bench_rados', 'benchmark'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid a52ba374-380a-11ec-b51d-53e6e728d2d3 -- ceph osd pool application enable bench_rados benchmark
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
enabled application 'benchmark' on pool 'bench_rados'
# ./benchmarks/bench-rados:270 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados noscrub 1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:21:54,546126721-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:21:54,549227955-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'a52ba374-380a-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'noscrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid a52ba374-380a-11ec-b51d-53e6e728d2d3 -- ceph osd pool set bench_rados noscrub 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 noscrub to 1
# ./benchmarks/bench-rados:271 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados nodeep-scrub 1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:21:59,387505047-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:21:59,390394312-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'a52ba374-380a-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'nodeep-scrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid a52ba374-380a-11ec-b51d-53e6e728d2d3 -- ceph osd pool set bench_rados nodeep-scrub 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 nodeep-scrub to 1
# ./benchmarks/bench-rados:272 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool ls detail
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:22:04,187532852-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:22:04,190424591-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'a52ba374-380a-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'ls', 'detail'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid a52ba374-380a-11ec-b51d-53e6e728d2d3 -- ceph osd pool ls detail
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
pool 1 'device_health_metrics' replicated size 1 min_size 1 crush_rule 0 object_hash rjenkins pg_num 64 pgp_num 64 autoscale_mode on last_change 18 lfor 0/0/16 flags hashpspool stripe_width 0 pg_num_min 1 application mgr_devicehealth
pool 2 'bench_rados' replicated size 1 min_size 1 crush_rule 0 object_hash rjenkins pg_num 128 pgp_num 128 autoscale_mode off last_change 20 flags hashpspool,noscrub,nodeep-scrub stripe_width 0 pg_num_min 128 application benchmark

# ./benchmarks/bench-rados:273 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd df tree
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:22:07,995128265-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:22:07,997958478-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'a52ba374-380a-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'df', 'tree'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid a52ba374-380a-11ec-b51d-53e6e728d2d3 -- ceph osd df tree
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
ID  CLASS  WEIGHT   REWEIGHT  SIZE     RAW USE  DATA     OMAP  META     AVAIL    %USE  VAR   PGS  STATUS  TYPE NAME    
-1         0.09769         -  100 GiB  5.7 MiB  168 KiB   0 B  5.6 MiB  100 GiB  0.01  1.00    -          root default 
-3         0.09769         -  100 GiB  5.7 MiB  168 KiB   0 B  5.6 MiB  100 GiB  0.01  1.00    -              host sm1 
 0    ssd  0.09769   1.00000  100 GiB  5.7 MiB  168 KiB   0 B  5.6 MiB  100 GiB  0.01  1.00  192      up          osd.0
                       TOTAL  100 GiB  5.7 MiB  168 KiB   0 B  5.6 MiB  100 GiB  0.01                                  
MIN/MAX VAR: 1.00/1.00  STDDEV: 0


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:22:11,729456524-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:22:35,436724890-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:22:44,317041176-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:22:53,207794841-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:23:02,132299127-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:23:02,140618003-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:23:11,054849171-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:23:11,062768184-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:23:19,920949403-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:23:19,928664371-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:23:28,658359992-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:23:28,666177854-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:23:37,601708412-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:23:37,609919315-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:23:37,616229696-07:00] INFO: > The cluster is idle now.[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:23:37,620425683-07:00] STAGE: Start benchmarking of mode write ...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:23:37,627722203-07:00] INFO: > Start system activity collection services[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:23:37,633816397-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:341 - rados_bench() > rm --force /mnt/sda8/ceph-research/scripts/output/sys_activity_4MB_write.dat.rados_bench_host.2
# ./benchmarks/bench-rados:342 - rados_bench() > sar_pid_rados_bench_host=489334
# ./benchmarks/bench-rados:342 - rados_bench() > S_TIME_FORMAT=ISO
# ./benchmarks/bench-rados:342 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/output/sys_activity_4MB_write.dat.rados_bench_host.2 2

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:23:37,641383977-07:00] INFO: >> for OSD host[0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:23:37,650508812-07:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.5 bash
10.10.2.5: b'174150\n'
[1] 09:23:38 [SUCCESS] ljishen@10.10.2.5
174150

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:23:38,751945534-07:00] INFO: > Run rados bench[0m
# ./benchmarks/bench-rados:370 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/output/rados_bench_4MB_write.log.2
## ./benchmarks/bench-rados:361 - rados_bench() > count_cpus 32-63,96-127
# ./benchmarks/bench-rados:361 - rados_bench() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 60 write --pool bench_rados -b 4194304 -O 4194304 --concurrent-ios 128 --show-time --write-object --write-omap --write-xattr --no-cleanup
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:23:38,772503162-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:23:38,775606290-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'a52ba374-380a-11ec-b51d-53e6e728d2d3', '--', 'rados', 'bench', '60', 'write', '--pool', 'bench_rados', '-b', '4194304', '-O', '4194304', '--concurrent-ios', '128', '--show-time', '--write-object', '--write-omap', '--write-xattr', '--no-cleanup'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid a52ba374-380a-11ec-b51d-53e6e728d2d3 -- rados bench 60 write --pool bench_rados -b 4194304 -O 4194304 --concurrent-ios 128 --show-time --write-object --write-omap --write-xattr --no-cleanup
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
hints = 1
2021-10-28T16:23:41.693490+0000 Maintaining 128 concurrent writes of 4194304 bytes to objects of size 4194304 for up to 60 seconds or 0 objects
2021-10-28T16:23:41.693505+0000 Object prefix: benchmark_data_node-2.bluefield2.ucsc-cmps10_7
2021-10-28T16:23:41.820208+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-28T16:23:41.820208+0000     0       0         0         0         0         0           -           0
2021-10-28T16:23:42.820342+0000     1      20        20         0         0         0           -           0
2021-10-28T16:23:43.820454+0000     2      39        39         0         0         0           -           0
2021-10-28T16:23:44.820545+0000     3      55        55         0         0         0           -           0
2021-10-28T16:23:45.820642+0000     4      74        74         0         0         0           -           0
2021-10-28T16:23:46.820737+0000     5      89        89         0         0         0           -           0
2021-10-28T16:23:47.820816+0000     6     108       108         0         0         0           -           0
2021-10-28T16:23:48.820913+0000     7     125       125         0         0         0           -           0
2021-10-28T16:23:49.821009+0000     8     127       136         9   4.49958       4.5     7.64576     7.47251
2021-10-28T16:23:50.821102+0000     9     127       144        17   7.55485        32     7.84479     7.66861
2021-10-28T16:23:51.821216+0000    10     127       152        25   9.99905        32     8.49165     7.90619
2021-10-28T16:23:52.821314+0000    11     127       164        37   13.4533        48     9.06018     8.24906
2021-10-28T16:23:53.821401+0000    12     127       172        45   14.9986        32     9.62166     8.47231
2021-10-28T16:23:54.821493+0000    13     127       180        53   16.3061        32     9.99394     8.69837
2021-10-28T16:23:55.821605+0000    14     127       194        67    19.141        56     10.1926     9.03272
2021-10-28T16:23:56.821698+0000    15     127       212        85   22.6645        72     10.1713     9.31332
2021-10-28T16:23:57.821794+0000    16     127       230       103   25.7475        72     10.2256     9.48596
2021-10-28T16:23:58.821875+0000    17     127       246       119   27.9973        64     10.2667     9.58893
2021-10-28T16:23:59.821994+0000    18     127       265       138   30.6637        76     9.83907      9.6768
2021-10-28T16:24:00.822084+0000    19     127       269       142   29.8919        16     9.85195     9.69208
2021-10-28T16:24:01.822182+0000 min lat: 7.17101 max lat: 10.5799 avg lat: 9.71294
2021-10-28T16:24:01.822182+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-28T16:24:01.822182+0000    20     127       273       146   29.1972        16     10.0761     9.71294
2021-10-28T16:24:02.822293+0000    21     127       277       150   28.5687        16     10.5748     9.74526
2021-10-28T16:24:03.822412+0000    22     127       281       154   27.9973        16     11.1356     9.79084
2021-10-28T16:24:04.822509+0000    23     127       289       162   28.1712        32     12.0305     9.90712
2021-10-28T16:24:05.822591+0000    24     127       293       166    27.664        16     12.5033     9.97771
2021-10-28T16:24:06.822690+0000    25     127       297       170   27.1974        16     12.9634     10.0562
2021-10-28T16:24:07.822811+0000    26     127       305       178   27.3819        32     13.0076     10.2043
2021-10-28T16:24:08.822907+0000    27     127       314       187    27.701        36     13.1264      10.351
2021-10-28T16:24:09.823004+0000    28     127       327       200   28.5686        52     13.8212     10.5665
2021-10-28T16:24:10.823095+0000    29     127       337       210   28.9627        40     14.0394      10.728
2021-10-28T16:24:11.823173+0000    30     127       350       223   29.7304        52     14.4171     10.9409
2021-10-28T16:24:12.823265+0000    31     127       360       233   30.0616        40     14.7668     11.1048
2021-10-28T16:24:13.823363+0000    32     127       371       244    30.497        44     15.1966     11.2864
2021-10-28T16:24:14.823455+0000    33     127       381       254   30.7849        40     15.5072     11.4533
2021-10-28T16:24:15.823574+0000    34     127       389       262   30.8205        32     16.0222      11.591
2021-10-28T16:24:16.823658+0000    35     127       397       270   30.8541        32     16.4696     11.7314
2021-10-28T16:24:17.823754+0000    36     127       405       278   30.8859        32     15.7706     11.8542
2021-10-28T16:24:18.823848+0000    37     127       413       286   30.9159        32     14.8018     11.9423
2021-10-28T16:24:19.823966+0000    38     127       421       294   30.9444        32     13.9672     12.0031
2021-10-28T16:24:20.824060+0000    39     127       429       302   30.9713        32     13.5159     12.0433
2021-10-28T16:24:21.824135+0000 min lat: 7.17101 max lat: 16.4696 avg lat: 12.0804
2021-10-28T16:24:21.824135+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-28T16:24:21.824135+0000    40     127       437       310    30.997        32     13.4895     12.0804
2021-10-28T16:24:22.824236+0000    41     127       456       329   32.0945        76     12.7604      12.144
2021-10-28T16:24:23.824352+0000    42     127       473       346   32.9492        68     12.3462     12.1645
2021-10-28T16:24:24.824444+0000    43     127       488       361   33.5781        60     11.8955     12.1595
2021-10-28T16:24:25.824543+0000    44     127       508       381    34.633        80     11.0276     12.1219
2021-10-28T16:24:26.824632+0000    45     127       523       396   35.1966        60     10.0051     12.0667
2021-10-28T16:24:27.824747+0000    46     127       528       401   34.8662        20      10.132     12.0448
2021-10-28T16:24:28.824837+0000    47     127       531       404   34.3796        12     10.3204     12.0332
2021-10-28T16:24:29.824916+0000    48     127       537       410   34.1634        24     11.3881     12.0205
2021-10-28T16:24:30.825006+0000    49     127       543       416   33.9559        24     11.6064      12.019
2021-10-28T16:24:31.825120+0000    50     127       545       418   33.4368         8     12.0784     12.0193
2021-10-28T16:24:32.825193+0000    51     127       550       423   33.1733        20     12.2226     12.0262
2021-10-28T16:24:33.825296+0000    52     127       553       426   32.7661        12     12.9776     12.0329
2021-10-28T16:24:34.825386+0000    53     127       566       439   33.1289        52      12.951     12.0682
2021-10-28T16:24:35.825467+0000    54     127       574       447   33.1079        32     13.3485     12.0911
2021-10-28T16:24:36.825556+0000    55     127       585       458   33.3059        44     13.7744     12.1297
2021-10-28T16:24:37.825661+0000    56     127       596       469   33.4968        44     14.1017     12.1776
2021-10-28T16:24:38.825759+0000    57     127       605       478   33.5406        36     14.6677     12.2229
2021-10-28T16:24:39.825878+0000    58     127       613       486    33.514        32     14.8559     12.2664
2021-10-28T16:24:40.825967+0000    59     127       626       499   33.8272        52     15.5566     12.3475
2021-10-28T16:24:41.826048+0000 min lat: 7.17101 max lat: 16.4696 avg lat: 12.4168
2021-10-28T16:24:41.826048+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-28T16:24:41.826048+0000    60     127       636       509   33.9301        40     15.9972     12.4168
2021-10-28T16:24:42.826190+0000 Total time run:         60.702
Total writes made:      637
Write size:             4194304
Object size:            4194304
Bandwidth (MB/sec):     41.9756
Stddev Bandwidth:       21.6087
Max bandwidth (MB/sec): 80
Min bandwidth (MB/sec): 0
Average IOPS:           10
Stddev IOPS:            5.40504
Max IOPS:               20
Min IOPS:               0
Average Latency(s):     11.4323
Stddev Latency(s):      3.54333
Max latency(s):         16.4696
Min latency(s):         0.706193

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:24:43,479984081-07:00] INFO: > Stop system activity collection services[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:24:43,486202629-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:391 - rados_bench() > kill -INT 489334

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:24:43,492415035-07:00] INFO: >> for OSD host[0m
# ./benchmarks/bench-rados:396 - rados_bench() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.5 kill -INT 174150
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:24:43,500352082-07:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.5 kill -INT 174150
[1] 09:24:45 [SUCCESS] ljishen@10.10.2.5

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:24:45,385277114-07:00] INFO: >>> Copy system activity file from OSD host[0m
## ./benchmarks/bench-rados:403 - rados_bench() > basename -- /tmp/bench-rados/sys_activity_4MB_write.dat.osd_host.2
# ./benchmarks/bench-rados:403 - rados_bench() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.5:/tmp/bench-rados/sys_activity_4MB_write.dat.osd_host.2 /mnt/sda8/ceph-research/scripts/output/sys_activity_4MB_write.dat.osd_host.2


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:24:46,899495884-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:25:10,668608969-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 638 objects, 2.5 GiB
    usage:   7.5 GiB used, 93 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:25:10,676875477-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:25:19,451751528-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 638 objects, 2.5 GiB
    usage:   7.5 GiB used, 93 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:25:19,460332499-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:25:28,455186940-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 638 objects, 2.5 GiB
    usage:   7.5 GiB used, 93 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:25:28,463261666-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:25:37,352050678-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 638 objects, 2.5 GiB
    usage:   7.5 GiB used, 93 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:25:37,360141875-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:25:46,468842441-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 638 objects, 2.5 GiB
    usage:   7.5 GiB used, 93 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:25:46,477255805-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:25:46,483500322-07:00] INFO: > The cluster is idle now.[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:25:46,487407124-07:00] STAGE: Start benchmarking of mode seq ...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:25:46,495057020-07:00] INFO: > Start system activity collection services[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:25:46,500974871-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:341 - rados_bench() > rm --force /mnt/sda8/ceph-research/scripts/output/sys_activity_4MB_seq.dat.rados_bench_host.2
# ./benchmarks/bench-rados:342 - rados_bench() > sar_pid_rados_bench_host=490732
# ./benchmarks/bench-rados:342 - rados_bench() > S_TIME_FORMAT=ISO
# ./benchmarks/bench-rados:342 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/output/sys_activity_4MB_seq.dat.rados_bench_host.2 2

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:25:46,508350359-07:00] INFO: >> for OSD host[0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:25:46,517448023-07:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.5 bash
10.10.2.5: b'174706\n'
[1] 09:25:47 [SUCCESS] ljishen@10.10.2.5
174706

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:25:47,830056380-07:00] INFO: > Run rados bench[0m
# ./benchmarks/bench-rados:379 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/output/rados_bench_4MB_seq.log.2
## ./benchmarks/bench-rados:376 - rados_bench() > count_cpus 32-63,96-127
# ./benchmarks/bench-rados:376 - rados_bench() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 128 --show-time
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:25:47,850447375-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:25:47,853330688-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'a52ba374-380a-11ec-b51d-53e6e728d2d3', '--', 'rados', 'bench', '99999999', 'seq', '--pool', 'bench_rados', '--concurrent-ios', '128', '--show-time'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid a52ba374-380a-11ec-b51d-53e6e728d2d3 -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 128 --show-time
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
hints = 1
2021-10-28T16:25:50.838736+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-28T16:25:50.838736+0000     0       0         0         0         0         0           -           0
2021-10-28T16:25:51.838893+0000     1     127       186        59   235.947       236    0.738279    0.729742
2021-10-28T16:25:52.839006+0000     2     127       364       237    473.92       712    0.722066    0.726108
2021-10-28T16:25:53.839403+0000     3     127       547       420   559.863       732    0.710795    0.715871
2021-10-28T16:25:54.839510+0000 Total time run:       3.58408
Total reads made:     637
Read size:            4194304
Object size:          4194304
Bandwidth (MB/sec):   710.922
Average IOPS:         177
Stddev IOPS:          70.1926
Max IOPS:             183
Min IOPS:             59
Average Latency(s):   0.642416
Max latency(s):       0.796436
Min latency(s):       0.111844

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:25:55,523324738-07:00] INFO: > Stop system activity collection services[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:25:55,529859802-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:391 - rados_bench() > kill -INT 490732

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:25:55,536640110-07:00] INFO: >> for OSD host[0m
# ./benchmarks/bench-rados:396 - rados_bench() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.5 kill -INT 174706
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:25:55,545111873-07:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.5 kill -INT 174706
[1] 09:25:56 [SUCCESS] ljishen@10.10.2.5

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:25:56,680802646-07:00] INFO: >>> Copy system activity file from OSD host[0m
## ./benchmarks/bench-rados:403 - rados_bench() > basename -- /tmp/bench-rados/sys_activity_4MB_seq.dat.osd_host.2
# ./benchmarks/bench-rados:403 - rados_bench() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.5:/tmp/bench-rados/sys_activity_4MB_seq.dat.osd_host.2 /mnt/sda8/ceph-research/scripts/output/sys_activity_4MB_seq.dat.osd_host.2


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:25:57,776152770-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:26:21,620674446-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 638 objects, 2.5 GiB
    usage:   7.5 GiB used, 93 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:26:21,628961002-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:26:30,479156965-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 638 objects, 2.5 GiB
    usage:   7.5 GiB used, 93 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:26:30,487478686-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:26:39,453452322-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 638 objects, 2.5 GiB
    usage:   7.5 GiB used, 93 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:26:39,461307444-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:26:48,460473858-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 638 objects, 2.5 GiB
    usage:   7.5 GiB used, 93 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:26:48,468380176-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:26:57,333902389-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 638 objects, 2.5 GiB
    usage:   7.5 GiB used, 93 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:26:57,341761327-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:26:57,347763287-07:00] INFO: > The cluster is idle now.[0m


[1;7;39;49m[2021-10-28T09:26:57,350266554-07:00][RUNNING][ROUND 3/6/21] object_size=4MB[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:26:57,353914809-07:00] STAGE: Launch a Ceph cluster (CEPH_HOST: 10.10.2.1, OSD_HOST: 10.10.2.5) ...[0m
# ./benchmarks/bench-rados:183 - launch_ceph_cluster() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.1 bash -euo pipefail
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:26:57,363298852-07:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.1 bash -euo pipefail
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T09:26:57,779822345-07:00] INFO: > Remove existing clusters\x1b[0m\n'
10.10.2.1: b'## bash:17 -  > basename -- /var/lib/ceph/a52ba374-380a-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'# bash:17 -  > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm --host 10.10.2.1 --host 10.10.2.5 rm-cluster --force --fsid a52ba374-380a-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T09:26:57,790450424-07:00] DEBUG: HOST_PARAMS: ['--host', '10.10.2.1', '--host', '10.10.2.5']\x1b[0m\n"
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T09:26:57,793969289-07:00] DEBUG: CEPHADM_PARAMS: ['rm-cluster', '--force', '--fsid', 'a52ba374-380a-11ec-b51d-53e6e728d2d3']\x1b[0m\n"
10.10.2.1: b"# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:63 -  > /tmp/bench-rados/ceph-research/scripts/parallel-ssh --host 10.10.2.1 --host 10.10.2.5 'sudo python3 -u - rm-cluster --force --fsid a52ba374-380a-11ec-b51d-53e6e728d2d3'\n"
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T09:26:57,803537035-07:00] DEBUG: command from stdin\x1b[0m\n'
10.10.2.1: b"# /tmp/bench-rados/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host 10.10.2.1 --host 10.10.2.5 'sudo python3 -u - rm-cluster --force --fsid a52ba374-380a-11ec-b51d-53e6e728d2d3'\n"
10.10.2.1: b'[1] 09:27:03 [SUCCESS] 10.10.2.1\n[2] 09:27:09 [SUCCESS] 10.10.2.5\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T09:27:09,168892234-07:00] INFO: > Deploy a new cluster\x1b[0m\n'
10.10.2.1: b'# bash:24 -  > /tmp/bench-rados/ceph-research/scripts/ceph-deploy -m 10.10.2.1 -o sm1/10.10.2.5:/dev/nvme0n1\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T09:27:09,181250195-07:00] STAGE: Verify passwordless SSH to hosts, passwordless sudo via ljishen, and the existence of devices on hosts...\x1b[0m\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T09:27:09,185824854-07:00] INFO: > Check the host for the monitor\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:181 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@10.10.2.1 sudo --non-interactive --validate\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T09:27:09,336194908-07:00] INFO: > Check hosts for OSDs\x1b[0m\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T09:27:09,340727259-07:00] INFO: >> Check host 'sm1'\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:200 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 sudo --non-interactive --validate\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T09:27:10,434794118-07:00] INFO: >>> Check device '/dev/nvme0n1'\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:215 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 test -b /dev/nvme0n1\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:226 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 lsblk --nodeps --noheadings --output TYPE /dev/nvme0n1\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:227 - verify_parameters() > grep --word-regexp disk\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T09:27:12,565785356-07:00] STAGE: Reset devices on hosts...\x1b[0m\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T09:27:12,570858492-07:00] INFO: > Resetting device '/dev/nvme0n1' on host 'sm1'\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:288 - reset_host_devices() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 sudo sh\n'
10.10.2.1: b'  Removing ceph--982eab50--586d--447f--a22a--466639ac9a86-osd--block--6e1ee148--b4bc--4bdd--8097--2e9b9b0673f6 (252:0)\n'
10.10.2.1: b'  Archiving volume group "ceph-982eab50-586d-447f-a22a-466639ac9a86" metadata (seqno 5).\n'
10.10.2.1: b'  Releasing logical volume "osd-block-6e1ee148-b4bc-4bdd-8097-2e9b9b0673f6"\n'
10.10.2.1: b'  Creating volume group backup "/etc/lvm/backup/ceph-982eab50-586d-447f-a22a-466639ac9a86" (seqno 6).\n'
10.10.2.1: b'  Logical volume "osd-block-6e1ee148-b4bc-4bdd-8097-2e9b9b0673f6" successfully removed\n'
10.10.2.1: b'  Removing physical volume "/dev/nvme0n1" from volume group "ceph-982eab50-586d-447f-a22a-466639ac9a86"\n'
10.10.2.1: b'  Volume group "ceph-982eab50-586d-447f-a22a-466639ac9a86" successfully removed\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T09:27:14,880334878-07:00] STAGE: Bootstrap a new cluster...\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:317 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.1 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T09:27:14,889450093-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T09:27:14,893254845-07:00] DEBUG: CEPHADM_PARAMS: ['bootstrap', '--config', '/tmp/bench-rados/ceph-research/scripts/ceph.conf', '--mon-ip', '10.10.2.1', '--output-dir', '/tmp/bench-rados/deployment_data_root/etc/ceph', '--ssh-user', 'ljishen', '--skip-dashboard', '--no-minimize-config', '--allow-overwrite', '--allow-fqdn-hostname', '--orphan-initial-daemons', '--skip-monitoring-stack', '--single-host-defaults']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.1 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults\n'
10.10.2.1: b'Verifying podman|docker is present...\n'
10.10.2.1: b'Verifying lvm2 is present...\nVerifying time synchronization is in place...\n'
10.10.2.1: b'Unit ntp.service is enabled and running\n'
10.10.2.1: b'Repeating the final host check...\n'
10.10.2.1: b'podman|docker (/usr/bin/docker) is present\n'
10.10.2.1: b'systemctl is present\n'
10.10.2.1: b'lvcreate is present\n'
10.10.2.1: b'Unit ntp.service is enabled and running\nHost looks OK\n'
10.10.2.1: b'Cluster fsid: e94548ca-380b-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Verifying IP 10.10.2.1 port 3300 ...\n'
10.10.2.1: b'Verifying IP 10.10.2.1 port 6789 ...\n'
10.10.2.1: b'Mon IP `10.10.2.1` is in CIDR network `10.10.2.0/24`\n'
10.10.2.1: b'- internal network (--cluster-network) has not been provided, OSD replication will default to the public_network\n'
10.10.2.1: b'Adjusting default settings to suit single-host cluster...\n'
10.10.2.1: b'Pulling container image quay.io/ceph/ceph:v16...\n'
10.10.2.1: b'Ceph version: ceph version 16.2.6 (ee28fb57e47e9f88813e24bbf4c14496ca299d31) pacific (stable)\n'
10.10.2.1: b'Extracting ceph user uid/gid from container image...\n'
10.10.2.1: b'Creating initial keys...\n'
10.10.2.1: b'Creating initial monmap...\n'
10.10.2.1: b'Creating mon...\n'
10.10.2.1: b'Waiting for mon to start...\n'
10.10.2.1: b'Waiting for mon...\n'
10.10.2.1: b'mon is available\n'
10.10.2.1: b'Setting mon public_network to 10.10.2.0/24\n'
10.10.2.1: b'Wrote config to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf\n'
10.10.2.1: b'Wrote keyring to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring\n'
10.10.2.1: b'Creating mgr...\n'
10.10.2.1: b'Verifying port 9283 ...\n'
10.10.2.1: b'Waiting for mgr to start...\n'
10.10.2.1: b'Waiting for mgr...\n'
10.10.2.1: b'mgr not available, waiting (1/15)...\n'
10.10.2.1: b'mgr not available, waiting (2/15)...\n'
10.10.2.1: b'mgr is available\n'
10.10.2.1: b'Enabling cephadm module...\n'
10.10.2.1: b'Waiting for the mgr to restart...\n'
10.10.2.1: b'Waiting for mgr epoch 5...\n'
10.10.2.1: b'mgr epoch 5 is available\nSetting orchestrator backend to cephadm...\n'
10.10.2.1: b'Generating ssh key...\n'
10.10.2.1: b'Wrote public SSH key to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub\n'
10.10.2.1: b'Adding key to ljishen@localhost authorized_keys...\n'
10.10.2.1: b'Adding host node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us...\n'
10.10.2.1: b'Deploying unmanaged mon service...\n'
10.10.2.1: b'Deploying unmanaged mgr service...\n'
10.10.2.1: b'You can access the Ceph CLI with:\n\n\tsudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --fsid e94548ca-380b-11ec-b51d-53e6e728d2d3 -c /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf -k /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring\n\n'
10.10.2.1: b'Please consider enabling telemetry to help improve Ceph:\n\n\tceph telemetry on\n\nFor more information see:\n\n\thttps://docs.ceph.com/docs/pacific/mgr/telemetry/\n\n'
10.10.2.1: b'Bootstrap complete.\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T09:28:17,080404816-07:00] INFO: Sleep for 20 seconds\x1b[0m\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T09:28:37,087511003-07:00] INFO: > Set the default number of monitor daemons to 1\x1b[0m\n# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:337 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T09:28:37,097375456-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T09:28:37,100807929-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mon', '1']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1\n'
10.10.2.1: b'Inferring fsid e94548ca-380b-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/e94548ca-380b-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b'Scheduled mon update...\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T09:28:46,797036517-07:00] INFO: > Set the default number of manager daemons to 1\x1b[0m\n# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:342 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T09:28:46,807592832-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T09:28:46,811474699-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mgr', '1']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1\n'
10.10.2.1: b'Inferring fsid e94548ca-380b-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/e94548ca-380b-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b'Scheduled mgr update...\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T09:28:56,169121883-07:00] STAGE: Add hosts to the cluster...\x1b[0m\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T09:28:56,175738040-07:00] INFO: > Adding host 'sm1'\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:361 - add_hosts() > ssh-copy-id -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -f -i /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub ljishen@sm1\n'
10.10.2.1: b'/usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: "/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub"\n'
10.10.2.1: b'\nNumber of key(s) added: 1\n\nNow try logging into the machine, with:   "ssh -o \'GlobalKnownHostsFile=/dev/null\' -o \'LogLevel=ERROR\' -o \'PasswordAuthentication=no\' -o \'StrictHostKeyChecking=no\' -o \'UserKnownHostsFile=/dev/null\' \'ljishen@sm1\'"\nand check to make sure that only the key(s) you wanted were added.\n\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:367 - add_hosts() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add sm1 10.10.2.5\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T09:28:57,324875966-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T09:28:57,328932612-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'host', 'add', 'sm1', '10.10.2.5']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add sm1 10.10.2.5\n'
10.10.2.1: b'Inferring fsid e94548ca-380b-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/e94548ca-380b-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b"Added host 'sm1' with addr '10.10.2.5'\n"
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T09:29:08,437465973-07:00] INFO: Sleep for 20 seconds\x1b[0m\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T09:29:28,442638754-07:00] STAGE: Add OSDs to the cluster...\x1b[0m\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T09:29:28,449281381-07:00] INFO: > Adding devices ['/dev/nvme0n1'] on host 'sm1' as OSDs\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:388 - add_host_devices_as_osds() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd sm1:/dev/nvme0n1\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T09:29:28,459990703-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T09:29:28,463834990-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'daemon', 'add', 'osd', 'sm1:/dev/nvme0n1']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd sm1:/dev/nvme0n1\n'
10.10.2.1: b'Inferring fsid e94548ca-380b-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/e94548ca-380b-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b"Created osd(s) 0 on host 'sm1'\n"
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T09:29:52,322415402-07:00] INFO: > Sleep for 20 seconds\x1b[0m\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T09:30:12,327923960-07:00] STAGE: Show Cluster Status\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:408 - show_status() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T09:30:12,338027803-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T09:30:12,341465205-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', '--status']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status\n'
10.10.2.1: b'Inferring fsid e94548ca-380b-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/e94548ca-380b-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b'  cluster:\n    id:     e94548ca-380b-11ec-b51d-53e6e728d2d3\n    health: HEALTH_WARN\n            1 pool(s) have no replicas configured\n \n  services:\n    mon: 1 daemons, quorum node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us (age 2m)\n    mgr: node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us.qtvaha(active, since 2m)\n    osd: 1 osds: 1 up (since 19s), 1 in (since 39s)\n \n  data:\n    pools:   1 pools, 1 pgs\n    objects: 0 objects, 0 B\n    usage:   4.7 MiB used, 100 GiB / 100 GiB avail\n    pgs:     1 active+clean\n \n'
[1] 09:30:20 [SUCCESS] ljishen@10.10.2.1

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:26:57,779822345-07:00] INFO: > Remove existing clusters[0m
## bash:17 -  > basename -- /var/lib/ceph/a52ba374-380a-11ec-b51d-53e6e728d2d3
# bash:17 -  > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm --host 10.10.2.1 --host 10.10.2.5 rm-cluster --force --fsid a52ba374-380a-11ec-b51d-53e6e728d2d3
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:26:57,790450424-07:00] DEBUG: HOST_PARAMS: ['--host', '10.10.2.1', '--host', '10.10.2.5'][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:26:57,793969289-07:00] DEBUG: CEPHADM_PARAMS: ['rm-cluster', '--force', '--fsid', 'a52ba374-380a-11ec-b51d-53e6e728d2d3'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:63 -  > /tmp/bench-rados/ceph-research/scripts/parallel-ssh --host 10.10.2.1 --host 10.10.2.5 'sudo python3 -u - rm-cluster --force --fsid a52ba374-380a-11ec-b51d-53e6e728d2d3'
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:26:57,803537035-07:00] DEBUG: command from stdin[0m
# /tmp/bench-rados/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host 10.10.2.1 --host 10.10.2.5 'sudo python3 -u - rm-cluster --force --fsid a52ba374-380a-11ec-b51d-53e6e728d2d3'
[1] 09:27:03 [SUCCESS] 10.10.2.1
[2] 09:27:09 [SUCCESS] 10.10.2.5

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:27:09,168892234-07:00] INFO: > Deploy a new cluster[0m
# bash:24 -  > /tmp/bench-rados/ceph-research/scripts/ceph-deploy -m 10.10.2.1 -o sm1/10.10.2.5:/dev/nvme0n1


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:27:09,181250195-07:00] STAGE: Verify passwordless SSH to hosts, passwordless sudo via ljishen, and the existence of devices on hosts...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:27:09,185824854-07:00] INFO: > Check the host for the monitor[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:181 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@10.10.2.1 sudo --non-interactive --validate

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:27:09,336194908-07:00] INFO: > Check hosts for OSDs[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:27:09,340727259-07:00] INFO: >> Check host 'sm1'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:200 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 sudo --non-interactive --validate

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:27:10,434794118-07:00] INFO: >>> Check device '/dev/nvme0n1'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:215 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 test -b /dev/nvme0n1
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:226 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 lsblk --nodeps --noheadings --output TYPE /dev/nvme0n1
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:227 - verify_parameters() > grep --word-regexp disk


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:27:12,565785356-07:00] STAGE: Reset devices on hosts...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:27:12,570858492-07:00] INFO: > Resetting device '/dev/nvme0n1' on host 'sm1'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:288 - reset_host_devices() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 sudo sh
  Removing ceph--982eab50--586d--447f--a22a--466639ac9a86-osd--block--6e1ee148--b4bc--4bdd--8097--2e9b9b0673f6 (252:0)
  Archiving volume group "ceph-982eab50-586d-447f-a22a-466639ac9a86" metadata (seqno 5).
  Releasing logical volume "osd-block-6e1ee148-b4bc-4bdd-8097-2e9b9b0673f6"
  Creating volume group backup "/etc/lvm/backup/ceph-982eab50-586d-447f-a22a-466639ac9a86" (seqno 6).
  Logical volume "osd-block-6e1ee148-b4bc-4bdd-8097-2e9b9b0673f6" successfully removed
  Removing physical volume "/dev/nvme0n1" from volume group "ceph-982eab50-586d-447f-a22a-466639ac9a86"
  Volume group "ceph-982eab50-586d-447f-a22a-466639ac9a86" successfully removed


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:27:14,880334878-07:00] STAGE: Bootstrap a new cluster...[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:317 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.1 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:27:14,889450093-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:27:14,893254845-07:00] DEBUG: CEPHADM_PARAMS: ['bootstrap', '--config', '/tmp/bench-rados/ceph-research/scripts/ceph.conf', '--mon-ip', '10.10.2.1', '--output-dir', '/tmp/bench-rados/deployment_data_root/etc/ceph', '--ssh-user', 'ljishen', '--skip-dashboard', '--no-minimize-config', '--allow-overwrite', '--allow-fqdn-hostname', '--orphan-initial-daemons', '--skip-monitoring-stack', '--single-host-defaults'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.1 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults
Verifying podman|docker is present...
Verifying lvm2 is present...
Verifying time synchronization is in place...
Unit ntp.service is enabled and running
Repeating the final host check...
podman|docker (/usr/bin/docker) is present
systemctl is present
lvcreate is present
Unit ntp.service is enabled and running
Host looks OK
Cluster fsid: e94548ca-380b-11ec-b51d-53e6e728d2d3
Verifying IP 10.10.2.1 port 3300 ...
Verifying IP 10.10.2.1 port 6789 ...
Mon IP `10.10.2.1` is in CIDR network `10.10.2.0/24`
- internal network (--cluster-network) has not been provided, OSD replication will default to the public_network
Adjusting default settings to suit single-host cluster...
Pulling container image quay.io/ceph/ceph:v16...
Ceph version: ceph version 16.2.6 (ee28fb57e47e9f88813e24bbf4c14496ca299d31) pacific (stable)
Extracting ceph user uid/gid from container image...
Creating initial keys...
Creating initial monmap...
Creating mon...
Waiting for mon to start...
Waiting for mon...
mon is available
Setting mon public_network to 10.10.2.0/24
Wrote config to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf
Wrote keyring to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring
Creating mgr...
Verifying port 9283 ...
Waiting for mgr to start...
Waiting for mgr...
mgr not available, waiting (1/15)...
mgr not available, waiting (2/15)...
mgr is available
Enabling cephadm module...
Waiting for the mgr to restart...
Waiting for mgr epoch 5...
mgr epoch 5 is available
Setting orchestrator backend to cephadm...
Generating ssh key...
Wrote public SSH key to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub
Adding key to ljishen@localhost authorized_keys...
Adding host node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us...
Deploying unmanaged mon service...
Deploying unmanaged mgr service...
You can access the Ceph CLI with:

	sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --fsid e94548ca-380b-11ec-b51d-53e6e728d2d3 -c /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf -k /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring

Please consider enabling telemetry to help improve Ceph:

	ceph telemetry on

For more information see:

	https://docs.ceph.com/docs/pacific/mgr/telemetry/

Bootstrap complete.

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:28:17,080404816-07:00] INFO: Sleep for 20 seconds[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:28:37,087511003-07:00] INFO: > Set the default number of monitor daemons to 1[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:337 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:28:37,097375456-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:28:37,100807929-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mon', '1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1
Inferring fsid e94548ca-380b-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/e94548ca-380b-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Scheduled mon update...

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:28:46,797036517-07:00] INFO: > Set the default number of manager daemons to 1[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:342 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:28:46,807592832-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:28:46,811474699-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mgr', '1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1
Inferring fsid e94548ca-380b-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/e94548ca-380b-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Scheduled mgr update...


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:28:56,169121883-07:00] STAGE: Add hosts to the cluster...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:28:56,175738040-07:00] INFO: > Adding host 'sm1'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:361 - add_hosts() > ssh-copy-id -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -f -i /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub ljishen@sm1
/usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: "/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub"

Number of key(s) added: 1

Now try logging into the machine, with:   "ssh -o 'GlobalKnownHostsFile=/dev/null' -o 'LogLevel=ERROR' -o 'PasswordAuthentication=no' -o 'StrictHostKeyChecking=no' -o 'UserKnownHostsFile=/dev/null' 'ljishen@sm1'"
and check to make sure that only the key(s) you wanted were added.

# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:367 - add_hosts() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add sm1 10.10.2.5
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:28:57,324875966-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:28:57,328932612-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'host', 'add', 'sm1', '10.10.2.5'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add sm1 10.10.2.5
Inferring fsid e94548ca-380b-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/e94548ca-380b-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Added host 'sm1' with addr '10.10.2.5'

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:29:08,437465973-07:00] INFO: Sleep for 20 seconds[0m


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:29:28,442638754-07:00] STAGE: Add OSDs to the cluster...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:29:28,449281381-07:00] INFO: > Adding devices ['/dev/nvme0n1'] on host 'sm1' as OSDs[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:388 - add_host_devices_as_osds() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd sm1:/dev/nvme0n1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:29:28,459990703-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:29:28,463834990-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'daemon', 'add', 'osd', 'sm1:/dev/nvme0n1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd sm1:/dev/nvme0n1
Inferring fsid e94548ca-380b-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/e94548ca-380b-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Created osd(s) 0 on host 'sm1'

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:29:52,322415402-07:00] INFO: > Sleep for 20 seconds[0m


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:30:12,327923960-07:00] STAGE: Show Cluster Status[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:408 - show_status() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:30:12,338027803-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:30:12,341465205-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', '--status'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status
Inferring fsid e94548ca-380b-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/e94548ca-380b-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
  cluster:
    id:     e94548ca-380b-11ec-b51d-53e6e728d2d3
    health: HEALTH_WARN
            1 pool(s) have no replicas configured
 
  services:
    mon: 1 daemons, quorum node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us (age 2m)
    mgr: node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us.qtvaha(active, since 2m)
    osd: 1 osds: 1 up (since 19s), 1 in (since 39s)
 
  data:
    pools:   1 pools, 1 pgs
    objects: 0 objects, 0 B
    usage:   4.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     1 active+clean
 

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:30:20,782421152-07:00] INFO: > Copy configuration files to local[0m
# ./benchmarks/bench-rados:220 - launch_ceph_cluster() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.1 sudo chown --changes --recursive ljishen /tmp/bench-rados/deployment_data_root/etc/ceph
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:30:20,790191424-07:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.1 sudo chown --changes --recursive ljishen /tmp/bench-rados/deployment_data_root/etc/ceph
[1] 09:30:20 [SUCCESS] ljishen@10.10.2.1
# ./benchmarks/bench-rados:223 - launch_ceph_cluster() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.1:/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf
# ./benchmarks/bench-rados:226 - launch_ceph_cluster() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.1:/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:30:21,264952035-07:00] STAGE: Update local ceph.conf as a client...[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:30:21,269016113-07:00] STAGE: Configure Ceph pool...[0m
# ./benchmarks/bench-rados:257 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_max_backfills 32
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:30:21,291113994-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:30:21,294083599-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'e94548ca-380b-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'config', 'set', 'osd', 'osd_max_backfills', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid e94548ca-380b-11ec-b51d-53e6e728d2d3 -- ceph config set osd osd_max_backfills 32
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:258 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_active 32
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:30:25,334966387-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:30:25,338069464-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'e94548ca-380b-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_active', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid e94548ca-380b-11ec-b51d-53e6e728d2d3 -- ceph config set osd osd_recovery_max_active 32
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:259 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_single_start 8
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:30:29,136615120-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:30:29,139654848-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'e94548ca-380b-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_single_start', '8'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid e94548ca-380b-11ec-b51d-53e6e728d2d3 -- ceph config set osd osd_recovery_max_single_start 8
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:260 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_op_priority 63
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:30:33,004399148-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:30:33,007336182-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'e94548ca-380b-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_op_priority', '63'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid e94548ca-380b-11ec-b51d-53e6e728d2d3 -- ceph config set osd osd_recovery_op_priority 63
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:262 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config show-with-defaults osd.0
# ./benchmarks/bench-rados:263 - configure_ceph_pool() > grep 'osd_max_backfills\|osd_recovery'
# ./benchmarks/bench-rados:263 - configure_ceph_pool() > column -t -s ' '
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
osd_max_backfills                        32         mon
osd_recovery_cost                        20971520   default
osd_recovery_delay_start                 0.000000   default
osd_recovery_max_active                  32         mon
osd_recovery_max_active_hdd              3          default
osd_recovery_max_active_ssd              10         default
osd_recovery_max_chunk                   8388608    default
osd_recovery_max_omap_entries_per_chunk  8096       default
osd_recovery_max_single_start            8          mon
osd_recovery_op_priority                 63         mon
osd_recovery_op_warn_multiple            16         default
osd_recovery_priority                    5          default
osd_recovery_retry_interval              30.000000  default
osd_recovery_sleep                       0.000000   default
osd_recovery_sleep_hdd                   0.100000   default
osd_recovery_sleep_hybrid                0.025000   default
osd_recovery_sleep_ssd                   0.000000   default
# ./benchmarks/bench-rados:265 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool create bench_rados 128 128 replicated --size 1 --pg-num-min 128
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:30:40,715964683-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:30:40,718928789-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'e94548ca-380b-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'create', 'bench_rados', '128', '128', 'replicated', '--size', '1', '--pg-num-min', '128'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid e94548ca-380b-11ec-b51d-53e6e728d2d3 -- ceph osd pool create bench_rados 128 128 replicated --size 1 --pg-num-min 128
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
pool 'bench_rados' created
# ./benchmarks/bench-rados:267 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados min_size 1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:30:44,957037814-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:30:44,959853630-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'e94548ca-380b-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'min_size', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid e94548ca-380b-11ec-b51d-53e6e728d2d3 -- ceph osd pool set bench_rados min_size 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 min_size to 1
# ./benchmarks/bench-rados:268 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados pg_autoscale_mode off
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:30:49,315217125-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:30:49,318189736-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'e94548ca-380b-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'pg_autoscale_mode', 'off'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid e94548ca-380b-11ec-b51d-53e6e728d2d3 -- ceph osd pool set bench_rados pg_autoscale_mode off
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 pg_autoscale_mode to off
# ./benchmarks/bench-rados:269 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool application enable bench_rados benchmark
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:30:54,014660662-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:30:54,017601424-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'e94548ca-380b-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'application', 'enable', 'bench_rados', 'benchmark'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid e94548ca-380b-11ec-b51d-53e6e728d2d3 -- ceph osd pool application enable bench_rados benchmark
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
enabled application 'benchmark' on pool 'bench_rados'
# ./benchmarks/bench-rados:270 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados noscrub 1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:30:58,342466409-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:30:58,345503191-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'e94548ca-380b-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'noscrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid e94548ca-380b-11ec-b51d-53e6e728d2d3 -- ceph osd pool set bench_rados noscrub 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 noscrub to 1
# ./benchmarks/bench-rados:271 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados nodeep-scrub 1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:31:02,744204903-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:31:02,747182274-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'e94548ca-380b-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'nodeep-scrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid e94548ca-380b-11ec-b51d-53e6e728d2d3 -- ceph osd pool set bench_rados nodeep-scrub 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 nodeep-scrub to 1
# ./benchmarks/bench-rados:272 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool ls detail
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:31:07,074747509-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:31:07,077720912-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'e94548ca-380b-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'ls', 'detail'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid e94548ca-380b-11ec-b51d-53e6e728d2d3 -- ceph osd pool ls detail
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
pool 1 'device_health_metrics' replicated size 1 min_size 1 crush_rule 0 object_hash rjenkins pg_num 64 pgp_num 64 autoscale_mode on last_change 19 lfor 0/0/16 flags hashpspool stripe_width 0 pg_num_min 1 application mgr_devicehealth
pool 2 'bench_rados' replicated size 1 min_size 1 crush_rule 0 object_hash rjenkins pg_num 128 pgp_num 128 autoscale_mode off last_change 20 flags hashpspool,noscrub,nodeep-scrub stripe_width 0 pg_num_min 128 application benchmark

# ./benchmarks/bench-rados:273 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd df tree
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:31:10,926616359-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:31:10,929850613-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'e94548ca-380b-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'df', 'tree'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid e94548ca-380b-11ec-b51d-53e6e728d2d3 -- ceph osd df tree
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
ID  CLASS  WEIGHT   REWEIGHT  SIZE     RAW USE  DATA     OMAP  META     AVAIL    %USE  VAR   PGS  STATUS  TYPE NAME    
-1         0.09769         -  100 GiB  5.7 MiB  168 KiB   0 B  5.6 MiB  100 GiB  0.01  1.00    -          root default 
-3         0.09769         -  100 GiB  5.7 MiB  168 KiB   0 B  5.6 MiB  100 GiB  0.01  1.00    -              host sm1 
 0    ssd  0.09769   1.00000  100 GiB  5.7 MiB  168 KiB   0 B  5.6 MiB  100 GiB  0.01  1.00  192      up          osd.0
                       TOTAL  100 GiB  5.7 MiB  168 KiB   0 B  5.6 MiB  100 GiB  0.01                                  
MIN/MAX VAR: 1.00/1.00  STDDEV: 0


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:31:14,829314591-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:31:38,561333857-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:31:47,444123117-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:31:56,234712379-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:32:05,150527024-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:32:05,158935028-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:32:13,984889525-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:32:13,992935717-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:32:22,733253775-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:32:22,741133393-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:32:31,598008369-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:32:31,606281058-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:32:40,497885068-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:32:40,505881306-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:32:40,512276367-07:00] INFO: > The cluster is idle now.[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:32:40,515892561-07:00] STAGE: Start benchmarking of mode write ...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:32:40,523333522-07:00] INFO: > Start system activity collection services[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:32:40,529033664-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:341 - rados_bench() > rm --force /mnt/sda8/ceph-research/scripts/output/sys_activity_4MB_write.dat.rados_bench_host.3
# ./benchmarks/bench-rados:342 - rados_bench() > sar_pid_rados_bench_host=496517
# ./benchmarks/bench-rados:342 - rados_bench() > S_TIME_FORMAT=ISO
# ./benchmarks/bench-rados:342 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/output/sys_activity_4MB_write.dat.rados_bench_host.3 2

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:32:40,536712173-07:00] INFO: >> for OSD host[0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:32:40,546204179-07:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.5 bash
10.10.2.5: b'179525\n'
[1] 09:32:41 [SUCCESS] ljishen@10.10.2.5
179525

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:32:41,665081763-07:00] INFO: > Run rados bench[0m
# ./benchmarks/bench-rados:370 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/output/rados_bench_4MB_write.log.3
## ./benchmarks/bench-rados:361 - rados_bench() > count_cpus 32-63,96-127
# ./benchmarks/bench-rados:361 - rados_bench() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 60 write --pool bench_rados -b 4194304 -O 4194304 --concurrent-ios 128 --show-time --write-object --write-omap --write-xattr --no-cleanup
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:32:41,686136658-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:32:41,689169253-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'e94548ca-380b-11ec-b51d-53e6e728d2d3', '--', 'rados', 'bench', '60', 'write', '--pool', 'bench_rados', '-b', '4194304', '-O', '4194304', '--concurrent-ios', '128', '--show-time', '--write-object', '--write-omap', '--write-xattr', '--no-cleanup'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid e94548ca-380b-11ec-b51d-53e6e728d2d3 -- rados bench 60 write --pool bench_rados -b 4194304 -O 4194304 --concurrent-ios 128 --show-time --write-object --write-omap --write-xattr --no-cleanup
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
hints = 1
2021-10-28T16:32:44.611610+0000 Maintaining 128 concurrent writes of 4194304 bytes to objects of size 4194304 for up to 60 seconds or 0 objects
2021-10-28T16:32:44.611625+0000 Object prefix: benchmark_data_node-2.bluefield2.ucsc-cmps10_7
2021-10-28T16:32:44.737552+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-28T16:32:44.737552+0000     0       0         0         0         0         0           -           0
2021-10-28T16:32:45.737690+0000     1      20        20         0         0         0           -           0
2021-10-28T16:32:46.737811+0000     2      39        39         0         0         0           -           0
2021-10-28T16:32:47.737929+0000     3      55        55         0         0         0           -           0
2021-10-28T16:32:48.738046+0000     4      75        75         0         0         0           -           0
2021-10-28T16:32:49.738138+0000     5      89        89         0         0         0           -           0
2021-10-28T16:32:50.738258+0000     6     108       108         0         0         0           -           0
2021-10-28T16:32:51.738378+0000     7     125       125         0         0         0           -           0
2021-10-28T16:32:52.738497+0000     8     127       137        10   4.99945         5     7.49363     7.46519
2021-10-28T16:32:53.738589+0000     9     127       145        18   7.99914        32     7.84249     7.64042
2021-10-28T16:32:54.738714+0000    10     127       153        26   10.3989        32       8.343      7.8792
2021-10-28T16:32:55.738842+0000    11     127       164        37   13.4531        44     9.08939     8.21241
2021-10-28T16:32:56.738964+0000    12     127       172        45   14.9983        32      9.7025     8.45642
2021-10-28T16:32:57.739060+0000    13     127       180        53   16.3059        32     10.1145     8.70042
2021-10-28T16:32:58.739191+0000    14     127       195        68   19.4264        60     10.2971     9.08282
2021-10-28T16:32:59.739320+0000    15     127       209        82   21.8642        56     10.2846     9.32265
2021-10-28T16:33:00.739445+0000    16     127       228       101   25.2471        76     10.4406     9.53762
2021-10-28T16:33:01.739540+0000    17     127       247       120   28.2321        76     10.3567     9.67251
2021-10-28T16:33:02.739662+0000    18     127       264       137    30.441        68     10.2701     9.75716
2021-10-28T16:33:03.739783+0000    19     127       271       144   30.3123        28     10.1785     9.78684
2021-10-28T16:33:04.739904+0000 min lat: 7.26949 max lat: 10.9234 avg lat: 9.80726
2021-10-28T16:33:04.739904+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-28T16:33:04.739904+0000    20     127       274       147   29.3966        12     10.5175     9.80726
2021-10-28T16:33:05.740011+0000    21     127       279       152   28.9491        20     11.0712     9.85708
2021-10-28T16:33:06.740144+0000    22     127       282       155   28.1786        12     11.4609     9.89308
2021-10-28T16:33:07.740243+0000    23     127       287       160   27.8229        20     12.1815      9.9718
2021-10-28T16:33:08.740370+0000    24     127       290       163   27.1636        12     12.5231     10.0231
2021-10-28T16:33:09.740471+0000    25     127       295       168   26.8769        20     13.0874     10.1209
2021-10-28T16:33:10.740603+0000    26     127       303       176   27.0738        32      13.542      10.286
2021-10-28T16:33:11.740721+0000    27     127       312       185   27.4043        36     13.5769     10.4495
2021-10-28T16:33:12.740842+0000    28     127       320       193   27.5683        32     13.8483     10.5909
2021-10-28T16:33:13.740945+0000    29     127       332       205   28.2726        48     14.3138     10.8058
2021-10-28T16:33:14.741066+0000    30     127       341       214   28.5301        36     14.7496      10.966
2021-10-28T16:33:15.741185+0000    31     127       349       222   28.6419        32     14.9685     11.1105
2021-10-28T16:33:16.741305+0000    32     127       361       234   29.2466        48     15.4701     11.3289
2021-10-28T16:33:17.741398+0000    33     127       373       246   29.8148        48     15.7932     11.5464
2021-10-28T16:33:18.741519+0000    34     127       385       258   30.3495        48     16.2388     11.7614
2021-10-28T16:33:19.741643+0000    35     127       392       265   30.2822        28     16.5121     11.8848
2021-10-28T16:33:20.741761+0000    36     127       400       273   30.3298        32     16.0934     12.0246
2021-10-28T16:33:21.741855+0000    37     127       408       281   30.3749        32     15.1035     12.1335
2021-10-28T16:33:22.741973+0000    38     127       416       289   30.4176        32     14.0621     12.2067
2021-10-28T16:33:23.742093+0000    39     127       424       297    30.458        32     13.1385     12.2511
2021-10-28T16:33:24.742213+0000 min lat: 7.26949 max lat: 17.0649 avg lat: 12.3
2021-10-28T16:33:24.742213+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-28T16:33:24.742213+0000    40     127       437       310   30.9964        52     13.0825        12.3
2021-10-28T16:33:25.742313+0000    41     127       451       324   31.6061        56     12.6296     12.3293
2021-10-28T16:33:26.742434+0000    42     127       467       340   32.3772        64     12.3479     12.3377
2021-10-28T16:33:27.742557+0000    43     127       485       358   33.2985        72     11.6439     12.3146
2021-10-28T16:33:28.742683+0000    44     127       501       374   33.9961        64     11.2213     12.2736
2021-10-28T16:33:29.742793+0000    45     127       515       388   34.4849        56     10.6537     12.2248
2021-10-28T16:33:30.742916+0000    46     127       525       398   34.6047        40     10.4408     12.1833
2021-10-28T16:33:31.743040+0000    47     127       528       401   34.1237        12     10.4433     12.1703
2021-10-28T16:33:32.743171+0000    48     127       536       409   34.0794        32     11.4157     12.1508
2021-10-28T16:33:33.743272+0000    49     127       536       409   33.3839         0           -     12.1508
2021-10-28T16:33:34.743362+0000    50     127       541       414   33.1162        10     12.0289     12.1494
2021-10-28T16:33:35.743456+0000    51     127       544       417   32.7021        12       12.47     12.1517
2021-10-28T16:33:36.743584+0000    52     127       552       425   32.6886        32     13.4155     12.1707
2021-10-28T16:33:37.743685+0000    53     127       558       431   32.5246        24     13.3719     12.1899
2021-10-28T16:33:38.743767+0000    54     127       566       439   32.5148        32     13.5518     12.2168
2021-10-28T16:33:39.743888+0000    55     127       576       449   32.6508        40     14.0116     12.2569
2021-10-28T16:33:40.744018+0000    56     127       586       459    32.782        40     14.3353     12.3034
2021-10-28T16:33:41.744130+0000    57     127       596       469   32.9085        40     14.7464     12.3553
2021-10-28T16:33:42.744253+0000    58     127       607       480   33.0997        44     15.2485      12.419
2021-10-28T16:33:43.744379+0000    59     127       615       488    33.081        32     15.4921     12.4694
2021-10-28T16:33:44.744515+0000 min lat: 7.26949 max lat: 17.0649 avg lat: 12.5449
2021-10-28T16:33:44.744515+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-28T16:33:44.744515+0000    60     127       626       499   33.2628        44     15.9236     12.5449
2021-10-28T16:33:45.744655+0000 Total time run:         60.625
Total writes made:      627
Write size:             4194304
Object size:            4194304
Bandwidth (MB/sec):     41.3691
Stddev Bandwidth:       20.7442
Max bandwidth (MB/sec): 76
Min bandwidth (MB/sec): 0
Average IOPS:           10
Stddev IOPS:            5.20134
Max IOPS:               19
Min IOPS:               0
Average Latency(s):     11.5724
Stddev Latency(s):      3.67109
Max latency(s):         17.0649
Min latency(s):         0.4797

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:33:46,369061913-07:00] INFO: > Stop system activity collection services[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:33:46,375613859-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:391 - rados_bench() > kill -INT 496517

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:33:46,382576419-07:00] INFO: >> for OSD host[0m
# ./benchmarks/bench-rados:396 - rados_bench() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.5 kill -INT 179525
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:33:46,390521100-07:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.5 kill -INT 179525
[1] 09:33:48 [SUCCESS] ljishen@10.10.2.5

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:33:48,174207599-07:00] INFO: >>> Copy system activity file from OSD host[0m
## ./benchmarks/bench-rados:403 - rados_bench() > basename -- /tmp/bench-rados/sys_activity_4MB_write.dat.osd_host.3
# ./benchmarks/bench-rados:403 - rados_bench() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.5:/tmp/bench-rados/sys_activity_4MB_write.dat.osd_host.3 /mnt/sda8/ceph-research/scripts/output/sys_activity_4MB_write.dat.osd_host.3


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:33:49,594991167-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:34:13,505061849-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 628 objects, 2.4 GiB
    usage:   7.4 GiB used, 93 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:34:13,513419188-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:34:22,337449263-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 628 objects, 2.4 GiB
    usage:   7.4 GiB used, 93 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:34:22,345463806-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:34:31,185898455-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 628 objects, 2.4 GiB
    usage:   7.4 GiB used, 93 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:34:31,194307090-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:34:40,092892399-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 628 objects, 2.4 GiB
    usage:   7.4 GiB used, 93 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:34:40,100828864-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:34:48,908889512-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 628 objects, 2.4 GiB
    usage:   7.4 GiB used, 93 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:34:48,917197798-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:34:48,923947297-07:00] INFO: > The cluster is idle now.[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:34:48,927866262-07:00] STAGE: Start benchmarking of mode seq ...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:34:48,935094733-07:00] INFO: > Start system activity collection services[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:34:48,940899482-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:341 - rados_bench() > rm --force /mnt/sda8/ceph-research/scripts/output/sys_activity_4MB_seq.dat.rados_bench_host.3
# ./benchmarks/bench-rados:342 - rados_bench() > sar_pid_rados_bench_host=497870
# ./benchmarks/bench-rados:342 - rados_bench() > S_TIME_FORMAT=ISO
# ./benchmarks/bench-rados:342 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/output/sys_activity_4MB_seq.dat.rados_bench_host.3 2

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:34:48,948502619-07:00] INFO: >> for OSD host[0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:34:48,957953788-07:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.5 bash
Traceback (most recent call last):
  File "/usr/lib/python3.8/signal.py", line 47, in signal
    handler = _signal.signal(_enum_to_int(signalnum), _enum_to_int(handler))
OSError: Signal 17 ignored due to race condition
10.10.2.5: b'180073\n'
[1] 09:34:50 [SUCCESS] ljishen@10.10.2.5
180073

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:34:50,298936683-07:00] INFO: > Run rados bench[0m
# ./benchmarks/bench-rados:379 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/output/rados_bench_4MB_seq.log.3
## ./benchmarks/bench-rados:376 - rados_bench() > count_cpus 32-63,96-127
# ./benchmarks/bench-rados:376 - rados_bench() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 128 --show-time
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:34:50,319338879-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:34:50,322377945-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'e94548ca-380b-11ec-b51d-53e6e728d2d3', '--', 'rados', 'bench', '99999999', 'seq', '--pool', 'bench_rados', '--concurrent-ios', '128', '--show-time'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid e94548ca-380b-11ec-b51d-53e6e728d2d3 -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 128 --show-time
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
hints = 1
2021-10-28T16:34:53.300807+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-28T16:34:53.300807+0000     0       0         0         0         0         0           -           0
2021-10-28T16:34:54.300944+0000     1     127       190        63   251.943       252    0.712778     0.71386
2021-10-28T16:34:55.301024+0000     2     127       366       239   477.927       704     0.73048    0.720364
2021-10-28T16:34:56.301144+0000     3     127       545       418   557.254       716    0.725393    0.714718
2021-10-28T16:34:57.301262+0000 Total time run:       3.58332
Total reads made:     627
Read size:            4194304
Object size:          4194304
Bandwidth (MB/sec):   699.909
Average IOPS:         174
Stddev IOPS:          66.1236
Max IOPS:             179
Min IOPS:             63
Average Latency(s):   0.649227
Max latency(s):       0.790073
Min latency(s):       0.0858153

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:34:58,029357664-07:00] INFO: > Stop system activity collection services[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:34:58,036140916-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:391 - rados_bench() > kill -INT 497870

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:34:58,042143407-07:00] INFO: >> for OSD host[0m
# ./benchmarks/bench-rados:396 - rados_bench() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.5 kill -INT 180073
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:34:58,050181153-07:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.5 kill -INT 180073
[1] 09:34:59 [SUCCESS] ljishen@10.10.2.5

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:34:59,160916598-07:00] INFO: >>> Copy system activity file from OSD host[0m
## ./benchmarks/bench-rados:403 - rados_bench() > basename -- /tmp/bench-rados/sys_activity_4MB_seq.dat.osd_host.3
# ./benchmarks/bench-rados:403 - rados_bench() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.5:/tmp/bench-rados/sys_activity_4MB_seq.dat.osd_host.3 /mnt/sda8/ceph-research/scripts/output/sys_activity_4MB_seq.dat.osd_host.3


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:35:00,252586136-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:35:24,149382882-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 628 objects, 2.4 GiB
    usage:   7.4 GiB used, 93 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:35:24,157571352-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:35:32,971914617-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 628 objects, 2.4 GiB
    usage:   7.4 GiB used, 93 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:35:32,980226981-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:35:41,881737023-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 628 objects, 2.4 GiB
    usage:   7.4 GiB used, 93 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:35:41,889810817-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:35:50,710785081-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 628 objects, 2.4 GiB
    usage:   7.4 GiB used, 93 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:35:50,719072888-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:35:59,516966932-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 628 objects, 2.4 GiB
    usage:   7.4 GiB used, 93 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:35:59,525700950-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:35:59,532071835-07:00] INFO: > The cluster is idle now.[0m


[1;7;39;49m[2021-10-28T09:35:59,536123149-07:00][RUNNING][ROUND 1/7/21] object_size=16MB[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:35:59,539897993-07:00] STAGE: Launch a Ceph cluster (CEPH_HOST: 10.10.2.1, OSD_HOST: 10.10.2.5) ...[0m
# ./benchmarks/bench-rados:183 - launch_ceph_cluster() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.1 bash -euo pipefail
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:35:59,549687379-07:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.1 bash -euo pipefail
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T09:35:59,984205585-07:00] INFO: > Remove existing clusters\x1b[0m\n'
10.10.2.1: b'## bash:17 -  > basename -- /var/lib/ceph/e94548ca-380b-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'# bash:17 -  > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm --host 10.10.2.1 --host 10.10.2.5 rm-cluster --force --fsid e94548ca-380b-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T09:35:59,994771256-07:00] DEBUG: HOST_PARAMS: ['--host', '10.10.2.1', '--host', '10.10.2.5']\x1b[0m\n"
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T09:35:59,998479076-07:00] DEBUG: CEPHADM_PARAMS: ['rm-cluster', '--force', '--fsid', 'e94548ca-380b-11ec-b51d-53e6e728d2d3']\x1b[0m\n"
10.10.2.1: b"# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:63 -  > /tmp/bench-rados/ceph-research/scripts/parallel-ssh --host 10.10.2.1 --host 10.10.2.5 'sudo python3 -u - rm-cluster --force --fsid e94548ca-380b-11ec-b51d-53e6e728d2d3'\n"
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T09:36:00,007098298-07:00] DEBUG: command from stdin\x1b[0m\n'
10.10.2.1: b"# /tmp/bench-rados/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host 10.10.2.1 --host 10.10.2.5 'sudo python3 -u - rm-cluster --force --fsid e94548ca-380b-11ec-b51d-53e6e728d2d3'\n"
10.10.2.1: b'[1] 09:36:06 [SUCCESS] 10.10.2.1\n[2] 09:36:11 [SUCCESS] 10.10.2.5\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T09:36:11,955119446-07:00] INFO: > Deploy a new cluster\x1b[0m\n'
10.10.2.1: b'# bash:24 -  > /tmp/bench-rados/ceph-research/scripts/ceph-deploy -m 10.10.2.1 -o sm1/10.10.2.5:/dev/nvme0n1\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T09:36:11,966978728-07:00] STAGE: Verify passwordless SSH to hosts, passwordless sudo via ljishen, and the existence of devices on hosts...\x1b[0m\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T09:36:11,971802335-07:00] INFO: > Check the host for the monitor\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:181 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@10.10.2.1 sudo --non-interactive --validate\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T09:36:12,122942851-07:00] INFO: > Check hosts for OSDs\x1b[0m\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T09:36:12,127675156-07:00] INFO: >> Check host 'sm1'\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:200 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 sudo --non-interactive --validate\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T09:36:13,215137678-07:00] INFO: >>> Check device '/dev/nvme0n1'\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:215 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 test -b /dev/nvme0n1\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:226 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 lsblk --nodeps --noheadings --output TYPE /dev/nvme0n1\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:227 - verify_parameters() > grep --word-regexp disk\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T09:36:15,374253495-07:00] STAGE: Reset devices on hosts...\x1b[0m\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T09:36:15,379233346-07:00] INFO: > Resetting device '/dev/nvme0n1' on host 'sm1'\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:288 - reset_host_devices() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 sudo sh\n'
10.10.2.1: b'  Removing ceph--06f29e38--2516--4798--8e59--a792dbaea07d-osd--block--1be5fb76--406e--4eb0--b891--e7309cf53561 (252:0)\n'
10.10.2.1: b'  Archiving volume group "ceph-06f29e38-2516-4798-8e59-a792dbaea07d" metadata (seqno 5).\n'
10.10.2.1: b'  Releasing logical volume "osd-block-1be5fb76-406e-4eb0-b891-e7309cf53561"\n'
10.10.2.1: b'  Creating volume group backup "/etc/lvm/backup/ceph-06f29e38-2516-4798-8e59-a792dbaea07d" (seqno 6).\n'
10.10.2.1: b'  Logical volume "osd-block-1be5fb76-406e-4eb0-b891-e7309cf53561" successfully removed\n'
10.10.2.1: b'  Removing physical volume "/dev/nvme0n1" from volume group "ceph-06f29e38-2516-4798-8e59-a792dbaea07d"\n'
10.10.2.1: b'  Volume group "ceph-06f29e38-2516-4798-8e59-a792dbaea07d" successfully removed\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T09:36:17,683939124-07:00] STAGE: Bootstrap a new cluster...\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:317 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.1 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T09:36:17,693467615-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T09:36:17,697146922-07:00] DEBUG: CEPHADM_PARAMS: ['bootstrap', '--config', '/tmp/bench-rados/ceph-research/scripts/ceph.conf', '--mon-ip', '10.10.2.1', '--output-dir', '/tmp/bench-rados/deployment_data_root/etc/ceph', '--ssh-user', 'ljishen', '--skip-dashboard', '--no-minimize-config', '--allow-overwrite', '--allow-fqdn-hostname', '--orphan-initial-daemons', '--skip-monitoring-stack', '--single-host-defaults']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.1 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults\n'
10.10.2.1: b'Verifying podman|docker is present...\nVerifying lvm2 is present...\n'
10.10.2.1: b'Verifying time synchronization is in place...\n'
10.10.2.1: b'Unit ntp.service is enabled and running\n'
10.10.2.1: b'Repeating the final host check...\npodman|docker (/usr/bin/docker) is present\n'
10.10.2.1: b'systemctl is present\n'
10.10.2.1: b'lvcreate is present\n'
10.10.2.1: b'Unit ntp.service is enabled and running\n'
10.10.2.1: b'Host looks OK\n'
10.10.2.1: b'Cluster fsid: 2ccebeea-380d-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Verifying IP 10.10.2.1 port 3300 ...\n'
10.10.2.1: b'Verifying IP 10.10.2.1 port 6789 ...\n'
10.10.2.1: b'Mon IP `10.10.2.1` is in CIDR network `10.10.2.0/24`\n'
10.10.2.1: b'- internal network (--cluster-network) has not been provided, OSD replication will default to the public_network\n'
10.10.2.1: b'Adjusting default settings to suit single-host cluster...\n'
10.10.2.1: b'Pulling container image quay.io/ceph/ceph:v16...\n'
10.10.2.1: b'Ceph version: ceph version 16.2.6 (ee28fb57e47e9f88813e24bbf4c14496ca299d31) pacific (stable)\n'
10.10.2.1: b'Extracting ceph user uid/gid from container image...\n'
10.10.2.1: b'Creating initial keys...\n'
10.10.2.1: b'Creating initial monmap...\n'
10.10.2.1: b'Creating mon...\n'
10.10.2.1: b'Waiting for mon to start...\n'
10.10.2.1: b'Waiting for mon...\n'
10.10.2.1: b'mon is available\nSetting mon public_network to 10.10.2.0/24\n'
10.10.2.1: b'Wrote config to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf\n'
10.10.2.1: b'Wrote keyring to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring\n'
10.10.2.1: b'Creating mgr...\n'
10.10.2.1: b'Verifying port 9283 ...\n'
10.10.2.1: b'Waiting for mgr to start...\n'
10.10.2.1: b'Waiting for mgr...\n'
10.10.2.1: b'mgr not available, waiting (1/15)...\n'
10.10.2.1: b'mgr not available, waiting (2/15)...\n'
10.10.2.1: b'mgr is available\n'
10.10.2.1: b'Enabling cephadm module...\n'
10.10.2.1: b'Waiting for the mgr to restart...\n'
10.10.2.1: b'Waiting for mgr epoch 5...\n'
10.10.2.1: b'mgr epoch 5 is available\nSetting orchestrator backend to cephadm...\n'
10.10.2.1: b'Generating ssh key...\n'
10.10.2.1: b'Wrote public SSH key to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub\nAdding key to ljishen@localhost authorized_keys...\n'
10.10.2.1: b'Adding host node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us...\n'
10.10.2.1: b'Deploying unmanaged mon service...\n'
10.10.2.1: b'Deploying unmanaged mgr service...\n'
10.10.2.1: b'You can access the Ceph CLI with:\n\n\tsudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --fsid 2ccebeea-380d-11ec-b51d-53e6e728d2d3 -c /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf -k /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring\n\n'
10.10.2.1: b'Please consider enabling telemetry to help improve Ceph:\n\n\tceph telemetry on\n\nFor more information see:\n\n\thttps://docs.ceph.com/docs/pacific/mgr/telemetry/\n\n'
10.10.2.1: b'Bootstrap complete.\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T09:37:19,383574717-07:00] INFO: Sleep for 20 seconds\x1b[0m\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T09:37:39,390173911-07:00] INFO: > Set the default number of monitor daemons to 1\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:337 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T09:37:39,400805014-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T09:37:39,404596271-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mon', '1']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1\n'
10.10.2.1: b'Inferring fsid 2ccebeea-380d-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/2ccebeea-380d-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b'Scheduled mon update...\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T09:37:48,555873695-07:00] INFO: > Set the default number of manager daemons to 1\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:342 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T09:37:48,565710556-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T09:37:48,569787830-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mgr', '1']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1\n'
10.10.2.1: b'Inferring fsid 2ccebeea-380d-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/2ccebeea-380d-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b'Scheduled mgr update...\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T09:37:57,880815095-07:00] STAGE: Add hosts to the cluster...\x1b[0m\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T09:37:57,887386928-07:00] INFO: > Adding host 'sm1'\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:361 - add_hosts() > ssh-copy-id -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -f -i /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub ljishen@sm1\n'
10.10.2.1: b'/usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: "/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub"\n'
10.10.2.1: b'\nNumber of key(s) added: 1\n\nNow try logging into the machine, with:   "ssh -o \'GlobalKnownHostsFile=/dev/null\' -o \'LogLevel=ERROR\' -o \'PasswordAuthentication=no\' -o \'StrictHostKeyChecking=no\' -o \'UserKnownHostsFile=/dev/null\' \'ljishen@sm1\'"\nand check to make sure that only the key(s) you wanted were added.\n\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:367 - add_hosts() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add sm1 10.10.2.5\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T09:37:59,032816776-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T09:37:59,036445818-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'host', 'add', 'sm1', '10.10.2.5']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add sm1 10.10.2.5\n'
10.10.2.1: b'Inferring fsid 2ccebeea-380d-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/2ccebeea-380d-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b"Added host 'sm1' with addr '10.10.2.5'\n"
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T09:38:09,888876825-07:00] INFO: Sleep for 20 seconds\x1b[0m\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T09:38:29,893962216-07:00] STAGE: Add OSDs to the cluster...\x1b[0m\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T09:38:29,900701133-07:00] INFO: > Adding devices ['/dev/nvme0n1'] on host 'sm1' as OSDs\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:388 - add_host_devices_as_osds() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd sm1:/dev/nvme0n1\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T09:38:29,910466260-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T09:38:29,913987779-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'daemon', 'add', 'osd', 'sm1:/dev/nvme0n1']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd sm1:/dev/nvme0n1\n'
10.10.2.1: b'Inferring fsid 2ccebeea-380d-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/2ccebeea-380d-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b"Created osd(s) 0 on host 'sm1'\n"
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T09:38:54,320428098-07:00] INFO: > Sleep for 20 seconds\x1b[0m\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T09:39:14,325952303-07:00] STAGE: Show Cluster Status\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:408 - show_status() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T09:39:14,336415521-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T09:39:14,340791046-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', '--status']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status\n'
10.10.2.1: b'Inferring fsid 2ccebeea-380d-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/2ccebeea-380d-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b'  cluster:\n    id:     2ccebeea-380d-11ec-b51d-53e6e728d2d3\n    health: HEALTH_WARN\n            1 pool(s) have no replicas configured\n \n  services:\n    mon: 1 daemons, quorum node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us (age 2m)\n    mgr: node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us.kyipce(active, since 2m)\n    osd: 1 osds: 1 up (since 20s), 1 in (since 40s)\n \n  data:\n    pools:   1 pools, 1 pgs\n    objects: 0 objects, 0 B\n    usage:   4.7 MiB used, 100 GiB / 100 GiB avail\n    pgs:     1 active+clean\n \n'
[1] 09:39:22 [SUCCESS] ljishen@10.10.2.1

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:35:59,984205585-07:00] INFO: > Remove existing clusters[0m
## bash:17 -  > basename -- /var/lib/ceph/e94548ca-380b-11ec-b51d-53e6e728d2d3
# bash:17 -  > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm --host 10.10.2.1 --host 10.10.2.5 rm-cluster --force --fsid e94548ca-380b-11ec-b51d-53e6e728d2d3
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:35:59,994771256-07:00] DEBUG: HOST_PARAMS: ['--host', '10.10.2.1', '--host', '10.10.2.5'][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:35:59,998479076-07:00] DEBUG: CEPHADM_PARAMS: ['rm-cluster', '--force', '--fsid', 'e94548ca-380b-11ec-b51d-53e6e728d2d3'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:63 -  > /tmp/bench-rados/ceph-research/scripts/parallel-ssh --host 10.10.2.1 --host 10.10.2.5 'sudo python3 -u - rm-cluster --force --fsid e94548ca-380b-11ec-b51d-53e6e728d2d3'
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:36:00,007098298-07:00] DEBUG: command from stdin[0m
# /tmp/bench-rados/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host 10.10.2.1 --host 10.10.2.5 'sudo python3 -u - rm-cluster --force --fsid e94548ca-380b-11ec-b51d-53e6e728d2d3'
[1] 09:36:06 [SUCCESS] 10.10.2.1
[2] 09:36:11 [SUCCESS] 10.10.2.5

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:36:11,955119446-07:00] INFO: > Deploy a new cluster[0m
# bash:24 -  > /tmp/bench-rados/ceph-research/scripts/ceph-deploy -m 10.10.2.1 -o sm1/10.10.2.5:/dev/nvme0n1


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:36:11,966978728-07:00] STAGE: Verify passwordless SSH to hosts, passwordless sudo via ljishen, and the existence of devices on hosts...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:36:11,971802335-07:00] INFO: > Check the host for the monitor[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:181 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@10.10.2.1 sudo --non-interactive --validate

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:36:12,122942851-07:00] INFO: > Check hosts for OSDs[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:36:12,127675156-07:00] INFO: >> Check host 'sm1'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:200 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 sudo --non-interactive --validate

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:36:13,215137678-07:00] INFO: >>> Check device '/dev/nvme0n1'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:215 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 test -b /dev/nvme0n1
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:226 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 lsblk --nodeps --noheadings --output TYPE /dev/nvme0n1
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:227 - verify_parameters() > grep --word-regexp disk


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:36:15,374253495-07:00] STAGE: Reset devices on hosts...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:36:15,379233346-07:00] INFO: > Resetting device '/dev/nvme0n1' on host 'sm1'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:288 - reset_host_devices() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 sudo sh
  Removing ceph--06f29e38--2516--4798--8e59--a792dbaea07d-osd--block--1be5fb76--406e--4eb0--b891--e7309cf53561 (252:0)
  Archiving volume group "ceph-06f29e38-2516-4798-8e59-a792dbaea07d" metadata (seqno 5).
  Releasing logical volume "osd-block-1be5fb76-406e-4eb0-b891-e7309cf53561"
  Creating volume group backup "/etc/lvm/backup/ceph-06f29e38-2516-4798-8e59-a792dbaea07d" (seqno 6).
  Logical volume "osd-block-1be5fb76-406e-4eb0-b891-e7309cf53561" successfully removed
  Removing physical volume "/dev/nvme0n1" from volume group "ceph-06f29e38-2516-4798-8e59-a792dbaea07d"
  Volume group "ceph-06f29e38-2516-4798-8e59-a792dbaea07d" successfully removed


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:36:17,683939124-07:00] STAGE: Bootstrap a new cluster...[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:317 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.1 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:36:17,693467615-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:36:17,697146922-07:00] DEBUG: CEPHADM_PARAMS: ['bootstrap', '--config', '/tmp/bench-rados/ceph-research/scripts/ceph.conf', '--mon-ip', '10.10.2.1', '--output-dir', '/tmp/bench-rados/deployment_data_root/etc/ceph', '--ssh-user', 'ljishen', '--skip-dashboard', '--no-minimize-config', '--allow-overwrite', '--allow-fqdn-hostname', '--orphan-initial-daemons', '--skip-monitoring-stack', '--single-host-defaults'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.1 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults
Verifying podman|docker is present...
Verifying lvm2 is present...
Verifying time synchronization is in place...
Unit ntp.service is enabled and running
Repeating the final host check...
podman|docker (/usr/bin/docker) is present
systemctl is present
lvcreate is present
Unit ntp.service is enabled and running
Host looks OK
Cluster fsid: 2ccebeea-380d-11ec-b51d-53e6e728d2d3
Verifying IP 10.10.2.1 port 3300 ...
Verifying IP 10.10.2.1 port 6789 ...
Mon IP `10.10.2.1` is in CIDR network `10.10.2.0/24`
- internal network (--cluster-network) has not been provided, OSD replication will default to the public_network
Adjusting default settings to suit single-host cluster...
Pulling container image quay.io/ceph/ceph:v16...
Ceph version: ceph version 16.2.6 (ee28fb57e47e9f88813e24bbf4c14496ca299d31) pacific (stable)
Extracting ceph user uid/gid from container image...
Creating initial keys...
Creating initial monmap...
Creating mon...
Waiting for mon to start...
Waiting for mon...
mon is available
Setting mon public_network to 10.10.2.0/24
Wrote config to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf
Wrote keyring to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring
Creating mgr...
Verifying port 9283 ...
Waiting for mgr to start...
Waiting for mgr...
mgr not available, waiting (1/15)...
mgr not available, waiting (2/15)...
mgr is available
Enabling cephadm module...
Waiting for the mgr to restart...
Waiting for mgr epoch 5...
mgr epoch 5 is available
Setting orchestrator backend to cephadm...
Generating ssh key...
Wrote public SSH key to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub
Adding key to ljishen@localhost authorized_keys...
Adding host node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us...
Deploying unmanaged mon service...
Deploying unmanaged mgr service...
You can access the Ceph CLI with:

	sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --fsid 2ccebeea-380d-11ec-b51d-53e6e728d2d3 -c /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf -k /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring

Please consider enabling telemetry to help improve Ceph:

	ceph telemetry on

For more information see:

	https://docs.ceph.com/docs/pacific/mgr/telemetry/

Bootstrap complete.

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:37:19,383574717-07:00] INFO: Sleep for 20 seconds[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:37:39,390173911-07:00] INFO: > Set the default number of monitor daemons to 1[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:337 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:37:39,400805014-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:37:39,404596271-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mon', '1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1
Inferring fsid 2ccebeea-380d-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/2ccebeea-380d-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Scheduled mon update...

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:37:48,555873695-07:00] INFO: > Set the default number of manager daemons to 1[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:342 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:37:48,565710556-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:37:48,569787830-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mgr', '1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1
Inferring fsid 2ccebeea-380d-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/2ccebeea-380d-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Scheduled mgr update...


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:37:57,880815095-07:00] STAGE: Add hosts to the cluster...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:37:57,887386928-07:00] INFO: > Adding host 'sm1'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:361 - add_hosts() > ssh-copy-id -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -f -i /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub ljishen@sm1
/usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: "/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub"

Number of key(s) added: 1

Now try logging into the machine, with:   "ssh -o 'GlobalKnownHostsFile=/dev/null' -o 'LogLevel=ERROR' -o 'PasswordAuthentication=no' -o 'StrictHostKeyChecking=no' -o 'UserKnownHostsFile=/dev/null' 'ljishen@sm1'"
and check to make sure that only the key(s) you wanted were added.

# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:367 - add_hosts() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add sm1 10.10.2.5
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:37:59,032816776-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:37:59,036445818-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'host', 'add', 'sm1', '10.10.2.5'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add sm1 10.10.2.5
Inferring fsid 2ccebeea-380d-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/2ccebeea-380d-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Added host 'sm1' with addr '10.10.2.5'

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:38:09,888876825-07:00] INFO: Sleep for 20 seconds[0m


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:38:29,893962216-07:00] STAGE: Add OSDs to the cluster...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:38:29,900701133-07:00] INFO: > Adding devices ['/dev/nvme0n1'] on host 'sm1' as OSDs[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:388 - add_host_devices_as_osds() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd sm1:/dev/nvme0n1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:38:29,910466260-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:38:29,913987779-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'daemon', 'add', 'osd', 'sm1:/dev/nvme0n1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd sm1:/dev/nvme0n1
Inferring fsid 2ccebeea-380d-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/2ccebeea-380d-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Created osd(s) 0 on host 'sm1'

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:38:54,320428098-07:00] INFO: > Sleep for 20 seconds[0m


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:39:14,325952303-07:00] STAGE: Show Cluster Status[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:408 - show_status() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:39:14,336415521-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:39:14,340791046-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', '--status'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status
Inferring fsid 2ccebeea-380d-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/2ccebeea-380d-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
  cluster:
    id:     2ccebeea-380d-11ec-b51d-53e6e728d2d3
    health: HEALTH_WARN
            1 pool(s) have no replicas configured
 
  services:
    mon: 1 daemons, quorum node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us (age 2m)
    mgr: node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us.kyipce(active, since 2m)
    osd: 1 osds: 1 up (since 20s), 1 in (since 40s)
 
  data:
    pools:   1 pools, 1 pgs
    objects: 0 objects, 0 B
    usage:   4.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     1 active+clean
 

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:39:22,933178538-07:00] INFO: > Copy configuration files to local[0m
# ./benchmarks/bench-rados:220 - launch_ceph_cluster() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.1 sudo chown --changes --recursive ljishen /tmp/bench-rados/deployment_data_root/etc/ceph
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:39:22,941231924-07:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.1 sudo chown --changes --recursive ljishen /tmp/bench-rados/deployment_data_root/etc/ceph
[1] 09:39:23 [SUCCESS] ljishen@10.10.2.1
# ./benchmarks/bench-rados:223 - launch_ceph_cluster() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.1:/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf
# ./benchmarks/bench-rados:226 - launch_ceph_cluster() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.1:/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:39:23,417372951-07:00] STAGE: Update local ceph.conf as a client...[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:39:23,421243755-07:00] STAGE: Configure Ceph pool...[0m
# ./benchmarks/bench-rados:257 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_max_backfills 32
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:39:23,443905399-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:39:23,446914749-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '2ccebeea-380d-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'config', 'set', 'osd', 'osd_max_backfills', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 2ccebeea-380d-11ec-b51d-53e6e728d2d3 -- ceph config set osd osd_max_backfills 32
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:258 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_active 32
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:39:27,274468822-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:39:27,277638966-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '2ccebeea-380d-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_active', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 2ccebeea-380d-11ec-b51d-53e6e728d2d3 -- ceph config set osd osd_recovery_max_active 32
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:259 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_single_start 8
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:39:31,208818388-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:39:31,212144085-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '2ccebeea-380d-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_single_start', '8'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 2ccebeea-380d-11ec-b51d-53e6e728d2d3 -- ceph config set osd osd_recovery_max_single_start 8
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:260 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_op_priority 63
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:39:34,909814229-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:39:34,912784526-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '2ccebeea-380d-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_op_priority', '63'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 2ccebeea-380d-11ec-b51d-53e6e728d2d3 -- ceph config set osd osd_recovery_op_priority 63
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:262 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config show-with-defaults osd.0
# ./benchmarks/bench-rados:263 - configure_ceph_pool() > grep 'osd_max_backfills\|osd_recovery'
# ./benchmarks/bench-rados:263 - configure_ceph_pool() > column -t -s ' '
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
osd_max_backfills                        32         mon
osd_recovery_cost                        20971520   default
osd_recovery_delay_start                 0.000000   default
osd_recovery_max_active                  32         mon
osd_recovery_max_active_hdd              3          default
osd_recovery_max_active_ssd              10         default
osd_recovery_max_chunk                   8388608    default
osd_recovery_max_omap_entries_per_chunk  8096       default
osd_recovery_max_single_start            8          mon
osd_recovery_op_priority                 3          default
osd_recovery_op_warn_multiple            16         default
osd_recovery_priority                    5          default
osd_recovery_retry_interval              30.000000  default
osd_recovery_sleep                       0.000000   default
osd_recovery_sleep_hdd                   0.100000   default
osd_recovery_sleep_hybrid                0.025000   default
osd_recovery_sleep_ssd                   0.000000   default
# ./benchmarks/bench-rados:265 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool create bench_rados 128 128 replicated --size 1 --pg-num-min 128
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:39:42,724852375-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:39:42,727825408-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '2ccebeea-380d-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'create', 'bench_rados', '128', '128', 'replicated', '--size', '1', '--pg-num-min', '128'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 2ccebeea-380d-11ec-b51d-53e6e728d2d3 -- ceph osd pool create bench_rados 128 128 replicated --size 1 --pg-num-min 128
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
pool 'bench_rados' created
# ./benchmarks/bench-rados:267 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados min_size 1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:39:46,841591914-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:39:46,844763240-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '2ccebeea-380d-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'min_size', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 2ccebeea-380d-11ec-b51d-53e6e728d2d3 -- ceph osd pool set bench_rados min_size 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 min_size to 1
# ./benchmarks/bench-rados:268 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados pg_autoscale_mode off
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:39:51,731048931-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:39:51,733988731-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '2ccebeea-380d-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'pg_autoscale_mode', 'off'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 2ccebeea-380d-11ec-b51d-53e6e728d2d3 -- ceph osd pool set bench_rados pg_autoscale_mode off
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 pg_autoscale_mode to off
# ./benchmarks/bench-rados:269 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool application enable bench_rados benchmark
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:39:56,638835339-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:39:56,641918128-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '2ccebeea-380d-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'application', 'enable', 'bench_rados', 'benchmark'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 2ccebeea-380d-11ec-b51d-53e6e728d2d3 -- ceph osd pool application enable bench_rados benchmark
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
enabled application 'benchmark' on pool 'bench_rados'
# ./benchmarks/bench-rados:270 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados noscrub 1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:40:01,054548347-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:40:01,057738207-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '2ccebeea-380d-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'noscrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 2ccebeea-380d-11ec-b51d-53e6e728d2d3 -- ceph osd pool set bench_rados noscrub 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 noscrub to 1
# ./benchmarks/bench-rados:271 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados nodeep-scrub 1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:40:06,304972633-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:40:06,308087814-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '2ccebeea-380d-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'nodeep-scrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 2ccebeea-380d-11ec-b51d-53e6e728d2d3 -- ceph osd pool set bench_rados nodeep-scrub 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 nodeep-scrub to 1
# ./benchmarks/bench-rados:272 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool ls detail
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:40:10,661794178-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:40:10,664850357-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '2ccebeea-380d-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'ls', 'detail'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 2ccebeea-380d-11ec-b51d-53e6e728d2d3 -- ceph osd pool ls detail
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
pool 1 'device_health_metrics' replicated size 1 min_size 1 crush_rule 0 object_hash rjenkins pg_num 64 pgp_num 64 autoscale_mode on last_change 18 lfor 0/0/16 flags hashpspool stripe_width 0 pg_num_min 1 application mgr_devicehealth
pool 2 'bench_rados' replicated size 1 min_size 1 crush_rule 0 object_hash rjenkins pg_num 128 pgp_num 128 autoscale_mode off last_change 19 flags hashpspool,noscrub,nodeep-scrub stripe_width 0 pg_num_min 128 application benchmark

# ./benchmarks/bench-rados:273 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd df tree
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:40:14,462745715-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:40:14,466009946-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '2ccebeea-380d-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'df', 'tree'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 2ccebeea-380d-11ec-b51d-53e6e728d2d3 -- ceph osd df tree
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
ID  CLASS  WEIGHT   REWEIGHT  SIZE     RAW USE  DATA     OMAP  META     AVAIL    %USE  VAR   PGS  STATUS  TYPE NAME    
-1         0.09769         -  100 GiB  5.7 MiB  160 KiB   0 B  5.6 MiB  100 GiB  0.01  1.00    -          root default 
-3         0.09769         -  100 GiB  5.7 MiB  160 KiB   0 B  5.6 MiB  100 GiB  0.01  1.00    -              host sm1 
 0    ssd  0.09769   1.00000  100 GiB  5.7 MiB  160 KiB   0 B  5.6 MiB  100 GiB  0.01  1.00  192      up          osd.0
                       TOTAL  100 GiB  5.7 MiB  160 KiB   0 B  5.6 MiB  100 GiB  0.01                                  
MIN/MAX VAR: 1.00/1.00  STDDEV: 0


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:40:18,347649842-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:40:42,112852411-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:40:50,895781217-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:40:59,741780213-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:41:08,710134083-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:41:08,718433081-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:41:17,581188065-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:41:17,589456065-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:41:26,605680264-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:41:26,613931973-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:41:35,612957816-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:41:35,620959524-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:41:44,527524857-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:41:44,535577201-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:41:44,542148063-07:00] INFO: > The cluster is idle now.[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:41:44,546020951-07:00] STAGE: Start benchmarking of mode write ...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:41:44,553290419-07:00] INFO: > Start system activity collection services[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:41:44,559745913-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:341 - rados_bench() > rm --force /mnt/sda8/ceph-research/scripts/output/sys_activity_16MB_write.dat.rados_bench_host.1
# ./benchmarks/bench-rados:342 - rados_bench() > sar_pid_rados_bench_host=503673
# ./benchmarks/bench-rados:342 - rados_bench() > S_TIME_FORMAT=ISO
# ./benchmarks/bench-rados:342 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/output/sys_activity_16MB_write.dat.rados_bench_host.1 2

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:41:44,567348239-07:00] INFO: >> for OSD host[0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:41:44,576575096-07:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.5 bash
10.10.2.5: b'184953\n'
[1] 09:41:45 [SUCCESS] ljishen@10.10.2.5
184953

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:41:45,684520994-07:00] INFO: > Run rados bench[0m
# ./benchmarks/bench-rados:370 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/output/rados_bench_16MB_write.log.1
## ./benchmarks/bench-rados:361 - rados_bench() > count_cpus 32-63,96-127
# ./benchmarks/bench-rados:361 - rados_bench() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 60 write --pool bench_rados -b 16777216 -O 16777216 --concurrent-ios 128 --show-time --write-object --write-omap --write-xattr --no-cleanup
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:41:45,705254052-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:41:45,708120594-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '2ccebeea-380d-11ec-b51d-53e6e728d2d3', '--', 'rados', 'bench', '60', 'write', '--pool', 'bench_rados', '-b', '16777216', '-O', '16777216', '--concurrent-ios', '128', '--show-time', '--write-object', '--write-omap', '--write-xattr', '--no-cleanup'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 2ccebeea-380d-11ec-b51d-53e6e728d2d3 -- rados bench 60 write --pool bench_rados -b 16777216 -O 16777216 --concurrent-ios 128 --show-time --write-object --write-omap --write-xattr --no-cleanup
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
hints = 1
2021-10-28T16:41:48.595090+0000 Maintaining 128 concurrent writes of 16777216 bytes to objects of size 16777216 for up to 60 seconds or 0 objects
2021-10-28T16:41:48.595128+0000 Object prefix: benchmark_data_node-2.bluefield2.ucsc-cmps10_7
2021-10-28T16:41:49.266214+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-28T16:41:49.266214+0000     0       0         0         0         0         0           -           0
2021-10-28T16:41:50.266351+0000     1       4         4         0         0         0           -           0
2021-10-28T16:41:51.266427+0000     2       9         9         0         0         0           -           0
2021-10-28T16:41:52.266508+0000     3      14        14         0         0         0           -           0
2021-10-28T16:41:53.266584+0000     4      18        18         0         0         0           -           0
2021-10-28T16:41:54.266659+0000     5      23        23         0         0         0           -           0
2021-10-28T16:41:55.266735+0000     6      27        27         0         0         0           -           0
2021-10-28T16:41:56.266806+0000     7      32        32         0         0         0           -           0
2021-10-28T16:41:57.266879+0000     8      35        35         0         0         0           -           0
2021-10-28T16:41:58.266958+0000     9      37        37         0         0         0           -           0
2021-10-28T16:41:59.267037+0000    10      39        39         0         0         0           -           0
2021-10-28T16:42:00.267111+0000    11      41        41         0         0         0           -           0
2021-10-28T16:42:01.267190+0000    12      43        43         0         0         0           -           0
2021-10-28T16:42:02.267271+0000    13      46        46         0         0         0           -           0
2021-10-28T16:42:03.267387+0000    14      51        51         0         0         0           -           0
2021-10-28T16:42:04.267500+0000    15      55        55         0         0         0           -           0
2021-10-28T16:42:05.267571+0000    16      60        60         0         0         0           -           0
2021-10-28T16:42:06.267673+0000    17      64        64         0         0         0           -           0
2021-10-28T16:42:07.267785+0000    18      67        67         0         0         0           -           0
2021-10-28T16:42:08.267898+0000    19      69        69         0         0         0           -           0
2021-10-28T16:42:09.267969+0000 min lat: 9999 max lat: 0 avg lat: 0
2021-10-28T16:42:09.267969+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-28T16:42:09.267969+0000    20      70        70         0         0         0           -           0
2021-10-28T16:42:10.268097+0000    21      71        71         0         0         0           -           0
2021-10-28T16:42:11.268210+0000    22      72        72         0         0         0           -           0
2021-10-28T16:42:12.268318+0000    23      73        73         0         0         0           -           0
2021-10-28T16:42:13.268388+0000    24      76        76         0         0         0           -           0
2021-10-28T16:42:14.268501+0000    25      78        78         0         0         0           -           0
2021-10-28T16:42:15.268614+0000    26      81        81         0         0         0           -           0
2021-10-28T16:42:16.268723+0000    27      84        84         0         0         0           -           0
2021-10-28T16:42:17.268790+0000    28      86        86         0         0         0           -           0
2021-10-28T16:42:18.268901+0000    29      89        89         0         0         0           -           0
2021-10-28T16:42:19.269013+0000    30      92        92         0         0         0           -           0
2021-10-28T16:42:20.269124+0000    31      95        95         0         0         0           -           0
2021-10-28T16:42:21.269195+0000    32      98        98         0         0         0           -           0
2021-10-28T16:42:22.269314+0000    33      99        99         0         0         0           -           0
2021-10-28T16:42:23.269430+0000    34     100       100         0         0         0           -           0
2021-10-28T16:42:24.269543+0000    35     101       101         0         0         0           -           0
2021-10-28T16:42:25.269615+0000    36     102       102         0         0         0           -           0
2021-10-28T16:42:26.269726+0000    37     103       103         0         0         0           -           0
2021-10-28T16:42:27.269840+0000    38     106       106         0         0         0           -           0
2021-10-28T16:42:28.269953+0000    39     109       109         0         0         0           -           0
2021-10-28T16:42:29.270024+0000 min lat: 9999 max lat: 0 avg lat: 0
2021-10-28T16:42:29.270024+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-28T16:42:29.270024+0000    40     114       114         0         0         0           -           0
2021-10-28T16:42:30.270145+0000    41     119       119         0         0         0           -           0
2021-10-28T16:42:31.270260+0000    42     123       123         0         0         0           -           0
2021-10-28T16:42:32.270377+0000    43     126       126         0         0         0           -           0
2021-10-28T16:42:33.270447+0000    44     127       129         2  0.727204  0.727273     43.9075     43.6871
2021-10-28T16:42:34.270570+0000    45     127       130         3   1.06657        16     44.2019     43.8587
2021-10-28T16:42:35.270686+0000    46     127       131         4   1.39117        16     44.7267     44.0757
2021-10-28T16:42:36.270804+0000    47     127       132         5   1.70196        16     45.4766     44.3559
2021-10-28T16:42:37.270881+0000    48     127       133         6   1.99981        16     46.1764     44.6593
2021-10-28T16:42:38.270995+0000    49     127       134         7   2.28549        16      46.969     44.9892
2021-10-28T16:42:39.271109+0000    50     127       137        10   3.19969        48     47.9114     45.8107
2021-10-28T16:42:40.271229+0000    51     127       139        12   3.76434        32     48.3108     46.2145
2021-10-28T16:42:41.271308+0000    52     127       143        16    4.9226        64     48.8191     46.8242
2021-10-28T16:42:42.271376+0000    53     127       145        18   5.43344        32     49.0537      47.066
2021-10-28T16:42:43.271468+0000    54     127       148        21   6.22163        48     49.4481     47.3874
2021-10-28T16:42:44.271579+0000    55     127       151        24   6.98115        48     49.8454     47.6767
2021-10-28T16:42:45.271655+0000    56     127       154        27   7.71355        48     50.2746       47.95
2021-10-28T16:42:46.271771+0000    57     127       156        29   8.13957        32     50.5615     48.1262
2021-10-28T16:42:47.271889+0000    58     127       158        31    8.5509        32     50.9238     48.3013
2021-10-28T16:42:48.272001+0000    59     127       161        34   9.21945        48     51.7703     48.5957
2021-10-28T16:42:49.272073+0000 min lat: 43.4667 max lat: 51.881 avg lat: 48.6896
2021-10-28T16:42:49.272073+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-28T16:42:49.272073+0000    60     127       162        35   9.33243        16      51.881     48.6896
2021-10-28T16:42:50.272195+0000    61       2       163       161   42.2254      2016     1.60481     32.1499
2021-10-28T16:42:51.272306+0000    62       1       163       162   41.8024        16     2.27907     31.9655
2021-10-28T16:42:52.272440+0000 Total time run:         62.2094
Total writes made:      163
Write size:             16777216
Object size:            16777216
Bandwidth (MB/sec):     41.9229
Stddev Bandwidth:       255.433
Max bandwidth (MB/sec): 2016
Min bandwidth (MB/sec): 0
Average IOPS:           2
Stddev IOPS:            15.9647
Max IOPS:               126
Min IOPS:               0
Average Latency(s):     31.7876
Stddev Latency(s):      16.1896
Max latency(s):         52.3721
Min latency(s):         1.60481

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:42:53,115274160-07:00] INFO: > Stop system activity collection services[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:42:53,121849861-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:391 - rados_bench() > kill -INT 503673

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:42:53,127912526-07:00] INFO: >> for OSD host[0m
# ./benchmarks/bench-rados:396 - rados_bench() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.5 kill -INT 184953
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:42:53,136270836-07:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.5 kill -INT 184953
[1] 09:42:54 [SUCCESS] ljishen@10.10.2.5

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:42:55,009864855-07:00] INFO: >>> Copy system activity file from OSD host[0m
## ./benchmarks/bench-rados:403 - rados_bench() > basename -- /tmp/bench-rados/sys_activity_16MB_write.dat.osd_host.1
# ./benchmarks/bench-rados:403 - rados_bench() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.5:/tmp/bench-rados/sys_activity_16MB_write.dat.osd_host.1 /mnt/sda8/ceph-research/scripts/output/sys_activity_16MB_write.dat.osd_host.1


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:42:56,411043225-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:43:20,251526004-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 164 objects, 2.5 GiB
    usage:   7.8 GiB used, 92 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:43:20,259724343-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:43:29,117182307-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 164 objects, 2.5 GiB
    usage:   7.8 GiB used, 92 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:43:29,125397778-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:43:38,023877715-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 164 objects, 2.5 GiB
    usage:   7.8 GiB used, 92 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:43:38,032035357-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:43:46,989294337-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 164 objects, 2.5 GiB
    usage:   7.8 GiB used, 92 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:43:46,997231614-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:43:55,871532962-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 164 objects, 2.5 GiB
    usage:   7.8 GiB used, 92 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:43:55,879863900-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:43:55,886304797-07:00] INFO: > The cluster is idle now.[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:43:55,890383293-07:00] STAGE: Start benchmarking of mode seq ...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:43:55,897854933-07:00] INFO: > Start system activity collection services[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:43:55,904374438-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:341 - rados_bench() > rm --force /mnt/sda8/ceph-research/scripts/output/sys_activity_16MB_seq.dat.rados_bench_host.1
# ./benchmarks/bench-rados:342 - rados_bench() > sar_pid_rados_bench_host=505027
# ./benchmarks/bench-rados:342 - rados_bench() > S_TIME_FORMAT=ISO
# ./benchmarks/bench-rados:342 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/output/sys_activity_16MB_seq.dat.rados_bench_host.1 2

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:43:55,912008073-07:00] INFO: >> for OSD host[0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:43:55,921160619-07:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.5 bash
10.10.2.5: b'185503\n'
[1] 09:43:57 [SUCCESS] ljishen@10.10.2.5
185503

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:43:57,053384859-07:00] INFO: > Run rados bench[0m
# ./benchmarks/bench-rados:379 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/output/rados_bench_16MB_seq.log.1
## ./benchmarks/bench-rados:376 - rados_bench() > count_cpus 32-63,96-127
# ./benchmarks/bench-rados:376 - rados_bench() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 128 --show-time
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:43:57,073736469-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:43:57,076509685-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '2ccebeea-380d-11ec-b51d-53e6e728d2d3', '--', 'rados', 'bench', '99999999', 'seq', '--pool', 'bench_rados', '--concurrent-ios', '128', '--show-time'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 2ccebeea-380d-11ec-b51d-53e6e728d2d3 -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 128 --show-time
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
hints = 1
2021-10-28T16:44:00.027207+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-28T16:44:00.027207+0000     0       0         0         0         0         0           -           0
2021-10-28T16:44:01.027380+0000     1      48        48         0         0         0           -           0
2021-10-28T16:44:02.027500+0000     2      95        95         0         0         0           -           0
2021-10-28T16:44:03.027574+0000     3     127       138        11   58.6571   58.6667     2.86124     2.85274
2021-10-28T16:44:04.027665+0000 Total time run:       3.78903
Total reads made:     163
Read size:            16777216
Object size:          16777216
Bandwidth (MB/sec):   688.302
Average IOPS:         43
Stddev IOPS:          1.73205
Max IOPS:             3
Min IOPS:             0
Average Latency(s):   1.83995
Max latency(s):       2.90799
Min latency(s):       0.287279

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:44:04,769642916-07:00] INFO: > Stop system activity collection services[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:44:04,776409517-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:391 - rados_bench() > kill -INT 505027

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:44:04,783014323-07:00] INFO: >> for OSD host[0m
# ./benchmarks/bench-rados:396 - rados_bench() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.5 kill -INT 185503
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:44:04,791401317-07:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.5 kill -INT 185503
[1] 09:44:05 [SUCCESS] ljishen@10.10.2.5

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:44:05,900451178-07:00] INFO: >>> Copy system activity file from OSD host[0m
## ./benchmarks/bench-rados:403 - rados_bench() > basename -- /tmp/bench-rados/sys_activity_16MB_seq.dat.osd_host.1
# ./benchmarks/bench-rados:403 - rados_bench() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.5:/tmp/bench-rados/sys_activity_16MB_seq.dat.osd_host.1 /mnt/sda8/ceph-research/scripts/output/sys_activity_16MB_seq.dat.osd_host.1


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:44:07,012965457-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:44:30,814034066-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 164 objects, 2.5 GiB
    usage:   7.8 GiB used, 92 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:44:30,822428294-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:44:39,688927571-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 164 objects, 2.5 GiB
    usage:   7.8 GiB used, 92 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:44:39,697664946-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:44:48,619448686-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 164 objects, 2.5 GiB
    usage:   7.8 GiB used, 92 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:44:48,628038732-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:44:57,485334959-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 164 objects, 2.5 GiB
    usage:   7.8 GiB used, 92 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:44:57,493691996-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:45:06,650680967-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 164 objects, 2.5 GiB
    usage:   7.8 GiB used, 92 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:45:06,659114769-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:45:06,665603997-07:00] INFO: > The cluster is idle now.[0m


[1;7;39;49m[2021-10-28T09:45:06,668197363-07:00][RUNNING][ROUND 2/7/21] object_size=16MB[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:45:06,672125045-07:00] STAGE: Launch a Ceph cluster (CEPH_HOST: 10.10.2.1, OSD_HOST: 10.10.2.5) ...[0m
# ./benchmarks/bench-rados:183 - launch_ceph_cluster() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.1 bash -euo pipefail
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:45:06,681870368-07:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.1 bash -euo pipefail
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T09:45:07,091226445-07:00] INFO: > Remove existing clusters\x1b[0m\n'
10.10.2.1: b'## bash:17 -  > basename -- /var/lib/ceph/2ccebeea-380d-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'# bash:17 -  > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm --host 10.10.2.1 --host 10.10.2.5 rm-cluster --force --fsid 2ccebeea-380d-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T09:45:07,101757711-07:00] DEBUG: HOST_PARAMS: ['--host', '10.10.2.1', '--host', '10.10.2.5']\x1b[0m\n"
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T09:45:07,105600495-07:00] DEBUG: CEPHADM_PARAMS: ['rm-cluster', '--force', '--fsid', '2ccebeea-380d-11ec-b51d-53e6e728d2d3']\x1b[0m\n"
10.10.2.1: b"# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:63 -  > /tmp/bench-rados/ceph-research/scripts/parallel-ssh --host 10.10.2.1 --host 10.10.2.5 'sudo python3 -u - rm-cluster --force --fsid 2ccebeea-380d-11ec-b51d-53e6e728d2d3'\n"
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T09:45:07,114950671-07:00] DEBUG: command from stdin\x1b[0m\n'
10.10.2.1: b"# /tmp/bench-rados/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host 10.10.2.1 --host 10.10.2.5 'sudo python3 -u - rm-cluster --force --fsid 2ccebeea-380d-11ec-b51d-53e6e728d2d3'\n"
10.10.2.1: b'[1] 09:45:12 [SUCCESS] 10.10.2.1\n[2] 09:45:18 [SUCCESS] 10.10.2.5\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T09:45:18,769037383-07:00] INFO: > Deploy a new cluster\x1b[0m\n'
10.10.2.1: b'# bash:24 -  > /tmp/bench-rados/ceph-research/scripts/ceph-deploy -m 10.10.2.1 -o sm1/10.10.2.5:/dev/nvme0n1\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T09:45:18,780723139-07:00] STAGE: Verify passwordless SSH to hosts, passwordless sudo via ljishen, and the existence of devices on hosts...\x1b[0m\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T09:45:18,785440807-07:00] INFO: > Check the host for the monitor\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:181 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@10.10.2.1 sudo --non-interactive --validate\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T09:45:18,934629867-07:00] INFO: > Check hosts for OSDs\x1b[0m\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T09:45:18,939504490-07:00] INFO: >> Check host 'sm1'\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:200 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 sudo --non-interactive --validate\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T09:45:20,019139841-07:00] INFO: >>> Check device '/dev/nvme0n1'\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:215 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 test -b /dev/nvme0n1\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:226 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 lsblk --nodeps --noheadings --output TYPE /dev/nvme0n1\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:227 - verify_parameters() > grep --word-regexp disk\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T09:45:22,149882068-07:00] STAGE: Reset devices on hosts...\x1b[0m\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T09:45:22,155385583-07:00] INFO: > Resetting device '/dev/nvme0n1' on host 'sm1'\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:288 - reset_host_devices() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 sudo sh\n'
10.10.2.1: b'  Removing ceph--7029ea8d--1eca--4a72--a792--848f49e0f818-osd--block--3df683c0--e433--419f--a1ab--f7d24d633aaf (252:0)\n'
10.10.2.1: b'  Archiving volume group "ceph-7029ea8d-1eca-4a72-a792-848f49e0f818" metadata (seqno 5).\n'
10.10.2.1: b'  Releasing logical volume "osd-block-3df683c0-e433-419f-a1ab-f7d24d633aaf"\n'
10.10.2.1: b'  Creating volume group backup "/etc/lvm/backup/ceph-7029ea8d-1eca-4a72-a792-848f49e0f818" (seqno 6).\n'
10.10.2.1: b'  Logical volume "osd-block-3df683c0-e433-419f-a1ab-f7d24d633aaf" successfully removed\n'
10.10.2.1: b'  Removing physical volume "/dev/nvme0n1" from volume group "ceph-7029ea8d-1eca-4a72-a792-848f49e0f818"\n'
10.10.2.1: b'  Volume group "ceph-7029ea8d-1eca-4a72-a792-848f49e0f818" successfully removed\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T09:45:24,464377355-07:00] STAGE: Bootstrap a new cluster...\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:317 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.1 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T09:45:24,473939269-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T09:45:24,477599139-07:00] DEBUG: CEPHADM_PARAMS: ['bootstrap', '--config', '/tmp/bench-rados/ceph-research/scripts/ceph.conf', '--mon-ip', '10.10.2.1', '--output-dir', '/tmp/bench-rados/deployment_data_root/etc/ceph', '--ssh-user', 'ljishen', '--skip-dashboard', '--no-minimize-config', '--allow-overwrite', '--allow-fqdn-hostname', '--orphan-initial-daemons', '--skip-monitoring-stack', '--single-host-defaults']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.1 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults\n'
10.10.2.1: b'Verifying podman|docker is present...\nVerifying lvm2 is present...\n'
10.10.2.1: b'Verifying time synchronization is in place...\n'
10.10.2.1: b'Unit ntp.service is enabled and running\n'
10.10.2.1: b'Repeating the final host check...\npodman|docker (/usr/bin/docker) is present\n'
10.10.2.1: b'systemctl is present\n'
10.10.2.1: b'lvcreate is present\n'
10.10.2.1: b'Unit ntp.service is enabled and running\n'
10.10.2.1: b'Host looks OK\n'
10.10.2.1: b'Cluster fsid: 72b70010-380e-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Verifying IP 10.10.2.1 port 3300 ...\n'
10.10.2.1: b'Verifying IP 10.10.2.1 port 6789 ...\n'
10.10.2.1: b'Mon IP `10.10.2.1` is in CIDR network `10.10.2.0/24`\n'
10.10.2.1: b'- internal network (--cluster-network) has not been provided, OSD replication will default to the public_network\n'
10.10.2.1: b'Adjusting default settings to suit single-host cluster...\n'
10.10.2.1: b'Pulling container image quay.io/ceph/ceph:v16...\n'
10.10.2.1: b'Ceph version: ceph version 16.2.6 (ee28fb57e47e9f88813e24bbf4c14496ca299d31) pacific (stable)\n'
10.10.2.1: b'Extracting ceph user uid/gid from container image...\n'
10.10.2.1: b'Creating initial keys...\n'
10.10.2.1: b'Creating initial monmap...\n'
10.10.2.1: b'Creating mon...\n'
10.10.2.1: b'Waiting for mon to start...\n'
10.10.2.1: b'Waiting for mon...\n'
10.10.2.1: b'mon is available\n'
10.10.2.1: b'Setting mon public_network to 10.10.2.0/24\n'
10.10.2.1: b'Wrote config to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf\n'
10.10.2.1: b'Wrote keyring to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring\n'
10.10.2.1: b'Creating mgr...\n'
10.10.2.1: b'Verifying port 9283 ...\n'
10.10.2.1: b'Waiting for mgr to start...\nWaiting for mgr...\n'
10.10.2.1: b'mgr not available, waiting (1/15)...\n'
10.10.2.1: b'mgr not available, waiting (2/15)...\n'
10.10.2.1: b'mgr is available\n'
10.10.2.1: b'Enabling cephadm module...\n'
10.10.2.1: b'Waiting for the mgr to restart...\n'
10.10.2.1: b'Waiting for mgr epoch 5...\n'
10.10.2.1: b'mgr epoch 5 is available\n'
10.10.2.1: b'Setting orchestrator backend to cephadm...\n'
10.10.2.1: b'Generating ssh key...\n'
10.10.2.1: b'Wrote public SSH key to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub\n'
10.10.2.1: b'Adding key to ljishen@localhost authorized_keys...\n'
10.10.2.1: b'Adding host node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us...\n'
10.10.2.1: b'Deploying unmanaged mon service...\n'
10.10.2.1: b'Deploying unmanaged mgr service...\n'
10.10.2.1: b'You can access the Ceph CLI with:\n\n\tsudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --fsid 72b70010-380e-11ec-b51d-53e6e728d2d3 -c /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf -k /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring\n\n'
10.10.2.1: b'Please consider enabling telemetry to help improve Ceph:\n\n\tceph telemetry on\n\nFor more information see:\n\n\thttps://docs.ceph.com/docs/pacific/mgr/telemetry/\n\nBootstrap complete.\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T09:46:27,615063658-07:00] INFO: Sleep for 20 seconds\x1b[0m\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T09:46:47,621970781-07:00] INFO: > Set the default number of monitor daemons to 1\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:337 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T09:46:47,632948927-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T09:46:47,636553192-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mon', '1']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1\n'
10.10.2.1: b'Inferring fsid 72b70010-380e-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/72b70010-380e-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b'Scheduled mon update...\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T09:46:56,984873013-07:00] INFO: > Set the default number of manager daemons to 1\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:342 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T09:46:56,994808800-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T09:46:56,998364454-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mgr', '1']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1\n'
10.10.2.1: b'Inferring fsid 72b70010-380e-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/72b70010-380e-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b'Scheduled mgr update...\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T09:47:06,326433420-07:00] STAGE: Add hosts to the cluster...\x1b[0m\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T09:47:06,332828501-07:00] INFO: > Adding host 'sm1'\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:361 - add_hosts() > ssh-copy-id -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -f -i /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub ljishen@sm1\n'
10.10.2.1: b'/usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: "/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub"\n'
10.10.2.1: b'\nNumber of key(s) added: 1\n\nNow try logging into the machine, with:   "ssh -o \'GlobalKnownHostsFile=/dev/null\' -o \'LogLevel=ERROR\' -o \'PasswordAuthentication=no\' -o \'StrictHostKeyChecking=no\' -o \'UserKnownHostsFile=/dev/null\' \'ljishen@sm1\'"\nand check to make sure that only the key(s) you wanted were added.\n\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:367 - add_hosts() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add sm1 10.10.2.5\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T09:47:07,485432498-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T09:47:07,489017447-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'host', 'add', 'sm1', '10.10.2.5']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add sm1 10.10.2.5\n'
10.10.2.1: b'Inferring fsid 72b70010-380e-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/72b70010-380e-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b"Added host 'sm1' with addr '10.10.2.5'\n"
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T09:47:18,482976826-07:00] INFO: Sleep for 20 seconds\x1b[0m\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T09:47:38,488005909-07:00] STAGE: Add OSDs to the cluster...\x1b[0m\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T09:47:38,494482283-07:00] INFO: > Adding devices ['/dev/nvme0n1'] on host 'sm1' as OSDs\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:388 - add_host_devices_as_osds() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd sm1:/dev/nvme0n1\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T09:47:38,504489004-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T09:47:38,508322960-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'daemon', 'add', 'osd', 'sm1:/dev/nvme0n1']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd sm1:/dev/nvme0n1\n'
10.10.2.1: b'Inferring fsid 72b70010-380e-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/72b70010-380e-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b"Created osd(s) 0 on host 'sm1'\n"
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T09:48:02,811358551-07:00] INFO: > Sleep for 20 seconds\x1b[0m\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T09:48:22,816455028-07:00] STAGE: Show Cluster Status\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:408 - show_status() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T09:48:22,826758105-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T09:48:22,830290635-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', '--status']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status\n'
10.10.2.1: b'Inferring fsid 72b70010-380e-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/72b70010-380e-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b'  cluster:\n    id:     72b70010-380e-11ec-b51d-53e6e728d2d3\n    health: HEALTH_WARN\n            1 pool(s) have no replicas configured\n \n  services:\n    mon: 1 daemons, quorum node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us (age 2m)\n    mgr: node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us.byfjvz(active, since 2m)\n    osd: 1 osds: 1 up (since 20s), 1 in (since 39s)\n \n  data:\n    pools:   1 pools, 1 pgs\n    objects: 0 objects, 0 B\n    usage:   4.7 MiB used, 100 GiB / 100 GiB avail\n    pgs:     1 active+clean\n \n'
[1] 09:48:31 [SUCCESS] ljishen@10.10.2.1

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:45:07,091226445-07:00] INFO: > Remove existing clusters[0m
## bash:17 -  > basename -- /var/lib/ceph/2ccebeea-380d-11ec-b51d-53e6e728d2d3
# bash:17 -  > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm --host 10.10.2.1 --host 10.10.2.5 rm-cluster --force --fsid 2ccebeea-380d-11ec-b51d-53e6e728d2d3
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:45:07,101757711-07:00] DEBUG: HOST_PARAMS: ['--host', '10.10.2.1', '--host', '10.10.2.5'][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:45:07,105600495-07:00] DEBUG: CEPHADM_PARAMS: ['rm-cluster', '--force', '--fsid', '2ccebeea-380d-11ec-b51d-53e6e728d2d3'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:63 -  > /tmp/bench-rados/ceph-research/scripts/parallel-ssh --host 10.10.2.1 --host 10.10.2.5 'sudo python3 -u - rm-cluster --force --fsid 2ccebeea-380d-11ec-b51d-53e6e728d2d3'
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:45:07,114950671-07:00] DEBUG: command from stdin[0m
# /tmp/bench-rados/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host 10.10.2.1 --host 10.10.2.5 'sudo python3 -u - rm-cluster --force --fsid 2ccebeea-380d-11ec-b51d-53e6e728d2d3'
[1] 09:45:12 [SUCCESS] 10.10.2.1
[2] 09:45:18 [SUCCESS] 10.10.2.5

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:45:18,769037383-07:00] INFO: > Deploy a new cluster[0m
# bash:24 -  > /tmp/bench-rados/ceph-research/scripts/ceph-deploy -m 10.10.2.1 -o sm1/10.10.2.5:/dev/nvme0n1


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:45:18,780723139-07:00] STAGE: Verify passwordless SSH to hosts, passwordless sudo via ljishen, and the existence of devices on hosts...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:45:18,785440807-07:00] INFO: > Check the host for the monitor[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:181 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@10.10.2.1 sudo --non-interactive --validate

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:45:18,934629867-07:00] INFO: > Check hosts for OSDs[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:45:18,939504490-07:00] INFO: >> Check host 'sm1'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:200 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 sudo --non-interactive --validate

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:45:20,019139841-07:00] INFO: >>> Check device '/dev/nvme0n1'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:215 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 test -b /dev/nvme0n1
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:226 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 lsblk --nodeps --noheadings --output TYPE /dev/nvme0n1
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:227 - verify_parameters() > grep --word-regexp disk


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:45:22,149882068-07:00] STAGE: Reset devices on hosts...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:45:22,155385583-07:00] INFO: > Resetting device '/dev/nvme0n1' on host 'sm1'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:288 - reset_host_devices() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 sudo sh
  Removing ceph--7029ea8d--1eca--4a72--a792--848f49e0f818-osd--block--3df683c0--e433--419f--a1ab--f7d24d633aaf (252:0)
  Archiving volume group "ceph-7029ea8d-1eca-4a72-a792-848f49e0f818" metadata (seqno 5).
  Releasing logical volume "osd-block-3df683c0-e433-419f-a1ab-f7d24d633aaf"
  Creating volume group backup "/etc/lvm/backup/ceph-7029ea8d-1eca-4a72-a792-848f49e0f818" (seqno 6).
  Logical volume "osd-block-3df683c0-e433-419f-a1ab-f7d24d633aaf" successfully removed
  Removing physical volume "/dev/nvme0n1" from volume group "ceph-7029ea8d-1eca-4a72-a792-848f49e0f818"
  Volume group "ceph-7029ea8d-1eca-4a72-a792-848f49e0f818" successfully removed


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:45:24,464377355-07:00] STAGE: Bootstrap a new cluster...[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:317 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.1 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:45:24,473939269-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:45:24,477599139-07:00] DEBUG: CEPHADM_PARAMS: ['bootstrap', '--config', '/tmp/bench-rados/ceph-research/scripts/ceph.conf', '--mon-ip', '10.10.2.1', '--output-dir', '/tmp/bench-rados/deployment_data_root/etc/ceph', '--ssh-user', 'ljishen', '--skip-dashboard', '--no-minimize-config', '--allow-overwrite', '--allow-fqdn-hostname', '--orphan-initial-daemons', '--skip-monitoring-stack', '--single-host-defaults'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.1 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults
Verifying podman|docker is present...
Verifying lvm2 is present...
Verifying time synchronization is in place...
Unit ntp.service is enabled and running
Repeating the final host check...
podman|docker (/usr/bin/docker) is present
systemctl is present
lvcreate is present
Unit ntp.service is enabled and running
Host looks OK
Cluster fsid: 72b70010-380e-11ec-b51d-53e6e728d2d3
Verifying IP 10.10.2.1 port 3300 ...
Verifying IP 10.10.2.1 port 6789 ...
Mon IP `10.10.2.1` is in CIDR network `10.10.2.0/24`
- internal network (--cluster-network) has not been provided, OSD replication will default to the public_network
Adjusting default settings to suit single-host cluster...
Pulling container image quay.io/ceph/ceph:v16...
Ceph version: ceph version 16.2.6 (ee28fb57e47e9f88813e24bbf4c14496ca299d31) pacific (stable)
Extracting ceph user uid/gid from container image...
Creating initial keys...
Creating initial monmap...
Creating mon...
Waiting for mon to start...
Waiting for mon...
mon is available
Setting mon public_network to 10.10.2.0/24
Wrote config to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf
Wrote keyring to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring
Creating mgr...
Verifying port 9283 ...
Waiting for mgr to start...
Waiting for mgr...
mgr not available, waiting (1/15)...
mgr not available, waiting (2/15)...
mgr is available
Enabling cephadm module...
Waiting for the mgr to restart...
Waiting for mgr epoch 5...
mgr epoch 5 is available
Setting orchestrator backend to cephadm...
Generating ssh key...
Wrote public SSH key to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub
Adding key to ljishen@localhost authorized_keys...
Adding host node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us...
Deploying unmanaged mon service...
Deploying unmanaged mgr service...
You can access the Ceph CLI with:

	sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --fsid 72b70010-380e-11ec-b51d-53e6e728d2d3 -c /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf -k /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring

Please consider enabling telemetry to help improve Ceph:

	ceph telemetry on

For more information see:

	https://docs.ceph.com/docs/pacific/mgr/telemetry/

Bootstrap complete.

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:46:27,615063658-07:00] INFO: Sleep for 20 seconds[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:46:47,621970781-07:00] INFO: > Set the default number of monitor daemons to 1[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:337 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:46:47,632948927-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:46:47,636553192-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mon', '1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1
Inferring fsid 72b70010-380e-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/72b70010-380e-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Scheduled mon update...

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:46:56,984873013-07:00] INFO: > Set the default number of manager daemons to 1[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:342 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:46:56,994808800-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:46:56,998364454-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mgr', '1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1
Inferring fsid 72b70010-380e-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/72b70010-380e-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Scheduled mgr update...


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:47:06,326433420-07:00] STAGE: Add hosts to the cluster...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:47:06,332828501-07:00] INFO: > Adding host 'sm1'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:361 - add_hosts() > ssh-copy-id -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -f -i /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub ljishen@sm1
/usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: "/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub"

Number of key(s) added: 1

Now try logging into the machine, with:   "ssh -o 'GlobalKnownHostsFile=/dev/null' -o 'LogLevel=ERROR' -o 'PasswordAuthentication=no' -o 'StrictHostKeyChecking=no' -o 'UserKnownHostsFile=/dev/null' 'ljishen@sm1'"
and check to make sure that only the key(s) you wanted were added.

# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:367 - add_hosts() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add sm1 10.10.2.5
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:47:07,485432498-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:47:07,489017447-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'host', 'add', 'sm1', '10.10.2.5'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add sm1 10.10.2.5
Inferring fsid 72b70010-380e-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/72b70010-380e-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Added host 'sm1' with addr '10.10.2.5'

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:47:18,482976826-07:00] INFO: Sleep for 20 seconds[0m


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:47:38,488005909-07:00] STAGE: Add OSDs to the cluster...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:47:38,494482283-07:00] INFO: > Adding devices ['/dev/nvme0n1'] on host 'sm1' as OSDs[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:388 - add_host_devices_as_osds() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd sm1:/dev/nvme0n1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:47:38,504489004-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:47:38,508322960-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'daemon', 'add', 'osd', 'sm1:/dev/nvme0n1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd sm1:/dev/nvme0n1
Inferring fsid 72b70010-380e-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/72b70010-380e-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Created osd(s) 0 on host 'sm1'

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:48:02,811358551-07:00] INFO: > Sleep for 20 seconds[0m


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:48:22,816455028-07:00] STAGE: Show Cluster Status[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:408 - show_status() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:48:22,826758105-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:48:22,830290635-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', '--status'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status
Inferring fsid 72b70010-380e-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/72b70010-380e-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
  cluster:
    id:     72b70010-380e-11ec-b51d-53e6e728d2d3
    health: HEALTH_WARN
            1 pool(s) have no replicas configured
 
  services:
    mon: 1 daemons, quorum node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us (age 2m)
    mgr: node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us.byfjvz(active, since 2m)
    osd: 1 osds: 1 up (since 20s), 1 in (since 39s)
 
  data:
    pools:   1 pools, 1 pgs
    objects: 0 objects, 0 B
    usage:   4.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     1 active+clean
 

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:48:31,573103870-07:00] INFO: > Copy configuration files to local[0m
# ./benchmarks/bench-rados:220 - launch_ceph_cluster() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.1 sudo chown --changes --recursive ljishen /tmp/bench-rados/deployment_data_root/etc/ceph
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:48:31,580928774-07:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.1 sudo chown --changes --recursive ljishen /tmp/bench-rados/deployment_data_root/etc/ceph
[1] 09:48:31 [SUCCESS] ljishen@10.10.2.1
# ./benchmarks/bench-rados:223 - launch_ceph_cluster() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.1:/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf
# ./benchmarks/bench-rados:226 - launch_ceph_cluster() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.1:/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:48:32,057004834-07:00] STAGE: Update local ceph.conf as a client...[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:48:32,060747857-07:00] STAGE: Configure Ceph pool...[0m
# ./benchmarks/bench-rados:257 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_max_backfills 32
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:48:32,083030315-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:48:32,085999349-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '72b70010-380e-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'config', 'set', 'osd', 'osd_max_backfills', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 72b70010-380e-11ec-b51d-53e6e728d2d3 -- ceph config set osd osd_max_backfills 32
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:258 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_active 32
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:48:35,891435176-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:48:35,894408168-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '72b70010-380e-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_active', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 72b70010-380e-11ec-b51d-53e6e728d2d3 -- ceph config set osd osd_recovery_max_active 32
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:259 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_single_start 8
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:48:39,992700678-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:48:39,995920185-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '72b70010-380e-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_single_start', '8'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 72b70010-380e-11ec-b51d-53e6e728d2d3 -- ceph config set osd osd_recovery_max_single_start 8
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:260 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_op_priority 63
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:48:43,999069331-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:48:44,002042363-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '72b70010-380e-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_op_priority', '63'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 72b70010-380e-11ec-b51d-53e6e728d2d3 -- ceph config set osd osd_recovery_op_priority 63
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:262 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config show-with-defaults osd.0
# ./benchmarks/bench-rados:263 - configure_ceph_pool() > grep 'osd_max_backfills\|osd_recovery'
# ./benchmarks/bench-rados:263 - configure_ceph_pool() > column -t -s ' '
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
osd_max_backfills                        32         mon
osd_recovery_cost                        20971520   default
osd_recovery_delay_start                 0.000000   default
osd_recovery_max_active                  32         mon
osd_recovery_max_active_hdd              3          default
osd_recovery_max_active_ssd              10         default
osd_recovery_max_chunk                   8388608    default
osd_recovery_max_omap_entries_per_chunk  8096       default
osd_recovery_max_single_start            8          mon
osd_recovery_op_priority                 3          default
osd_recovery_op_warn_multiple            16         default
osd_recovery_priority                    5          default
osd_recovery_retry_interval              30.000000  default
osd_recovery_sleep                       0.000000   default
osd_recovery_sleep_hdd                   0.100000   default
osd_recovery_sleep_hybrid                0.025000   default
osd_recovery_sleep_ssd                   0.000000   default
# ./benchmarks/bench-rados:265 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool create bench_rados 128 128 replicated --size 1 --pg-num-min 128
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:48:51,934960526-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:48:51,938003771-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '72b70010-380e-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'create', 'bench_rados', '128', '128', 'replicated', '--size', '1', '--pg-num-min', '128'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 72b70010-380e-11ec-b51d-53e6e728d2d3 -- ceph osd pool create bench_rados 128 128 replicated --size 1 --pg-num-min 128
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
pool 'bench_rados' created
# ./benchmarks/bench-rados:267 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados min_size 1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:48:56,358457804-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:48:56,361389709-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '72b70010-380e-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'min_size', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 72b70010-380e-11ec-b51d-53e6e728d2d3 -- ceph osd pool set bench_rados min_size 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 min_size to 1
# ./benchmarks/bench-rados:268 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados pg_autoscale_mode off
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:49:00,542079765-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:49:00,545136585-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '72b70010-380e-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'pg_autoscale_mode', 'off'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 72b70010-380e-11ec-b51d-53e6e728d2d3 -- ceph osd pool set bench_rados pg_autoscale_mode off
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 pg_autoscale_mode to off
# ./benchmarks/bench-rados:269 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool application enable bench_rados benchmark
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:49:05,374844293-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:49:05,377999909-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '72b70010-380e-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'application', 'enable', 'bench_rados', 'benchmark'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 72b70010-380e-11ec-b51d-53e6e728d2d3 -- ceph osd pool application enable bench_rados benchmark
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
enabled application 'benchmark' on pool 'bench_rados'
# ./benchmarks/bench-rados:270 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados noscrub 1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:49:10,301022325-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:49:10,304286546-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '72b70010-380e-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'noscrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 72b70010-380e-11ec-b51d-53e6e728d2d3 -- ceph osd pool set bench_rados noscrub 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 noscrub to 1
# ./benchmarks/bench-rados:271 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados nodeep-scrub 1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:49:14,542501620-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:49:14,545698965-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '72b70010-380e-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'nodeep-scrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 72b70010-380e-11ec-b51d-53e6e728d2d3 -- ceph osd pool set bench_rados nodeep-scrub 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 nodeep-scrub to 1
# ./benchmarks/bench-rados:272 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool ls detail
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:49:19,192970824-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:49:19,195848917-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '72b70010-380e-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'ls', 'detail'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 72b70010-380e-11ec-b51d-53e6e728d2d3 -- ceph osd pool ls detail
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
pool 1 'device_health_metrics' replicated size 1 min_size 1 crush_rule 0 object_hash rjenkins pg_num 64 pgp_num 64 autoscale_mode on last_change 19 lfor 0/0/16 flags hashpspool stripe_width 0 pg_num_min 1 application mgr_devicehealth
pool 2 'bench_rados' replicated size 1 min_size 1 crush_rule 0 object_hash rjenkins pg_num 128 pgp_num 128 autoscale_mode off last_change 21 flags hashpspool,noscrub,nodeep-scrub stripe_width 0 pg_num_min 128 application benchmark

# ./benchmarks/bench-rados:273 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd df tree
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:49:23,245394575-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:49:23,248538730-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '72b70010-380e-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'df', 'tree'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 72b70010-380e-11ec-b51d-53e6e728d2d3 -- ceph osd df tree
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
ID  CLASS  WEIGHT   REWEIGHT  SIZE     RAW USE  DATA     OMAP  META     AVAIL    %USE  VAR   PGS  STATUS  TYPE NAME    
-1         0.09769         -  100 GiB  5.7 MiB  176 KiB   0 B  5.6 MiB  100 GiB  0.01  1.00    -          root default 
-3         0.09769         -  100 GiB  5.7 MiB  176 KiB   0 B  5.6 MiB  100 GiB  0.01  1.00    -              host sm1 
 0    ssd  0.09769   1.00000  100 GiB  5.7 MiB  176 KiB   0 B  5.6 MiB  100 GiB  0.01  1.00  192      up          osd.0
                       TOTAL  100 GiB  5.7 MiB  176 KiB   0 B  5.6 MiB  100 GiB  0.01                                  
MIN/MAX VAR: 1.00/1.00  STDDEV: 0


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:49:27,056381327-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:49:50,973096857-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:50:00,073100025-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:50:09,146688017-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:50:09,154862862-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:50:18,063313010-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:50:18,071621877-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:50:26,981154637-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:50:26,989460518-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:50:35,880229493-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:50:35,888408625-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:50:44,763459184-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:50:44,771894829-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:50:44,778616455-07:00] INFO: > The cluster is idle now.[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:50:44,782728053-07:00] STAGE: Start benchmarking of mode write ...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:50:44,790161049-07:00] INFO: > Start system activity collection services[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:50:44,796158160-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:341 - rados_bench() > rm --force /mnt/sda8/ceph-research/scripts/output/sys_activity_16MB_write.dat.rados_bench_host.2
# ./benchmarks/bench-rados:342 - rados_bench() > sar_pid_rados_bench_host=510598
# ./benchmarks/bench-rados:342 - rados_bench() > S_TIME_FORMAT=ISO
# ./benchmarks/bench-rados:342 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/output/sys_activity_16MB_write.dat.rados_bench_host.2 2

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:50:44,803471230-07:00] INFO: >> for OSD host[0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:50:44,812633996-07:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.5 bash
10.10.2.5: b'190490\n'
[1] 09:50:45 [SUCCESS] ljishen@10.10.2.5
190490

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:50:45,912908512-07:00] INFO: > Run rados bench[0m
# ./benchmarks/bench-rados:370 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/output/rados_bench_16MB_write.log.2
## ./benchmarks/bench-rados:361 - rados_bench() > count_cpus 32-63,96-127
# ./benchmarks/bench-rados:361 - rados_bench() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 60 write --pool bench_rados -b 16777216 -O 16777216 --concurrent-ios 128 --show-time --write-object --write-omap --write-xattr --no-cleanup
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:50:45,934088943-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:50:45,936940356-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '72b70010-380e-11ec-b51d-53e6e728d2d3', '--', 'rados', 'bench', '60', 'write', '--pool', 'bench_rados', '-b', '16777216', '-O', '16777216', '--concurrent-ios', '128', '--show-time', '--write-object', '--write-omap', '--write-xattr', '--no-cleanup'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 72b70010-380e-11ec-b51d-53e6e728d2d3 -- rados bench 60 write --pool bench_rados -b 16777216 -O 16777216 --concurrent-ios 128 --show-time --write-object --write-omap --write-xattr --no-cleanup
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
hints = 1
2021-10-28T16:50:48.995306+0000 Maintaining 128 concurrent writes of 16777216 bytes to objects of size 16777216 for up to 60 seconds or 0 objects
2021-10-28T16:50:48.995340+0000 Object prefix: benchmark_data_node-2.bluefield2.ucsc-cmps10_7
2021-10-28T16:50:49.671269+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-28T16:50:49.671269+0000     0       0         0         0         0         0           -           0
2021-10-28T16:50:50.671406+0000     1       4         4         0         0         0           -           0
2021-10-28T16:50:51.671481+0000     2       8         8         0         0         0           -           0
2021-10-28T16:50:52.671556+0000     3      13        13         0         0         0           -           0
2021-10-28T16:50:53.671625+0000     4      17        17         0         0         0           -           0
2021-10-28T16:50:54.671697+0000     5      23        23         0         0         0           -           0
2021-10-28T16:50:55.671765+0000     6      27        27         0         0         0           -           0
2021-10-28T16:50:56.671832+0000     7      32        32         0         0         0           -           0
2021-10-28T16:50:57.671897+0000     8      35        35         0         0         0           -           0
2021-10-28T16:50:58.671973+0000     9      37        37         0         0         0           -           0
2021-10-28T16:50:59.672046+0000    10      39        39         0         0         0           -           0
2021-10-28T16:51:00.672120+0000    11      41        41         0         0         0           -           0
2021-10-28T16:51:01.672194+0000    12      43        43         0         0         0           -           0
2021-10-28T16:51:02.672265+0000    13      47        47         0         0         0           -           0
2021-10-28T16:51:03.672336+0000    14      51        51         0         0         0           -           0
2021-10-28T16:51:04.672410+0000    15      56        56         0         0         0           -           0
2021-10-28T16:51:05.672482+0000    16      61        61         0         0         0           -           0
2021-10-28T16:51:06.672554+0000    17      65        65         0         0         0           -           0
2021-10-28T16:51:07.672625+0000    18      67        67         0         0         0           -           0
2021-10-28T16:51:08.672701+0000    19      69        69         0         0         0           -           0
2021-10-28T16:51:09.672775+0000 min lat: 9999 max lat: 0 avg lat: 0
2021-10-28T16:51:09.672775+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-28T16:51:09.672775+0000    20      70        70         0         0         0           -           0
2021-10-28T16:51:10.672858+0000    21      71        71         0         0         0           -           0
2021-10-28T16:51:11.672931+0000    22      72        72         0         0         0           -           0
2021-10-28T16:51:12.673004+0000    23      73        73         0         0         0           -           0
2021-10-28T16:51:13.673077+0000    24      75        75         0         0         0           -           0
2021-10-28T16:51:14.673155+0000    25      78        78         0         0         0           -           0
2021-10-28T16:51:15.673227+0000    26      80        80         0         0         0           -           0
2021-10-28T16:51:16.673298+0000    27      83        83         0         0         0           -           0
2021-10-28T16:51:17.673370+0000    28      86        86         0         0         0           -           0
2021-10-28T16:51:18.673446+0000    29      89        89         0         0         0           -           0
2021-10-28T16:51:19.673520+0000    30      92        92         0         0         0           -           0
2021-10-28T16:51:20.673593+0000    31      94        94         0         0         0           -           0
2021-10-28T16:51:21.673666+0000    32      97        97         0         0         0           -           0
2021-10-28T16:51:22.673737+0000    33      98        98         0         0         0           -           0
2021-10-28T16:51:23.673810+0000    34     100       100         0         0         0           -           0
2021-10-28T16:51:24.673889+0000    35     101       101         0         0         0           -           0
2021-10-28T16:51:25.673961+0000    36     102       102         0         0         0           -           0
2021-10-28T16:51:26.674029+0000    37     103       103         0         0         0           -           0
2021-10-28T16:51:27.674095+0000    38     106       106         0         0         0           -           0
2021-10-28T16:51:28.674172+0000    39     110       110         0         0         0           -           0
2021-10-28T16:51:29.674253+0000 min lat: 9999 max lat: 0 avg lat: 0
2021-10-28T16:51:29.674253+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-28T16:51:29.674253+0000    40     114       114         0         0         0           -           0
2021-10-28T16:51:30.674344+0000    41     119       119         0         0         0           -           0
2021-10-28T16:51:31.674423+0000    42     123       123         0         0         0           -           0
2021-10-28T16:51:32.674501+0000    43     127       127         0         0         0           -           0
2021-10-28T16:51:33.674575+0000    44     127       129         2  0.727219  0.727273     43.9038      43.707
2021-10-28T16:51:34.674655+0000    45     127       130         3   1.06659        16     44.2672     43.8937
2021-10-28T16:51:35.674731+0000    46     127       131         4    1.3912        16     44.4431     44.0311
2021-10-28T16:51:36.674801+0000    47     127       132         5     1.702        16     45.2406      44.273
2021-10-28T16:51:37.674870+0000    48     127       133         6   1.99985        16      45.864     44.5381
2021-10-28T16:51:38.674950+0000    49     127       134         7   2.28555        16     46.6886     44.8454
2021-10-28T16:51:39.675031+0000    50     127       137        10   3.19976        48     47.7936     45.6755
2021-10-28T16:51:40.675113+0000    51     127       139        12   3.76443        32     48.1087     46.0662
2021-10-28T16:51:41.675190+0000    52     127       142        15   4.61504        48     48.5469      46.541
2021-10-28T16:51:42.675266+0000    53     127       145        18   5.43356        48     48.9752     46.9229
2021-10-28T16:51:43.675343+0000    54     127       148        21   6.22176        48     49.5763      47.276
2021-10-28T16:51:44.675421+0000    55     127       150        23   6.69041        32     49.9646     47.5025
2021-10-28T16:51:45.675495+0000    56     127       153        26   7.42802        48      50.425      47.821
2021-10-28T16:51:46.675570+0000    57     127       155        28   7.85907        32     50.8362      48.028
2021-10-28T16:51:47.675636+0000    58     127       158        31   8.55109        48     51.2724     48.3287
2021-10-28T16:51:48.675704+0000    59     127       160        33   8.94849        32     52.0894     48.5538
2021-10-28T16:51:49.675782+0000 min lat: 43.5101 max lat: 52.3683 avg lat: 48.7678
2021-10-28T16:51:49.675782+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-28T16:51:49.675782+0000    60     127       162        35   9.33264        32     52.3683     48.7678
2021-10-28T16:51:50.675867+0000    61       2       163       161   42.2264      2016     1.66373     32.5229
2021-10-28T16:51:51.675948+0000    62       1       163       162   41.8034        16     2.14745     32.3354
2021-10-28T16:51:52.676046+0000 Total time run:         62.4458
Total writes made:      163
Write size:             16777216
Object size:            16777216
Bandwidth (MB/sec):     41.7642
Stddev Bandwidth:       255.4
Max bandwidth (MB/sec): 2016
Min bandwidth (MB/sec): 0
Average IOPS:           2
Stddev IOPS:            15.9626
Max IOPS:               126
Min IOPS:               0
Average Latency(s):     32.1538
Stddev Latency(s):      16.1833
Max latency(s):         52.9347
Min latency(s):         1.66373

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:51:53,317566626-07:00] INFO: > Stop system activity collection services[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:51:53,324158597-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:391 - rados_bench() > kill -INT 510598

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:51:53,330509845-07:00] INFO: >> for OSD host[0m
# ./benchmarks/bench-rados:396 - rados_bench() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.5 kill -INT 190490
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:51:53,338774218-07:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.5 kill -INT 190490
[1] 09:51:55 [SUCCESS] ljishen@10.10.2.5

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:51:55,329964159-07:00] INFO: >>> Copy system activity file from OSD host[0m
## ./benchmarks/bench-rados:403 - rados_bench() > basename -- /tmp/bench-rados/sys_activity_16MB_write.dat.osd_host.2
# ./benchmarks/bench-rados:403 - rados_bench() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.5:/tmp/bench-rados/sys_activity_16MB_write.dat.osd_host.2 /mnt/sda8/ceph-research/scripts/output/sys_activity_16MB_write.dat.osd_host.2


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:51:56,661535040-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:52:20,532683281-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 164 objects, 2.5 GiB
    usage:   7.8 GiB used, 92 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:52:20,541324774-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:52:29,515414471-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 164 objects, 2.5 GiB
    usage:   7.8 GiB used, 92 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:52:29,524076945-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:52:38,389704263-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 164 objects, 2.5 GiB
    usage:   7.8 GiB used, 92 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:52:38,398279582-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:52:47,366386099-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 164 objects, 2.5 GiB
    usage:   7.8 GiB used, 92 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:52:47,375205046-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:52:56,296208477-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 164 objects, 2.5 GiB
    usage:   7.8 GiB used, 92 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:52:56,304938297-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:52:56,311773617-07:00] INFO: > The cluster is idle now.[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:52:56,316033815-07:00] STAGE: Start benchmarking of mode seq ...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:52:56,323610572-07:00] INFO: > Start system activity collection services[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:52:56,329671243-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:341 - rados_bench() > rm --force /mnt/sda8/ceph-research/scripts/output/sys_activity_16MB_seq.dat.rados_bench_host.2
# ./benchmarks/bench-rados:342 - rados_bench() > sar_pid_rados_bench_host=511957
# ./benchmarks/bench-rados:342 - rados_bench() > S_TIME_FORMAT=ISO
# ./benchmarks/bench-rados:342 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/output/sys_activity_16MB_seq.dat.rados_bench_host.2 2

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:52:56,337287083-07:00] INFO: >> for OSD host[0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:52:56,346540670-07:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.5 bash
10.10.2.5: b'191039\n'
[1] 09:52:57 [SUCCESS] ljishen@10.10.2.5
191039

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:52:57,474305231-07:00] INFO: > Run rados bench[0m
# ./benchmarks/bench-rados:379 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/output/rados_bench_16MB_seq.log.2
## ./benchmarks/bench-rados:376 - rados_bench() > count_cpus 32-63,96-127
# ./benchmarks/bench-rados:376 - rados_bench() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 128 --show-time
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:52:57,494808276-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:52:57,497751361-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '72b70010-380e-11ec-b51d-53e6e728d2d3', '--', 'rados', 'bench', '99999999', 'seq', '--pool', 'bench_rados', '--concurrent-ios', '128', '--show-time'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 72b70010-380e-11ec-b51d-53e6e728d2d3 -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 128 --show-time
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
hints = 1
2021-10-28T16:53:00.468082+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-28T16:53:00.468082+0000     0       0         0         0         0         0           -           0
2021-10-28T16:53:01.468218+0000     1      46        46         0         0         0           -           0
2021-10-28T16:53:02.468334+0000     2      91        91         0         0         0           -           0
2021-10-28T16:53:03.468407+0000     3     127       136         9    47.993        48     2.89489      2.8887
2021-10-28T16:53:04.468492+0000 Total time run:       3.82138
Total reads made:     163
Read size:            16777216
Object size:          16777216
Bandwidth (MB/sec):   682.476
Average IOPS:         42
Stddev IOPS:          1.73205
Max IOPS:             3
Min IOPS:             0
Average Latency(s):   1.8166
Max latency(s):       2.94679
Min latency(s):       0.276849

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:53:05,220974479-07:00] INFO: > Stop system activity collection services[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:53:05,227830929-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:391 - rados_bench() > kill -INT 511957

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:53:05,234457617-07:00] INFO: >> for OSD host[0m
# ./benchmarks/bench-rados:396 - rados_bench() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.5 kill -INT 191039
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:53:05,242459926-07:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.5 kill -INT 191039
[1] 09:53:06 [SUCCESS] ljishen@10.10.2.5

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:53:06,352610085-07:00] INFO: >>> Copy system activity file from OSD host[0m
## ./benchmarks/bench-rados:403 - rados_bench() > basename -- /tmp/bench-rados/sys_activity_16MB_seq.dat.osd_host.2
# ./benchmarks/bench-rados:403 - rados_bench() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.5:/tmp/bench-rados/sys_activity_16MB_seq.dat.osd_host.2 /mnt/sda8/ceph-research/scripts/output/sys_activity_16MB_seq.dat.osd_host.2


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:53:07,428673051-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:53:31,217475308-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 164 objects, 2.5 GiB
    usage:   7.8 GiB used, 92 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:53:31,225255318-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:53:40,174368718-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 164 objects, 2.5 GiB
    usage:   7.8 GiB used, 92 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:53:40,182981367-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:53:48,990274230-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 164 objects, 2.5 GiB
    usage:   7.8 GiB used, 92 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:53:48,998850580-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:53:57,857124451-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 164 objects, 2.5 GiB
    usage:   7.8 GiB used, 92 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:53:57,865300647-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:54:06,750307794-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 164 objects, 2.5 GiB
    usage:   7.8 GiB used, 92 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:54:06,758557009-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:54:06,765152156-07:00] INFO: > The cluster is idle now.[0m


[1;7;39;49m[2021-10-28T09:54:06,767774357-07:00][RUNNING][ROUND 3/7/21] object_size=16MB[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:54:06,771501881-07:00] STAGE: Launch a Ceph cluster (CEPH_HOST: 10.10.2.1, OSD_HOST: 10.10.2.5) ...[0m
# ./benchmarks/bench-rados:183 - launch_ceph_cluster() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.1 bash -euo pipefail
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:54:06,781147447-07:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.1 bash -euo pipefail
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T09:54:07,233078063-07:00] INFO: > Remove existing clusters\x1b[0m\n'
10.10.2.1: b'## bash:17 -  > basename -- /var/lib/ceph/72b70010-380e-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'# bash:17 -  > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm --host 10.10.2.1 --host 10.10.2.5 rm-cluster --force --fsid 72b70010-380e-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T09:54:07,243903021-07:00] DEBUG: HOST_PARAMS: ['--host', '10.10.2.1', '--host', '10.10.2.5']\x1b[0m\n"
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T09:54:07,247545909-07:00] DEBUG: CEPHADM_PARAMS: ['rm-cluster', '--force', '--fsid', '72b70010-380e-11ec-b51d-53e6e728d2d3']\x1b[0m\n"
10.10.2.1: b"# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:63 -  > /tmp/bench-rados/ceph-research/scripts/parallel-ssh --host 10.10.2.1 --host 10.10.2.5 'sudo python3 -u - rm-cluster --force --fsid 72b70010-380e-11ec-b51d-53e6e728d2d3'\n"
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T09:54:07,256237357-07:00] DEBUG: command from stdin\x1b[0m\n'
10.10.2.1: b"# /tmp/bench-rados/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host 10.10.2.1 --host 10.10.2.5 'sudo python3 -u - rm-cluster --force --fsid 72b70010-380e-11ec-b51d-53e6e728d2d3'\n"
10.10.2.1: b'[1] 09:54:12 [SUCCESS] 10.10.2.1\n[2] 09:54:19 [SUCCESS] 10.10.2.5\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T09:54:19,205207818-07:00] INFO: > Deploy a new cluster\x1b[0m\n'
10.10.2.1: b'# bash:24 -  > /tmp/bench-rados/ceph-research/scripts/ceph-deploy -m 10.10.2.1 -o sm1/10.10.2.5:/dev/nvme0n1\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T09:54:19,216820116-07:00] STAGE: Verify passwordless SSH to hosts, passwordless sudo via ljishen, and the existence of devices on hosts...\x1b[0m\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T09:54:19,221787524-07:00] INFO: > Check the host for the monitor\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:181 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@10.10.2.1 sudo --non-interactive --validate\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T09:54:19,371419496-07:00] INFO: > Check hosts for OSDs\x1b[0m\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T09:54:19,375854032-07:00] INFO: >> Check host 'sm1'\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:200 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 sudo --non-interactive --validate\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T09:54:20,479936015-07:00] INFO: >>> Check device '/dev/nvme0n1'\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:215 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 test -b /dev/nvme0n1\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:226 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 lsblk --nodeps --noheadings --output TYPE /dev/nvme0n1\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:227 - verify_parameters() > grep --word-regexp disk\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T09:54:22,649828961-07:00] STAGE: Reset devices on hosts...\x1b[0m\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T09:54:22,654905193-07:00] INFO: > Resetting device '/dev/nvme0n1' on host 'sm1'\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:288 - reset_host_devices() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 sudo sh\n'
10.10.2.1: b'  Removing ceph--deb649a8--e848--477b--82de--b1752ef8e09b-osd--block--7f68f36c--a01b--485e--b5c7--279b02b5bb51 (252:0)\n'
10.10.2.1: b'  Archiving volume group "ceph-deb649a8-e848-477b-82de-b1752ef8e09b" metadata (seqno 5).\n'
10.10.2.1: b'  Releasing logical volume "osd-block-7f68f36c-a01b-485e-b5c7-279b02b5bb51"\n'
10.10.2.1: b'  Creating volume group backup "/etc/lvm/backup/ceph-deb649a8-e848-477b-82de-b1752ef8e09b" (seqno 6).\n'
10.10.2.1: b'  Logical volume "osd-block-7f68f36c-a01b-485e-b5c7-279b02b5bb51" successfully removed\n'
10.10.2.1: b'  Removing physical volume "/dev/nvme0n1" from volume group "ceph-deb649a8-e848-477b-82de-b1752ef8e09b"\n'
10.10.2.1: b'  Volume group "ceph-deb649a8-e848-477b-82de-b1752ef8e09b" successfully removed\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T09:54:25,012736459-07:00] STAGE: Bootstrap a new cluster...\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:317 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.1 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T09:54:25,022528696-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T09:54:25,026267113-07:00] DEBUG: CEPHADM_PARAMS: ['bootstrap', '--config', '/tmp/bench-rados/ceph-research/scripts/ceph.conf', '--mon-ip', '10.10.2.1', '--output-dir', '/tmp/bench-rados/deployment_data_root/etc/ceph', '--ssh-user', 'ljishen', '--skip-dashboard', '--no-minimize-config', '--allow-overwrite', '--allow-fqdn-hostname', '--orphan-initial-daemons', '--skip-monitoring-stack', '--single-host-defaults']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.1 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults\n'
10.10.2.1: b'Verifying podman|docker is present...\n'
10.10.2.1: b'Verifying lvm2 is present...\nVerifying time synchronization is in place...\n'
10.10.2.1: b'Unit ntp.service is enabled and running\n'
10.10.2.1: b'Repeating the final host check...\n'
10.10.2.1: b'podman|docker (/usr/bin/docker) is present\n'
10.10.2.1: b'systemctl is present\n'
10.10.2.1: b'lvcreate is present\n'
10.10.2.1: b'Unit ntp.service is enabled and running\n'
10.10.2.1: b'Host looks OK\nCluster fsid: b4e7b2d0-380f-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Verifying IP 10.10.2.1 port 3300 ...\n'
10.10.2.1: b'Verifying IP 10.10.2.1 port 6789 ...\n'
10.10.2.1: b'Mon IP `10.10.2.1` is in CIDR network `10.10.2.0/24`\n'
10.10.2.1: b'- internal network (--cluster-network) has not been provided, OSD replication will default to the public_network\n'
10.10.2.1: b'Adjusting default settings to suit single-host cluster...\n'
10.10.2.1: b'Pulling container image quay.io/ceph/ceph:v16...\n'
10.10.2.1: b'Ceph version: ceph version 16.2.6 (ee28fb57e47e9f88813e24bbf4c14496ca299d31) pacific (stable)\n'
10.10.2.1: b'Extracting ceph user uid/gid from container image...\n'
10.10.2.1: b'Creating initial keys...\n'
10.10.2.1: b'Creating initial monmap...\n'
10.10.2.1: b'Creating mon...\n'
10.10.2.1: b'Waiting for mon to start...\n'
10.10.2.1: b'Waiting for mon...\n'
10.10.2.1: b'mon is available\nSetting mon public_network to 10.10.2.0/24\n'
10.10.2.1: b'Wrote config to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf\n'
10.10.2.1: b'Wrote keyring to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring\n'
10.10.2.1: b'Creating mgr...\n'
10.10.2.1: b'Verifying port 9283 ...\n'
10.10.2.1: b'Waiting for mgr to start...\nWaiting for mgr...\n'
10.10.2.1: b'mgr not available, waiting (1/15)...\n'
10.10.2.1: b'mgr not available, waiting (2/15)...\n'
10.10.2.1: b'mgr is available\n'
10.10.2.1: b'Enabling cephadm module...\n'
10.10.2.1: b'Waiting for the mgr to restart...\n'
10.10.2.1: b'Waiting for mgr epoch 5...\n'
10.10.2.1: b'mgr epoch 5 is available\nSetting orchestrator backend to cephadm...\n'
10.10.2.1: b'Generating ssh key...\n'
10.10.2.1: b'Wrote public SSH key to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub\n'
10.10.2.1: b'Adding key to ljishen@localhost authorized_keys...\n'
10.10.2.1: b'Adding host node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us...\n'
10.10.2.1: b'Deploying unmanaged mon service...\n'
10.10.2.1: b'Deploying unmanaged mgr service...\n'
10.10.2.1: b'You can access the Ceph CLI with:\n\n\tsudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --fsid b4e7b2d0-380f-11ec-b51d-53e6e728d2d3 -c /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf -k /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring\n\n'
10.10.2.1: b'Please consider enabling telemetry to help improve Ceph:\n\n\tceph telemetry on\n\nFor more information see:\n\n\thttps://docs.ceph.com/docs/pacific/mgr/telemetry/\n\nBootstrap complete.\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T09:55:27,529151729-07:00] INFO: Sleep for 20 seconds\x1b[0m\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T09:55:47,536112566-07:00] INFO: > Set the default number of monitor daemons to 1\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:337 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T09:55:47,546068601-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T09:55:47,549896597-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mon', '1']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1\n'
10.10.2.1: b'Inferring fsid b4e7b2d0-380f-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/b4e7b2d0-380f-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b'Scheduled mon update...\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T09:55:56,739753211-07:00] INFO: > Set the default number of manager daemons to 1\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:342 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T09:55:56,749947414-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T09:55:56,753594089-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mgr', '1']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1\n'
10.10.2.1: b'Inferring fsid b4e7b2d0-380f-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/b4e7b2d0-380f-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b'Scheduled mgr update...\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T09:56:06,368206595-07:00] STAGE: Add hosts to the cluster...\x1b[0m\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T09:56:06,374153424-07:00] INFO: > Adding host 'sm1'\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:361 - add_hosts() > ssh-copy-id -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -f -i /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub ljishen@sm1\n'
10.10.2.1: b'/usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: "/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub"\n'
10.10.2.1: b'\nNumber of key(s) added: 1\n\nNow try logging into the machine, with:   "ssh -o \'GlobalKnownHostsFile=/dev/null\' -o \'LogLevel=ERROR\' -o \'PasswordAuthentication=no\' -o \'StrictHostKeyChecking=no\' -o \'UserKnownHostsFile=/dev/null\' \'ljishen@sm1\'"\nand check to make sure that only the key(s) you wanted were added.\n\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:367 - add_hosts() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add sm1 10.10.2.5\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T09:56:07,542362864-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T09:56:07,546066326-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'host', 'add', 'sm1', '10.10.2.5']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add sm1 10.10.2.5\n'
10.10.2.1: b'Inferring fsid b4e7b2d0-380f-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/b4e7b2d0-380f-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b"Added host 'sm1' with addr '10.10.2.5'\n"
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T09:56:18,173831003-07:00] INFO: Sleep for 20 seconds\x1b[0m\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T09:56:38,178724912-07:00] STAGE: Add OSDs to the cluster...\x1b[0m\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T09:56:38,185680778-07:00] INFO: > Adding devices ['/dev/nvme0n1'] on host 'sm1' as OSDs\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:388 - add_host_devices_as_osds() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd sm1:/dev/nvme0n1\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T09:56:38,196163803-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T09:56:38,200150127-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'daemon', 'add', 'osd', 'sm1:/dev/nvme0n1']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd sm1:/dev/nvme0n1\n'
10.10.2.1: b'Inferring fsid b4e7b2d0-380f-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/b4e7b2d0-380f-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b"Created osd(s) 0 on host 'sm1'\n"
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T09:57:02,692051939-07:00] INFO: > Sleep for 20 seconds\x1b[0m\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T09:57:22,697475720-07:00] STAGE: Show Cluster Status\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:408 - show_status() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T09:57:22,707132383-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T09:57:22,710544206-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', '--status']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status\n'
10.10.2.1: b'Inferring fsid b4e7b2d0-380f-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/b4e7b2d0-380f-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b'  cluster:\n    id:     b4e7b2d0-380f-11ec-b51d-53e6e728d2d3\n    health: HEALTH_WARN\n            1 pool(s) have no replicas configured\n \n  services:\n    mon: 1 daemons, quorum node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us (age 2m)\n    mgr: node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us.nzmgmg(active, since 2m)\n    osd: 1 osds: 1 up (since 20s), 1 in (since 40s)\n \n  data:\n    pools:   1 pools, 1 pgs\n    objects: 0 objects, 0 B\n    usage:   4.7 MiB used, 100 GiB / 100 GiB avail\n    pgs:     1 active+clean\n \n'
[1] 09:57:31 [SUCCESS] ljishen@10.10.2.1

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:54:07,233078063-07:00] INFO: > Remove existing clusters[0m
## bash:17 -  > basename -- /var/lib/ceph/72b70010-380e-11ec-b51d-53e6e728d2d3
# bash:17 -  > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm --host 10.10.2.1 --host 10.10.2.5 rm-cluster --force --fsid 72b70010-380e-11ec-b51d-53e6e728d2d3
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:54:07,243903021-07:00] DEBUG: HOST_PARAMS: ['--host', '10.10.2.1', '--host', '10.10.2.5'][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:54:07,247545909-07:00] DEBUG: CEPHADM_PARAMS: ['rm-cluster', '--force', '--fsid', '72b70010-380e-11ec-b51d-53e6e728d2d3'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:63 -  > /tmp/bench-rados/ceph-research/scripts/parallel-ssh --host 10.10.2.1 --host 10.10.2.5 'sudo python3 -u - rm-cluster --force --fsid 72b70010-380e-11ec-b51d-53e6e728d2d3'
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:54:07,256237357-07:00] DEBUG: command from stdin[0m
# /tmp/bench-rados/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host 10.10.2.1 --host 10.10.2.5 'sudo python3 -u - rm-cluster --force --fsid 72b70010-380e-11ec-b51d-53e6e728d2d3'
[1] 09:54:12 [SUCCESS] 10.10.2.1
[2] 09:54:19 [SUCCESS] 10.10.2.5

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:54:19,205207818-07:00] INFO: > Deploy a new cluster[0m
# bash:24 -  > /tmp/bench-rados/ceph-research/scripts/ceph-deploy -m 10.10.2.1 -o sm1/10.10.2.5:/dev/nvme0n1


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:54:19,216820116-07:00] STAGE: Verify passwordless SSH to hosts, passwordless sudo via ljishen, and the existence of devices on hosts...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:54:19,221787524-07:00] INFO: > Check the host for the monitor[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:181 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@10.10.2.1 sudo --non-interactive --validate

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:54:19,371419496-07:00] INFO: > Check hosts for OSDs[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:54:19,375854032-07:00] INFO: >> Check host 'sm1'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:200 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 sudo --non-interactive --validate

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:54:20,479936015-07:00] INFO: >>> Check device '/dev/nvme0n1'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:215 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 test -b /dev/nvme0n1
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:226 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 lsblk --nodeps --noheadings --output TYPE /dev/nvme0n1
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:227 - verify_parameters() > grep --word-regexp disk


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:54:22,649828961-07:00] STAGE: Reset devices on hosts...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:54:22,654905193-07:00] INFO: > Resetting device '/dev/nvme0n1' on host 'sm1'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:288 - reset_host_devices() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@sm1 sudo sh
  Removing ceph--deb649a8--e848--477b--82de--b1752ef8e09b-osd--block--7f68f36c--a01b--485e--b5c7--279b02b5bb51 (252:0)
  Archiving volume group "ceph-deb649a8-e848-477b-82de-b1752ef8e09b" metadata (seqno 5).
  Releasing logical volume "osd-block-7f68f36c-a01b-485e-b5c7-279b02b5bb51"
  Creating volume group backup "/etc/lvm/backup/ceph-deb649a8-e848-477b-82de-b1752ef8e09b" (seqno 6).
  Logical volume "osd-block-7f68f36c-a01b-485e-b5c7-279b02b5bb51" successfully removed
  Removing physical volume "/dev/nvme0n1" from volume group "ceph-deb649a8-e848-477b-82de-b1752ef8e09b"
  Volume group "ceph-deb649a8-e848-477b-82de-b1752ef8e09b" successfully removed


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:54:25,012736459-07:00] STAGE: Bootstrap a new cluster...[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:317 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.1 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:54:25,022528696-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:54:25,026267113-07:00] DEBUG: CEPHADM_PARAMS: ['bootstrap', '--config', '/tmp/bench-rados/ceph-research/scripts/ceph.conf', '--mon-ip', '10.10.2.1', '--output-dir', '/tmp/bench-rados/deployment_data_root/etc/ceph', '--ssh-user', 'ljishen', '--skip-dashboard', '--no-minimize-config', '--allow-overwrite', '--allow-fqdn-hostname', '--orphan-initial-daemons', '--skip-monitoring-stack', '--single-host-defaults'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.1 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults
Verifying podman|docker is present...
Verifying lvm2 is present...
Verifying time synchronization is in place...
Unit ntp.service is enabled and running
Repeating the final host check...
podman|docker (/usr/bin/docker) is present
systemctl is present
lvcreate is present
Unit ntp.service is enabled and running
Host looks OK
Cluster fsid: b4e7b2d0-380f-11ec-b51d-53e6e728d2d3
Verifying IP 10.10.2.1 port 3300 ...
Verifying IP 10.10.2.1 port 6789 ...
Mon IP `10.10.2.1` is in CIDR network `10.10.2.0/24`
- internal network (--cluster-network) has not been provided, OSD replication will default to the public_network
Adjusting default settings to suit single-host cluster...
Pulling container image quay.io/ceph/ceph:v16...
Ceph version: ceph version 16.2.6 (ee28fb57e47e9f88813e24bbf4c14496ca299d31) pacific (stable)
Extracting ceph user uid/gid from container image...
Creating initial keys...
Creating initial monmap...
Creating mon...
Waiting for mon to start...
Waiting for mon...
mon is available
Setting mon public_network to 10.10.2.0/24
Wrote config to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf
Wrote keyring to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring
Creating mgr...
Verifying port 9283 ...
Waiting for mgr to start...
Waiting for mgr...
mgr not available, waiting (1/15)...
mgr not available, waiting (2/15)...
mgr is available
Enabling cephadm module...
Waiting for the mgr to restart...
Waiting for mgr epoch 5...
mgr epoch 5 is available
Setting orchestrator backend to cephadm...
Generating ssh key...
Wrote public SSH key to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub
Adding key to ljishen@localhost authorized_keys...
Adding host node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us...
Deploying unmanaged mon service...
Deploying unmanaged mgr service...
You can access the Ceph CLI with:

	sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --fsid b4e7b2d0-380f-11ec-b51d-53e6e728d2d3 -c /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf -k /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring

Please consider enabling telemetry to help improve Ceph:

	ceph telemetry on

For more information see:

	https://docs.ceph.com/docs/pacific/mgr/telemetry/

Bootstrap complete.

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:55:27,529151729-07:00] INFO: Sleep for 20 seconds[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:55:47,536112566-07:00] INFO: > Set the default number of monitor daemons to 1[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:337 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:55:47,546068601-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:55:47,549896597-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mon', '1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1
Inferring fsid b4e7b2d0-380f-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/b4e7b2d0-380f-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Scheduled mon update...

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:55:56,739753211-07:00] INFO: > Set the default number of manager daemons to 1[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:342 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:55:56,749947414-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:55:56,753594089-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mgr', '1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1
Inferring fsid b4e7b2d0-380f-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/b4e7b2d0-380f-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Scheduled mgr update...


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:56:06,368206595-07:00] STAGE: Add hosts to the cluster...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:56:06,374153424-07:00] INFO: > Adding host 'sm1'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:361 - add_hosts() > ssh-copy-id -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -f -i /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub ljishen@sm1
/usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: "/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub"

Number of key(s) added: 1

Now try logging into the machine, with:   "ssh -o 'GlobalKnownHostsFile=/dev/null' -o 'LogLevel=ERROR' -o 'PasswordAuthentication=no' -o 'StrictHostKeyChecking=no' -o 'UserKnownHostsFile=/dev/null' 'ljishen@sm1'"
and check to make sure that only the key(s) you wanted were added.

# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:367 - add_hosts() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add sm1 10.10.2.5
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:56:07,542362864-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:56:07,546066326-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'host', 'add', 'sm1', '10.10.2.5'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add sm1 10.10.2.5
Inferring fsid b4e7b2d0-380f-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/b4e7b2d0-380f-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Added host 'sm1' with addr '10.10.2.5'

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:56:18,173831003-07:00] INFO: Sleep for 20 seconds[0m


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:56:38,178724912-07:00] STAGE: Add OSDs to the cluster...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:56:38,185680778-07:00] INFO: > Adding devices ['/dev/nvme0n1'] on host 'sm1' as OSDs[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:388 - add_host_devices_as_osds() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd sm1:/dev/nvme0n1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:56:38,196163803-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:56:38,200150127-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'daemon', 'add', 'osd', 'sm1:/dev/nvme0n1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd sm1:/dev/nvme0n1
Inferring fsid b4e7b2d0-380f-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/b4e7b2d0-380f-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Created osd(s) 0 on host 'sm1'

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:57:02,692051939-07:00] INFO: > Sleep for 20 seconds[0m


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:57:22,697475720-07:00] STAGE: Show Cluster Status[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:408 - show_status() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:57:22,707132383-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:57:22,710544206-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', '--status'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status
Inferring fsid b4e7b2d0-380f-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/b4e7b2d0-380f-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
  cluster:
    id:     b4e7b2d0-380f-11ec-b51d-53e6e728d2d3
    health: HEALTH_WARN
            1 pool(s) have no replicas configured
 
  services:
    mon: 1 daemons, quorum node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us (age 2m)
    mgr: node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us.nzmgmg(active, since 2m)
    osd: 1 osds: 1 up (since 20s), 1 in (since 40s)
 
  data:
    pools:   1 pools, 1 pgs
    objects: 0 objects, 0 B
    usage:   4.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     1 active+clean
 

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:57:31,162444252-07:00] INFO: > Copy configuration files to local[0m
# ./benchmarks/bench-rados:220 - launch_ceph_cluster() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.1 sudo chown --changes --recursive ljishen /tmp/bench-rados/deployment_data_root/etc/ceph
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:57:31,170624026-07:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.1 sudo chown --changes --recursive ljishen /tmp/bench-rados/deployment_data_root/etc/ceph
[1] 09:57:31 [SUCCESS] ljishen@10.10.2.1
# ./benchmarks/bench-rados:223 - launch_ceph_cluster() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.1:/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf
# ./benchmarks/bench-rados:226 - launch_ceph_cluster() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.1:/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:57:31,648958006-07:00] STAGE: Update local ceph.conf as a client...[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:57:31,652846854-07:00] STAGE: Configure Ceph pool...[0m
# ./benchmarks/bench-rados:257 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_max_backfills 32
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:57:31,675958936-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:57:31,678730378-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'b4e7b2d0-380f-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'config', 'set', 'osd', 'osd_max_backfills', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid b4e7b2d0-380f-11ec-b51d-53e6e728d2d3 -- ceph config set osd osd_max_backfills 32
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:258 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_active 32
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:57:35,623512887-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:57:35,626301592-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'b4e7b2d0-380f-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_active', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid b4e7b2d0-380f-11ec-b51d-53e6e728d2d3 -- ceph config set osd osd_recovery_max_active 32
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:259 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_single_start 8
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:57:39,582866524-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:57:39,585815691-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'b4e7b2d0-380f-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_single_start', '8'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid b4e7b2d0-380f-11ec-b51d-53e6e728d2d3 -- ceph config set osd osd_recovery_max_single_start 8
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:260 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_op_priority 63
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:57:43,411019783-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:57:43,414073768-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'b4e7b2d0-380f-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_op_priority', '63'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid b4e7b2d0-380f-11ec-b51d-53e6e728d2d3 -- ceph config set osd osd_recovery_op_priority 63
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:262 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config show-with-defaults osd.0
# ./benchmarks/bench-rados:263 - configure_ceph_pool() > grep 'osd_max_backfills\|osd_recovery'
# ./benchmarks/bench-rados:263 - configure_ceph_pool() > column -t -s ' '
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
osd_max_backfills                        32         mon
osd_recovery_cost                        20971520   default
osd_recovery_delay_start                 0.000000   default
osd_recovery_max_active                  32         mon
osd_recovery_max_active_hdd              3          default
osd_recovery_max_active_ssd              10         default
osd_recovery_max_chunk                   8388608    default
osd_recovery_max_omap_entries_per_chunk  8096       default
osd_recovery_max_single_start            8          mon
osd_recovery_op_priority                 3          default
osd_recovery_op_warn_multiple            16         default
osd_recovery_priority                    5          default
osd_recovery_retry_interval              30.000000  default
osd_recovery_sleep                       0.000000   default
osd_recovery_sleep_hdd                   0.100000   default
osd_recovery_sleep_hybrid                0.025000   default
osd_recovery_sleep_ssd                   0.000000   default
# ./benchmarks/bench-rados:265 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool create bench_rados 128 128 replicated --size 1 --pg-num-min 128
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:57:51,046391633-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:57:51,049465576-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'b4e7b2d0-380f-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'create', 'bench_rados', '128', '128', 'replicated', '--size', '1', '--pg-num-min', '128'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid b4e7b2d0-380f-11ec-b51d-53e6e728d2d3 -- ceph osd pool create bench_rados 128 128 replicated --size 1 --pg-num-min 128
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
pool 'bench_rados' created
# ./benchmarks/bench-rados:267 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados min_size 1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:57:55,194112492-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:57:55,197072239-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'b4e7b2d0-380f-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'min_size', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid b4e7b2d0-380f-11ec-b51d-53e6e728d2d3 -- ceph osd pool set bench_rados min_size 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 min_size to 1
# ./benchmarks/bench-rados:268 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados pg_autoscale_mode off
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:57:59,895604723-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:57:59,898553250-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'b4e7b2d0-380f-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'pg_autoscale_mode', 'off'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid b4e7b2d0-380f-11ec-b51d-53e6e728d2d3 -- ceph osd pool set bench_rados pg_autoscale_mode off
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 pg_autoscale_mode to off
# ./benchmarks/bench-rados:269 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool application enable bench_rados benchmark
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:58:04,010127506-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:58:04,012992855-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'b4e7b2d0-380f-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'application', 'enable', 'bench_rados', 'benchmark'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid b4e7b2d0-380f-11ec-b51d-53e6e728d2d3 -- ceph osd pool application enable bench_rados benchmark
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
enabled application 'benchmark' on pool 'bench_rados'
# ./benchmarks/bench-rados:270 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados noscrub 1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:58:08,850815753-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:58:08,853837788-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'b4e7b2d0-380f-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'noscrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid b4e7b2d0-380f-11ec-b51d-53e6e728d2d3 -- ceph osd pool set bench_rados noscrub 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 noscrub to 1
# ./benchmarks/bench-rados:271 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados nodeep-scrub 1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:58:13,158358775-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:58:13,161474787-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'b4e7b2d0-380f-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'nodeep-scrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid b4e7b2d0-380f-11ec-b51d-53e6e728d2d3 -- ceph osd pool set bench_rados nodeep-scrub 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 nodeep-scrub to 1
# ./benchmarks/bench-rados:272 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool ls detail
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:58:17,965813630-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:58:17,968789377-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'b4e7b2d0-380f-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'ls', 'detail'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid b4e7b2d0-380f-11ec-b51d-53e6e728d2d3 -- ceph osd pool ls detail
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
pool 1 'device_health_metrics' replicated size 1 min_size 1 crush_rule 0 object_hash rjenkins pg_num 64 pgp_num 64 autoscale_mode on last_change 18 lfor 0/0/16 flags hashpspool stripe_width 0 pg_num_min 1 application mgr_devicehealth
pool 2 'bench_rados' replicated size 1 min_size 1 crush_rule 0 object_hash rjenkins pg_num 128 pgp_num 128 autoscale_mode off last_change 20 flags hashpspool,noscrub,nodeep-scrub stripe_width 0 pg_num_min 128 application benchmark

# ./benchmarks/bench-rados:273 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd df tree
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:58:22,016947096-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:58:22,019945366-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'b4e7b2d0-380f-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'df', 'tree'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid b4e7b2d0-380f-11ec-b51d-53e6e728d2d3 -- ceph osd df tree
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
ID  CLASS  WEIGHT   REWEIGHT  SIZE     RAW USE  DATA     OMAP  META     AVAIL    %USE  VAR   PGS  STATUS  TYPE NAME    
-1         0.09769         -  100 GiB  5.7 MiB  168 KiB   0 B  5.6 MiB  100 GiB  0.01  1.00    -          root default 
-3         0.09769         -  100 GiB  5.7 MiB  168 KiB   0 B  5.6 MiB  100 GiB  0.01  1.00    -              host sm1 
 0    ssd  0.09769   1.00000  100 GiB  5.7 MiB  168 KiB   0 B  5.6 MiB  100 GiB  0.01  1.00  192      up          osd.0
                       TOTAL  100 GiB  5.7 MiB  168 KiB   0 B  5.6 MiB  100 GiB  0.01                                  
MIN/MAX VAR: 1.00/1.00  STDDEV: 0


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:58:25,894565569-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:58:49,739985933-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:58:58,688206621-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:59:07,587026783-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:59:16,438675339-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:59:16,447481132-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:59:25,293008775-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:59:25,301736912-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:59:34,239403670-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:59:34,247753674-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:59:43,107831991-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:59:43,116102155-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:59:52,061599202-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:59:52,070100752-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:59:52,076528213-07:00] INFO: > The cluster is idle now.[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:59:52,080715113-07:00] STAGE: Start benchmarking of mode write ...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:59:52,088670273-07:00] INFO: > Start system activity collection services[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:59:52,095016773-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:341 - rados_bench() > rm --force /mnt/sda8/ceph-research/scripts/output/sys_activity_16MB_write.dat.rados_bench_host.3
# ./benchmarks/bench-rados:342 - rados_bench() > sar_pid_rados_bench_host=517762
# ./benchmarks/bench-rados:342 - rados_bench() > S_TIME_FORMAT=ISO
# ./benchmarks/bench-rados:342 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/output/sys_activity_16MB_write.dat.rados_bench_host.3 2

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:59:52,102445490-07:00] INFO: >> for OSD host[0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:59:52,111954198-07:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.5 bash
10.10.2.5: b'195906\n'
[1] 09:59:53 [SUCCESS] ljishen@10.10.2.5
195906

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:59:53,237716798-07:00] INFO: > Run rados bench[0m
# ./benchmarks/bench-rados:370 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/output/rados_bench_16MB_write.log.3
## ./benchmarks/bench-rados:361 - rados_bench() > count_cpus 32-63,96-127
# ./benchmarks/bench-rados:361 - rados_bench() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 60 write --pool bench_rados -b 16777216 -O 16777216 --concurrent-ios 128 --show-time --write-object --write-omap --write-xattr --no-cleanup
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:59:53,258308801-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T09:59:53,261458446-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'b4e7b2d0-380f-11ec-b51d-53e6e728d2d3', '--', 'rados', 'bench', '60', 'write', '--pool', 'bench_rados', '-b', '16777216', '-O', '16777216', '--concurrent-ios', '128', '--show-time', '--write-object', '--write-omap', '--write-xattr', '--no-cleanup'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid b4e7b2d0-380f-11ec-b51d-53e6e728d2d3 -- rados bench 60 write --pool bench_rados -b 16777216 -O 16777216 --concurrent-ios 128 --show-time --write-object --write-omap --write-xattr --no-cleanup
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
hints = 1
2021-10-28T16:59:56.240594+0000 Maintaining 128 concurrent writes of 16777216 bytes to objects of size 16777216 for up to 60 seconds or 0 objects
2021-10-28T16:59:56.240628+0000 Object prefix: benchmark_data_node-2.bluefield2.ucsc-cmps10_7
2021-10-28T16:59:56.916055+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-28T16:59:56.916055+0000     0       0         0         0         0         0           -           0
2021-10-28T16:59:57.916147+0000     1       5         5         0         0         0           -           0
2021-10-28T16:59:58.916218+0000     2       9         9         0         0         0           -           0
2021-10-28T16:59:59.916289+0000     3      14        14         0         0         0           -           0
2021-10-28T17:00:00.916364+0000     4      18        18         0         0         0           -           0
2021-10-28T17:00:01.916445+0000     5      23        23         0         0         0           -           0
2021-10-28T17:00:02.916518+0000     6      28        28         0         0         0           -           0
2021-10-28T17:00:03.916595+0000     7      32        32         0         0         0           -           0
2021-10-28T17:00:04.916677+0000     8      35        35         0         0         0           -           0
2021-10-28T17:00:05.916758+0000     9      37        37         0         0         0           -           0
2021-10-28T17:00:06.916833+0000    10      40        40         0         0         0           -           0
2021-10-28T17:00:07.916904+0000    11      42        42         0         0         0           -           0
2021-10-28T17:00:08.916981+0000    12      44        44         0         0         0           -           0
2021-10-28T17:00:09.917056+0000    13      48        48         0         0         0           -           0
2021-10-28T17:00:10.917133+0000    14      52        52         0         0         0           -           0
2021-10-28T17:00:11.917202+0000    15      57        57         0         0         0           -           0
2021-10-28T17:00:12.917285+0000    16      61        61         0         0         0           -           0
2021-10-28T17:00:13.917355+0000    17      66        66         0         0         0           -           0
2021-10-28T17:00:14.917441+0000    18      67        67         0         0         0           -           0
2021-10-28T17:00:15.917516+0000    19      69        69         0         0         0           -           0
2021-10-28T17:00:16.917592+0000 min lat: 9999 max lat: 0 avg lat: 0
2021-10-28T17:00:16.917592+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-28T17:00:16.917592+0000    20      70        70         0         0         0           -           0
2021-10-28T17:00:17.917695+0000    21      70        70         0         0         0           -           0
2021-10-28T17:00:18.917761+0000    22      72        72         0         0         0           -           0
2021-10-28T17:00:19.917834+0000    23      73        73         0         0         0           -           0
2021-10-28T17:00:20.917913+0000    24      75        75         0         0         0           -           0
2021-10-28T17:00:21.917989+0000    25      78        78         0         0         0           -           0
2021-10-28T17:00:22.918055+0000    26      81        81         0         0         0           -           0
2021-10-28T17:00:23.918139+0000    27      83        83         0         0         0           -           0
2021-10-28T17:00:24.918212+0000    28      86        86         0         0         0           -           0
2021-10-28T17:00:25.918288+0000    29      89        89         0         0         0           -           0
2021-10-28T17:00:26.918354+0000    30      92        92         0         0         0           -           0
2021-10-28T17:00:27.918421+0000    31      95        95         0         0         0           -           0
2021-10-28T17:00:28.918499+0000    32      97        97         0         0         0           -           0
2021-10-28T17:00:29.918583+0000    33      99        99         0         0         0           -           0
2021-10-28T17:00:30.918680+0000    34     100       100         0         0         0           -           0
2021-10-28T17:00:31.918777+0000    35     101       101         0         0         0           -           0
2021-10-28T17:00:32.918882+0000    36     102       102         0         0         0           -           0
2021-10-28T17:00:33.918961+0000    37     104       104         0         0         0           -           0
2021-10-28T17:00:34.919041+0000    38     107       107         0         0         0           -           0
2021-10-28T17:00:35.919112+0000    39     111       111         0         0         0           -           0
2021-10-28T17:00:36.919180+0000 min lat: 9999 max lat: 0 avg lat: 0
2021-10-28T17:00:36.919180+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-28T17:00:36.919180+0000    40     115       115         0         0         0           -           0
2021-10-28T17:00:37.919254+0000    41     120       120         0         0         0           -           0
2021-10-28T17:00:38.919330+0000    42     125       125         0         0         0           -           0
2021-10-28T17:00:39.919418+0000    43     127       127         0         0         0           -           0
2021-10-28T17:00:40.919487+0000    44     127       130         3   1.09083   1.09091     43.8933     43.5489
2021-10-28T17:00:41.919558+0000    45     127       131         4   1.42211        16     44.4126     43.7648
2021-10-28T17:00:42.919625+0000    46     127       132         5     1.739        16     45.1836     44.0486
2021-10-28T17:00:43.919703+0000    47     127       133         6    2.0424        16     45.9833      44.371
2021-10-28T17:00:44.919785+0000    48     127       134         7   2.33315        16     46.5173     44.6776
2021-10-28T17:00:45.919869+0000    49     127       135         8   2.61204        16     47.2976     45.0051
2021-10-28T17:00:46.919937+0000    50     127       138        11   3.51973        48     47.8578     45.7331
2021-10-28T17:00:47.920018+0000    51     127       140        13   4.07812        32     48.2376     46.1008
2021-10-28T17:00:48.920097+0000    52     127       143        16    4.9227        48     48.5379     46.5429
2021-10-28T17:00:49.920173+0000    53     127       146        19   5.73541        48     48.9359     46.8987
2021-10-28T17:00:50.920252+0000    54     127       149        22   6.51802        48     49.3572     47.2125
2021-10-28T17:00:51.920330+0000    55     127       152        25   7.27217        48     49.8419     47.5146
2021-10-28T17:00:52.920397+0000    56     127       155        28   7.99939        48     50.2718     47.7935
2021-10-28T17:00:53.920490+0000    57     127       157        30    8.4204        32     50.5932     47.9746
2021-10-28T17:00:54.920558+0000    58     127       159        32   8.82691        32     51.3897     48.1674
2021-10-28T17:00:55.920643+0000    59     127       161        34   9.21963        32     51.6133     48.3676
2021-10-28T17:00:56.920758+0000 min lat: 43.1624 max lat: 51.7411 avg lat: 48.464
2021-10-28T17:00:56.920758+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-28T17:00:56.920758+0000    60     127       162        35   9.33261        16     51.7411      48.464
2021-10-28T17:00:57.920866+0000    61       2       163       161   42.2262      2016      1.6733     32.1248
2021-10-28T17:00:58.920983+0000    62       1       163       162   41.8032        16      2.3319     31.9409
2021-10-28T17:00:59.921126+0000 Total time run:         62.0502
Total writes made:      163
Write size:             16777216
Object size:            16777216
Bandwidth (MB/sec):     42.0305
Stddev Bandwidth:       255.417
Max bandwidth (MB/sec): 2016
Min bandwidth (MB/sec): 0
Average IOPS:           2
Stddev IOPS:            15.9637
Max IOPS:               126
Min IOPS:               0
Average Latency(s):     31.7635
Stddev Latency(s):      16.1044
Max latency(s):         52.3344
Min latency(s):         1.6733

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T10:01:00,516256060-07:00] INFO: > Stop system activity collection services[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T10:01:00,523016960-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:391 - rados_bench() > kill -INT 517762

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T10:01:00,529453368-07:00] INFO: >> for OSD host[0m
# ./benchmarks/bench-rados:396 - rados_bench() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.5 kill -INT 195906
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T10:01:00,537528004-07:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.5 kill -INT 195906
[1] 10:01:02 [SUCCESS] ljishen@10.10.2.5

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T10:01:02,461569951-07:00] INFO: >>> Copy system activity file from OSD host[0m
## ./benchmarks/bench-rados:403 - rados_bench() > basename -- /tmp/bench-rados/sys_activity_16MB_write.dat.osd_host.3
# ./benchmarks/bench-rados:403 - rados_bench() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.5:/tmp/bench-rados/sys_activity_16MB_write.dat.osd_host.3 /mnt/sda8/ceph-research/scripts/output/sys_activity_16MB_write.dat.osd_host.3


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T10:01:03,835418695-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T10:01:27,627701799-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 164 objects, 2.5 GiB
    usage:   7.8 GiB used, 92 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T10:01:27,636400950-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T10:01:36,611147196-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 164 objects, 2.5 GiB
    usage:   7.8 GiB used, 92 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T10:01:36,619861266-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T10:01:45,432637603-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 164 objects, 2.5 GiB
    usage:   7.8 GiB used, 92 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T10:01:45,441382992-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T10:01:54,331527230-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 164 objects, 2.5 GiB
    usage:   7.8 GiB used, 92 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T10:01:54,340339947-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T10:02:03,256915642-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 164 objects, 2.5 GiB
    usage:   7.8 GiB used, 92 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T10:02:03,265891706-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T10:02:03,272621869-07:00] INFO: > The cluster is idle now.[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T10:02:03,276641343-07:00] STAGE: Start benchmarking of mode seq ...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T10:02:03,284546479-07:00] INFO: > Start system activity collection services[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T10:02:03,290788852-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:341 - rados_bench() > rm --force /mnt/sda8/ceph-research/scripts/output/sys_activity_16MB_seq.dat.rados_bench_host.3
# ./benchmarks/bench-rados:342 - rados_bench() > sar_pid_rados_bench_host=519104
# ./benchmarks/bench-rados:342 - rados_bench() > S_TIME_FORMAT=ISO
# ./benchmarks/bench-rados:342 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/output/sys_activity_16MB_seq.dat.rados_bench_host.3 2

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T10:02:03,298527454-07:00] INFO: >> for OSD host[0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T10:02:03,307915815-07:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.5 bash
10.10.2.5: b'196462\n'
[1] 10:02:04 [SUCCESS] ljishen@10.10.2.5
196462

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T10:02:04,438507648-07:00] INFO: > Run rados bench[0m
# ./benchmarks/bench-rados:379 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/output/rados_bench_16MB_seq.log.3
## ./benchmarks/bench-rados:376 - rados_bench() > count_cpus 32-63,96-127
# ./benchmarks/bench-rados:376 - rados_bench() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 128 --show-time
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T10:02:04,459436075-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T10:02:04,462389621-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'b4e7b2d0-380f-11ec-b51d-53e6e728d2d3', '--', 'rados', 'bench', '99999999', 'seq', '--pool', 'bench_rados', '--concurrent-ios', '128', '--show-time'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid b4e7b2d0-380f-11ec-b51d-53e6e728d2d3 -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 128 --show-time
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
hints = 1
2021-10-28T17:02:07.433188+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-28T17:02:07.433188+0000     0       0         0         0         0         0           -           0
2021-10-28T17:02:08.433305+0000     1      46        46         0         0         0           -           0
2021-10-28T17:02:09.433378+0000     2      92        92         0         0         0           -           0
2021-10-28T17:02:10.433452+0000     3     127       139        12   63.9921        64     2.83344     2.83313
2021-10-28T17:02:11.433548+0000 Total time run:       3.6825
Total reads made:     163
Read size:            16777216
Object size:          16777216
Bandwidth (MB/sec):   708.215
Average IOPS:         44
Stddev IOPS:          2.3094
Max IOPS:             4
Min IOPS:             0
Average Latency(s):   1.75297
Max latency(s):       2.9156
Min latency(s):       0.233138

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T10:02:12,244927381-07:00] INFO: > Stop system activity collection services[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T10:02:12,251801314-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:391 - rados_bench() > kill -INT 519104

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T10:02:12,258228726-07:00] INFO: >> for OSD host[0m
# ./benchmarks/bench-rados:396 - rados_bench() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.5 kill -INT 196462
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T10:02:12,266643382-07:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.5 kill -INT 196462
[1] 10:02:13 [SUCCESS] ljishen@10.10.2.5

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T10:02:13,381152040-07:00] INFO: >>> Copy system activity file from OSD host[0m
## ./benchmarks/bench-rados:403 - rados_bench() > basename -- /tmp/bench-rados/sys_activity_16MB_seq.dat.osd_host.3
# ./benchmarks/bench-rados:403 - rados_bench() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.5:/tmp/bench-rados/sys_activity_16MB_seq.dat.osd_host.3 /mnt/sda8/ceph-research/scripts/output/sys_activity_16MB_seq.dat.osd_host.3


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T10:02:14,464638559-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T10:02:38,298061783-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 164 objects, 2.5 GiB
    usage:   7.8 GiB used, 92 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T10:02:38,306827521-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T10:02:47,155189256-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 164 objects, 2.5 GiB
    usage:   7.8 GiB used, 92 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T10:02:47,164029605-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T10:02:55,887060774-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 164 objects, 2.5 GiB
    usage:   7.8 GiB used, 92 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T10:02:55,895391441-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T10:03:04,853591896-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 164 objects, 2.5 GiB
    usage:   7.8 GiB used, 92 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T10:03:04,862520380-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T10:03:13,868583708-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 164 objects, 2.5 GiB
    usage:   7.8 GiB used, 92 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T10:03:13,877204302-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T10:03:13,884383220-07:00] INFO: > The cluster is idle now.[0m
