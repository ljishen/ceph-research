[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:20:41,711011613-07:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.1 mkdir --parents /tmp/bench-rados
[1] 01:20:41 [SUCCESS] ljishen@10.10.2.1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:20:41,899107365-07:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.2 mkdir --parents /tmp/bench-rados
[1] 01:20:42 [SUCCESS] ljishen@10.10.2.2
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:20:42,095062016-07:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.2 bash
[1] 01:20:42 [SUCCESS] ljishen@10.10.2.2


[1;7;39;49m[2021-10-28T01:20:42,283071136-07:00][RUNNING][ROUND 1/1/21] object_size=4KB[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:20:42,285828121-07:00] STAGE: Launch a Ceph cluster (CEPH_HOST: 10.10.2.1, OSD_HOST: 10.10.2.2) ...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:20:42,290278537-07:00] INFO: > Get OSD hostname[0m
## ./benchmarks/bench-rados:178 - launch_ceph_cluster() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.2 uname --nodename
## ./benchmarks/bench-rados:178 - launch_ceph_cluster() > tail -n1
# ./benchmarks/bench-rados:178 - launch_ceph_cluster() > OSD_HOSTNAME=node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us
# ./benchmarks/bench-rados:183 - launch_ceph_cluster() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.1 bash -euo pipefail
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:20:42,485453029-07:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.1 bash -euo pipefail
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T01:20:42,893317047-07:00] INFO: > Remove existing clusters\x1b[0m\n'
10.10.2.1: b'## bash:17 -  > basename -- /var/lib/ceph/8506a566-37c0-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'# bash:17 -  > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm --host 10.10.2.1 --host 10.10.2.2 rm-cluster --force --fsid 8506a566-37c0-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T01:20:42,904853293-07:00] DEBUG: HOST_PARAMS: ['--host', '10.10.2.1', '--host', '10.10.2.2']\x1b[0m\n"
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T01:20:42,909021118-07:00] DEBUG: CEPHADM_PARAMS: ['rm-cluster', '--force', '--fsid', '8506a566-37c0-11ec-b51d-53e6e728d2d3']\x1b[0m\n"
10.10.2.1: b"# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:63 -  > /tmp/bench-rados/ceph-research/scripts/parallel-ssh --host 10.10.2.1 --host 10.10.2.2 'sudo python3 -u - rm-cluster --force --fsid 8506a566-37c0-11ec-b51d-53e6e728d2d3'\n"
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T01:20:42,917386744-07:00] DEBUG: command from stdin\x1b[0m\n'
10.10.2.1: b"# /tmp/bench-rados/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host 10.10.2.1 --host 10.10.2.2 'sudo python3 -u - rm-cluster --force --fsid 8506a566-37c0-11ec-b51d-53e6e728d2d3'\n"
10.10.2.1: b'[1] 01:20:48 [SUCCESS] 10.10.2.1\n[2] 01:20:52 [SUCCESS] 10.10.2.2\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T01:20:52,392411383-07:00] INFO: > Deploy a new cluster\x1b[0m\n'
10.10.2.1: b'# bash:24 -  > /tmp/bench-rados/ceph-research/scripts/ceph-deploy -m 10.10.2.1 -o node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/10.10.2.2:/dev/nvme0n1\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T01:20:52,404945204-07:00] STAGE: Verify passwordless SSH to hosts, passwordless sudo via ljishen, and the existence of devices on hosts...\x1b[0m\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T01:20:52,410124731-07:00] INFO: > Check the host for the monitor\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:181 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@10.10.2.1 sudo --non-interactive --validate\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T01:20:52,558795343-07:00] INFO: > Check hosts for OSDs\x1b[0m\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T01:20:52,563848272-07:00] INFO: >> Check host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:200 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us sudo --non-interactive --validate\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T01:20:52,714707318-07:00] INFO: >>> Check device '/dev/nvme0n1'\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:215 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us test -b /dev/nvme0n1\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:227 - verify_parameters() > grep --word-regexp disk\n# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:226 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us lsblk --nodeps --noheadings --output TYPE /dev/nvme0n1\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T01:20:52,998849420-07:00] STAGE: Reset devices on hosts...\x1b[0m\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T01:20:53,004100751-07:00] INFO: > Resetting device '/dev/nvme0n1' on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:288 - reset_host_devices() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us sudo sh\n'
10.10.2.1: b'  Removing ceph--c0a786bd--ec44--410b--86be--bc3a0250fc4c-osd--block--12820196--890c--496b--b1cf--308c44bf4d8c (253:0)\n'
10.10.2.1: b'  Archiving volume group "ceph-c0a786bd-ec44-410b-86be-bc3a0250fc4c" metadata (seqno 5).\n'
10.10.2.1: b'  Releasing logical volume "osd-block-12820196-890c-496b-b1cf-308c44bf4d8c"\n'
10.10.2.1: b'  Creating volume group backup "/etc/lvm/backup/ceph-c0a786bd-ec44-410b-86be-bc3a0250fc4c" (seqno 6).\n'
10.10.2.1: b'  Logical volume "osd-block-12820196-890c-496b-b1cf-308c44bf4d8c" successfully removed\n'
10.10.2.1: b'  Removing physical volume "/dev/nvme0n1" from volume group "ceph-c0a786bd-ec44-410b-86be-bc3a0250fc4c"\n'
10.10.2.1: b'  Volume group "ceph-c0a786bd-ec44-410b-86be-bc3a0250fc4c" successfully removed\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T01:20:53,301781854-07:00] STAGE: Bootstrap a new cluster...\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:317 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.1 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T01:20:53,312139014-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T01:20:53,316190089-07:00] DEBUG: CEPHADM_PARAMS: ['bootstrap', '--config', '/tmp/bench-rados/ceph-research/scripts/ceph.conf', '--mon-ip', '10.10.2.1', '--output-dir', '/tmp/bench-rados/deployment_data_root/etc/ceph', '--ssh-user', 'ljishen', '--skip-dashboard', '--no-minimize-config', '--allow-overwrite', '--allow-fqdn-hostname', '--orphan-initial-daemons', '--skip-monitoring-stack', '--single-host-defaults']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.1 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults\n'
10.10.2.1: b'Verifying podman|docker is present...\nVerifying lvm2 is present...\n'
10.10.2.1: b'Verifying time synchronization is in place...\n'
10.10.2.1: b'Unit ntp.service is enabled and running\n'
10.10.2.1: b'Repeating the final host check...\n'
10.10.2.1: b'podman|docker (/usr/bin/docker) is present\n'
10.10.2.1: b'systemctl is present\n'
10.10.2.1: b'lvcreate is present\n'
10.10.2.1: b'Unit ntp.service is enabled and running\n'
10.10.2.1: b'Host looks OK\n'
10.10.2.1: b'Cluster fsid: f7b2a0f4-37c7-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Verifying IP 10.10.2.1 port 3300 ...\n'
10.10.2.1: b'Verifying IP 10.10.2.1 port 6789 ...\n'
10.10.2.1: b'Mon IP `10.10.2.1` is in CIDR network `10.10.2.0/24`\n'
10.10.2.1: b'- internal network (--cluster-network) has not been provided, OSD replication will default to the public_network\n'
10.10.2.1: b'Adjusting default settings to suit single-host cluster...\n'
10.10.2.1: b'Pulling container image quay.io/ceph/ceph:v16...\n'
10.10.2.1: b'Ceph version: ceph version 16.2.6 (ee28fb57e47e9f88813e24bbf4c14496ca299d31) pacific (stable)\n'
10.10.2.1: b'Extracting ceph user uid/gid from container image...\n'
10.10.2.1: b'Creating initial keys...\n'
10.10.2.1: b'Creating initial monmap...\n'
10.10.2.1: b'Creating mon...\n'
10.10.2.1: b'Waiting for mon to start...\n'
10.10.2.1: b'Waiting for mon...\n'
10.10.2.1: b'mon is available\nSetting mon public_network to 10.10.2.0/24\n'
10.10.2.1: b'Wrote config to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf\n'
10.10.2.1: b'Wrote keyring to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring\n'
10.10.2.1: b'Creating mgr...\n'
10.10.2.1: b'Verifying port 9283 ...\n'
10.10.2.1: b'Waiting for mgr to start...\nWaiting for mgr...\n'
10.10.2.1: b'mgr not available, waiting (1/15)...\n'
10.10.2.1: b'mgr not available, waiting (2/15)...\n'
10.10.2.1: b'mgr is available\n'
10.10.2.1: b'Enabling cephadm module...\n'
10.10.2.1: b'Waiting for the mgr to restart...\n'
10.10.2.1: b'Waiting for mgr epoch 5...\n'
10.10.2.1: b'mgr epoch 5 is available\nSetting orchestrator backend to cephadm...\n'
10.10.2.1: b'Generating ssh key...\n'
10.10.2.1: b'Wrote public SSH key to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub\n'
10.10.2.1: b'Adding key to ljishen@localhost authorized_keys...\n'
10.10.2.1: b'Adding host node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us...\n'
10.10.2.1: b'Deploying unmanaged mon service...\n'
10.10.2.1: b'Deploying unmanaged mgr service...\n'
10.10.2.1: b'You can access the Ceph CLI with:\n\n\tsudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --fsid f7b2a0f4-37c7-11ec-b51d-53e6e728d2d3 -c /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf -k /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring\n\n'
10.10.2.1: b'Please consider enabling telemetry to help improve Ceph:\n\n\tceph telemetry on\n\nFor more information see:\n\n\thttps://docs.ceph.com/docs/pacific/mgr/telemetry/\n\n'
10.10.2.1: b'Bootstrap complete.\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T01:21:50,333414443-07:00] INFO: Sleep for 20 seconds\x1b[0m\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T01:22:10,341240943-07:00] INFO: > Set the default number of monitor daemons to 1\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:337 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T01:22:10,351469040-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T01:22:10,355300953-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mon', '1']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1\n'
10.10.2.1: b'Inferring fsid f7b2a0f4-37c7-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/f7b2a0f4-37c7-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b'Scheduled mon update...\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T01:22:18,964712484-07:00] INFO: > Set the default number of manager daemons to 1\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:342 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T01:22:18,974004992-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T01:22:18,978336835-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mgr', '1']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1\n'
10.10.2.1: b'Inferring fsid f7b2a0f4-37c7-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/f7b2a0f4-37c7-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b'Scheduled mgr update...\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T01:22:27,465270321-07:00] STAGE: Add hosts to the cluster...\x1b[0m\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T01:22:27,471551379-07:00] INFO: > Adding host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:361 - add_hosts() > ssh-copy-id -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -f -i /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\n'
10.10.2.1: b'/usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: "/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub"\n'
10.10.2.1: b'\nNumber of key(s) added: 1\n\nNow try logging into the machine, with:   "ssh -o \'GlobalKnownHostsFile=/dev/null\' -o \'LogLevel=ERROR\' -o \'PasswordAuthentication=no\' -o \'StrictHostKeyChecking=no\' -o \'UserKnownHostsFile=/dev/null\' \'ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\'"\nand check to make sure that only the key(s) you wanted were added.\n\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:367 - add_hosts() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us 10.10.2.2\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T01:22:27,681217982-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T01:22:27,684969704-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'host', 'add', 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us', '10.10.2.2']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us 10.10.2.2\n'
10.10.2.1: b'Inferring fsid f7b2a0f4-37c7-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/f7b2a0f4-37c7-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b"Added host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us' with addr '10.10.2.2'\n"
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T01:22:36,818314944-07:00] INFO: Sleep for 20 seconds\x1b[0m\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T01:22:56,823263034-07:00] STAGE: Add OSDs to the cluster...\x1b[0m\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T01:22:56,830242584-07:00] INFO: > Adding devices ['/dev/nvme0n1'] on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us' as OSDs\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:388 - add_host_devices_as_osds() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T01:22:56,840948509-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T01:22:56,844661458-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'daemon', 'add', 'osd', 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1\n'
10.10.2.1: b'Inferring fsid f7b2a0f4-37c7-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/f7b2a0f4-37c7-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b"Created osd(s) 0 on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'\n"
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T01:23:21,245188323-07:00] INFO: > Sleep for 20 seconds\x1b[0m\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T01:23:41,250544759-07:00] STAGE: Show Cluster Status\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:408 - show_status() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T01:23:41,260866472-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T01:23:41,264963313-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', '--status']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status\n'
10.10.2.1: b'Inferring fsid f7b2a0f4-37c7-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/f7b2a0f4-37c7-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b'  cluster:\n    id:     f7b2a0f4-37c7-11ec-b51d-53e6e728d2d3\n    health: HEALTH_WARN\n            1 pool(s) have no replicas configured\n \n  services:\n    mon: 1 daemons, quorum node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us (age 2m)\n    mgr: node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us.fxegxa(active, since 2m)\n    osd: 1 osds: 1 up (since 17s), 1 in (since 40s)\n \n  data:\n    pools:   1 pools, 1 pgs\n    objects: 0 objects, 0 B\n    usage:   4.7 MiB used, 100 GiB / 100 GiB avail\n    pgs:     1 active+clean\n \n'
[1] 01:23:49 [SUCCESS] ljishen@10.10.2.1

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:20:42,893317047-07:00] INFO: > Remove existing clusters[0m
## bash:17 -  > basename -- /var/lib/ceph/8506a566-37c0-11ec-b51d-53e6e728d2d3
# bash:17 -  > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm --host 10.10.2.1 --host 10.10.2.2 rm-cluster --force --fsid 8506a566-37c0-11ec-b51d-53e6e728d2d3
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:20:42,904853293-07:00] DEBUG: HOST_PARAMS: ['--host', '10.10.2.1', '--host', '10.10.2.2'][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:20:42,909021118-07:00] DEBUG: CEPHADM_PARAMS: ['rm-cluster', '--force', '--fsid', '8506a566-37c0-11ec-b51d-53e6e728d2d3'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:63 -  > /tmp/bench-rados/ceph-research/scripts/parallel-ssh --host 10.10.2.1 --host 10.10.2.2 'sudo python3 -u - rm-cluster --force --fsid 8506a566-37c0-11ec-b51d-53e6e728d2d3'
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:20:42,917386744-07:00] DEBUG: command from stdin[0m
# /tmp/bench-rados/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host 10.10.2.1 --host 10.10.2.2 'sudo python3 -u - rm-cluster --force --fsid 8506a566-37c0-11ec-b51d-53e6e728d2d3'
[1] 01:20:48 [SUCCESS] 10.10.2.1
[2] 01:20:52 [SUCCESS] 10.10.2.2

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:20:52,392411383-07:00] INFO: > Deploy a new cluster[0m
# bash:24 -  > /tmp/bench-rados/ceph-research/scripts/ceph-deploy -m 10.10.2.1 -o node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/10.10.2.2:/dev/nvme0n1


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:20:52,404945204-07:00] STAGE: Verify passwordless SSH to hosts, passwordless sudo via ljishen, and the existence of devices on hosts...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:20:52,410124731-07:00] INFO: > Check the host for the monitor[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:181 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@10.10.2.1 sudo --non-interactive --validate

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:20:52,558795343-07:00] INFO: > Check hosts for OSDs[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:20:52,563848272-07:00] INFO: >> Check host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:200 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us sudo --non-interactive --validate

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:20:52,714707318-07:00] INFO: >>> Check device '/dev/nvme0n1'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:215 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us test -b /dev/nvme0n1
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:227 - verify_parameters() > grep --word-regexp disk
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:226 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us lsblk --nodeps --noheadings --output TYPE /dev/nvme0n1


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:20:52,998849420-07:00] STAGE: Reset devices on hosts...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:20:53,004100751-07:00] INFO: > Resetting device '/dev/nvme0n1' on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:288 - reset_host_devices() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us sudo sh
  Removing ceph--c0a786bd--ec44--410b--86be--bc3a0250fc4c-osd--block--12820196--890c--496b--b1cf--308c44bf4d8c (253:0)
  Archiving volume group "ceph-c0a786bd-ec44-410b-86be-bc3a0250fc4c" metadata (seqno 5).
  Releasing logical volume "osd-block-12820196-890c-496b-b1cf-308c44bf4d8c"
  Creating volume group backup "/etc/lvm/backup/ceph-c0a786bd-ec44-410b-86be-bc3a0250fc4c" (seqno 6).
  Logical volume "osd-block-12820196-890c-496b-b1cf-308c44bf4d8c" successfully removed
  Removing physical volume "/dev/nvme0n1" from volume group "ceph-c0a786bd-ec44-410b-86be-bc3a0250fc4c"
  Volume group "ceph-c0a786bd-ec44-410b-86be-bc3a0250fc4c" successfully removed


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:20:53,301781854-07:00] STAGE: Bootstrap a new cluster...[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:317 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.1 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:20:53,312139014-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:20:53,316190089-07:00] DEBUG: CEPHADM_PARAMS: ['bootstrap', '--config', '/tmp/bench-rados/ceph-research/scripts/ceph.conf', '--mon-ip', '10.10.2.1', '--output-dir', '/tmp/bench-rados/deployment_data_root/etc/ceph', '--ssh-user', 'ljishen', '--skip-dashboard', '--no-minimize-config', '--allow-overwrite', '--allow-fqdn-hostname', '--orphan-initial-daemons', '--skip-monitoring-stack', '--single-host-defaults'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.1 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults
Verifying podman|docker is present...
Verifying lvm2 is present...
Verifying time synchronization is in place...
Unit ntp.service is enabled and running
Repeating the final host check...
podman|docker (/usr/bin/docker) is present
systemctl is present
lvcreate is present
Unit ntp.service is enabled and running
Host looks OK
Cluster fsid: f7b2a0f4-37c7-11ec-b51d-53e6e728d2d3
Verifying IP 10.10.2.1 port 3300 ...
Verifying IP 10.10.2.1 port 6789 ...
Mon IP `10.10.2.1` is in CIDR network `10.10.2.0/24`
- internal network (--cluster-network) has not been provided, OSD replication will default to the public_network
Adjusting default settings to suit single-host cluster...
Pulling container image quay.io/ceph/ceph:v16...
Ceph version: ceph version 16.2.6 (ee28fb57e47e9f88813e24bbf4c14496ca299d31) pacific (stable)
Extracting ceph user uid/gid from container image...
Creating initial keys...
Creating initial monmap...
Creating mon...
Waiting for mon to start...
Waiting for mon...
mon is available
Setting mon public_network to 10.10.2.0/24
Wrote config to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf
Wrote keyring to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring
Creating mgr...
Verifying port 9283 ...
Waiting for mgr to start...
Waiting for mgr...
mgr not available, waiting (1/15)...
mgr not available, waiting (2/15)...
mgr is available
Enabling cephadm module...
Waiting for the mgr to restart...
Waiting for mgr epoch 5...
mgr epoch 5 is available
Setting orchestrator backend to cephadm...
Generating ssh key...
Wrote public SSH key to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub
Adding key to ljishen@localhost authorized_keys...
Adding host node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us...
Deploying unmanaged mon service...
Deploying unmanaged mgr service...
You can access the Ceph CLI with:

	sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --fsid f7b2a0f4-37c7-11ec-b51d-53e6e728d2d3 -c /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf -k /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring

Please consider enabling telemetry to help improve Ceph:

	ceph telemetry on

For more information see:

	https://docs.ceph.com/docs/pacific/mgr/telemetry/

Bootstrap complete.

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:21:50,333414443-07:00] INFO: Sleep for 20 seconds[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:22:10,341240943-07:00] INFO: > Set the default number of monitor daemons to 1[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:337 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:22:10,351469040-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:22:10,355300953-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mon', '1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1
Inferring fsid f7b2a0f4-37c7-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/f7b2a0f4-37c7-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Scheduled mon update...

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:22:18,964712484-07:00] INFO: > Set the default number of manager daemons to 1[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:342 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:22:18,974004992-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:22:18,978336835-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mgr', '1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1
Inferring fsid f7b2a0f4-37c7-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/f7b2a0f4-37c7-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Scheduled mgr update...


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:22:27,465270321-07:00] STAGE: Add hosts to the cluster...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:22:27,471551379-07:00] INFO: > Adding host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:361 - add_hosts() > ssh-copy-id -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -f -i /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us
/usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: "/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub"

Number of key(s) added: 1

Now try logging into the machine, with:   "ssh -o 'GlobalKnownHostsFile=/dev/null' -o 'LogLevel=ERROR' -o 'PasswordAuthentication=no' -o 'StrictHostKeyChecking=no' -o 'UserKnownHostsFile=/dev/null' 'ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'"
and check to make sure that only the key(s) you wanted were added.

# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:367 - add_hosts() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us 10.10.2.2
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:22:27,681217982-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:22:27,684969704-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'host', 'add', 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us', '10.10.2.2'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us 10.10.2.2
Inferring fsid f7b2a0f4-37c7-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/f7b2a0f4-37c7-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Added host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us' with addr '10.10.2.2'

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:22:36,818314944-07:00] INFO: Sleep for 20 seconds[0m


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:22:56,823263034-07:00] STAGE: Add OSDs to the cluster...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:22:56,830242584-07:00] INFO: > Adding devices ['/dev/nvme0n1'] on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us' as OSDs[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:388 - add_host_devices_as_osds() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:22:56,840948509-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:22:56,844661458-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'daemon', 'add', 'osd', 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1
Inferring fsid f7b2a0f4-37c7-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/f7b2a0f4-37c7-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Created osd(s) 0 on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:23:21,245188323-07:00] INFO: > Sleep for 20 seconds[0m


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:23:41,250544759-07:00] STAGE: Show Cluster Status[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:408 - show_status() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:23:41,260866472-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:23:41,264963313-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', '--status'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status
Inferring fsid f7b2a0f4-37c7-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/f7b2a0f4-37c7-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
  cluster:
    id:     f7b2a0f4-37c7-11ec-b51d-53e6e728d2d3
    health: HEALTH_WARN
            1 pool(s) have no replicas configured
 
  services:
    mon: 1 daemons, quorum node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us (age 2m)
    mgr: node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us.fxegxa(active, since 2m)
    osd: 1 osds: 1 up (since 17s), 1 in (since 40s)
 
  data:
    pools:   1 pools, 1 pgs
    objects: 0 objects, 0 B
    usage:   4.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     1 active+clean
 

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:23:49,652242655-07:00] INFO: > Copy configuration files to local[0m
# ./benchmarks/bench-rados:220 - launch_ceph_cluster() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.1 sudo chown --changes --recursive ljishen /tmp/bench-rados/deployment_data_root/etc/ceph
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:23:49,659756524-07:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.1 sudo chown --changes --recursive ljishen /tmp/bench-rados/deployment_data_root/etc/ceph
[1] 01:23:49 [SUCCESS] ljishen@10.10.2.1
# ./benchmarks/bench-rados:223 - launch_ceph_cluster() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.1:/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf
# ./benchmarks/bench-rados:226 - launch_ceph_cluster() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.1:/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:23:50,132261149-07:00] STAGE: Update local ceph.conf as a client...[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:23:50,134711326-07:00] STAGE: Configure Ceph pool...[0m
# ./benchmarks/bench-rados:257 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_max_backfills 32
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:23:50,157506791-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:23:50,160508878-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'f7b2a0f4-37c7-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'config', 'set', 'osd', 'osd_max_backfills', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid f7b2a0f4-37c7-11ec-b51d-53e6e728d2d3 -- ceph config set osd osd_max_backfills 32
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:258 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_active 32
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:23:54,486034738-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:23:54,489199402-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'f7b2a0f4-37c7-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_active', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid f7b2a0f4-37c7-11ec-b51d-53e6e728d2d3 -- ceph config set osd osd_recovery_max_active 32
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:259 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_single_start 8
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:23:58,752023809-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:23:58,754879801-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'f7b2a0f4-37c7-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_single_start', '8'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid f7b2a0f4-37c7-11ec-b51d-53e6e728d2d3 -- ceph config set osd osd_recovery_max_single_start 8
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:260 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_op_priority 63
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:24:02,952860619-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:24:02,955912460-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'f7b2a0f4-37c7-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_op_priority', '63'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid f7b2a0f4-37c7-11ec-b51d-53e6e728d2d3 -- ceph config set osd osd_recovery_op_priority 63
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:262 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config show-with-defaults osd.0
# ./benchmarks/bench-rados:263 - configure_ceph_pool() > grep 'osd_max_backfills\|osd_recovery'
# ./benchmarks/bench-rados:263 - configure_ceph_pool() > column -t -s ' '
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
osd_max_backfills                        32         mon
osd_recovery_cost                        20971520   default
osd_recovery_delay_start                 0.000000   default
osd_recovery_max_active                  32         mon
osd_recovery_max_active_hdd              3          default
osd_recovery_max_active_ssd              10         default
osd_recovery_max_chunk                   8388608    default
osd_recovery_max_omap_entries_per_chunk  8096       default
osd_recovery_max_single_start            8          mon
osd_recovery_op_priority                 63         mon
osd_recovery_op_warn_multiple            16         default
osd_recovery_priority                    5          default
osd_recovery_retry_interval              30.000000  default
osd_recovery_sleep                       0.000000   default
osd_recovery_sleep_hdd                   0.100000   default
osd_recovery_sleep_hybrid                0.025000   default
osd_recovery_sleep_ssd                   0.000000   default
# ./benchmarks/bench-rados:265 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool create bench_rados 128 128 replicated --size 1 --pg-num-min 128
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:24:11,736694751-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:24:11,739801375-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'f7b2a0f4-37c7-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'create', 'bench_rados', '128', '128', 'replicated', '--size', '1', '--pg-num-min', '128'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid f7b2a0f4-37c7-11ec-b51d-53e6e728d2d3 -- ceph osd pool create bench_rados 128 128 replicated --size 1 --pg-num-min 128
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
pool 'bench_rados' created
# ./benchmarks/bench-rados:267 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados min_size 1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:24:16,634812457-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:24:16,637857085-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'f7b2a0f4-37c7-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'min_size', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid f7b2a0f4-37c7-11ec-b51d-53e6e728d2d3 -- ceph osd pool set bench_rados min_size 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 min_size to 1
# ./benchmarks/bench-rados:268 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados pg_autoscale_mode off
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:24:21,149826595-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:24:21,153070247-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'f7b2a0f4-37c7-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'pg_autoscale_mode', 'off'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid f7b2a0f4-37c7-11ec-b51d-53e6e728d2d3 -- ceph osd pool set bench_rados pg_autoscale_mode off
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 pg_autoscale_mode to off
# ./benchmarks/bench-rados:269 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool application enable bench_rados benchmark
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:24:25,325198459-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:24:25,328430420-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'f7b2a0f4-37c7-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'application', 'enable', 'bench_rados', 'benchmark'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid f7b2a0f4-37c7-11ec-b51d-53e6e728d2d3 -- ceph osd pool application enable bench_rados benchmark
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
enabled application 'benchmark' on pool 'bench_rados'
# ./benchmarks/bench-rados:270 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados noscrub 1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:24:28,534540514-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:24:28,537688857-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'f7b2a0f4-37c7-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'noscrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid f7b2a0f4-37c7-11ec-b51d-53e6e728d2d3 -- ceph osd pool set bench_rados noscrub 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 noscrub to 1
# ./benchmarks/bench-rados:271 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados nodeep-scrub 1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:24:32,149432386-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:24:32,152367126-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'f7b2a0f4-37c7-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'nodeep-scrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid f7b2a0f4-37c7-11ec-b51d-53e6e728d2d3 -- ceph osd pool set bench_rados nodeep-scrub 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 nodeep-scrub to 1
# ./benchmarks/bench-rados:272 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool ls detail
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:24:35,715697207-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:24:35,718520266-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'f7b2a0f4-37c7-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'ls', 'detail'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid f7b2a0f4-37c7-11ec-b51d-53e6e728d2d3 -- ceph osd pool ls detail
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
pool 1 'device_health_metrics' replicated size 1 min_size 1 crush_rule 0 object_hash rjenkins pg_num 64 pgp_num 64 autoscale_mode on last_change 20 lfor 0/0/17 flags hashpspool stripe_width 0 pg_num_min 1 application mgr_devicehealth
pool 2 'bench_rados' replicated size 1 min_size 1 crush_rule 0 object_hash rjenkins pg_num 128 pgp_num 128 autoscale_mode off last_change 19 flags hashpspool,noscrub,nodeep-scrub stripe_width 0 pg_num_min 128 application benchmark

# ./benchmarks/bench-rados:273 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd df tree
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:24:40,198409946-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:24:40,201377107-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'f7b2a0f4-37c7-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'df', 'tree'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid f7b2a0f4-37c7-11ec-b51d-53e6e728d2d3 -- ceph osd df tree
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
ID  CLASS  WEIGHT   REWEIGHT  SIZE     RAW USE  DATA     OMAP  META     AVAIL    %USE  VAR   PGS  STATUS  TYPE NAME      
-1         0.09769         -  100 GiB  5.7 MiB  168 KiB   0 B  5.6 MiB  100 GiB  0.01  1.00    -          root default   
-3         0.09769         -  100 GiB  5.7 MiB  168 KiB   0 B  5.6 MiB  100 GiB  0.01  1.00    -              host node-1
 0    ssd  0.09769   1.00000  100 GiB  5.7 MiB  168 KiB   0 B  5.6 MiB  100 GiB  0.01  1.00  192      up          osd.0  
                       TOTAL  100 GiB  5.7 MiB  168 KiB   0 B  5.6 MiB  100 GiB  0.01                                    
MIN/MAX VAR: 1.00/1.00  STDDEV: 0


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:24:44,506486897-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:25:08,884905805-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:25:18,282652449-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:25:27,365370142-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:25:36,771265833-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:25:36,777473751-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:25:46,329429934-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:25:46,335270811-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:25:55,691577347-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:25:55,697631826-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:26:05,041619691-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:26:05,047795408-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:26:14,323225153-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:26:14,329537768-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:26:14,333979869-07:00] INFO: > The cluster is idle now.[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:26:14,336733157-07:00] STAGE: Start benchmarking of mode write ...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:26:14,341699556-07:00] INFO: > Start system activity collection services[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:26:14,345875695-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:341 - rados_bench() > rm --force /mnt/sda8/ceph-research/scripts/output/sys_activity_4KB_write.dat.rados_bench_host.1
# ./benchmarks/bench-rados:342 - rados_bench() > sar_pid_rados_bench_host=208980
# ./benchmarks/bench-rados:342 - rados_bench() > S_TIME_FORMAT=ISO
# ./benchmarks/bench-rados:342 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/output/sys_activity_4KB_write.dat.rados_bench_host.1 2

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:26:14,351183447-07:00] INFO: >> for OSD host[0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:26:14,359376747-07:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.2 bash
10.10.2.2: b'126902\n'
[1] 01:26:14 [SUCCESS] ljishen@10.10.2.2
126902

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:26:14,549379141-07:00] INFO: > Run rados bench[0m
# ./benchmarks/bench-rados:370 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/output/rados_bench_4KB_write.log.1
## ./benchmarks/bench-rados:361 - rados_bench() > count_cpus 32-63,96-127
# ./benchmarks/bench-rados:361 - rados_bench() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 60 write --pool bench_rados -b 4096 -O 4096 --concurrent-ios 128 --show-time --write-object --write-omap --write-xattr --no-cleanup
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:26:14,568771484-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:26:14,571527177-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'f7b2a0f4-37c7-11ec-b51d-53e6e728d2d3', '--', 'rados', 'bench', '60', 'write', '--pool', 'bench_rados', '-b', '4096', '-O', '4096', '--concurrent-ios', '128', '--show-time', '--write-object', '--write-omap', '--write-xattr', '--no-cleanup'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid f7b2a0f4-37c7-11ec-b51d-53e6e728d2d3 -- rados bench 60 write --pool bench_rados -b 4096 -O 4096 --concurrent-ios 128 --show-time --write-object --write-omap --write-xattr --no-cleanup
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
hints = 1
2021-10-28T08:26:18.015353+0000 Maintaining 128 concurrent writes of 4096 bytes to objects of size 4096 for up to 60 seconds or 0 objects
2021-10-28T08:26:18.015364+0000 Object prefix: benchmark_data_node-2.bluefield2.ucsc-cmps10_7
2021-10-28T08:26:18.015873+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-28T08:26:18.015873+0000     0       0         0         0         0         0           -           0
2021-10-28T08:26:19.015970+0000     1     127     22729     22602   88.2839   88.2891  0.00670787  0.00563761
2021-10-28T08:26:20.016071+0000     2     127     44838     44711   87.3192   86.3633  0.00546146  0.00571533
2021-10-28T08:26:21.016157+0000     3     128     65915     65787   85.6531   82.3281  0.00476738  0.00582791
2021-10-28T08:26:22.016260+0000     4     127     85118     84991   82.9918   75.0156   0.0067044   0.0060192
2021-10-28T08:26:23.016376+0000     5     128    104829    104701   81.7901   76.9922  0.00584038  0.00610789
2021-10-28T08:26:24.016473+0000     6     128    120039    119911   78.0597   59.4141  0.00656647  0.00639719
2021-10-28T08:26:25.016579+0000     7     128    133483    133355   74.4098   52.5156  0.00946584  0.00671263
2021-10-28T08:26:26.016667+0000     8     128    151287    151159   73.8011   69.5469  0.00479673  0.00677021
2021-10-28T08:26:27.016751+0000     9     127    171773    171646   74.4922   80.0273  0.00638957  0.00670836
2021-10-28T08:26:28.016836+0000    10     127    192640    192513   75.1934   81.5117  0.00654169  0.00664585
2021-10-28T08:26:29.016920+0000    11     127    213611    213484   75.8041    81.918  0.00745303  0.00659197
2021-10-28T08:26:30.017014+0000    12     127    219212    219085   71.3102   21.8789   0.0291376   0.0069961
2021-10-28T08:26:31.017104+0000    13     128    221318    221190   66.4572   8.22266   0.0583217  0.00750179
2021-10-28T08:26:32.017183+0000    14     128    231053    230925   64.4263   38.0273  0.00926782  0.00775603
2021-10-28T08:26:33.017270+0000    15     128    244237    244109   63.5643      51.5   0.0104151  0.00786119
2021-10-28T08:26:34.017360+0000    16     127    261423    261296   63.7872   67.1367  0.00441675  0.00783572
2021-10-28T08:26:35.017458+0000    17     127    283333    283206    65.069   85.5859  0.00774755  0.00768083
2021-10-28T08:26:36.017572+0000    18     127    303901    303774   65.9171   80.3438  0.00621286  0.00758246
2021-10-28T08:26:37.017689+0000    19     127    322606    322479   66.2929   73.0664   0.0100817   0.0075385
2021-10-28T08:26:38.017792+0000 min lat: 0.00300398 max lat: 0.100068 avg lat: 0.00761223
2021-10-28T08:26:38.017792+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-28T08:26:38.017792+0000    20     128    336289    336161   65.6503   53.4453   0.0102673  0.00761223
2021-10-28T08:26:39.017905+0000    21     127    352265    352138   65.4956   62.4102  0.00534546  0.00763173
2021-10-28T08:26:40.017993+0000    22     128    372809    372681   66.1658   80.2461  0.00472933  0.00755423
2021-10-28T08:26:41.018078+0000    23     127    394120    393993   66.9083     83.25  0.00791658  0.00747025
2021-10-28T08:26:42.018178+0000    24     127    414402    414275   67.4212   79.2266  0.00854976  0.00741336
2021-10-28T08:26:43.018296+0000    25     128    426302    426174   66.5833   46.4805   0.0275308  0.00749868
2021-10-28T08:26:44.018366+0000    26     128    428414    428286   64.3398      8.25   0.0776633  0.00775715
2021-10-28T08:26:45.018479+0000    27     127    433211    433084   62.6509   18.7422  0.00852349  0.00797807
2021-10-28T08:26:46.018555+0000    28     127    445319    445192   62.1024   47.2969  0.00877799  0.00804817
2021-10-28T08:26:47.018661+0000    29     127    458827    458700   61.7802   52.7656  0.00939585  0.00809035
2021-10-28T08:26:48.018738+0000    30     128    471598    471470   61.3835   49.8828   0.0102859  0.00814264
2021-10-28T08:26:49.018824+0000    31     127    484178    484051   60.9886   49.1445   0.0108189  0.00819581
2021-10-28T08:26:50.018914+0000    32     128    496803    496675   60.6236   49.3125   0.0052388  0.00824499
2021-10-28T08:26:51.019004+0000    33     127    517596    517469   61.2477   81.2266  0.00461786  0.00816152
2021-10-28T08:26:52.019121+0000    34     128    534057    533929   61.3372   64.2969  0.00912708  0.00814899
2021-10-28T08:26:53.019199+0000    35     127    547671    547544   61.1041   53.1836   0.0101754  0.00818017
2021-10-28T08:26:54.019283+0000    36     128    567095    566967   61.5141   75.8711   0.0051395  0.00812607
2021-10-28T08:26:55.019371+0000    37     127    588196    588069   62.0792   82.4297   0.0077354  0.00805172
2021-10-28T08:26:56.019461+0000    38     128    609491    609363   62.6343   83.1797  0.00497043  0.00798062
2021-10-28T08:26:57.019568+0000    39     128    630344    630216   63.1167    81.457   0.0099405  0.00791974
2021-10-28T08:26:58.019661+0000 min lat: 0.0029433 max lat: 0.253304 avg lat: 0.00806536
2021-10-28T08:26:58.019661+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-28T08:26:58.019661+0000    40     128    634552    634424   61.9497   16.4375   0.0666445  0.00806536
2021-10-28T08:26:59.019757+0000    41     128    636931    636803   60.6653   9.29297   0.0119645  0.00823888
2021-10-28T08:27:00.019847+0000    42     128    648592    648464   60.3054   45.5508  0.00906369  0.00828902
2021-10-28T08:27:01.019929+0000    43     128    663459    663331   60.2534   58.0742  0.00834571  0.00829583
2021-10-28T08:27:02.020011+0000    44     128    676105    675977   60.0066   49.3984  0.00986355   0.0083297
2021-10-28T08:27:03.020112+0000    45     127    688779    688652   59.7732   49.5117  0.00882348  0.00836254
2021-10-28T08:27:04.020214+0000    46     127    702392    702265   59.6297   53.1758   0.0104814  0.00838246
2021-10-28T08:27:05.020305+0000    47     128    715729    715601   59.4693   52.0938   0.0113347  0.00840525
2021-10-28T08:27:06.020408+0000    48     128    728750    728622   59.2899   50.8633   0.0105549  0.00843112
2021-10-28T08:27:07.020484+0000    49     128    735938    735810   58.6528   28.0781  0.00911863  0.00852234
2021-10-28T08:27:08.020571+0000    50     128    748608    748480   58.4696   49.4922   0.0111427  0.00854901
2021-10-28T08:27:09.020681+0000    51     127    769602    769475    58.931   82.0117   0.0046927  0.00848269
2021-10-28T08:27:10.020770+0000    52     128    791722    791594   59.4591   86.4023  0.00394957  0.00840718
2021-10-28T08:27:11.020856+0000    53     127    813220    813093   59.9217   83.9805  0.00847364  0.00834238
2021-10-28T08:27:12.020934+0000    54     127    834897    834770   60.3799   84.6758  0.00480442  0.00827911
2021-10-28T08:27:13.021054+0000    55     127    842434    842307   59.8173   29.4414    0.064535  0.00835408
2021-10-28T08:27:14.021150+0000    56     128    844539    844411   58.8959   8.21875   0.0820505  0.00848446
2021-10-28T08:27:15.021276+0000    57     128    855459    855331   58.6109   42.6562   0.0077943  0.00852481
2021-10-28T08:27:16.021370+0000    58     128    869340    869212   58.5352   54.2227  0.00911091  0.00853972
2021-10-28T08:27:17.021462+0000    59     127    883232    883105   58.4628   54.2695   0.0090057  0.00855045
2021-10-28T08:27:18.021572+0000 min lat: 0.0029433 max lat: 0.253304 avg lat: 0.00856637
2021-10-28T08:27:18.021572+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-28T08:27:18.021572+0000    60      46    896451    896405   58.3542   51.9531  0.00918946  0.00856637
2021-10-28T08:27:19.021704+0000 Total time run:         60.0075
Total writes made:      896451
Write size:             4096
Object size:            4096
Bandwidth (MB/sec):     58.3554
Stddev Bandwidth:       22.9975
Max bandwidth (MB/sec): 88.2891
Min bandwidth (MB/sec): 8.21875
Average IOPS:           14938
Stddev IOPS:            5887.35
Max IOPS:               22602
Min IOPS:               2104
Average Latency(s):     0.00856634
Stddev Latency(s):      0.00859994
Max latency(s):         0.253304
Min latency(s):         0.0029433

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:27:19,727246306-07:00] INFO: > Stop system activity collection services[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:27:19,731762917-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:391 - rados_bench() > kill -INT 208980

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:27:19,736170602-07:00] INFO: >> for OSD host[0m
# ./benchmarks/bench-rados:396 - rados_bench() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.2 kill -INT 126902
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:27:19,743462503-07:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.2 kill -INT 126902
[1] 01:27:19 [SUCCESS] ljishen@10.10.2.2

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:27:19,928345922-07:00] INFO: >>> Copy system activity file from OSD host[0m
## ./benchmarks/bench-rados:403 - rados_bench() > basename -- /tmp/bench-rados/sys_activity_4KB_write.dat.osd_host.1
# ./benchmarks/bench-rados:403 - rados_bench() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.2:/tmp/bench-rados/sys_activity_4KB_write.dat.osd_host.1 /mnt/sda8/ceph-research/scripts/output/sys_activity_4KB_write.dat.osd_host.1


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:27:20,102768820-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:27:44,330895825-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 896.45k objects, 3.4 GiB
    usage:   11 GiB used, 89 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:27:44,337501052-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:27:53,629669582-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 896.45k objects, 3.4 GiB
    usage:   11 GiB used, 89 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:27:53,635861300-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:28:02,793750949-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 896.45k objects, 3.4 GiB
    usage:   11 GiB used, 89 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:28:02,799611873-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:28:12,225980578-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 896.45k objects, 3.4 GiB
    usage:   11 GiB used, 89 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:28:12,232060675-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:28:21,409472683-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 896.45k objects, 3.4 GiB
    usage:   11 GiB used, 89 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:28:21,415857133-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:28:21,420522735-07:00] INFO: > The cluster is idle now.[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:28:21,423564948-07:00] STAGE: Start benchmarking of mode seq ...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:28:21,428667773-07:00] INFO: > Start system activity collection services[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:28:21,432894949-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:341 - rados_bench() > rm --force /mnt/sda8/ceph-research/scripts/output/sys_activity_4KB_seq.dat.rados_bench_host.1
# ./benchmarks/bench-rados:342 - rados_bench() > sar_pid_rados_bench_host=212451
# ./benchmarks/bench-rados:342 - rados_bench() > S_TIME_FORMAT=ISO
# ./benchmarks/bench-rados:342 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/output/sys_activity_4KB_seq.dat.rados_bench_host.1 2

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:28:21,439002588-07:00] INFO: >> for OSD host[0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:28:21,447616029-07:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.2 bash
10.10.2.2: b'127001\n'
[1] 01:28:21 [SUCCESS] ljishen@10.10.2.2
127001

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:28:21,637359436-07:00] INFO: > Run rados bench[0m
# ./benchmarks/bench-rados:379 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/output/rados_bench_4KB_seq.log.1
## ./benchmarks/bench-rados:376 - rados_bench() > count_cpus 32-63,96-127
# ./benchmarks/bench-rados:376 - rados_bench() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 128 --show-time
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:28:21,656971693-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:28:21,659752102-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'f7b2a0f4-37c7-11ec-b51d-53e6e728d2d3', '--', 'rados', 'bench', '99999999', 'seq', '--pool', 'bench_rados', '--concurrent-ios', '128', '--show-time'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid f7b2a0f4-37c7-11ec-b51d-53e6e728d2d3 -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 128 --show-time
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
hints = 1
2021-10-28T08:28:24.938288+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-28T08:28:24.938288+0000     0       0         0         0         0         0           -           0
2021-10-28T08:28:25.938410+0000     1     128     41446     41318   161.367   161.398  0.00303573  0.00308745
2021-10-28T08:28:26.938533+0000     2     128     83143     83015   162.113   162.879  0.00309269   0.0030769
2021-10-28T08:28:27.938653+0000     3     128    124748    124620   162.242    162.52  0.00203319  0.00307558
2021-10-28T08:28:28.938764+0000     4     128    166719    166591   162.664   163.949  0.00198305  0.00306845
2021-10-28T08:28:29.938839+0000     5     128    206570    206442   161.263   155.668  0.00306669  0.00309526
2021-10-28T08:28:30.938936+0000     6     128    247970    247842   161.336   161.719  0.00396356  0.00309365
2021-10-28T08:28:31.939048+0000     7     128    288595    288467   160.956   158.691  0.00201604  0.00310141
2021-10-28T08:28:32.939163+0000     8     128    329285    329157   160.702   158.945  0.00193702  0.00310642
2021-10-28T08:28:33.939278+0000     9     127    369672    369545   160.374   157.766  0.00272923  0.00311262
2021-10-28T08:28:34.939389+0000    10     127    406944    406817   158.894   145.594  0.00126879  0.00313825
2021-10-28T08:28:35.939514+0000    11     127    442748    442621   157.162   139.859  0.00220341  0.00317664
2021-10-28T08:28:36.939624+0000    12     128    482651    482523   157.053   155.867  0.00351962  0.00317872
2021-10-28T08:28:37.939733+0000    13     128    522082    521954   156.819   154.027  0.00206435  0.00318351
2021-10-28T08:28:38.939847+0000    14     128    562933    562805   157.014   159.574  0.00286385   0.0031795
2021-10-28T08:28:39.939964+0000    15     127    602567    602440   156.867   154.824  0.00302954  0.00318271
2021-10-28T08:28:40.940076+0000    16     128    641970    641842   156.681   153.914  0.00316513  0.00318644
2021-10-28T08:28:41.940183+0000    17     127    682243    682116   156.718    157.32  0.00286073  0.00318579
2021-10-28T08:28:42.940294+0000    18     127    721910    721783   156.619   154.949  0.00307373  0.00318774
2021-10-28T08:28:43.940411+0000    19     127    762766    762639   156.774   159.594  0.00308224  0.00318454
2021-10-28T08:28:44.940520+0000 min lat: 0.000243188 max lat: 0.12228 avg lat: 0.00318727
2021-10-28T08:28:44.940520+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-28T08:28:44.940520+0000    20     128    802261    802133   156.649   154.273  0.00257796  0.00318727
2021-10-28T08:28:45.940633+0000    21     127    840919    840792   156.379   151.012  0.00340295  0.00319257
2021-10-28T08:28:46.940746+0000    22     127    880922    880795   156.373   156.262  0.00340019  0.00319282
2021-10-28T08:28:47.940893+0000 Total time run:       22.3928
Total reads made:     896451
Read size:            4096
Object size:          4096
Bandwidth (MB/sec):   156.379
Average IOPS:         40033
Stddev IOPS:          1441.2
Max IOPS:             41971
Min IOPS:             35804
Average Latency(s):   0.00319278
Max latency(s):       0.12228
Min latency(s):       0.000243188

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:28:48,583629131-07:00] INFO: > Stop system activity collection services[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:28:48,588584649-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:391 - rados_bench() > kill -INT 212451

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:28:48,594205230-07:00] INFO: >> for OSD host[0m
# ./benchmarks/bench-rados:396 - rados_bench() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.2 kill -INT 127001
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:28:48,601622437-07:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.2 kill -INT 127001
[1] 01:28:48 [SUCCESS] ljishen@10.10.2.2

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:28:48,785913691-07:00] INFO: >>> Copy system activity file from OSD host[0m
## ./benchmarks/bench-rados:403 - rados_bench() > basename -- /tmp/bench-rados/sys_activity_4KB_seq.dat.osd_host.1
# ./benchmarks/bench-rados:403 - rados_bench() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.2:/tmp/bench-rados/sys_activity_4KB_seq.dat.osd_host.1 /mnt/sda8/ceph-research/scripts/output/sys_activity_4KB_seq.dat.osd_host.1


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:28:48,944681027-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:29:13,146161577-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 896.45k objects, 3.4 GiB
    usage:   11 GiB used, 89 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:29:13,153402532-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:29:22,545789414-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 896.45k objects, 3.4 GiB
    usage:   11 GiB used, 89 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:29:22,552376657-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:29:31,736762949-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 896.45k objects, 3.4 GiB
    usage:   11 GiB used, 89 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:29:31,743827581-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:29:40,970948661-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 896.45k objects, 3.4 GiB
    usage:   11 GiB used, 89 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:29:40,978236054-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:29:50,295353711-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 896.45k objects, 3.4 GiB
    usage:   11 GiB used, 89 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:29:50,302068875-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:29:50,307195856-07:00] INFO: > The cluster is idle now.[0m


[1;7;39;49m[2021-10-28T01:29:50,309244817-07:00][RUNNING][ROUND 2/1/21] object_size=4KB[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:29:50,312234370-07:00] STAGE: Launch a Ceph cluster (CEPH_HOST: 10.10.2.1, OSD_HOST: 10.10.2.2) ...[0m
# ./benchmarks/bench-rados:183 - launch_ceph_cluster() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.1 bash -euo pipefail
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:29:50,321238207-07:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.1 bash -euo pipefail
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T01:29:50,653450446-07:00] INFO: > Remove existing clusters\x1b[0m\n'
10.10.2.1: b'## bash:17 -  > basename -- /var/lib/ceph/f7b2a0f4-37c7-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'# bash:17 -  > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm --host 10.10.2.1 --host 10.10.2.2 rm-cluster --force --fsid f7b2a0f4-37c7-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T01:29:50,664706416-07:00] DEBUG: HOST_PARAMS: ['--host', '10.10.2.1', '--host', '10.10.2.2']\x1b[0m\n"
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T01:29:50,668256760-07:00] DEBUG: CEPHADM_PARAMS: ['rm-cluster', '--force', '--fsid', 'f7b2a0f4-37c7-11ec-b51d-53e6e728d2d3']\x1b[0m\n"
10.10.2.1: b"# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:63 -  > /tmp/bench-rados/ceph-research/scripts/parallel-ssh --host 10.10.2.1 --host 10.10.2.2 'sudo python3 -u - rm-cluster --force --fsid f7b2a0f4-37c7-11ec-b51d-53e6e728d2d3'\n"
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T01:29:50,676830917-07:00] DEBUG: command from stdin\x1b[0m\n'
10.10.2.1: b"# /tmp/bench-rados/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host 10.10.2.1 --host 10.10.2.2 'sudo python3 -u - rm-cluster --force --fsid f7b2a0f4-37c7-11ec-b51d-53e6e728d2d3'\n"
10.10.2.1: b'[1] 01:29:56 [SUCCESS] 10.10.2.1\n[2] 01:29:58 [SUCCESS] 10.10.2.2\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T01:29:58,959185059-07:00] INFO: > Deploy a new cluster\x1b[0m\n'
10.10.2.1: b'# bash:24 -  > /tmp/bench-rados/ceph-research/scripts/ceph-deploy -m 10.10.2.1 -o node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/10.10.2.2:/dev/nvme0n1\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T01:29:58,971562646-07:00] STAGE: Verify passwordless SSH to hosts, passwordless sudo via ljishen, and the existence of devices on hosts...\x1b[0m\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T01:29:58,976078876-07:00] INFO: > Check the host for the monitor\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:181 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@10.10.2.1 sudo --non-interactive --validate\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T01:29:59,128109727-07:00] INFO: > Check hosts for OSDs\x1b[0m\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T01:29:59,132799543-07:00] INFO: >> Check host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:200 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us sudo --non-interactive --validate\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T01:29:59,286863074-07:00] INFO: >>> Check device '/dev/nvme0n1'\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:215 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us test -b /dev/nvme0n1\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:226 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us lsblk --nodeps --noheadings --output TYPE /dev/nvme0n1\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:227 - verify_parameters() > grep --word-regexp disk\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T01:29:59,575850989-07:00] STAGE: Reset devices on hosts...\x1b[0m\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T01:29:59,580794973-07:00] INFO: > Resetting device '/dev/nvme0n1' on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:288 - reset_host_devices() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us sudo sh\n'
10.10.2.1: b'  Removing ceph--df6c11fd--fc8f--4e75--bec1--cfe861f534f4-osd--block--dfbb0976--8c53--4f48--8038--e109853410b1 (253:0)\n'
10.10.2.1: b'  Archiving volume group "ceph-df6c11fd-fc8f-4e75-bec1-cfe861f534f4" metadata (seqno 5).\n'
10.10.2.1: b'  Releasing logical volume "osd-block-dfbb0976-8c53-4f48-8038-e109853410b1"\n'
10.10.2.1: b'  Creating volume group backup "/etc/lvm/backup/ceph-df6c11fd-fc8f-4e75-bec1-cfe861f534f4" (seqno 6).\n'
10.10.2.1: b'  Logical volume "osd-block-dfbb0976-8c53-4f48-8038-e109853410b1" successfully removed\n'
10.10.2.1: b'  Removing physical volume "/dev/nvme0n1" from volume group "ceph-df6c11fd-fc8f-4e75-bec1-cfe861f534f4"\n'
10.10.2.1: b'  Volume group "ceph-df6c11fd-fc8f-4e75-bec1-cfe861f534f4" successfully removed\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T01:30:00,001553543-07:00] STAGE: Bootstrap a new cluster...\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:317 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.1 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T01:30:00,011268585-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T01:30:00,014768775-07:00] DEBUG: CEPHADM_PARAMS: ['bootstrap', '--config', '/tmp/bench-rados/ceph-research/scripts/ceph.conf', '--mon-ip', '10.10.2.1', '--output-dir', '/tmp/bench-rados/deployment_data_root/etc/ceph', '--ssh-user', 'ljishen', '--skip-dashboard', '--no-minimize-config', '--allow-overwrite', '--allow-fqdn-hostname', '--orphan-initial-daemons', '--skip-monitoring-stack', '--single-host-defaults']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.1 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults\n'
10.10.2.1: b'Verifying podman|docker is present...\n'
10.10.2.1: b'Verifying lvm2 is present...\nVerifying time synchronization is in place...\n'
10.10.2.1: b'Unit ntp.service is enabled and running\n'
10.10.2.1: b'Repeating the final host check...\n'
10.10.2.1: b'podman|docker (/usr/bin/docker) is present\n'
10.10.2.1: b'systemctl is present\n'
10.10.2.1: b'lvcreate is present\n'
10.10.2.1: b'Unit ntp.service is enabled and running\n'
10.10.2.1: b'Host looks OK\n'
10.10.2.1: b'Cluster fsid: 3d8df47e-37c9-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Verifying IP 10.10.2.1 port 3300 ...\n'
10.10.2.1: b'Verifying IP 10.10.2.1 port 6789 ...\n'
10.10.2.1: b'Mon IP `10.10.2.1` is in CIDR network `10.10.2.0/24`\n'
10.10.2.1: b'- internal network (--cluster-network) has not been provided, OSD replication will default to the public_network\n'
10.10.2.1: b'Adjusting default settings to suit single-host cluster...\n'
10.10.2.1: b'Pulling container image quay.io/ceph/ceph:v16...\n'
10.10.2.1: b'Ceph version: ceph version 16.2.6 (ee28fb57e47e9f88813e24bbf4c14496ca299d31) pacific (stable)\nExtracting ceph user uid/gid from container image...\n'
10.10.2.1: b'Creating initial keys...\n'
10.10.2.1: b'Creating initial monmap...\n'
10.10.2.1: b'Creating mon...\n'
10.10.2.1: b'Waiting for mon to start...\n'
10.10.2.1: b'Waiting for mon...\n'
10.10.2.1: b'mon is available\n'
10.10.2.1: b'Setting mon public_network to 10.10.2.0/24\n'
10.10.2.1: b'Wrote config to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf\n'
10.10.2.1: b'Wrote keyring to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring\n'
10.10.2.1: b'Creating mgr...\n'
10.10.2.1: b'Verifying port 9283 ...\n'
10.10.2.1: b'Waiting for mgr to start...\nWaiting for mgr...\n'
10.10.2.1: b'mgr not available, waiting (1/15)...\n'
10.10.2.1: b'mgr not available, waiting (2/15)...\n'
10.10.2.1: b'mgr is available\n'
10.10.2.1: b'Enabling cephadm module...\n'
10.10.2.1: b'Waiting for the mgr to restart...\nWaiting for mgr epoch 5...\n'
10.10.2.1: b'mgr epoch 5 is available\n'
10.10.2.1: b'Setting orchestrator backend to cephadm...\n'
10.10.2.1: b'Generating ssh key...\n'
10.10.2.1: b'Wrote public SSH key to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub\n'
10.10.2.1: b'Adding key to ljishen@localhost authorized_keys...\n'
10.10.2.1: b'Adding host node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us...\n'
10.10.2.1: b'Deploying unmanaged mon service...\n'
10.10.2.1: b'Deploying unmanaged mgr service...\n'
10.10.2.1: b'You can access the Ceph CLI with:\n\n\tsudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --fsid 3d8df47e-37c9-11ec-b51d-53e6e728d2d3 -c /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf -k /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring\n\nPlease consider enabling telemetry to help improve Ceph:\n\n\tceph telemetry on\n\nFor more information see:\n\n\thttps://docs.ceph.com/docs/pacific/mgr/telemetry/\n\n'
10.10.2.1: b'Bootstrap complete.\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T01:30:58,113343951-07:00] INFO: Sleep for 20 seconds\x1b[0m\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T01:31:18,120748414-07:00] INFO: > Set the default number of monitor daemons to 1\x1b[0m\n# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:337 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T01:31:18,130960551-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T01:31:18,134937136-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mon', '1']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1\n'
10.10.2.1: b'Inferring fsid 3d8df47e-37c9-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/3d8df47e-37c9-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b'Scheduled mon update...\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T01:31:27,216514457-07:00] INFO: > Set the default number of manager daemons to 1\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:342 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T01:31:27,226613331-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T01:31:27,230198850-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mgr', '1']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1\n'
10.10.2.1: b'Inferring fsid 3d8df47e-37c9-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/3d8df47e-37c9-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b'Scheduled mgr update...\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T01:31:36,322175702-07:00] STAGE: Add hosts to the cluster...\x1b[0m\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T01:31:36,328419609-07:00] INFO: > Adding host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:361 - add_hosts() > ssh-copy-id -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -f -i /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\n'
10.10.2.1: b'/usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: "/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub"\n'
10.10.2.1: b'\nNumber of key(s) added: 1\n\nNow try logging into the machine, with:   "ssh -o \'GlobalKnownHostsFile=/dev/null\' -o \'LogLevel=ERROR\' -o \'PasswordAuthentication=no\' -o \'StrictHostKeyChecking=no\' -o \'UserKnownHostsFile=/dev/null\' \'ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\'"\nand check to make sure that only the key(s) you wanted were added.\n\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:367 - add_hosts() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us 10.10.2.2\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T01:31:36,542611101-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T01:31:36,546267223-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'host', 'add', 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us', '10.10.2.2']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us 10.10.2.2\n'
10.10.2.1: b'Inferring fsid 3d8df47e-37c9-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/3d8df47e-37c9-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b"Added host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us' with addr '10.10.2.2'\n"
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T01:31:45,508839298-07:00] INFO: Sleep for 20 seconds\x1b[0m\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T01:32:05,513207330-07:00] STAGE: Add OSDs to the cluster...\x1b[0m\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T01:32:05,520116788-07:00] INFO: > Adding devices ['/dev/nvme0n1'] on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us' as OSDs\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:388 - add_host_devices_as_osds() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T01:32:05,530757170-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T01:32:05,534608450-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'daemon', 'add', 'osd', 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1\n'
10.10.2.1: b'Inferring fsid 3d8df47e-37c9-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/3d8df47e-37c9-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b"Created osd(s) 0 on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'\n"
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T01:32:29,820593642-07:00] INFO: > Sleep for 20 seconds\x1b[0m\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T01:32:49,825969161-07:00] STAGE: Show Cluster Status\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:408 - show_status() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T01:32:49,836450043-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T01:32:49,840296995-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', '--status']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status\n'
10.10.2.1: b'Inferring fsid 3d8df47e-37c9-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/3d8df47e-37c9-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b'  cluster:\n    id:     3d8df47e-37c9-11ec-b51d-53e6e728d2d3\n    health: HEALTH_WARN\n            1 pool(s) have no replicas configured\n \n  services:\n    mon: 1 daemons, quorum node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us (age 2m)\n    mgr: node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us.kgisbk(active, since 2m)\n    osd: 1 osds: 1 up (since 17s), 1 in (since 40s)\n \n  data:\n    pools:   1 pools, 1 pgs\n    objects: 0 objects, 0 B\n    usage:   4.7 MiB used, 100 GiB / 100 GiB avail\n    pgs:     1 active+clean\n \n'
[1] 01:32:58 [SUCCESS] ljishen@10.10.2.1

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:29:50,653450446-07:00] INFO: > Remove existing clusters[0m
## bash:17 -  > basename -- /var/lib/ceph/f7b2a0f4-37c7-11ec-b51d-53e6e728d2d3
# bash:17 -  > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm --host 10.10.2.1 --host 10.10.2.2 rm-cluster --force --fsid f7b2a0f4-37c7-11ec-b51d-53e6e728d2d3
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:29:50,664706416-07:00] DEBUG: HOST_PARAMS: ['--host', '10.10.2.1', '--host', '10.10.2.2'][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:29:50,668256760-07:00] DEBUG: CEPHADM_PARAMS: ['rm-cluster', '--force', '--fsid', 'f7b2a0f4-37c7-11ec-b51d-53e6e728d2d3'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:63 -  > /tmp/bench-rados/ceph-research/scripts/parallel-ssh --host 10.10.2.1 --host 10.10.2.2 'sudo python3 -u - rm-cluster --force --fsid f7b2a0f4-37c7-11ec-b51d-53e6e728d2d3'
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:29:50,676830917-07:00] DEBUG: command from stdin[0m
# /tmp/bench-rados/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host 10.10.2.1 --host 10.10.2.2 'sudo python3 -u - rm-cluster --force --fsid f7b2a0f4-37c7-11ec-b51d-53e6e728d2d3'
[1] 01:29:56 [SUCCESS] 10.10.2.1
[2] 01:29:58 [SUCCESS] 10.10.2.2

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:29:58,959185059-07:00] INFO: > Deploy a new cluster[0m
# bash:24 -  > /tmp/bench-rados/ceph-research/scripts/ceph-deploy -m 10.10.2.1 -o node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/10.10.2.2:/dev/nvme0n1


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:29:58,971562646-07:00] STAGE: Verify passwordless SSH to hosts, passwordless sudo via ljishen, and the existence of devices on hosts...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:29:58,976078876-07:00] INFO: > Check the host for the monitor[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:181 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@10.10.2.1 sudo --non-interactive --validate

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:29:59,128109727-07:00] INFO: > Check hosts for OSDs[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:29:59,132799543-07:00] INFO: >> Check host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:200 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us sudo --non-interactive --validate

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:29:59,286863074-07:00] INFO: >>> Check device '/dev/nvme0n1'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:215 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us test -b /dev/nvme0n1
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:226 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us lsblk --nodeps --noheadings --output TYPE /dev/nvme0n1
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:227 - verify_parameters() > grep --word-regexp disk


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:29:59,575850989-07:00] STAGE: Reset devices on hosts...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:29:59,580794973-07:00] INFO: > Resetting device '/dev/nvme0n1' on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:288 - reset_host_devices() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us sudo sh
  Removing ceph--df6c11fd--fc8f--4e75--bec1--cfe861f534f4-osd--block--dfbb0976--8c53--4f48--8038--e109853410b1 (253:0)
  Archiving volume group "ceph-df6c11fd-fc8f-4e75-bec1-cfe861f534f4" metadata (seqno 5).
  Releasing logical volume "osd-block-dfbb0976-8c53-4f48-8038-e109853410b1"
  Creating volume group backup "/etc/lvm/backup/ceph-df6c11fd-fc8f-4e75-bec1-cfe861f534f4" (seqno 6).
  Logical volume "osd-block-dfbb0976-8c53-4f48-8038-e109853410b1" successfully removed
  Removing physical volume "/dev/nvme0n1" from volume group "ceph-df6c11fd-fc8f-4e75-bec1-cfe861f534f4"
  Volume group "ceph-df6c11fd-fc8f-4e75-bec1-cfe861f534f4" successfully removed


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:30:00,001553543-07:00] STAGE: Bootstrap a new cluster...[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:317 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.1 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:30:00,011268585-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:30:00,014768775-07:00] DEBUG: CEPHADM_PARAMS: ['bootstrap', '--config', '/tmp/bench-rados/ceph-research/scripts/ceph.conf', '--mon-ip', '10.10.2.1', '--output-dir', '/tmp/bench-rados/deployment_data_root/etc/ceph', '--ssh-user', 'ljishen', '--skip-dashboard', '--no-minimize-config', '--allow-overwrite', '--allow-fqdn-hostname', '--orphan-initial-daemons', '--skip-monitoring-stack', '--single-host-defaults'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.1 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults
Verifying podman|docker is present...
Verifying lvm2 is present...
Verifying time synchronization is in place...
Unit ntp.service is enabled and running
Repeating the final host check...
podman|docker (/usr/bin/docker) is present
systemctl is present
lvcreate is present
Unit ntp.service is enabled and running
Host looks OK
Cluster fsid: 3d8df47e-37c9-11ec-b51d-53e6e728d2d3
Verifying IP 10.10.2.1 port 3300 ...
Verifying IP 10.10.2.1 port 6789 ...
Mon IP `10.10.2.1` is in CIDR network `10.10.2.0/24`
- internal network (--cluster-network) has not been provided, OSD replication will default to the public_network
Adjusting default settings to suit single-host cluster...
Pulling container image quay.io/ceph/ceph:v16...
Ceph version: ceph version 16.2.6 (ee28fb57e47e9f88813e24bbf4c14496ca299d31) pacific (stable)
Extracting ceph user uid/gid from container image...
Creating initial keys...
Creating initial monmap...
Creating mon...
Waiting for mon to start...
Waiting for mon...
mon is available
Setting mon public_network to 10.10.2.0/24
Wrote config to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf
Wrote keyring to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring
Creating mgr...
Verifying port 9283 ...
Waiting for mgr to start...
Waiting for mgr...
mgr not available, waiting (1/15)...
mgr not available, waiting (2/15)...
mgr is available
Enabling cephadm module...
Waiting for the mgr to restart...
Waiting for mgr epoch 5...
mgr epoch 5 is available
Setting orchestrator backend to cephadm...
Generating ssh key...
Wrote public SSH key to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub
Adding key to ljishen@localhost authorized_keys...
Adding host node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us...
Deploying unmanaged mon service...
Deploying unmanaged mgr service...
You can access the Ceph CLI with:

	sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --fsid 3d8df47e-37c9-11ec-b51d-53e6e728d2d3 -c /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf -k /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring

Please consider enabling telemetry to help improve Ceph:

	ceph telemetry on

For more information see:

	https://docs.ceph.com/docs/pacific/mgr/telemetry/

Bootstrap complete.

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:30:58,113343951-07:00] INFO: Sleep for 20 seconds[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:31:18,120748414-07:00] INFO: > Set the default number of monitor daemons to 1[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:337 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:31:18,130960551-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:31:18,134937136-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mon', '1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1
Inferring fsid 3d8df47e-37c9-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/3d8df47e-37c9-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Scheduled mon update...

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:31:27,216514457-07:00] INFO: > Set the default number of manager daemons to 1[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:342 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:31:27,226613331-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:31:27,230198850-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mgr', '1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1
Inferring fsid 3d8df47e-37c9-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/3d8df47e-37c9-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Scheduled mgr update...


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:31:36,322175702-07:00] STAGE: Add hosts to the cluster...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:31:36,328419609-07:00] INFO: > Adding host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:361 - add_hosts() > ssh-copy-id -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -f -i /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us
/usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: "/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub"

Number of key(s) added: 1

Now try logging into the machine, with:   "ssh -o 'GlobalKnownHostsFile=/dev/null' -o 'LogLevel=ERROR' -o 'PasswordAuthentication=no' -o 'StrictHostKeyChecking=no' -o 'UserKnownHostsFile=/dev/null' 'ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'"
and check to make sure that only the key(s) you wanted were added.

# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:367 - add_hosts() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us 10.10.2.2
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:31:36,542611101-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:31:36,546267223-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'host', 'add', 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us', '10.10.2.2'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us 10.10.2.2
Inferring fsid 3d8df47e-37c9-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/3d8df47e-37c9-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Added host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us' with addr '10.10.2.2'

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:31:45,508839298-07:00] INFO: Sleep for 20 seconds[0m


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:32:05,513207330-07:00] STAGE: Add OSDs to the cluster...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:32:05,520116788-07:00] INFO: > Adding devices ['/dev/nvme0n1'] on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us' as OSDs[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:388 - add_host_devices_as_osds() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:32:05,530757170-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:32:05,534608450-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'daemon', 'add', 'osd', 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1
Inferring fsid 3d8df47e-37c9-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/3d8df47e-37c9-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Created osd(s) 0 on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:32:29,820593642-07:00] INFO: > Sleep for 20 seconds[0m


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:32:49,825969161-07:00] STAGE: Show Cluster Status[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:408 - show_status() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:32:49,836450043-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:32:49,840296995-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', '--status'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status
Inferring fsid 3d8df47e-37c9-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/3d8df47e-37c9-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
  cluster:
    id:     3d8df47e-37c9-11ec-b51d-53e6e728d2d3
    health: HEALTH_WARN
            1 pool(s) have no replicas configured
 
  services:
    mon: 1 daemons, quorum node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us (age 2m)
    mgr: node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us.kgisbk(active, since 2m)
    osd: 1 osds: 1 up (since 17s), 1 in (since 40s)
 
  data:
    pools:   1 pools, 1 pgs
    objects: 0 objects, 0 B
    usage:   4.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     1 active+clean
 

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:32:58,642686178-07:00] INFO: > Copy configuration files to local[0m
# ./benchmarks/bench-rados:220 - launch_ceph_cluster() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.1 sudo chown --changes --recursive ljishen /tmp/bench-rados/deployment_data_root/etc/ceph
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:32:58,650262936-07:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.1 sudo chown --changes --recursive ljishen /tmp/bench-rados/deployment_data_root/etc/ceph
[1] 01:32:58 [SUCCESS] ljishen@10.10.2.1
# ./benchmarks/bench-rados:223 - launch_ceph_cluster() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.1:/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf
# ./benchmarks/bench-rados:226 - launch_ceph_cluster() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.1:/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:32:59,129012048-07:00] STAGE: Update local ceph.conf as a client...[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:32:59,132271339-07:00] STAGE: Configure Ceph pool...[0m
# ./benchmarks/bench-rados:257 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_max_backfills 32
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:32:59,153904825-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:32:59,156623989-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '3d8df47e-37c9-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'config', 'set', 'osd', 'osd_max_backfills', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 3d8df47e-37c9-11ec-b51d-53e6e728d2d3 -- ceph config set osd osd_max_backfills 32
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:258 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_active 32
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:33:03,473859004-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:33:03,476798513-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '3d8df47e-37c9-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_active', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 3d8df47e-37c9-11ec-b51d-53e6e728d2d3 -- ceph config set osd osd_recovery_max_active 32
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:259 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_single_start 8
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:33:07,742640554-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:33:07,745928710-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '3d8df47e-37c9-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_single_start', '8'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 3d8df47e-37c9-11ec-b51d-53e6e728d2d3 -- ceph config set osd osd_recovery_max_single_start 8
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:260 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_op_priority 63
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:33:12,051137093-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:33:12,054249067-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '3d8df47e-37c9-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_op_priority', '63'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 3d8df47e-37c9-11ec-b51d-53e6e728d2d3 -- ceph config set osd osd_recovery_op_priority 63
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:262 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config show-with-defaults osd.0
# ./benchmarks/bench-rados:263 - configure_ceph_pool() > grep 'osd_max_backfills\|osd_recovery'
# ./benchmarks/bench-rados:263 - configure_ceph_pool() > column -t -s ' '
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
osd_max_backfills                        32         mon
osd_recovery_cost                        20971520   default
osd_recovery_delay_start                 0.000000   default
osd_recovery_max_active                  32         mon
osd_recovery_max_active_hdd              3          default
osd_recovery_max_active_ssd              10         default
osd_recovery_max_chunk                   8388608    default
osd_recovery_max_omap_entries_per_chunk  8096       default
osd_recovery_max_single_start            8          mon
osd_recovery_op_priority                 63         mon
osd_recovery_op_warn_multiple            16         default
osd_recovery_priority                    5          default
osd_recovery_retry_interval              30.000000  default
osd_recovery_sleep                       0.000000   default
osd_recovery_sleep_hdd                   0.100000   default
osd_recovery_sleep_hybrid                0.025000   default
osd_recovery_sleep_ssd                   0.000000   default
# ./benchmarks/bench-rados:265 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool create bench_rados 128 128 replicated --size 1 --pg-num-min 128
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:33:20,569679581-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:33:20,572688421-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '3d8df47e-37c9-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'create', 'bench_rados', '128', '128', 'replicated', '--size', '1', '--pg-num-min', '128'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 3d8df47e-37c9-11ec-b51d-53e6e728d2d3 -- ceph osd pool create bench_rados 128 128 replicated --size 1 --pg-num-min 128
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
pool 'bench_rados' created
# ./benchmarks/bench-rados:267 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados min_size 1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:33:25,136387908-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:33:25,139472129-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '3d8df47e-37c9-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'min_size', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 3d8df47e-37c9-11ec-b51d-53e6e728d2d3 -- ceph osd pool set bench_rados min_size 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 min_size to 1
# ./benchmarks/bench-rados:268 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados pg_autoscale_mode off
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:33:29,461768362-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:33:29,464867722-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '3d8df47e-37c9-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'pg_autoscale_mode', 'off'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 3d8df47e-37c9-11ec-b51d-53e6e728d2d3 -- ceph osd pool set bench_rados pg_autoscale_mode off
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 pg_autoscale_mode to off
# ./benchmarks/bench-rados:269 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool application enable bench_rados benchmark
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:33:34,090406367-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:33:34,093396201-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '3d8df47e-37c9-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'application', 'enable', 'bench_rados', 'benchmark'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 3d8df47e-37c9-11ec-b51d-53e6e728d2d3 -- ceph osd pool application enable bench_rados benchmark
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
enabled application 'benchmark' on pool 'bench_rados'
# ./benchmarks/bench-rados:270 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados noscrub 1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:33:38,402863629-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:33:38,405864914-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '3d8df47e-37c9-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'noscrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 3d8df47e-37c9-11ec-b51d-53e6e728d2d3 -- ceph osd pool set bench_rados noscrub 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 noscrub to 1
# ./benchmarks/bench-rados:271 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados nodeep-scrub 1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:33:43,629190303-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:33:43,632591872-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '3d8df47e-37c9-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'nodeep-scrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 3d8df47e-37c9-11ec-b51d-53e6e728d2d3 -- ceph osd pool set bench_rados nodeep-scrub 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 nodeep-scrub to 1
# ./benchmarks/bench-rados:272 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool ls detail
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:33:48,129203811-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:33:48,132159351-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '3d8df47e-37c9-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'ls', 'detail'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 3d8df47e-37c9-11ec-b51d-53e6e728d2d3 -- ceph osd pool ls detail
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
pool 1 'device_health_metrics' replicated size 1 min_size 1 crush_rule 0 object_hash rjenkins pg_num 64 pgp_num 64 autoscale_mode on last_change 21 lfor 0/0/17 flags hashpspool stripe_width 0 pg_num_min 1 application mgr_devicehealth
pool 2 'bench_rados' replicated size 1 min_size 1 crush_rule 0 object_hash rjenkins pg_num 128 pgp_num 128 autoscale_mode off last_change 20 flags hashpspool,noscrub,nodeep-scrub stripe_width 0 pg_num_min 128 application benchmark

# ./benchmarks/bench-rados:273 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd df tree
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:33:52,382241074-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:33:52,385399646-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '3d8df47e-37c9-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'df', 'tree'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 3d8df47e-37c9-11ec-b51d-53e6e728d2d3 -- ceph osd df tree
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
ID  CLASS  WEIGHT   REWEIGHT  SIZE     RAW USE  DATA     OMAP  META     AVAIL    %USE  VAR   PGS  STATUS  TYPE NAME      
-1         0.09769         -  100 GiB  5.7 MiB  176 KiB   0 B  5.6 MiB  100 GiB  0.01  1.00    -          root default   
-3         0.09769         -  100 GiB  5.7 MiB  176 KiB   0 B  5.6 MiB  100 GiB  0.01  1.00    -              host node-1
 0    ssd  0.09769   1.00000  100 GiB  5.7 MiB  176 KiB   0 B  5.6 MiB  100 GiB  0.01  1.00  192      up          osd.0  
                       TOTAL  100 GiB  5.7 MiB  176 KiB   0 B  5.6 MiB  100 GiB  0.01                                    
MIN/MAX VAR: 1.00/1.00  STDDEV: 0


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:33:56,540813920-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:34:20,805198898-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:34:30,138104874-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:34:39,486651162-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:34:48,909357607-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:34:48,916257008-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:34:58,124986219-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:34:58,131827170-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:35:07,526443363-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:35:07,533412365-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:35:16,816857208-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:35:16,824003976-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:35:26,015768660-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:35:26,022402120-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:35:26,027260806-07:00] INFO: > The cluster is idle now.[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:35:26,030443022-07:00] STAGE: Start benchmarking of mode write ...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:35:26,036159374-07:00] INFO: > Start system activity collection services[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:35:26,041058055-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:341 - rados_bench() > rm --force /mnt/sda8/ceph-research/scripts/output/sys_activity_4KB_write.dat.rados_bench_host.2
# ./benchmarks/bench-rados:342 - rados_bench() > sar_pid_rados_bench_host=218465
# ./benchmarks/bench-rados:342 - rados_bench() > S_TIME_FORMAT=ISO
# ./benchmarks/bench-rados:342 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/output/sys_activity_4KB_write.dat.rados_bench_host.2 2

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:35:26,047407400-07:00] INFO: >> for OSD host[0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:35:26,055992327-07:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.2 bash
10.10.2.2: b'130635\n'
[1] 01:35:26 [SUCCESS] ljishen@10.10.2.2
130635

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:35:26,245822137-07:00] INFO: > Run rados bench[0m
# ./benchmarks/bench-rados:370 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/output/rados_bench_4KB_write.log.2
## ./benchmarks/bench-rados:361 - rados_bench() > count_cpus 32-63,96-127
# ./benchmarks/bench-rados:361 - rados_bench() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 60 write --pool bench_rados -b 4096 -O 4096 --concurrent-ios 128 --show-time --write-object --write-omap --write-xattr --no-cleanup
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:35:26,265538771-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:35:26,268515129-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '3d8df47e-37c9-11ec-b51d-53e6e728d2d3', '--', 'rados', 'bench', '60', 'write', '--pool', 'bench_rados', '-b', '4096', '-O', '4096', '--concurrent-ios', '128', '--show-time', '--write-object', '--write-omap', '--write-xattr', '--no-cleanup'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 3d8df47e-37c9-11ec-b51d-53e6e728d2d3 -- rados bench 60 write --pool bench_rados -b 4096 -O 4096 --concurrent-ios 128 --show-time --write-object --write-omap --write-xattr --no-cleanup
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
hints = 1
2021-10-28T08:35:29.614718+0000 Maintaining 128 concurrent writes of 4096 bytes to objects of size 4096 for up to 60 seconds or 0 objects
2021-10-28T08:35:29.614728+0000 Object prefix: benchmark_data_node-2.bluefield2.ucsc-cmps10_7
2021-10-28T08:35:29.615222+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-28T08:35:29.615222+0000     0       0         0         0         0         0           -           0
2021-10-28T08:35:30.615344+0000     1     128     23100     22972   89.7291   89.7344   0.0058183  0.00554863
2021-10-28T08:35:31.615425+0000     2     128     45153     45025   87.9333   86.1445  0.00568212  0.00567255
2021-10-28T08:35:32.615507+0000     3     128     66314     66186   86.1733   82.6602   0.0057681  0.00579334
2021-10-28T08:35:33.615584+0000     4     128     86211     86083   84.0591   77.7227  0.00622704   0.0059407
2021-10-28T08:35:34.615659+0000     5     127    105762    105635   82.5212    76.375  0.00631355  0.00605317
2021-10-28T08:35:35.615728+0000     6     128    121626    121498   79.0944   61.9648  0.00780039  0.00631592
2021-10-28T08:35:36.615795+0000     7     127    134968    134841   75.2406   52.1211   0.0102637  0.00663771
2021-10-28T08:35:37.615872+0000     8     127    151713    151586   74.0112   65.4102  0.00780214  0.00675142
2021-10-28T08:35:38.615946+0000     9     127    173036    172909   75.0418    83.293  0.00419155  0.00665977
2021-10-28T08:35:39.616028+0000    10     127    194481    194354   75.9139   83.7695   0.0042969  0.00658311
2021-10-28T08:35:40.616109+0000    11     127    214747    214620   76.2088   79.1641  0.00794662   0.0065561
2021-10-28T08:35:41.616187+0000    12     128    218971    218843   71.2326   16.4961   0.0598414  0.00700456
2021-10-28T08:35:42.616258+0000    13     128    221275    221147   66.4454         9   0.0144264  0.00752039
2021-10-28T08:35:43.616333+0000    14     128    233088    232960   64.9951   46.1445  0.00953234  0.00768806
2021-10-28T08:35:44.616412+0000    15     127    246157    246030   64.0655   51.0547  0.00942648  0.00779969
2021-10-28T08:35:45.616501+0000    16     127    265582    265455   64.8034   75.8789  0.00595868  0.00771284
2021-10-28T08:35:46.616576+0000    17     128    287002    286874   65.9127    83.668  0.00391557  0.00758276
2021-10-28T08:35:47.616640+0000    18     128    309536    309408   67.1408   88.0234  0.00548001  0.00744436
2021-10-28T08:35:48.616714+0000    19     128    326837    326709   67.1637    67.582  0.00946642  0.00744115
2021-10-28T08:35:49.616793+0000 min lat: 0.00302166 max lat: 0.0921763 avg lat: 0.00751302
2021-10-28T08:35:49.616793+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-28T08:35:49.616793+0000    20     128    340744    340616   66.5215   54.3242  0.00901257  0.00751302
2021-10-28T08:35:50.616877+0000    21     128    359456    359328   66.8342   73.0938  0.00449261  0.00747872
2021-10-28T08:35:51.616956+0000    22     127    380045    379918   67.4519   80.4297  0.00548215  0.00741032
2021-10-28T08:35:52.617030+0000    23     127    401247    401120   68.1198   82.8203  0.00470758  0.00733762
2021-10-28T08:35:53.617106+0000    24     127    421428    421301   68.5659    78.832  0.00535226  0.00728973
2021-10-28T08:35:54.617177+0000    25     128    429981    429853   67.1594   33.4062   0.0714742  0.00743477
2021-10-28T08:35:55.617245+0000    26     128    432285    432157   64.9225         9   0.0477808  0.00768828
2021-10-28T08:35:56.617314+0000    27     128    442005    441877   63.9242   37.9688  0.00856287  0.00781938
2021-10-28T08:35:57.617383+0000    28     128    456134    456006   63.6121   55.1914   0.0106763  0.00785756
2021-10-28T08:35:58.617459+0000    29     128    466104    465976   62.7615   38.9453  0.00900427  0.00796361
2021-10-28T08:35:59.617534+0000    30     128    479070    478942   62.3576   50.6484  0.00990754   0.0080157
2021-10-28T08:36:00.617614+0000    31     128    491636    491508   61.9293   49.0859   0.0104786  0.00807042
2021-10-28T08:36:01.617690+0000    32     128    506692    506564   61.8318   58.8125  0.00609686  0.00808422
2021-10-28T08:36:02.617765+0000    33     128    526661    526533   62.3217   78.0039  0.00796207  0.00802067
2021-10-28T08:36:03.617839+0000    34     128    540431    540303   62.0706   53.7891   0.0107988  0.00805301
2021-10-28T08:36:04.617911+0000    35     127    556905    556778   62.1357   64.3555  0.00558458  0.00804488
2021-10-28T08:36:05.617987+0000    36     127    578669    578542   62.7711   85.0156  0.00425938  0.00796356
2021-10-28T08:36:06.618072+0000    37     128    600085    599957   63.3353   83.6523  0.00415978   0.0078926
2021-10-28T08:36:07.618146+0000    38     127    620792    620665   63.7971   80.8906  0.00837545  0.00783497
2021-10-28T08:36:08.618226+0000    39     128    633694    633566   63.4533   50.3945   0.0461429  0.00787462
2021-10-28T08:36:09.618309+0000 min lat: 0.0030086 max lat: 0.277395 avg lat: 0.00804688
2021-10-28T08:36:09.618309+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-28T08:36:09.618309+0000    40     128    635798    635670   62.0725   8.21875   0.0607886  0.00804688
2021-10-28T08:36:10.618394+0000    41     128    643759    643631   61.3169   31.0977   0.0079313  0.00815184
2021-10-28T08:36:11.618481+0000    42     127    658651    658524    61.242   58.1758  0.00913204  0.00816186
2021-10-28T08:36:12.618564+0000    43     128    672990    672862   61.1202   56.0078  0.00856741  0.00817826
2021-10-28T08:36:13.618645+0000    44     127    685923    685796   60.8792   50.5234    0.010413  0.00821066
2021-10-28T08:36:14.618719+0000    45     128    698763    698635   60.6408   50.1523   0.0102942  0.00824292
2021-10-28T08:36:15.618792+0000    46     128    711265    711137   60.3841   48.8359   0.0105324  0.00827822
2021-10-28T08:36:16.618860+0000    47     128    722652    722524   60.0456   44.4805  0.00967814  0.00832465
2021-10-28T08:36:17.618929+0000    48     128    732948    732820   59.6325   40.2188   0.0472648  0.00837658
2021-10-28T08:36:18.619005+0000    49     127    741767    741640   59.1186   34.4531  0.00963768   0.0084551
2021-10-28T08:36:19.619079+0000    50     127    758247    758120   59.2236    64.375  0.00539016  0.00844052
2021-10-28T08:36:20.619153+0000    51     127    781008    780881   59.8056   88.9102  0.00547706  0.00835851
2021-10-28T08:36:21.619228+0000    52     128    803590    803462   60.3516    88.207  0.00486663   0.0082828
2021-10-28T08:36:22.619299+0000    53     128    825038    824910   60.7936   83.7812  0.00330299  0.00822288
2021-10-28T08:36:23.619376+0000    54     128    840496    840368   60.7859   60.3828   0.0482696  0.00822008
2021-10-28T08:36:24.619459+0000    55     128    842544    842416   59.8261         8   0.0488012  0.00835016
2021-10-28T08:36:25.619532+0000    56     128    848470    848342   59.1712   23.1484  0.00875815  0.00844612
2021-10-28T08:36:26.619604+0000    57     128    861537    861409   59.0285    51.043  0.00899866  0.00846847
2021-10-28T08:36:27.619678+0000    58     128    874170    874042   58.8615   49.3477   0.0111523  0.00849213
2021-10-28T08:36:28.619757+0000    59     127    886928    886801   58.7085   49.8398   0.0101743  0.00851448
2021-10-28T08:36:29.619837+0000 min lat: 0.00249328 max lat: 0.277395 avg lat: 0.00853931
2021-10-28T08:36:29.619837+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-28T08:36:29.619837+0000    60      74    899276    899202   58.5374   48.4414  0.00908349  0.00853931
2021-10-28T08:36:30.619966+0000 Total time run:         60.0065
Total writes made:      899276
Write size:             4096
Object size:            4096
Bandwidth (MB/sec):     58.5403
Stddev Bandwidth:       22.7095
Max bandwidth (MB/sec): 89.7344
Min bandwidth (MB/sec): 8
Average IOPS:           14986
Stddev IOPS:            5813.63
Max IOPS:               22972
Min IOPS:               2048
Average Latency(s):     0.0085393
Stddev Latency(s):      0.00834841
Max latency(s):         0.277395
Min latency(s):         0.00249328

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:36:31,345883162-07:00] INFO: > Stop system activity collection services[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:36:31,351355473-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:391 - rados_bench() > kill -INT 218465

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:36:31,356519786-07:00] INFO: >> for OSD host[0m
# ./benchmarks/bench-rados:396 - rados_bench() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.2 kill -INT 130635
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:36:31,364154131-07:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.2 kill -INT 130635
[1] 01:36:31 [SUCCESS] ljishen@10.10.2.2

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:36:31,548924889-07:00] INFO: >>> Copy system activity file from OSD host[0m
## ./benchmarks/bench-rados:403 - rados_bench() > basename -- /tmp/bench-rados/sys_activity_4KB_write.dat.osd_host.2
# ./benchmarks/bench-rados:403 - rados_bench() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.2:/tmp/bench-rados/sys_activity_4KB_write.dat.osd_host.2 /mnt/sda8/ceph-research/scripts/output/sys_activity_4KB_write.dat.osd_host.2


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:36:31,724197748-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:36:55,895129675-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 899.28k objects, 3.4 GiB
    usage:   11 GiB used, 89 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:36:55,901773284-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:37:05,059113812-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 899.28k objects, 3.4 GiB
    usage:   11 GiB used, 89 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:37:05,066476647-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:37:14,351706998-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 899.28k objects, 3.4 GiB
    usage:   11 GiB used, 89 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:37:14,358405641-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:37:23,630912497-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 899.28k objects, 3.4 GiB
    usage:   11 GiB used, 89 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:37:23,637951742-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:37:33,028879868-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 899.28k objects, 3.4 GiB
    usage:   11 GiB used, 89 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:37:33,035702605-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:37:33,040878778-07:00] INFO: > The cluster is idle now.[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:37:33,044073208-07:00] STAGE: Start benchmarking of mode seq ...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:37:33,050134209-07:00] INFO: > Start system activity collection services[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:37:33,055386246-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:341 - rados_bench() > rm --force /mnt/sda8/ceph-research/scripts/output/sys_activity_4KB_seq.dat.rados_bench_host.2
# ./benchmarks/bench-rados:342 - rados_bench() > sar_pid_rados_bench_host=219809
# ./benchmarks/bench-rados:342 - rados_bench() > S_TIME_FORMAT=ISO
# ./benchmarks/bench-rados:342 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/output/sys_activity_4KB_seq.dat.rados_bench_host.2 2

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:37:33,061505246-07:00] INFO: >> for OSD host[0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:37:33,070093229-07:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.2 bash
10.10.2.2: b'130733\n'
[1] 01:37:33 [SUCCESS] ljishen@10.10.2.2
130733

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:37:33,257651759-07:00] INFO: > Run rados bench[0m
# ./benchmarks/bench-rados:379 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/output/rados_bench_4KB_seq.log.2
## ./benchmarks/bench-rados:376 - rados_bench() > count_cpus 32-63,96-127
# ./benchmarks/bench-rados:376 - rados_bench() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 128 --show-time
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:37:33,277036358-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:37:33,280400437-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '3d8df47e-37c9-11ec-b51d-53e6e728d2d3', '--', 'rados', 'bench', '99999999', 'seq', '--pool', 'bench_rados', '--concurrent-ios', '128', '--show-time'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 3d8df47e-37c9-11ec-b51d-53e6e728d2d3 -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 128 --show-time
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
hints = 1
2021-10-28T08:37:36.509413+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-28T08:37:36.509413+0000     0       0         0         0         0         0           -           0
2021-10-28T08:37:37.509520+0000     1     127     38305     38178   149.107   149.133  0.00379663  0.00333929
2021-10-28T08:37:38.509642+0000     2     128     79029     78901   154.081   159.074   0.0029888  0.00323781
2021-10-28T08:37:39.509768+0000     3     127    118986    118859   154.743   156.086  0.00273519  0.00322452
2021-10-28T08:37:40.509880+0000     4     127    160167    160040   156.268   160.863  0.00275166  0.00319336
2021-10-28T08:37:41.509947+0000     5     128    200563    200435   156.571   157.793  0.00313005  0.00318786
2021-10-28T08:37:42.510029+0000     6     128    242185    242057   157.571   162.586  0.00294112  0.00316772
2021-10-28T08:37:43.510153+0000     7     128    279530    279402   155.898   145.879 0.000324772  0.00318334
2021-10-28T08:37:44.510274+0000     8     127    315359    315232   153.904   139.961  0.00313877   0.0032434
2021-10-28T08:37:45.510372+0000     9     128    355236    355108   154.109   155.766  0.00322915  0.00323937
2021-10-28T08:37:46.510447+0000    10     127    394659    394532   154.097       154  0.00374783  0.00323949
2021-10-28T08:37:47.510570+0000    11     128    433235    433107   153.785   150.684   0.0033588  0.00324597
2021-10-28T08:37:48.510683+0000    12     128    470768    470640   153.186   146.613  0.00364384  0.00325886
2021-10-28T08:37:49.510770+0000    13     127    510189    510062   153.247   153.992  0.00304036  0.00325775
2021-10-28T08:37:50.510891+0000    14     127    549370    549243   153.232   153.051  0.00354951  0.00325814
2021-10-28T08:37:51.510955+0000    15     127    588397    588270   153.179   152.449  0.00386406  0.00325926
2021-10-28T08:37:52.511069+0000    16     128    626165    626037   152.825   147.527  0.00358373  0.00326669
2021-10-28T08:37:53.511153+0000    17     128    664452    664324   152.632   149.559  0.00261973  0.00327097
2021-10-28T08:37:54.511268+0000    18     127    702899    702772   152.495   150.188  0.00372625  0.00327391
2021-10-28T08:37:55.511389+0000    19     128    739681    739553    152.03   143.676  0.00328458  0.00328394
2021-10-28T08:37:56.511455+0000 min lat: 0.000220495 max lat: 0.110132 avg lat: 0.00328588
2021-10-28T08:37:56.511455+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-28T08:37:56.511455+0000    20     127    778133    778006   151.938   150.207  0.00331105  0.00328588
2021-10-28T08:37:57.511556+0000    21     127    815423    815296   151.639   145.664  0.00327524  0.00329236
2021-10-28T08:37:58.511674+0000    22     127    853263    853136   151.464   147.812  0.00372324  0.00329634
2021-10-28T08:37:59.511795+0000    23     128    889992    889864   151.116   143.469  0.00263717  0.00330388
2021-10-28T08:38:00.511944+0000 Total time run:       23.2489
Total reads made:     899276
Read size:            4096
Object size:          4096
Bandwidth (MB/sec):   151.095
Average IOPS:         38680
Stddev IOPS:          1495.35
Max IOPS:             41622
Min IOPS:             35830
Average Latency(s):   0.00330444
Max latency(s):       0.110132
Min latency(s):       0.000220495

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:38:01,169181618-07:00] INFO: > Stop system activity collection services[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:38:01,174603064-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:391 - rados_bench() > kill -INT 219809

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:38:01,179677166-07:00] INFO: >> for OSD host[0m
# ./benchmarks/bench-rados:396 - rados_bench() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.2 kill -INT 130733
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:38:01,187274892-07:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.2 kill -INT 130733
[1] 01:38:01 [SUCCESS] ljishen@10.10.2.2

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:38:01,373503597-07:00] INFO: >>> Copy system activity file from OSD host[0m
## ./benchmarks/bench-rados:403 - rados_bench() > basename -- /tmp/bench-rados/sys_activity_4KB_seq.dat.osd_host.2
# ./benchmarks/bench-rados:403 - rados_bench() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.2:/tmp/bench-rados/sys_activity_4KB_seq.dat.osd_host.2 /mnt/sda8/ceph-research/scripts/output/sys_activity_4KB_seq.dat.osd_host.2


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:38:01,533497074-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:38:25,598829567-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 899.28k objects, 3.4 GiB
    usage:   11 GiB used, 89 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:38:25,606003586-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:38:34,720888374-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 899.28k objects, 3.4 GiB
    usage:   11 GiB used, 89 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:38:34,727778017-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:38:44,060866274-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 899.28k objects, 3.4 GiB
    usage:   11 GiB used, 89 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:38:44,067696175-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:38:53,143090819-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 899.28k objects, 3.4 GiB
    usage:   11 GiB used, 89 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:38:53,150421873-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:39:02,508364685-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 899.28k objects, 3.4 GiB
    usage:   11 GiB used, 89 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:39:02,515600029-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:39:02,521366465-07:00] INFO: > The cluster is idle now.[0m


[1;7;39;49m[2021-10-28T01:39:02,523676077-07:00][RUNNING][ROUND 3/1/21] object_size=4KB[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:39:02,527226668-07:00] STAGE: Launch a Ceph cluster (CEPH_HOST: 10.10.2.1, OSD_HOST: 10.10.2.2) ...[0m
# ./benchmarks/bench-rados:183 - launch_ceph_cluster() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.1 bash -euo pipefail
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:39:02,536320995-07:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.1 bash -euo pipefail
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T01:39:03,139011702-07:00] INFO: > Remove existing clusters\x1b[0m\n'
10.10.2.1: b'## bash:17 -  > basename -- /var/lib/ceph/3d8df47e-37c9-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'# bash:17 -  > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm --host 10.10.2.1 --host 10.10.2.2 rm-cluster --force --fsid 3d8df47e-37c9-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T01:39:03,150003585-07:00] DEBUG: HOST_PARAMS: ['--host', '10.10.2.1', '--host', '10.10.2.2']\x1b[0m\n"
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T01:39:03,153630293-07:00] DEBUG: CEPHADM_PARAMS: ['rm-cluster', '--force', '--fsid', '3d8df47e-37c9-11ec-b51d-53e6e728d2d3']\x1b[0m\n"
10.10.2.1: b"# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:63 -  > /tmp/bench-rados/ceph-research/scripts/parallel-ssh --host 10.10.2.1 --host 10.10.2.2 'sudo python3 -u - rm-cluster --force --fsid 3d8df47e-37c9-11ec-b51d-53e6e728d2d3'\n"
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T01:39:03,160665037-07:00] DEBUG: command from stdin\x1b[0m\n'
10.10.2.1: b"# /tmp/bench-rados/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host 10.10.2.1 --host 10.10.2.2 'sudo python3 -u - rm-cluster --force --fsid 3d8df47e-37c9-11ec-b51d-53e6e728d2d3'\n"
10.10.2.1: b'[1] 01:39:08 [SUCCESS] 10.10.2.1\n[2] 01:39:11 [SUCCESS] 10.10.2.2\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T01:39:11,595661291-07:00] INFO: > Deploy a new cluster\x1b[0m\n'
10.10.2.1: b'# bash:24 -  > /tmp/bench-rados/ceph-research/scripts/ceph-deploy -m 10.10.2.1 -o node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/10.10.2.2:/dev/nvme0n1\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T01:39:11,608954330-07:00] STAGE: Verify passwordless SSH to hosts, passwordless sudo via ljishen, and the existence of devices on hosts...\x1b[0m\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T01:39:11,613911920-07:00] INFO: > Check the host for the monitor\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:181 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@10.10.2.1 sudo --non-interactive --validate\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T01:39:11,763138129-07:00] INFO: > Check hosts for OSDs\x1b[0m\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T01:39:11,767591881-07:00] INFO: >> Check host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:200 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us sudo --non-interactive --validate\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T01:39:11,918896207-07:00] INFO: >>> Check device '/dev/nvme0n1'\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:215 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us test -b /dev/nvme0n1\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:226 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us lsblk --nodeps --noheadings --output TYPE /dev/nvme0n1\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:227 - verify_parameters() > grep --word-regexp disk\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T01:39:12,203490515-07:00] STAGE: Reset devices on hosts...\x1b[0m\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T01:39:12,208507687-07:00] INFO: > Resetting device '/dev/nvme0n1' on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:288 - reset_host_devices() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us sudo sh\n'
10.10.2.1: b'  Removing ceph--b6def27c--a800--4138--bf68--3dbc7a167145-osd--block--e2cd6d7f--d263--40ba--b6a2--7f3635a246bc (253:0)\n'
10.10.2.1: b'  Archiving volume group "ceph-b6def27c-a800-4138-bf68-3dbc7a167145" metadata (seqno 5).\n'
10.10.2.1: b'  Releasing logical volume "osd-block-e2cd6d7f-d263-40ba-b6a2-7f3635a246bc"\n'
10.10.2.1: b'  Creating volume group backup "/etc/lvm/backup/ceph-b6def27c-a800-4138-bf68-3dbc7a167145" (seqno 6).\n'
10.10.2.1: b'  Logical volume "osd-block-e2cd6d7f-d263-40ba-b6a2-7f3635a246bc" successfully removed\n'
10.10.2.1: b'  Removing physical volume "/dev/nvme0n1" from volume group "ceph-b6def27c-a800-4138-bf68-3dbc7a167145"\n'
10.10.2.1: b'  Volume group "ceph-b6def27c-a800-4138-bf68-3dbc7a167145" successfully removed\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T01:39:12,561785117-07:00] STAGE: Bootstrap a new cluster...\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:317 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.1 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T01:39:12,571876387-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T01:39:12,575290164-07:00] DEBUG: CEPHADM_PARAMS: ['bootstrap', '--config', '/tmp/bench-rados/ceph-research/scripts/ceph.conf', '--mon-ip', '10.10.2.1', '--output-dir', '/tmp/bench-rados/deployment_data_root/etc/ceph', '--ssh-user', 'ljishen', '--skip-dashboard', '--no-minimize-config', '--allow-overwrite', '--allow-fqdn-hostname', '--orphan-initial-daemons', '--skip-monitoring-stack', '--single-host-defaults']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.1 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults\n'
10.10.2.1: b'Verifying podman|docker is present...\nVerifying lvm2 is present...\n'
10.10.2.1: b'Verifying time synchronization is in place...\n'
10.10.2.1: b'Unit ntp.service is enabled and running\n'
10.10.2.1: b'Repeating the final host check...\n'
10.10.2.1: b'podman|docker (/usr/bin/docker) is present\n'
10.10.2.1: b'systemctl is present\n'
10.10.2.1: b'lvcreate is present\n'
10.10.2.1: b'Unit ntp.service is enabled and running\n'
10.10.2.1: b'Host looks OK\n'
10.10.2.1: b'Cluster fsid: 86e7af38-37ca-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Verifying IP 10.10.2.1 port 3300 ...\n'
10.10.2.1: b'Verifying IP 10.10.2.1 port 6789 ...\n'
10.10.2.1: b'Mon IP `10.10.2.1` is in CIDR network `10.10.2.0/24`\n'
10.10.2.1: b'- internal network (--cluster-network) has not been provided, OSD replication will default to the public_network\n'
10.10.2.1: b'Adjusting default settings to suit single-host cluster...\n'
10.10.2.1: b'Pulling container image quay.io/ceph/ceph:v16...\n'
10.10.2.1: b'Ceph version: ceph version 16.2.6 (ee28fb57e47e9f88813e24bbf4c14496ca299d31) pacific (stable)\n'
10.10.2.1: b'Extracting ceph user uid/gid from container image...\n'
10.10.2.1: b'Creating initial keys...\n'
10.10.2.1: b'Creating initial monmap...\n'
10.10.2.1: b'Creating mon...\n'
10.10.2.1: b'Waiting for mon to start...\n'
10.10.2.1: b'Waiting for mon...\n'
10.10.2.1: b'mon is available\n'
10.10.2.1: b'Setting mon public_network to 10.10.2.0/24\n'
10.10.2.1: b'Wrote config to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf\n'
10.10.2.1: b'Wrote keyring to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring\n'
10.10.2.1: b'Creating mgr...\n'
10.10.2.1: b'Verifying port 9283 ...\n'
10.10.2.1: b'Waiting for mgr to start...\nWaiting for mgr...\n'
10.10.2.1: b'mgr not available, waiting (1/15)...\n'
10.10.2.1: b'mgr not available, waiting (2/15)...\n'
10.10.2.1: b'mgr is available\n'
10.10.2.1: b'Enabling cephadm module...\n'
10.10.2.1: b'Waiting for the mgr to restart...\n'
10.10.2.1: b'Waiting for mgr epoch 5...\n'
10.10.2.1: b'mgr epoch 5 is available\nSetting orchestrator backend to cephadm...\n'
10.10.2.1: b'Generating ssh key...\n'
10.10.2.1: b'Wrote public SSH key to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub\n'
10.10.2.1: b'Adding key to ljishen@localhost authorized_keys...\n'
10.10.2.1: b'Adding host node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us...\n'
10.10.2.1: b'Deploying unmanaged mon service...\n'
10.10.2.1: b'Deploying unmanaged mgr service...\n'
10.10.2.1: b'You can access the Ceph CLI with:\n\n\tsudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --fsid 86e7af38-37ca-11ec-b51d-53e6e728d2d3 -c /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf -k /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring\n\n'
10.10.2.1: b'Please consider enabling telemetry to help improve Ceph:\n\n\tceph telemetry on\n\nFor more information see:\n\n\thttps://docs.ceph.com/docs/pacific/mgr/telemetry/\n\n'
10.10.2.1: b'Bootstrap complete.\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T01:40:10,968266612-07:00] INFO: Sleep for 20 seconds\x1b[0m\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T01:40:30,975099498-07:00] INFO: > Set the default number of monitor daemons to 1\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:337 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T01:40:30,985464683-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T01:40:30,988856900-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mon', '1']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1\n'
10.10.2.1: b'Inferring fsid 86e7af38-37ca-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/86e7af38-37ca-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b'Scheduled mon update...\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T01:40:39,974412932-07:00] INFO: > Set the default number of manager daemons to 1\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:342 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T01:40:39,984703517-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T01:40:39,988513549-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mgr', '1']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1\n'
10.10.2.1: b'Inferring fsid 86e7af38-37ca-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/86e7af38-37ca-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b'Scheduled mgr update...\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T01:40:48,800621489-07:00] STAGE: Add hosts to the cluster...\x1b[0m\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T01:40:48,806934166-07:00] INFO: > Adding host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:361 - add_hosts() > ssh-copy-id -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -f -i /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\n'
10.10.2.1: b'/usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: "/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub"\n'
10.10.2.1: b'\nNumber of key(s) added: 1\n\nNow try logging into the machine, with:   "ssh -o \'GlobalKnownHostsFile=/dev/null\' -o \'LogLevel=ERROR\' -o \'PasswordAuthentication=no\' -o \'StrictHostKeyChecking=no\' -o \'UserKnownHostsFile=/dev/null\' \'ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\'"\nand check to make sure that only the key(s) you wanted were added.\n\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:367 - add_hosts() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us 10.10.2.2\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T01:40:49,017065026-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T01:40:49,020537664-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'host', 'add', 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us', '10.10.2.2']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us 10.10.2.2\n'
10.10.2.1: b'Inferring fsid 86e7af38-37ca-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/86e7af38-37ca-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b"Added host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us' with addr '10.10.2.2'\n"
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T01:40:58,018819621-07:00] INFO: Sleep for 20 seconds\x1b[0m\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T01:41:18,023613940-07:00] STAGE: Add OSDs to the cluster...\x1b[0m\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T01:41:18,030316319-07:00] INFO: > Adding devices ['/dev/nvme0n1'] on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us' as OSDs\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:388 - add_host_devices_as_osds() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T01:41:18,040016814-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T01:41:18,043893923-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'daemon', 'add', 'osd', 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1\n'
10.10.2.1: b'Inferring fsid 86e7af38-37ca-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/86e7af38-37ca-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b"Created osd(s) 0 on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'\n"
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T01:41:42,380120602-07:00] INFO: > Sleep for 20 seconds\x1b[0m\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T01:42:02,385201230-07:00] STAGE: Show Cluster Status\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:408 - show_status() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T01:42:02,395697401-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T01:42:02,399473460-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', '--status']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status\n'
10.10.2.1: b'Inferring fsid 86e7af38-37ca-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/86e7af38-37ca-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b'  cluster:\n    id:     86e7af38-37ca-11ec-b51d-53e6e728d2d3\n    health: HEALTH_WARN\n            1 pool(s) have no replicas configured\n \n  services:\n    mon: 1 daemons, quorum node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us (age 2m)\n    mgr: node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us.ohzjts(active, since 2m)\n    osd: 1 osds: 1 up (since 18s), 1 in (since 40s)\n \n  data:\n    pools:   1 pools, 1 pgs\n    objects: 0 objects, 0 B\n    usage:   4.7 MiB used, 100 GiB / 100 GiB avail\n    pgs:     1 active+clean\n \n'
[1] 01:42:11 [SUCCESS] ljishen@10.10.2.1

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:39:03,139011702-07:00] INFO: > Remove existing clusters[0m
## bash:17 -  > basename -- /var/lib/ceph/3d8df47e-37c9-11ec-b51d-53e6e728d2d3
# bash:17 -  > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm --host 10.10.2.1 --host 10.10.2.2 rm-cluster --force --fsid 3d8df47e-37c9-11ec-b51d-53e6e728d2d3
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:39:03,150003585-07:00] DEBUG: HOST_PARAMS: ['--host', '10.10.2.1', '--host', '10.10.2.2'][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:39:03,153630293-07:00] DEBUG: CEPHADM_PARAMS: ['rm-cluster', '--force', '--fsid', '3d8df47e-37c9-11ec-b51d-53e6e728d2d3'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:63 -  > /tmp/bench-rados/ceph-research/scripts/parallel-ssh --host 10.10.2.1 --host 10.10.2.2 'sudo python3 -u - rm-cluster --force --fsid 3d8df47e-37c9-11ec-b51d-53e6e728d2d3'
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:39:03,160665037-07:00] DEBUG: command from stdin[0m
# /tmp/bench-rados/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host 10.10.2.1 --host 10.10.2.2 'sudo python3 -u - rm-cluster --force --fsid 3d8df47e-37c9-11ec-b51d-53e6e728d2d3'
[1] 01:39:08 [SUCCESS] 10.10.2.1
[2] 01:39:11 [SUCCESS] 10.10.2.2

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:39:11,595661291-07:00] INFO: > Deploy a new cluster[0m
# bash:24 -  > /tmp/bench-rados/ceph-research/scripts/ceph-deploy -m 10.10.2.1 -o node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/10.10.2.2:/dev/nvme0n1


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:39:11,608954330-07:00] STAGE: Verify passwordless SSH to hosts, passwordless sudo via ljishen, and the existence of devices on hosts...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:39:11,613911920-07:00] INFO: > Check the host for the monitor[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:181 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@10.10.2.1 sudo --non-interactive --validate

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:39:11,763138129-07:00] INFO: > Check hosts for OSDs[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:39:11,767591881-07:00] INFO: >> Check host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:200 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us sudo --non-interactive --validate

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:39:11,918896207-07:00] INFO: >>> Check device '/dev/nvme0n1'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:215 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us test -b /dev/nvme0n1
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:226 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us lsblk --nodeps --noheadings --output TYPE /dev/nvme0n1
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:227 - verify_parameters() > grep --word-regexp disk


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:39:12,203490515-07:00] STAGE: Reset devices on hosts...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:39:12,208507687-07:00] INFO: > Resetting device '/dev/nvme0n1' on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:288 - reset_host_devices() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us sudo sh
  Removing ceph--b6def27c--a800--4138--bf68--3dbc7a167145-osd--block--e2cd6d7f--d263--40ba--b6a2--7f3635a246bc (253:0)
  Archiving volume group "ceph-b6def27c-a800-4138-bf68-3dbc7a167145" metadata (seqno 5).
  Releasing logical volume "osd-block-e2cd6d7f-d263-40ba-b6a2-7f3635a246bc"
  Creating volume group backup "/etc/lvm/backup/ceph-b6def27c-a800-4138-bf68-3dbc7a167145" (seqno 6).
  Logical volume "osd-block-e2cd6d7f-d263-40ba-b6a2-7f3635a246bc" successfully removed
  Removing physical volume "/dev/nvme0n1" from volume group "ceph-b6def27c-a800-4138-bf68-3dbc7a167145"
  Volume group "ceph-b6def27c-a800-4138-bf68-3dbc7a167145" successfully removed


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:39:12,561785117-07:00] STAGE: Bootstrap a new cluster...[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:317 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.1 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:39:12,571876387-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:39:12,575290164-07:00] DEBUG: CEPHADM_PARAMS: ['bootstrap', '--config', '/tmp/bench-rados/ceph-research/scripts/ceph.conf', '--mon-ip', '10.10.2.1', '--output-dir', '/tmp/bench-rados/deployment_data_root/etc/ceph', '--ssh-user', 'ljishen', '--skip-dashboard', '--no-minimize-config', '--allow-overwrite', '--allow-fqdn-hostname', '--orphan-initial-daemons', '--skip-monitoring-stack', '--single-host-defaults'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.1 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults
Verifying podman|docker is present...
Verifying lvm2 is present...
Verifying time synchronization is in place...
Unit ntp.service is enabled and running
Repeating the final host check...
podman|docker (/usr/bin/docker) is present
systemctl is present
lvcreate is present
Unit ntp.service is enabled and running
Host looks OK
Cluster fsid: 86e7af38-37ca-11ec-b51d-53e6e728d2d3
Verifying IP 10.10.2.1 port 3300 ...
Verifying IP 10.10.2.1 port 6789 ...
Mon IP `10.10.2.1` is in CIDR network `10.10.2.0/24`
- internal network (--cluster-network) has not been provided, OSD replication will default to the public_network
Adjusting default settings to suit single-host cluster...
Pulling container image quay.io/ceph/ceph:v16...
Ceph version: ceph version 16.2.6 (ee28fb57e47e9f88813e24bbf4c14496ca299d31) pacific (stable)
Extracting ceph user uid/gid from container image...
Creating initial keys...
Creating initial monmap...
Creating mon...
Waiting for mon to start...
Waiting for mon...
mon is available
Setting mon public_network to 10.10.2.0/24
Wrote config to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf
Wrote keyring to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring
Creating mgr...
Verifying port 9283 ...
Waiting for mgr to start...
Waiting for mgr...
mgr not available, waiting (1/15)...
mgr not available, waiting (2/15)...
mgr is available
Enabling cephadm module...
Waiting for the mgr to restart...
Waiting for mgr epoch 5...
mgr epoch 5 is available
Setting orchestrator backend to cephadm...
Generating ssh key...
Wrote public SSH key to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub
Adding key to ljishen@localhost authorized_keys...
Adding host node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us...
Deploying unmanaged mon service...
Deploying unmanaged mgr service...
You can access the Ceph CLI with:

	sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --fsid 86e7af38-37ca-11ec-b51d-53e6e728d2d3 -c /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf -k /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring

Please consider enabling telemetry to help improve Ceph:

	ceph telemetry on

For more information see:

	https://docs.ceph.com/docs/pacific/mgr/telemetry/

Bootstrap complete.

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:40:10,968266612-07:00] INFO: Sleep for 20 seconds[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:40:30,975099498-07:00] INFO: > Set the default number of monitor daemons to 1[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:337 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:40:30,985464683-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:40:30,988856900-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mon', '1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1
Inferring fsid 86e7af38-37ca-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/86e7af38-37ca-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Scheduled mon update...

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:40:39,974412932-07:00] INFO: > Set the default number of manager daemons to 1[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:342 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:40:39,984703517-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:40:39,988513549-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mgr', '1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1
Inferring fsid 86e7af38-37ca-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/86e7af38-37ca-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Scheduled mgr update...


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:40:48,800621489-07:00] STAGE: Add hosts to the cluster...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:40:48,806934166-07:00] INFO: > Adding host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:361 - add_hosts() > ssh-copy-id -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -f -i /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us
/usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: "/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub"

Number of key(s) added: 1

Now try logging into the machine, with:   "ssh -o 'GlobalKnownHostsFile=/dev/null' -o 'LogLevel=ERROR' -o 'PasswordAuthentication=no' -o 'StrictHostKeyChecking=no' -o 'UserKnownHostsFile=/dev/null' 'ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'"
and check to make sure that only the key(s) you wanted were added.

# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:367 - add_hosts() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us 10.10.2.2
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:40:49,017065026-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:40:49,020537664-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'host', 'add', 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us', '10.10.2.2'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us 10.10.2.2
Inferring fsid 86e7af38-37ca-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/86e7af38-37ca-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Added host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us' with addr '10.10.2.2'

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:40:58,018819621-07:00] INFO: Sleep for 20 seconds[0m


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:41:18,023613940-07:00] STAGE: Add OSDs to the cluster...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:41:18,030316319-07:00] INFO: > Adding devices ['/dev/nvme0n1'] on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us' as OSDs[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:388 - add_host_devices_as_osds() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:41:18,040016814-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:41:18,043893923-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'daemon', 'add', 'osd', 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1
Inferring fsid 86e7af38-37ca-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/86e7af38-37ca-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Created osd(s) 0 on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:41:42,380120602-07:00] INFO: > Sleep for 20 seconds[0m


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:42:02,385201230-07:00] STAGE: Show Cluster Status[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:408 - show_status() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:42:02,395697401-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:42:02,399473460-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', '--status'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status
Inferring fsid 86e7af38-37ca-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/86e7af38-37ca-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
  cluster:
    id:     86e7af38-37ca-11ec-b51d-53e6e728d2d3
    health: HEALTH_WARN
            1 pool(s) have no replicas configured
 
  services:
    mon: 1 daemons, quorum node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us (age 2m)
    mgr: node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us.ohzjts(active, since 2m)
    osd: 1 osds: 1 up (since 18s), 1 in (since 40s)
 
  data:
    pools:   1 pools, 1 pgs
    objects: 0 objects, 0 B
    usage:   4.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     1 active+clean
 

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:42:11,068648380-07:00] INFO: > Copy configuration files to local[0m
# ./benchmarks/bench-rados:220 - launch_ceph_cluster() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.1 sudo chown --changes --recursive ljishen /tmp/bench-rados/deployment_data_root/etc/ceph
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:42:11,076346296-07:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.1 sudo chown --changes --recursive ljishen /tmp/bench-rados/deployment_data_root/etc/ceph
[1] 01:42:11 [SUCCESS] ljishen@10.10.2.1
# ./benchmarks/bench-rados:223 - launch_ceph_cluster() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.1:/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf
# ./benchmarks/bench-rados:226 - launch_ceph_cluster() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.1:/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:42:11,552905167-07:00] STAGE: Update local ceph.conf as a client...[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:42:11,556002994-07:00] STAGE: Configure Ceph pool...[0m
# ./benchmarks/bench-rados:257 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_max_backfills 32
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:42:11,577722002-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:42:11,580439353-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '86e7af38-37ca-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'config', 'set', 'osd', 'osd_max_backfills', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 86e7af38-37ca-11ec-b51d-53e6e728d2d3 -- ceph config set osd osd_max_backfills 32
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:258 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_active 32
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:42:16,021888306-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:42:16,024920260-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '86e7af38-37ca-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_active', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 86e7af38-37ca-11ec-b51d-53e6e728d2d3 -- ceph config set osd osd_recovery_max_active 32
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:259 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_single_start 8
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:42:20,363312480-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:42:20,366374500-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '86e7af38-37ca-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_single_start', '8'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 86e7af38-37ca-11ec-b51d-53e6e728d2d3 -- ceph config set osd osd_recovery_max_single_start 8
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:260 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_op_priority 63
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:42:24,530982586-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:42:24,533927215-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '86e7af38-37ca-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_op_priority', '63'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 86e7af38-37ca-11ec-b51d-53e6e728d2d3 -- ceph config set osd osd_recovery_op_priority 63
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:262 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config show-with-defaults osd.0
# ./benchmarks/bench-rados:263 - configure_ceph_pool() > grep 'osd_max_backfills\|osd_recovery'
# ./benchmarks/bench-rados:263 - configure_ceph_pool() > column -t -s ' '
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
osd_max_backfills                        32         mon
osd_recovery_cost                        20971520   default
osd_recovery_delay_start                 0.000000   default
osd_recovery_max_active                  32         mon
osd_recovery_max_active_hdd              3          default
osd_recovery_max_active_ssd              10         default
osd_recovery_max_chunk                   8388608    default
osd_recovery_max_omap_entries_per_chunk  8096       default
osd_recovery_max_single_start            8          mon
osd_recovery_op_priority                 3          default
osd_recovery_op_warn_multiple            16         default
osd_recovery_priority                    5          default
osd_recovery_retry_interval              30.000000  default
osd_recovery_sleep                       0.000000   default
osd_recovery_sleep_hdd                   0.100000   default
osd_recovery_sleep_hybrid                0.025000   default
osd_recovery_sleep_ssd                   0.000000   default
# ./benchmarks/bench-rados:265 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool create bench_rados 128 128 replicated --size 1 --pg-num-min 128
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:42:32,995523247-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:42:32,998662262-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '86e7af38-37ca-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'create', 'bench_rados', '128', '128', 'replicated', '--size', '1', '--pg-num-min', '128'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 86e7af38-37ca-11ec-b51d-53e6e728d2d3 -- ceph osd pool create bench_rados 128 128 replicated --size 1 --pg-num-min 128
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
pool 'bench_rados' created
# ./benchmarks/bench-rados:267 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados min_size 1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:42:37,509866783-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:42:37,512906130-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '86e7af38-37ca-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'min_size', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 86e7af38-37ca-11ec-b51d-53e6e728d2d3 -- ceph osd pool set bench_rados min_size 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 min_size to 1
# ./benchmarks/bench-rados:268 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados pg_autoscale_mode off
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:42:42,967185428-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:42:42,970123174-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '86e7af38-37ca-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'pg_autoscale_mode', 'off'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 86e7af38-37ca-11ec-b51d-53e6e728d2d3 -- ceph osd pool set bench_rados pg_autoscale_mode off
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 pg_autoscale_mode to off
# ./benchmarks/bench-rados:269 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool application enable bench_rados benchmark
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:42:47,418885145-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:42:47,421996609-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '86e7af38-37ca-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'application', 'enable', 'bench_rados', 'benchmark'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 86e7af38-37ca-11ec-b51d-53e6e728d2d3 -- ceph osd pool application enable bench_rados benchmark
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
enabled application 'benchmark' on pool 'bench_rados'
# ./benchmarks/bench-rados:270 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados noscrub 1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:42:52,416386967-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:42:52,419509221-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '86e7af38-37ca-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'noscrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 86e7af38-37ca-11ec-b51d-53e6e728d2d3 -- ceph osd pool set bench_rados noscrub 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 noscrub to 1
# ./benchmarks/bench-rados:271 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados nodeep-scrub 1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:42:56,710459217-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:42:56,713475992-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '86e7af38-37ca-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'nodeep-scrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 86e7af38-37ca-11ec-b51d-53e6e728d2d3 -- ceph osd pool set bench_rados nodeep-scrub 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 nodeep-scrub to 1
# ./benchmarks/bench-rados:272 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool ls detail
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:43:01,149852603-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:43:01,152808162-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '86e7af38-37ca-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'ls', 'detail'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 86e7af38-37ca-11ec-b51d-53e6e728d2d3 -- ceph osd pool ls detail
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
pool 1 'device_health_metrics' replicated size 1 min_size 1 crush_rule 0 object_hash rjenkins pg_num 64 pgp_num 64 autoscale_mode on last_change 19 lfor 0/0/16 flags hashpspool stripe_width 0 pg_num_min 1 application mgr_devicehealth
pool 2 'bench_rados' replicated size 1 min_size 1 crush_rule 0 object_hash rjenkins pg_num 128 pgp_num 128 autoscale_mode off last_change 20 flags hashpspool,noscrub,nodeep-scrub stripe_width 0 pg_num_min 128 application benchmark

# ./benchmarks/bench-rados:273 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd df tree
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:43:05,333160835-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:43:05,336302124-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '86e7af38-37ca-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'df', 'tree'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 86e7af38-37ca-11ec-b51d-53e6e728d2d3 -- ceph osd df tree
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
ID  CLASS  WEIGHT   REWEIGHT  SIZE     RAW USE  DATA     OMAP  META     AVAIL    %USE  VAR   PGS  STATUS  TYPE NAME      
-1         0.09769         -  100 GiB  5.7 MiB  168 KiB   0 B  5.6 MiB  100 GiB  0.01  1.00    -          root default   
-3         0.09769         -  100 GiB  5.7 MiB  168 KiB   0 B  5.6 MiB  100 GiB  0.01  1.00    -              host node-1
 0    ssd  0.09769   1.00000  100 GiB  5.7 MiB  168 KiB   0 B  5.6 MiB  100 GiB  0.01  1.00  192      up          osd.0  
                       TOTAL  100 GiB  5.7 MiB  168 KiB   0 B  5.6 MiB  100 GiB  0.01                                    
MIN/MAX VAR: 1.00/1.00  STDDEV: 0


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:43:09,791995095-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:43:34,012292016-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:43:43,312179686-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:43:52,709898741-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:44:02,035651219-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:44:02,042434381-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:44:11,132387971-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:44:11,139369277-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:44:20,391304609-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:44:20,397995387-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:44:29,589670962-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:44:29,596332165-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:44:38,679958848-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:44:38,686851637-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:44:38,692391667-07:00] INFO: > The cluster is idle now.[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:44:38,695692958-07:00] STAGE: Start benchmarking of mode write ...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:44:38,701609468-07:00] INFO: > Start system activity collection services[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:44:38,706559325-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:341 - rados_bench() > rm --force /mnt/sda8/ceph-research/scripts/output/sys_activity_4KB_write.dat.rados_bench_host.3
# ./benchmarks/bench-rados:342 - rados_bench() > sar_pid_rados_bench_host=225609
# ./benchmarks/bench-rados:342 - rados_bench() > S_TIME_FORMAT=ISO
# ./benchmarks/bench-rados:342 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/output/sys_activity_4KB_write.dat.rados_bench_host.3 2

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:44:38,712736596-07:00] INFO: >> for OSD host[0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:44:38,721453452-07:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.2 bash
10.10.2.2: b'134366\n'
[1] 01:44:38 [SUCCESS] ljishen@10.10.2.2
134366

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:44:38,913768936-07:00] INFO: > Run rados bench[0m
# ./benchmarks/bench-rados:370 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/output/rados_bench_4KB_write.log.3
## ./benchmarks/bench-rados:361 - rados_bench() > count_cpus 32-63,96-127
# ./benchmarks/bench-rados:361 - rados_bench() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 60 write --pool bench_rados -b 4096 -O 4096 --concurrent-ios 128 --show-time --write-object --write-omap --write-xattr --no-cleanup
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:44:38,933705545-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:44:38,936564873-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '86e7af38-37ca-11ec-b51d-53e6e728d2d3', '--', 'rados', 'bench', '60', 'write', '--pool', 'bench_rados', '-b', '4096', '-O', '4096', '--concurrent-ios', '128', '--show-time', '--write-object', '--write-omap', '--write-xattr', '--no-cleanup'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 86e7af38-37ca-11ec-b51d-53e6e728d2d3 -- rados bench 60 write --pool bench_rados -b 4096 -O 4096 --concurrent-ios 128 --show-time --write-object --write-omap --write-xattr --no-cleanup
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
hints = 1
2021-10-28T08:44:42.307568+0000 Maintaining 128 concurrent writes of 4096 bytes to objects of size 4096 for up to 60 seconds or 0 objects
2021-10-28T08:44:42.307578+0000 Object prefix: benchmark_data_node-2.bluefield2.ucsc-cmps10_7
2021-10-28T08:44:42.308073+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-28T08:44:42.308073+0000     0       0         0         0         0         0           -           0
2021-10-28T08:44:43.308158+0000     1     128     24218     24090    94.097   94.1016  0.00506502  0.00529358
2021-10-28T08:44:44.308237+0000     2     127     49288     49161   96.0114   97.9336  0.00376621  0.00519935
2021-10-28T08:44:45.308311+0000     3     127     73635     73508   95.7071   95.1055  0.00496955  0.00521791
2021-10-28T08:44:46.308425+0000     4     128     94324     94196    91.981   80.8125  0.00521234  0.00542999
2021-10-28T08:44:47.308535+0000     5     127    113104    112977   88.2558   73.3633  0.00908805  0.00565923
2021-10-28T08:44:48.308645+0000     6     127    126008    125881   81.9465   50.4062   0.0116623  0.00609394
2021-10-28T08:44:49.308718+0000     7     127    145120    144993   80.9042   74.6562  0.00692868  0.00617442
2021-10-28T08:44:50.308829+0000     8     128    167141    167013    81.542   86.0156  0.00560425  0.00612648
2021-10-28T08:44:51.308943+0000     9     127    189782    189655   82.3079   88.4453  0.00593702  0.00607152
2021-10-28T08:44:52.309059+0000    10     127    211054    210927   82.3855   83.0938  0.00388402   0.0060661
2021-10-28T08:44:53.309137+0000    11     128    219562    219434   77.9167   33.2305    0.063564  0.00638888
2021-10-28T08:44:54.309256+0000    12     128    222122    221994   72.2568        10     0.04591  0.00689837
2021-10-28T08:44:55.309342+0000    13     127    231071    230944   69.3876   34.9609   0.0100461  0.00720072
2021-10-28T08:44:56.309472+0000    14     128    244186    244058   68.0899   51.2266   0.0099341  0.00733905
2021-10-28T08:44:57.309551+0000    15     128    260418    260290   67.7773   63.4062  0.00551411  0.00737432
2021-10-28T08:44:58.309653+0000    16     127    283558    283431   69.1903   90.3945  0.00562295  0.00722363
2021-10-28T08:44:59.309762+0000    17     127    306249    306122   70.3337   88.6367  0.00712466  0.00710554
2021-10-28T08:45:00.309871+0000    18     127    324358    324231   70.3557   70.7383  0.00800153  0.00710338
2021-10-28T08:45:01.309947+0000    19     128    339254    339126   69.7149   58.1836  0.00921365  0.00716806
2021-10-28T08:45:02.310029+0000 min lat: 0.0028016 max lat: 0.0825237 avg lat: 0.0072427
2021-10-28T08:45:02.310029+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-28T08:45:02.310029+0000    20     127    353472    353345   69.0061    55.543  0.00691721   0.0072427
2021-10-28T08:45:03.310155+0000    21     127    374860    374733    69.698   83.5469  0.00581937  0.00717151
2021-10-28T08:45:04.310277+0000    22     127    397939    397812   70.6273   90.1523  0.00843643   0.0070771
2021-10-28T08:45:05.310353+0000    23     128    421270    421142   71.5185   91.1328  0.00455007  0.00698897
2021-10-28T08:45:06.310466+0000    24     128    429268    429140   69.8401   31.2422   0.0408015  0.00715043
2021-10-28T08:45:07.310579+0000    25     128    431696    431568   67.4258   9.48438   0.0413679  0.00740629
2021-10-28T08:45:08.310690+0000    26     128    441051    440923   66.2379    36.543  0.00783587  0.00754594
2021-10-28T08:45:09.310766+0000    27     128    454214    454086   65.6888    51.418   0.0100049  0.00760818
2021-10-28T08:45:10.310882+0000    28     127    460564    460437   64.2287   24.8086   0.0122879  0.00778197
2021-10-28T08:45:11.310991+0000    29     127    471158    471031   63.4408   41.3828   0.0104097  0.00787804
2021-10-28T08:45:12.311065+0000    30     128    483477    483349   62.9299   48.1172   0.0101481  0.00794213
2021-10-28T08:45:13.311144+0000    31     128    496635    496507   62.5578   51.3984  0.00685825  0.00799044
2021-10-28T08:45:14.311254+0000    32     127    517330    517203   63.1289   80.8438  0.00584854  0.00791804
2021-10-28T08:45:15.311369+0000    33     128    534372    534244   63.2329   66.5664  0.00871627  0.00790493
2021-10-28T08:45:16.311485+0000    34     128    547680    547552   62.9018   51.9844  0.00950708  0.00794661
2021-10-28T08:45:17.311559+0000    35     128    566229    566101   63.1747    72.457  0.00389128   0.0079126
2021-10-28T08:45:18.311668+0000    36     128    588975    588847   63.8877   88.8516  0.00449006  0.00782427
2021-10-28T08:45:19.311778+0000    37     127    611317    611190   64.5196   87.2773  0.00608401  0.00774727
2021-10-28T08:45:20.311888+0000    38     128    632505    632377   64.9994   82.7617  0.00899066  0.00769002
2021-10-28T08:45:21.311962+0000    39     128    635252    635124   63.6079   10.7305   0.0701821  0.00784932
2021-10-28T08:45:22.312072+0000 min lat: 0.00107466 max lat: 0.381037 avg lat: 0.00802027
2021-10-28T08:45:22.312072+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-28T08:45:22.312072+0000    40     128    638337    638209   62.3189   12.0508   0.0136733  0.00802027
2021-10-28T08:45:23.312188+0000    41     128    651043    650915   62.0094   49.6328  0.00922952  0.00806123
2021-10-28T08:45:24.312299+0000    42     128    665793    665665   61.9046   57.6172  0.00947687  0.00807436
2021-10-28T08:45:25.312373+0000    43     128    679389    679261      61.7   53.1094  0.00863472  0.00810161
2021-10-28T08:45:26.312484+0000    44     128    692236    692108   61.4381   50.1836    0.010561  0.00813594
2021-10-28T08:45:27.312600+0000    45     128    704677    704549   61.1527   48.5977  0.00943403  0.00817368
2021-10-28T08:45:28.312718+0000    46     128    716860    716732   60.8577   47.5898   0.0104268  0.00821335
2021-10-28T08:45:29.312793+0000    47     128    729145    729017   60.5838   47.9883   0.0103468  0.00825048
2021-10-28T08:45:30.312907+0000    48     127    738132    738005    60.053   35.1094  0.00861514  0.00832393
2021-10-28T08:45:31.313015+0000    49     128    751495    751367   59.8925   52.1953  0.00563272  0.00834632
2021-10-28T08:45:32.313124+0000    50     128    773688    773560   60.4283   86.6914  0.00422103  0.00826753
2021-10-28T08:45:33.313197+0000    51     127    796235    796108   60.9703   88.0781  0.00555974  0.00819871
2021-10-28T08:45:34.313306+0000    52     127    818768    818641   61.4903   88.0195  0.00551037  0.00812941
2021-10-28T08:45:35.313415+0000    53     127    839733    839606   61.8751   81.8945  0.00960845   0.0080784
2021-10-28T08:45:36.313526+0000    54     128    843902    843774   61.0308   16.2812   0.0561115  0.00818607
2021-10-28T08:45:37.313601+0000    55     128    846386    846258   60.0976   9.70312   0.0163935  0.00831695
2021-10-28T08:45:38.313712+0000    56     127    861209    861082   60.0583   57.9062  0.00850933  0.00832323
2021-10-28T08:45:39.313824+0000    57     127    876042    875915   60.0211   57.9414  0.00889521  0.00832845
2021-10-28T08:45:40.313941+0000    58     127    889620    889493   59.9006   53.0391   0.0134508  0.00834496
2021-10-28T08:45:41.314020+0000    59     127    902919    902792   59.7657   51.9492   0.0101678  0.00836384
2021-10-28T08:45:42.314133+0000 Total time run:         60.0034
Total writes made:      916880
Write size:             4096
Object size:            4096
Bandwidth (MB/sec):     59.6893
Stddev Bandwidth:       25.2681
Max bandwidth (MB/sec): 97.9336
Min bandwidth (MB/sec): 9.48438
Average IOPS:           15280
Stddev IOPS:            6468.63
Max IOPS:               25071
Min IOPS:               2428
Average Latency(s):     0.00837507
Stddev Latency(s):      0.00878465
Max latency(s):         0.381037
Min latency(s):         0.000569394

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:45:43,041018553-07:00] INFO: > Stop system activity collection services[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:45:43,046282973-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:391 - rados_bench() > kill -INT 225609

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:45:43,051065626-07:00] INFO: >> for OSD host[0m
# ./benchmarks/bench-rados:396 - rados_bench() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.2 kill -INT 134366
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:45:43,059271820-07:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.2 kill -INT 134366
[1] 01:45:43 [SUCCESS] ljishen@10.10.2.2

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:45:43,244989591-07:00] INFO: >>> Copy system activity file from OSD host[0m
## ./benchmarks/bench-rados:403 - rados_bench() > basename -- /tmp/bench-rados/sys_activity_4KB_write.dat.osd_host.3
# ./benchmarks/bench-rados:403 - rados_bench() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.2:/tmp/bench-rados/sys_activity_4KB_write.dat.osd_host.3 /mnt/sda8/ceph-research/scripts/output/sys_activity_4KB_write.dat.osd_host.3


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:45:43,419322789-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:46:07,636231611-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 916.88k objects, 3.5 GiB
    usage:   11 GiB used, 88 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:46:07,643218858-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:46:16,892759208-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 916.88k objects, 3.5 GiB
    usage:   11 GiB used, 88 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:46:16,900216431-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:46:26,033759011-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 916.88k objects, 3.5 GiB
    usage:   11 GiB used, 88 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:46:26,040766426-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:46:35,278016724-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 916.88k objects, 3.5 GiB
    usage:   11 GiB used, 88 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:46:35,285210721-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:46:44,546491972-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 916.88k objects, 3.5 GiB
    usage:   11 GiB used, 88 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:46:44,553263452-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:46:44,558368503-07:00] INFO: > The cluster is idle now.[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:46:44,561758912-07:00] STAGE: Start benchmarking of mode seq ...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:46:44,567548903-07:00] INFO: > Start system activity collection services[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:46:44,572370649-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:341 - rados_bench() > rm --force /mnt/sda8/ceph-research/scripts/output/sys_activity_4KB_seq.dat.rados_bench_host.3
# ./benchmarks/bench-rados:342 - rados_bench() > sar_pid_rados_bench_host=226978
# ./benchmarks/bench-rados:342 - rados_bench() > S_TIME_FORMAT=ISO
# ./benchmarks/bench-rados:342 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/output/sys_activity_4KB_seq.dat.rados_bench_host.3 2

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:46:44,578753988-07:00] INFO: >> for OSD host[0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:46:44,587337863-07:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.2 bash
10.10.2.2: b'134470\n'
[1] 01:46:44 [SUCCESS] ljishen@10.10.2.2
134470

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:46:44,778134364-07:00] INFO: > Run rados bench[0m
# ./benchmarks/bench-rados:379 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/output/rados_bench_4KB_seq.log.3
## ./benchmarks/bench-rados:376 - rados_bench() > count_cpus 32-63,96-127
# ./benchmarks/bench-rados:376 - rados_bench() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 128 --show-time
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:46:44,797363891-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:46:44,800207981-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '86e7af38-37ca-11ec-b51d-53e6e728d2d3', '--', 'rados', 'bench', '99999999', 'seq', '--pool', 'bench_rados', '--concurrent-ios', '128', '--show-time'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 86e7af38-37ca-11ec-b51d-53e6e728d2d3 -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 128 --show-time
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
hints = 1
2021-10-28T08:46:48.083651+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-28T08:46:48.083651+0000     0       0         0         0         0         0           -           0
2021-10-28T08:46:49.083756+0000     1     127     42650     42523   166.076   166.105  0.00276451  0.00300049
2021-10-28T08:46:50.083835+0000     2     127     79910     79783   155.806   145.547  0.00372712  0.00320156
2021-10-28T08:46:51.083915+0000     3     127    116503    116376   151.514   142.941  0.00392325  0.00329453
2021-10-28T08:46:52.083993+0000     4     127    152341    152214   148.631   139.992  0.00383128  0.00335796
2021-10-28T08:46:53.084062+0000     5     128    188309    188181   147.002   140.496  0.00410007  0.00339654
2021-10-28T08:46:54.084129+0000     6     127    227095    226968   147.752   151.512  0.00315734  0.00337966
2021-10-28T08:46:55.084196+0000     7     128    265761    265633    148.22   151.035  0.00339841  0.00336882
2021-10-28T08:46:56.084274+0000     8     128    300256    300128   146.534   134.746  0.00315943  0.00340809
2021-10-28T08:46:57.084357+0000     9     127    338476    338349    146.84   149.301  0.00322574   0.0034009
2021-10-28T08:46:58.084427+0000    10     127    376860    376733   147.149   149.938  0.00305189  0.00339389
2021-10-28T08:46:59.084496+0000    11     127    414621    414494    147.18   147.504  0.00343665  0.00339312
2021-10-28T08:47:00.084574+0000    12     128    453308    453180   147.507   151.117  0.00304975  0.00338543
2021-10-28T08:47:01.084644+0000    13     128    491502    491374   147.636   149.195  0.00322359  0.00338298
2021-10-28T08:47:02.084710+0000    14     128    529985    529857   147.828   150.324  0.00308392  0.00337861
2021-10-28T08:47:03.084779+0000    15     127    568786    568659   148.076    151.57  0.00324313  0.00337284
2021-10-28T08:47:04.084854+0000    16     128    608832    608704   148.598   156.426  0.00176738  0.00336097
2021-10-28T08:47:05.084929+0000    17     127    648519    648392   148.975   155.031  0.00314966  0.00335243
2021-10-28T08:47:06.084996+0000    18     127    687429    687302   149.142   151.992  0.00338632   0.0033488
2021-10-28T08:47:07.085066+0000    19     128    724210    724082   148.854   143.672  0.00232429  0.00335529
2021-10-28T08:47:08.085148+0000 min lat: 0.000259759 max lat: 0.0952858 avg lat: 0.00335891
2021-10-28T08:47:08.085148+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-28T08:47:08.085148+0000    20     128    761490    761362   148.692   145.625  0.00260999  0.00335891
2021-10-28T08:47:09.085227+0000    21     127    799893    799766   148.754   150.016  0.00341986   0.0033577
2021-10-28T08:47:10.085294+0000    22     128    842590    842462   149.573   166.781  0.00296708   0.0033393
2021-10-28T08:47:11.085369+0000    23     127    885601    885474   150.375   168.016  0.00288335  0.00332139
2021-10-28T08:47:12.085462+0000 Total time run:       23.7738
Total reads made:     916880
Read size:            4096
Object size:          4096
Bandwidth (MB/sec):   150.651
Average IOPS:         38566
Stddev IOPS:          2113.32
Max IOPS:             43012
Min IOPS:             34495
Average Latency(s):   0.0033154
Max latency(s):       0.0952858
Min latency(s):       0.000259759

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:47:12,745745616-07:00] INFO: > Stop system activity collection services[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:47:12,750719920-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:391 - rados_bench() > kill -INT 226978

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:47:12,755813749-07:00] INFO: >> for OSD host[0m
# ./benchmarks/bench-rados:396 - rados_bench() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.2 kill -INT 134470
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:47:12,763203344-07:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.2 kill -INT 134470
[1] 01:47:12 [SUCCESS] ljishen@10.10.2.2

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:47:12,949288627-07:00] INFO: >>> Copy system activity file from OSD host[0m
## ./benchmarks/bench-rados:403 - rados_bench() > basename -- /tmp/bench-rados/sys_activity_4KB_seq.dat.osd_host.3
# ./benchmarks/bench-rados:403 - rados_bench() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.2:/tmp/bench-rados/sys_activity_4KB_seq.dat.osd_host.3 /mnt/sda8/ceph-research/scripts/output/sys_activity_4KB_seq.dat.osd_host.3


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:47:13,113150042-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:47:37,324552143-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 916.88k objects, 3.5 GiB
    usage:   11 GiB used, 88 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:47:37,331685435-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:47:46,740252927-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 916.88k objects, 3.5 GiB
    usage:   11 GiB used, 88 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:47:46,747716552-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:47:55,879975189-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 916.88k objects, 3.5 GiB
    usage:   11 GiB used, 88 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:47:55,887177802-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:48:05,136634419-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 916.88k objects, 3.5 GiB
    usage:   11 GiB used, 88 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:48:05,143666851-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:48:14,350778891-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 916.88k objects, 3.5 GiB
    usage:   11 GiB used, 88 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:48:14,357716014-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:48:14,362824220-07:00] INFO: > The cluster is idle now.[0m


[1;7;39;49m[2021-10-28T01:48:14,366182869-07:00][RUNNING][ROUND 1/2/21] object_size=16KB[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:48:14,369623793-07:00] STAGE: Launch a Ceph cluster (CEPH_HOST: 10.10.2.1, OSD_HOST: 10.10.2.2) ...[0m
# ./benchmarks/bench-rados:183 - launch_ceph_cluster() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.1 bash -euo pipefail
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:48:14,378952512-07:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.1 bash -euo pipefail
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T01:48:14,773658276-07:00] INFO: > Remove existing clusters\x1b[0m\n'
10.10.2.1: b'## bash:17 -  > basename -- /var/lib/ceph/86e7af38-37ca-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'# bash:17 -  > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm --host 10.10.2.1 --host 10.10.2.2 rm-cluster --force --fsid 86e7af38-37ca-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T01:48:14,784471243-07:00] DEBUG: HOST_PARAMS: ['--host', '10.10.2.1', '--host', '10.10.2.2']\x1b[0m\n"
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T01:48:14,787882184-07:00] DEBUG: CEPHADM_PARAMS: ['rm-cluster', '--force', '--fsid', '86e7af38-37ca-11ec-b51d-53e6e728d2d3']\x1b[0m\n"
10.10.2.1: b"# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:63 -  > /tmp/bench-rados/ceph-research/scripts/parallel-ssh --host 10.10.2.1 --host 10.10.2.2 'sudo python3 -u - rm-cluster --force --fsid 86e7af38-37ca-11ec-b51d-53e6e728d2d3'\n"
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T01:48:14,796493332-07:00] DEBUG: command from stdin\x1b[0m\n'
10.10.2.1: b"# /tmp/bench-rados/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host 10.10.2.1 --host 10.10.2.2 'sudo python3 -u - rm-cluster --force --fsid 86e7af38-37ca-11ec-b51d-53e6e728d2d3'\n"
10.10.2.1: b'[1] 01:48:20 [SUCCESS] 10.10.2.1\n[2] 01:48:23 [SUCCESS] 10.10.2.2\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T01:48:23,495022575-07:00] INFO: > Deploy a new cluster\x1b[0m\n'
10.10.2.1: b'# bash:24 -  > /tmp/bench-rados/ceph-research/scripts/ceph-deploy -m 10.10.2.1 -o node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/10.10.2.2:/dev/nvme0n1\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T01:48:23,508829970-07:00] STAGE: Verify passwordless SSH to hosts, passwordless sudo via ljishen, and the existence of devices on hosts...\x1b[0m\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T01:48:23,513376517-07:00] INFO: > Check the host for the monitor\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:181 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@10.10.2.1 sudo --non-interactive --validate\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T01:48:23,663593799-07:00] INFO: > Check hosts for OSDs\x1b[0m\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T01:48:23,668450619-07:00] INFO: >> Check host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:200 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us sudo --non-interactive --validate\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T01:48:23,818694231-07:00] INFO: >>> Check device '/dev/nvme0n1'\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:215 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us test -b /dev/nvme0n1\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:226 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us lsblk --nodeps --noheadings --output TYPE /dev/nvme0n1\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:227 - verify_parameters() > grep --word-regexp disk\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T01:48:24,103442457-07:00] STAGE: Reset devices on hosts...\x1b[0m\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T01:48:24,108506868-07:00] INFO: > Resetting device '/dev/nvme0n1' on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:288 - reset_host_devices() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us sudo sh\n'
10.10.2.1: b'  Removing ceph--4b054872--e847--4782--987b--1c822b968bea-osd--block--dbedcc6e--039f--4c09--a682--8d06af66a2b8 (253:0)\n'
10.10.2.1: b'  Archiving volume group "ceph-4b054872-e847-4782-987b-1c822b968bea" metadata (seqno 5).\n'
10.10.2.1: b'  Releasing logical volume "osd-block-dbedcc6e-039f-4c09-a682-8d06af66a2b8"\n'
10.10.2.1: b'  Creating volume group backup "/etc/lvm/backup/ceph-4b054872-e847-4782-987b-1c822b968bea" (seqno 6).\n'
10.10.2.1: b'  Logical volume "osd-block-dbedcc6e-039f-4c09-a682-8d06af66a2b8" successfully removed\n'
10.10.2.1: b'  Removing physical volume "/dev/nvme0n1" from volume group "ceph-4b054872-e847-4782-987b-1c822b968bea"\n'
10.10.2.1: b'  Volume group "ceph-4b054872-e847-4782-987b-1c822b968bea" successfully removed\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T01:48:24,485607610-07:00] STAGE: Bootstrap a new cluster...\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:317 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.1 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T01:48:24,495529101-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T01:48:24,499231942-07:00] DEBUG: CEPHADM_PARAMS: ['bootstrap', '--config', '/tmp/bench-rados/ceph-research/scripts/ceph.conf', '--mon-ip', '10.10.2.1', '--output-dir', '/tmp/bench-rados/deployment_data_root/etc/ceph', '--ssh-user', 'ljishen', '--skip-dashboard', '--no-minimize-config', '--allow-overwrite', '--allow-fqdn-hostname', '--orphan-initial-daemons', '--skip-monitoring-stack', '--single-host-defaults']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.1 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults\n'
10.10.2.1: b'Verifying podman|docker is present...\nVerifying lvm2 is present...\n'
10.10.2.1: b'Verifying time synchronization is in place...\n'
10.10.2.1: b'Unit ntp.service is enabled and running\n'
10.10.2.1: b'Repeating the final host check...\n'
10.10.2.1: b'podman|docker (/usr/bin/docker) is present\n'
10.10.2.1: b'systemctl is present\n'
10.10.2.1: b'lvcreate is present\n'
10.10.2.1: b'Unit ntp.service is enabled and running\n'
10.10.2.1: b'Host looks OK\n'
10.10.2.1: b'Cluster fsid: cfe0a75c-37cb-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Verifying IP 10.10.2.1 port 3300 ...\n'
10.10.2.1: b'Verifying IP 10.10.2.1 port 6789 ...\n'
10.10.2.1: b'Mon IP `10.10.2.1` is in CIDR network `10.10.2.0/24`\n'
10.10.2.1: b'- internal network (--cluster-network) has not been provided, OSD replication will default to the public_network\n'
10.10.2.1: b'Adjusting default settings to suit single-host cluster...\n'
10.10.2.1: b'Pulling container image quay.io/ceph/ceph:v16...\n'
10.10.2.1: b'Ceph version: ceph version 16.2.6 (ee28fb57e47e9f88813e24bbf4c14496ca299d31) pacific (stable)\n'
10.10.2.1: b'Extracting ceph user uid/gid from container image...\n'
10.10.2.1: b'Creating initial keys...\n'
10.10.2.1: b'Creating initial monmap...\n'
10.10.2.1: b'Creating mon...\n'
10.10.2.1: b'Waiting for mon to start...\n'
10.10.2.1: b'Waiting for mon...\n'
10.10.2.1: b'mon is available\n'
10.10.2.1: b'Setting mon public_network to 10.10.2.0/24\n'
10.10.2.1: b'Wrote config to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf\n'
10.10.2.1: b'Wrote keyring to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring\n'
10.10.2.1: b'Creating mgr...\n'
10.10.2.1: b'Verifying port 9283 ...\n'
10.10.2.1: b'Waiting for mgr to start...\nWaiting for mgr...\n'
10.10.2.1: b'mgr not available, waiting (1/15)...\n'
10.10.2.1: b'mgr not available, waiting (2/15)...\n'
10.10.2.1: b'mgr is available\n'
10.10.2.1: b'Enabling cephadm module...\n'
10.10.2.1: b'Waiting for the mgr to restart...\n'
10.10.2.1: b'Waiting for mgr epoch 5...\n'
10.10.2.1: b'mgr epoch 5 is available\n'
10.10.2.1: b'Setting orchestrator backend to cephadm...\n'
10.10.2.1: b'Generating ssh key...\n'
10.10.2.1: b'Wrote public SSH key to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub\n'
10.10.2.1: b'Adding key to ljishen@localhost authorized_keys...\n'
10.10.2.1: b'Adding host node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us...\n'
10.10.2.1: b'Deploying unmanaged mon service...\n'
10.10.2.1: b'Deploying unmanaged mgr service...\n'
10.10.2.1: b'You can access the Ceph CLI with:\n\n\tsudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --fsid cfe0a75c-37cb-11ec-b51d-53e6e728d2d3 -c /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf -k /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring\n\n'
10.10.2.1: b'Please consider enabling telemetry to help improve Ceph:\n\n\tceph telemetry on\n\nFor more information see:\n\n\thttps://docs.ceph.com/docs/pacific/mgr/telemetry/\n\n'
10.10.2.1: b'Bootstrap complete.\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T01:49:20,613024149-07:00] INFO: Sleep for 20 seconds\x1b[0m\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T01:49:40,620480609-07:00] INFO: > Set the default number of monitor daemons to 1\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:337 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T01:49:40,630551801-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T01:49:40,634064093-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mon', '1']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1\n'
10.10.2.1: b'Inferring fsid cfe0a75c-37cb-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/cfe0a75c-37cb-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b'Scheduled mon update...\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T01:49:49,480526573-07:00] INFO: > Set the default number of manager daemons to 1\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:342 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T01:49:49,490778445-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T01:49:49,494516421-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mgr', '1']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1\n'
10.10.2.1: b'Inferring fsid cfe0a75c-37cb-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/cfe0a75c-37cb-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b'Scheduled mgr update...\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T01:49:58,093892703-07:00] STAGE: Add hosts to the cluster...\x1b[0m\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T01:49:58,100663391-07:00] INFO: > Adding host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:361 - add_hosts() > ssh-copy-id -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -f -i /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\n'
10.10.2.1: b'/usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: "/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub"\n'
10.10.2.1: b'\nNumber of key(s) added: 1\n\nNow try logging into the machine, with:   "ssh -o \'GlobalKnownHostsFile=/dev/null\' -o \'LogLevel=ERROR\' -o \'PasswordAuthentication=no\' -o \'StrictHostKeyChecking=no\' -o \'UserKnownHostsFile=/dev/null\' \'ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\'"\nand check to make sure that only the key(s) you wanted were added.\n\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:367 - add_hosts() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us 10.10.2.2\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T01:49:58,314064958-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T01:49:58,317443279-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'host', 'add', 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us', '10.10.2.2']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us 10.10.2.2\n'
10.10.2.1: b'Inferring fsid cfe0a75c-37cb-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/cfe0a75c-37cb-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b"Added host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us' with addr '10.10.2.2'\n"
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T01:50:07,619382738-07:00] INFO: Sleep for 20 seconds\x1b[0m\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T01:50:27,624083349-07:00] STAGE: Add OSDs to the cluster...\x1b[0m\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T01:50:27,630992627-07:00] INFO: > Adding devices ['/dev/nvme0n1'] on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us' as OSDs\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:388 - add_host_devices_as_osds() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T01:50:27,641148869-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T01:50:27,644970052-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'daemon', 'add', 'osd', 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1\n'
10.10.2.1: b'Inferring fsid cfe0a75c-37cb-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/cfe0a75c-37cb-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b"Created osd(s) 0 on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'\n"
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T01:50:51,530936579-07:00] INFO: > Sleep for 20 seconds\x1b[0m\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T01:51:11,536757946-07:00] STAGE: Show Cluster Status\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:408 - show_status() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T01:51:11,547277370-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T01:51:11,550721785-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', '--status']\x1b[0m\n# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status\n"
10.10.2.1: b'Inferring fsid cfe0a75c-37cb-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/cfe0a75c-37cb-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b'  cluster:\n    id:     cfe0a75c-37cb-11ec-b51d-53e6e728d2d3\n    health: HEALTH_WARN\n            1 pool(s) have no replicas configured\n \n  services:\n    mon: 1 daemons, quorum node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us (age 2m)\n    mgr: node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us.ochgih(active, since 2m)\n    osd: 1 osds: 1 up (since 16s), 1 in (since 40s)\n \n  data:\n    pools:   1 pools, 1 pgs\n    objects: 0 objects, 0 B\n    usage:   4.7 MiB used, 100 GiB / 100 GiB avail\n    pgs:     1 active+clean\n \n'
[1] 01:51:19 [SUCCESS] ljishen@10.10.2.1

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:48:14,773658276-07:00] INFO: > Remove existing clusters[0m
## bash:17 -  > basename -- /var/lib/ceph/86e7af38-37ca-11ec-b51d-53e6e728d2d3
# bash:17 -  > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm --host 10.10.2.1 --host 10.10.2.2 rm-cluster --force --fsid 86e7af38-37ca-11ec-b51d-53e6e728d2d3
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:48:14,784471243-07:00] DEBUG: HOST_PARAMS: ['--host', '10.10.2.1', '--host', '10.10.2.2'][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:48:14,787882184-07:00] DEBUG: CEPHADM_PARAMS: ['rm-cluster', '--force', '--fsid', '86e7af38-37ca-11ec-b51d-53e6e728d2d3'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:63 -  > /tmp/bench-rados/ceph-research/scripts/parallel-ssh --host 10.10.2.1 --host 10.10.2.2 'sudo python3 -u - rm-cluster --force --fsid 86e7af38-37ca-11ec-b51d-53e6e728d2d3'
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:48:14,796493332-07:00] DEBUG: command from stdin[0m
# /tmp/bench-rados/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host 10.10.2.1 --host 10.10.2.2 'sudo python3 -u - rm-cluster --force --fsid 86e7af38-37ca-11ec-b51d-53e6e728d2d3'
[1] 01:48:20 [SUCCESS] 10.10.2.1
[2] 01:48:23 [SUCCESS] 10.10.2.2

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:48:23,495022575-07:00] INFO: > Deploy a new cluster[0m
# bash:24 -  > /tmp/bench-rados/ceph-research/scripts/ceph-deploy -m 10.10.2.1 -o node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/10.10.2.2:/dev/nvme0n1


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:48:23,508829970-07:00] STAGE: Verify passwordless SSH to hosts, passwordless sudo via ljishen, and the existence of devices on hosts...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:48:23,513376517-07:00] INFO: > Check the host for the monitor[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:181 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@10.10.2.1 sudo --non-interactive --validate

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:48:23,663593799-07:00] INFO: > Check hosts for OSDs[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:48:23,668450619-07:00] INFO: >> Check host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:200 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us sudo --non-interactive --validate

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:48:23,818694231-07:00] INFO: >>> Check device '/dev/nvme0n1'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:215 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us test -b /dev/nvme0n1
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:226 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us lsblk --nodeps --noheadings --output TYPE /dev/nvme0n1
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:227 - verify_parameters() > grep --word-regexp disk


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:48:24,103442457-07:00] STAGE: Reset devices on hosts...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:48:24,108506868-07:00] INFO: > Resetting device '/dev/nvme0n1' on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:288 - reset_host_devices() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us sudo sh
  Removing ceph--4b054872--e847--4782--987b--1c822b968bea-osd--block--dbedcc6e--039f--4c09--a682--8d06af66a2b8 (253:0)
  Archiving volume group "ceph-4b054872-e847-4782-987b-1c822b968bea" metadata (seqno 5).
  Releasing logical volume "osd-block-dbedcc6e-039f-4c09-a682-8d06af66a2b8"
  Creating volume group backup "/etc/lvm/backup/ceph-4b054872-e847-4782-987b-1c822b968bea" (seqno 6).
  Logical volume "osd-block-dbedcc6e-039f-4c09-a682-8d06af66a2b8" successfully removed
  Removing physical volume "/dev/nvme0n1" from volume group "ceph-4b054872-e847-4782-987b-1c822b968bea"
  Volume group "ceph-4b054872-e847-4782-987b-1c822b968bea" successfully removed


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:48:24,485607610-07:00] STAGE: Bootstrap a new cluster...[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:317 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.1 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:48:24,495529101-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:48:24,499231942-07:00] DEBUG: CEPHADM_PARAMS: ['bootstrap', '--config', '/tmp/bench-rados/ceph-research/scripts/ceph.conf', '--mon-ip', '10.10.2.1', '--output-dir', '/tmp/bench-rados/deployment_data_root/etc/ceph', '--ssh-user', 'ljishen', '--skip-dashboard', '--no-minimize-config', '--allow-overwrite', '--allow-fqdn-hostname', '--orphan-initial-daemons', '--skip-monitoring-stack', '--single-host-defaults'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.1 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults
Verifying podman|docker is present...
Verifying lvm2 is present...
Verifying time synchronization is in place...
Unit ntp.service is enabled and running
Repeating the final host check...
podman|docker (/usr/bin/docker) is present
systemctl is present
lvcreate is present
Unit ntp.service is enabled and running
Host looks OK
Cluster fsid: cfe0a75c-37cb-11ec-b51d-53e6e728d2d3
Verifying IP 10.10.2.1 port 3300 ...
Verifying IP 10.10.2.1 port 6789 ...
Mon IP `10.10.2.1` is in CIDR network `10.10.2.0/24`
- internal network (--cluster-network) has not been provided, OSD replication will default to the public_network
Adjusting default settings to suit single-host cluster...
Pulling container image quay.io/ceph/ceph:v16...
Ceph version: ceph version 16.2.6 (ee28fb57e47e9f88813e24bbf4c14496ca299d31) pacific (stable)
Extracting ceph user uid/gid from container image...
Creating initial keys...
Creating initial monmap...
Creating mon...
Waiting for mon to start...
Waiting for mon...
mon is available
Setting mon public_network to 10.10.2.0/24
Wrote config to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf
Wrote keyring to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring
Creating mgr...
Verifying port 9283 ...
Waiting for mgr to start...
Waiting for mgr...
mgr not available, waiting (1/15)...
mgr not available, waiting (2/15)...
mgr is available
Enabling cephadm module...
Waiting for the mgr to restart...
Waiting for mgr epoch 5...
mgr epoch 5 is available
Setting orchestrator backend to cephadm...
Generating ssh key...
Wrote public SSH key to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub
Adding key to ljishen@localhost authorized_keys...
Adding host node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us...
Deploying unmanaged mon service...
Deploying unmanaged mgr service...
You can access the Ceph CLI with:

	sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --fsid cfe0a75c-37cb-11ec-b51d-53e6e728d2d3 -c /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf -k /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring

Please consider enabling telemetry to help improve Ceph:

	ceph telemetry on

For more information see:

	https://docs.ceph.com/docs/pacific/mgr/telemetry/

Bootstrap complete.

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:49:20,613024149-07:00] INFO: Sleep for 20 seconds[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:49:40,620480609-07:00] INFO: > Set the default number of monitor daemons to 1[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:337 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:49:40,630551801-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:49:40,634064093-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mon', '1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1
Inferring fsid cfe0a75c-37cb-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/cfe0a75c-37cb-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Scheduled mon update...

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:49:49,480526573-07:00] INFO: > Set the default number of manager daemons to 1[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:342 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:49:49,490778445-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:49:49,494516421-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mgr', '1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1
Inferring fsid cfe0a75c-37cb-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/cfe0a75c-37cb-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Scheduled mgr update...


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:49:58,093892703-07:00] STAGE: Add hosts to the cluster...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:49:58,100663391-07:00] INFO: > Adding host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:361 - add_hosts() > ssh-copy-id -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -f -i /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us
/usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: "/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub"

Number of key(s) added: 1

Now try logging into the machine, with:   "ssh -o 'GlobalKnownHostsFile=/dev/null' -o 'LogLevel=ERROR' -o 'PasswordAuthentication=no' -o 'StrictHostKeyChecking=no' -o 'UserKnownHostsFile=/dev/null' 'ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'"
and check to make sure that only the key(s) you wanted were added.

# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:367 - add_hosts() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us 10.10.2.2
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:49:58,314064958-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:49:58,317443279-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'host', 'add', 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us', '10.10.2.2'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us 10.10.2.2
Inferring fsid cfe0a75c-37cb-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/cfe0a75c-37cb-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Added host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us' with addr '10.10.2.2'

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:50:07,619382738-07:00] INFO: Sleep for 20 seconds[0m


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:50:27,624083349-07:00] STAGE: Add OSDs to the cluster...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:50:27,630992627-07:00] INFO: > Adding devices ['/dev/nvme0n1'] on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us' as OSDs[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:388 - add_host_devices_as_osds() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:50:27,641148869-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:50:27,644970052-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'daemon', 'add', 'osd', 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1
Inferring fsid cfe0a75c-37cb-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/cfe0a75c-37cb-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Created osd(s) 0 on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:50:51,530936579-07:00] INFO: > Sleep for 20 seconds[0m


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:51:11,536757946-07:00] STAGE: Show Cluster Status[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:408 - show_status() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:51:11,547277370-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:51:11,550721785-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', '--status'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status
Inferring fsid cfe0a75c-37cb-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/cfe0a75c-37cb-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
  cluster:
    id:     cfe0a75c-37cb-11ec-b51d-53e6e728d2d3
    health: HEALTH_WARN
            1 pool(s) have no replicas configured
 
  services:
    mon: 1 daemons, quorum node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us (age 2m)
    mgr: node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us.ochgih(active, since 2m)
    osd: 1 osds: 1 up (since 16s), 1 in (since 40s)
 
  data:
    pools:   1 pools, 1 pgs
    objects: 0 objects, 0 B
    usage:   4.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     1 active+clean
 

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:51:19,847475362-07:00] INFO: > Copy configuration files to local[0m
# ./benchmarks/bench-rados:220 - launch_ceph_cluster() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.1 sudo chown --changes --recursive ljishen /tmp/bench-rados/deployment_data_root/etc/ceph
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:51:19,855232861-07:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.1 sudo chown --changes --recursive ljishen /tmp/bench-rados/deployment_data_root/etc/ceph
[1] 01:51:20 [SUCCESS] ljishen@10.10.2.1
# ./benchmarks/bench-rados:223 - launch_ceph_cluster() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.1:/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf
# ./benchmarks/bench-rados:226 - launch_ceph_cluster() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.1:/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:51:20,329735583-07:00] STAGE: Update local ceph.conf as a client...[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:51:20,332993442-07:00] STAGE: Configure Ceph pool...[0m
# ./benchmarks/bench-rados:257 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_max_backfills 32
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:51:20,354391565-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:51:20,357263978-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'cfe0a75c-37cb-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'config', 'set', 'osd', 'osd_max_backfills', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid cfe0a75c-37cb-11ec-b51d-53e6e728d2d3 -- ceph config set osd osd_max_backfills 32
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:258 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_active 32
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:51:24,665699470-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:51:24,668863111-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'cfe0a75c-37cb-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_active', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid cfe0a75c-37cb-11ec-b51d-53e6e728d2d3 -- ceph config set osd osd_recovery_max_active 32
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:259 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_single_start 8
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:51:28,815273700-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:51:28,818200084-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'cfe0a75c-37cb-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_single_start', '8'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid cfe0a75c-37cb-11ec-b51d-53e6e728d2d3 -- ceph config set osd osd_recovery_max_single_start 8
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:260 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_op_priority 63
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:51:33,091385058-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:51:33,094415008-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'cfe0a75c-37cb-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_op_priority', '63'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid cfe0a75c-37cb-11ec-b51d-53e6e728d2d3 -- ceph config set osd osd_recovery_op_priority 63
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:262 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config show-with-defaults osd.0
# ./benchmarks/bench-rados:263 - configure_ceph_pool() > grep 'osd_max_backfills\|osd_recovery'
# ./benchmarks/bench-rados:263 - configure_ceph_pool() > column -t -s ' '
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
osd_max_backfills                        32         mon
osd_recovery_cost                        20971520   default
osd_recovery_delay_start                 0.000000   default
osd_recovery_max_active                  32         mon
osd_recovery_max_active_hdd              3          default
osd_recovery_max_active_ssd              10         default
osd_recovery_max_chunk                   8388608    default
osd_recovery_max_omap_entries_per_chunk  8096       default
osd_recovery_max_single_start            8          mon
osd_recovery_op_priority                 63         mon
osd_recovery_op_warn_multiple            16         default
osd_recovery_priority                    5          default
osd_recovery_retry_interval              30.000000  default
osd_recovery_sleep                       0.000000   default
osd_recovery_sleep_hdd                   0.100000   default
osd_recovery_sleep_hybrid                0.025000   default
osd_recovery_sleep_ssd                   0.000000   default
# ./benchmarks/bench-rados:265 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool create bench_rados 128 128 replicated --size 1 --pg-num-min 128
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:51:41,615486463-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:51:41,618602274-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'cfe0a75c-37cb-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'create', 'bench_rados', '128', '128', 'replicated', '--size', '1', '--pg-num-min', '128'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid cfe0a75c-37cb-11ec-b51d-53e6e728d2d3 -- ceph osd pool create bench_rados 128 128 replicated --size 1 --pg-num-min 128
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
pool 'bench_rados' created
# ./benchmarks/bench-rados:267 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados min_size 1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:51:46,081415867-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:51:46,084643850-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'cfe0a75c-37cb-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'min_size', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid cfe0a75c-37cb-11ec-b51d-53e6e728d2d3 -- ceph osd pool set bench_rados min_size 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 min_size to 1
# ./benchmarks/bench-rados:268 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados pg_autoscale_mode off
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:51:50,437069967-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:51:50,440228579-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'cfe0a75c-37cb-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'pg_autoscale_mode', 'off'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid cfe0a75c-37cb-11ec-b51d-53e6e728d2d3 -- ceph osd pool set bench_rados pg_autoscale_mode off
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 pg_autoscale_mode to off
# ./benchmarks/bench-rados:269 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool application enable bench_rados benchmark
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:51:55,020234822-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:51:55,023515353-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'cfe0a75c-37cb-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'application', 'enable', 'bench_rados', 'benchmark'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid cfe0a75c-37cb-11ec-b51d-53e6e728d2d3 -- ceph osd pool application enable bench_rados benchmark
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
enabled application 'benchmark' on pool 'bench_rados'
# ./benchmarks/bench-rados:270 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados noscrub 1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:52:00,103947698-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:52:00,107273535-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'cfe0a75c-37cb-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'noscrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid cfe0a75c-37cb-11ec-b51d-53e6e728d2d3 -- ceph osd pool set bench_rados noscrub 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 noscrub to 1
# ./benchmarks/bench-rados:271 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados nodeep-scrub 1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:52:04,444547619-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:52:04,447540639-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'cfe0a75c-37cb-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'nodeep-scrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid cfe0a75c-37cb-11ec-b51d-53e6e728d2d3 -- ceph osd pool set bench_rados nodeep-scrub 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 nodeep-scrub to 1
# ./benchmarks/bench-rados:272 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool ls detail
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:52:09,052965290-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:52:09,055978398-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'cfe0a75c-37cb-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'ls', 'detail'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid cfe0a75c-37cb-11ec-b51d-53e6e728d2d3 -- ceph osd pool ls detail
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
pool 1 'device_health_metrics' replicated size 1 min_size 1 crush_rule 0 object_hash rjenkins pg_num 64 pgp_num 64 autoscale_mode on last_change 20 lfor 0/0/17 flags hashpspool stripe_width 0 pg_num_min 1 application mgr_devicehealth
pool 2 'bench_rados' replicated size 1 min_size 1 crush_rule 0 object_hash rjenkins pg_num 128 pgp_num 128 autoscale_mode off last_change 19 flags hashpspool,noscrub,nodeep-scrub stripe_width 0 pg_num_min 128 application benchmark

# ./benchmarks/bench-rados:273 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd df tree
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:52:13,370992644-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:52:13,373863253-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'cfe0a75c-37cb-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'df', 'tree'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid cfe0a75c-37cb-11ec-b51d-53e6e728d2d3 -- ceph osd df tree
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
ID  CLASS  WEIGHT   REWEIGHT  SIZE     RAW USE  DATA     OMAP  META     AVAIL    %USE  VAR   PGS  STATUS  TYPE NAME      
-1         0.09769         -  100 GiB  5.7 MiB  168 KiB   0 B  5.6 MiB  100 GiB  0.01  1.00    -          root default   
-3         0.09769         -  100 GiB  5.7 MiB  168 KiB   0 B  5.6 MiB  100 GiB  0.01  1.00    -              host node-1
 0    ssd  0.09769   1.00000  100 GiB  5.7 MiB  168 KiB   0 B  5.6 MiB  100 GiB  0.01  1.00  192      up          osd.0  
                       TOTAL  100 GiB  5.7 MiB  168 KiB   0 B  5.6 MiB  100 GiB  0.01                                    
MIN/MAX VAR: 1.00/1.00  STDDEV: 0


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:52:17,673384803-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:52:41,926590824-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:52:51,260057013-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:53:00,628010839-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:53:09,834925049-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:53:09,841804733-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:53:19,042487338-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:53:19,049620721-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:53:28,399347457-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:53:28,406205560-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:53:37,715753201-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:53:37,722843902-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:53:46,981074569-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:53:46,988055604-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:53:46,993489253-07:00] INFO: > The cluster is idle now.[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:53:46,996897035-07:00] STAGE: Start benchmarking of mode write ...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:53:47,002810729-07:00] INFO: > Start system activity collection services[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:53:47,007686176-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:341 - rados_bench() > rm --force /mnt/sda8/ceph-research/scripts/output/sys_activity_16KB_write.dat.rados_bench_host.1
# ./benchmarks/bench-rados:342 - rados_bench() > sar_pid_rados_bench_host=232808
# ./benchmarks/bench-rados:342 - rados_bench() > S_TIME_FORMAT=ISO
# ./benchmarks/bench-rados:342 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/output/sys_activity_16KB_write.dat.rados_bench_host.1 2

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:53:47,014240627-07:00] INFO: >> for OSD host[0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:53:47,022845923-07:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.2 bash
10.10.2.2: b'138094\n'
[1] 01:53:47 [SUCCESS] ljishen@10.10.2.2
138094

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:53:47,214931563-07:00] INFO: > Run rados bench[0m
# ./benchmarks/bench-rados:370 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/output/rados_bench_16KB_write.log.1
## ./benchmarks/bench-rados:361 - rados_bench() > count_cpus 32-63,96-127
# ./benchmarks/bench-rados:361 - rados_bench() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 60 write --pool bench_rados -b 16384 -O 16384 --concurrent-ios 128 --show-time --write-object --write-omap --write-xattr --no-cleanup
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:53:47,234882429-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:53:47,238005113-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'cfe0a75c-37cb-11ec-b51d-53e6e728d2d3', '--', 'rados', 'bench', '60', 'write', '--pool', 'bench_rados', '-b', '16384', '-O', '16384', '--concurrent-ios', '128', '--show-time', '--write-object', '--write-omap', '--write-xattr', '--no-cleanup'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid cfe0a75c-37cb-11ec-b51d-53e6e728d2d3 -- rados bench 60 write --pool bench_rados -b 16384 -O 16384 --concurrent-ios 128 --show-time --write-object --write-omap --write-xattr --no-cleanup
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
hints = 1
2021-10-28T08:53:50.528439+0000 Maintaining 128 concurrent writes of 16384 bytes to objects of size 16384 for up to 60 seconds or 0 objects
2021-10-28T08:53:50.528450+0000 Object prefix: benchmark_data_node-2.bluefield2.ucsc-cmps10_7
2021-10-28T08:53:50.529443+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-28T08:53:50.529443+0000     0       0         0         0         0         0           -           0
2021-10-28T08:53:51.529603+0000     1     127     14297     14170   221.384   221.406  0.00867192  0.00897543
2021-10-28T08:53:52.529733+0000     2     127     28153     28026   218.928     216.5  0.00920255  0.00910749
2021-10-28T08:53:53.529824+0000     3     127     35369     35242   183.532    112.75   0.0245135   0.0108339
2021-10-28T08:53:54.529940+0000     4     128     42066     41938   163.802   104.625  0.00741204   0.0121967
2021-10-28T08:53:55.530038+0000     5     128     56575     56447   176.378   226.703  0.00910115   0.0113232
2021-10-28T08:53:56.530151+0000     6     128     64063     63935   166.479       117   0.0907906   0.0119087
2021-10-28T08:53:57.530231+0000     7     128     65599     65471   146.125        24   0.0880566   0.0136159
2021-10-28T08:53:58.530344+0000     8     128     68607     68479   133.734        47   0.0181726   0.0149302
2021-10-28T08:53:59.530462+0000     9     128     74815     74687   129.651        97   0.0195081   0.0154021
2021-10-28T08:54:00.530577+0000    10     128     81983     81855   127.885       112  0.00956393   0.0156294
2021-10-28T08:54:01.530659+0000    11     128     94143     94015    133.53       190   0.0250162   0.0149641
2021-10-28T08:54:02.530779+0000    12     128    100159    100031   130.235        94   0.0202919   0.0153444
2021-10-28T08:54:03.530900+0000    13     127    107055    106928   128.505   107.766  0.00960482   0.0155566
2021-10-28T08:54:04.531020+0000    14     128    119613    119485   133.339   196.203  0.00994711   0.0149913
2021-10-28T08:54:05.531115+0000    15     128    127037    126909   132.183       116   0.0839058   0.0150866
2021-10-28T08:54:06.531243+0000    16     128    128505    128377   125.355   22.9375   0.0838622   0.0158979
2021-10-28T08:54:07.531363+0000    17     128    130169    130041    119.51        26   0.0362009   0.0167108
2021-10-28T08:54:08.531462+0000    18     128    135929    135801    117.87        90   0.0222057   0.0169566
2021-10-28T08:54:09.531551+0000    19     128    142013    141885   116.669   95.0625   0.0204062   0.0171296
2021-10-28T08:54:10.531673+0000 min lat: 0.00508635 max lat: 0.0977839 avg lat: 0.017293
2021-10-28T08:54:10.531673+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-28T08:54:10.531673+0000    20     128    148089    147961   115.582   94.9375   0.0210705    0.017293
2021-10-28T08:54:11.531807+0000    21     128    154105    153977   114.554        94   0.0224177   0.0174471
2021-10-28T08:54:12.531917+0000    22     128    157501    157373   111.758   53.0625   0.0183873   0.0178852
2021-10-28T08:54:13.532025+0000    23     128    163961    163833   111.287   100.938   0.0216166   0.0179649
2021-10-28T08:54:14.532125+0000    24     127    172531    172404    112.23   133.922   0.0101828   0.0178159
2021-10-28T08:54:15.532242+0000    25     127    184997    184870   115.531   194.781   0.0123106    0.017305
2021-10-28T08:54:16.532369+0000    26     127    189360    189233   113.709   68.1719   0.0751266   0.0175659
2021-10-28T08:54:17.532463+0000    27     128    191106    190978   110.508   27.2656   0.0824495   0.0180789
2021-10-28T08:54:18.532593+0000    28     128    193349    193221   107.812   35.0469   0.0272241   0.0185414
2021-10-28T08:54:19.532688+0000    29     127    198170    198043   106.692   75.3438   0.0355398   0.0187304
2021-10-28T08:54:20.532794+0000    30     128    201412    201284   104.824   50.6406   0.0369279   0.0190689
2021-10-28T08:54:21.532899+0000    31     128    205314    205186   103.409   60.9688   0.0262522    0.019331
2021-10-28T08:54:22.533019+0000    32     128    210372    210244   102.647   79.0312    0.026498   0.0194753
2021-10-28T08:54:23.533141+0000    33     128    215236    215108   101.839        76   0.0284099   0.0196311
2021-10-28T08:54:24.533257+0000    34     128    217668    217540   99.9614        38   0.0794395   0.0199703
2021-10-28T08:54:25.533343+0000    35     128    219204    219076   97.7911        24   0.0786718   0.0204271
2021-10-28T08:54:26.533462+0000    36     127    220869    220742   95.7976   26.0312   0.0820111   0.0208333
2021-10-28T08:54:27.533541+0000    37     128    226178    226050   95.4499   82.9375   0.0253291   0.0209464
2021-10-28T08:54:28.533663+0000    38     128    231170    231042   94.9904        78   0.0273989   0.0210465
2021-10-28T08:54:29.533751+0000    39     128    236356    236228   94.6323   81.0312   0.0111941   0.0211294
2021-10-28T08:54:30.533831+0000 min lat: 0.00441004 max lat: 0.100118 avg lat: 0.0206438
2021-10-28T08:54:30.533831+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-28T08:54:30.533831+0000    40     128    248065    247937   96.8399   182.953   0.0269219   0.0206438
2021-10-28T08:54:31.533926+0000    41     128    250050    249922   95.2344   31.0156   0.0889246   0.0209671
2021-10-28T08:54:32.534043+0000    42     128    251521    251393   93.5141   22.9844   0.0895033   0.0213525
2021-10-28T08:54:33.534126+0000    43     128    256450    256322   93.1302   77.0156   0.0217095   0.0214685
2021-10-28T08:54:34.534247+0000    44     127    261708    261581    92.881   82.1719   0.0253829   0.0215233
2021-10-28T08:54:35.534361+0000    45     128    266946    266818   92.6351   81.8281    0.024798   0.0215805
2021-10-28T08:54:36.534482+0000    46     128    272116    271988   92.3772   80.7812   0.0257005   0.0216441
2021-10-28T08:54:37.534571+0000    47     127    277292    277165   92.1327   80.8906   0.0251177    0.021701
2021-10-28T08:54:38.534689+0000    48     128    279746    279618   91.0116   38.3281   0.0898723   0.0219598
2021-10-28T08:54:39.534807+0000    49     128    281282    281154    89.644        24   0.0800383   0.0222972
2021-10-28T08:54:40.534892+0000    50     127    284912    284785   88.9857   56.7344   0.0205109   0.0224701
2021-10-28T08:54:41.534973+0000    51     128    290561    290433   88.9712     88.25    0.022556   0.0224702
2021-10-28T08:54:42.535055+0000    52     128    295809    295681    88.837        82    0.024219    0.022505
2021-10-28T08:54:43.535165+0000    53     128    300994    300866   88.6892   81.0156   0.0238721   0.0225455
2021-10-28T08:54:44.535280+0000    54     128    306049    305921   88.5093   78.9844   0.0245801   0.0225901
2021-10-28T08:54:45.535357+0000    55     128    309633    309505   87.9182        56    0.087251    0.022724
2021-10-28T08:54:46.535474+0000    56     128    311169    311041   86.7767        24   0.0930251   0.0230348
2021-10-28T08:54:47.535589+0000    57     128    312577    312449   85.6403        22   0.0939228   0.0233371
2021-10-28T08:54:48.535697+0000    58     128    317441    317313   85.4739        76   0.0227167   0.0233926
2021-10-28T08:54:49.535785+0000    59     128    322754    322626   85.4321   83.0156   0.0251429   0.0234019
2021-10-28T08:54:50.535865+0000 min lat: 0.00441004 max lat: 0.102893 avg lat: 0.0234174
2021-10-28T08:54:50.535865+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-28T08:54:50.535865+0000    60      65    328002    327937   85.3912   82.9844   0.0232814   0.0234174
2021-10-28T08:54:51.536018+0000 Total time run:         60.0181
Total writes made:      328002
Write size:             16384
Object size:            16384
Bandwidth (MB/sec):     85.3915
Stddev Bandwidth:       52.386
Max bandwidth (MB/sec): 226.703
Min bandwidth (MB/sec): 22
Average IOPS:           5465
Stddev IOPS:            3352.7
Max IOPS:               14509
Min IOPS:               1408
Average Latency(s):     0.0234174
Stddev Latency(s):      0.0193032
Max latency(s):         0.102893
Min latency(s):         0.00441004

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:54:52,243433099-07:00] INFO: > Stop system activity collection services[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:54:52,249180920-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:391 - rados_bench() > kill -INT 232808

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:54:52,253995112-07:00] INFO: >> for OSD host[0m
# ./benchmarks/bench-rados:396 - rados_bench() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.2 kill -INT 138094
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:54:52,261401298-07:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.2 kill -INT 138094
[1] 01:54:52 [SUCCESS] ljishen@10.10.2.2

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:54:52,449144505-07:00] INFO: >>> Copy system activity file from OSD host[0m
## ./benchmarks/bench-rados:403 - rados_bench() > basename -- /tmp/bench-rados/sys_activity_16KB_write.dat.osd_host.1
# ./benchmarks/bench-rados:403 - rados_bench() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.2:/tmp/bench-rados/sys_activity_16KB_write.dat.osd_host.1 /mnt/sda8/ceph-research/scripts/output/sys_activity_16KB_write.dat.osd_host.1


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:54:52,623498120-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:55:16,972844630-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 328.00k objects, 5.0 GiB
    usage:   16 GiB used, 84 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:55:16,979820987-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:55:26,283169021-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 328.00k objects, 5.0 GiB
    usage:   16 GiB used, 84 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:55:26,290188869-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:55:35,540115966-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 328.00k objects, 5.0 GiB
    usage:   16 GiB used, 84 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:55:35,547693986-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:55:44,887727449-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 328.00k objects, 5.0 GiB
    usage:   16 GiB used, 84 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:55:44,894810566-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:55:54,171750684-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 328.00k objects, 5.0 GiB
    usage:   16 GiB used, 84 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:55:54,178596595-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:55:54,183803346-07:00] INFO: > The cluster is idle now.[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:55:54,187274367-07:00] STAGE: Start benchmarking of mode seq ...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:55:54,193426861-07:00] INFO: > Start system activity collection services[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:55:54,198680611-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:341 - rados_bench() > rm --force /mnt/sda8/ceph-research/scripts/output/sys_activity_16KB_seq.dat.rados_bench_host.1
# ./benchmarks/bench-rados:342 - rados_bench() > sar_pid_rados_bench_host=234180
# ./benchmarks/bench-rados:342 - rados_bench() > S_TIME_FORMAT=ISO
# ./benchmarks/bench-rados:342 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/output/sys_activity_16KB_seq.dat.rados_bench_host.1 2

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:55:54,204951068-07:00] INFO: >> for OSD host[0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:55:54,213644961-07:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.2 bash
10.10.2.2: b'138196\n'
[1] 01:55:54 [SUCCESS] ljishen@10.10.2.2
138196

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:55:54,401722898-07:00] INFO: > Run rados bench[0m
# ./benchmarks/bench-rados:379 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/output/rados_bench_16KB_seq.log.1
## ./benchmarks/bench-rados:376 - rados_bench() > count_cpus 32-63,96-127
# ./benchmarks/bench-rados:376 - rados_bench() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 128 --show-time
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:55:54,421007448-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:55:54,423815469-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'cfe0a75c-37cb-11ec-b51d-53e6e728d2d3', '--', 'rados', 'bench', '99999999', 'seq', '--pool', 'bench_rados', '--concurrent-ios', '128', '--show-time'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid cfe0a75c-37cb-11ec-b51d-53e6e728d2d3 -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 128 --show-time
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
hints = 1
2021-10-28T08:55:57.721645+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-28T08:55:57.721645+0000     0       0         0         0         0         0           -           0
2021-10-28T08:55:58.721787+0000     1     128     29520     29392   459.155    459.25  0.00472574  0.00433933
2021-10-28T08:55:59.721913+0000     2     128     58492     58364   455.893   452.688  0.00433669  0.00437199
2021-10-28T08:56:00.722035+0000     3     128     87370     87242   454.316   451.219  0.00419996  0.00439024
2021-10-28T08:56:01.722132+0000     4     127    115916    115789   452.238   446.047  0.00435051  0.00441095
2021-10-28T08:56:02.722198+0000     5     128    144737    144609   451.847   450.312  0.00468132  0.00441597
2021-10-28T08:56:03.722311+0000     6     128    174555    174427   454.182   465.906  0.00438546  0.00439354
2021-10-28T08:56:04.722399+0000     7     128    203658    203530   454.255   454.734   0.0046431   0.0043932
2021-10-28T08:56:05.722497+0000     8     128    232861    232733   454.504   456.297  0.00425337  0.00439105
2021-10-28T08:56:06.722615+0000     9     127    261841    261714   454.312   452.828   0.0046202  0.00439317
2021-10-28T08:56:07.722691+0000    10     128    291231    291103   454.798   459.203  0.00447907  0.00438846
2021-10-28T08:56:08.722816+0000    11     128    320241    320113   454.655   453.281  0.00442188  0.00438995
2021-10-28T08:56:09.722941+0000 Total time run:       11.2735
Total reads made:     328002
Read size:            16384
Object size:          16384
Bandwidth (MB/sec):   454.609
Average IOPS:         29094
Stddev IOPS:          341.621
Max IOPS:             29818
Min IOPS:             28547
Average Latency(s):   0.00439075
Max latency(s):       0.0155479
Min latency(s):       0.000343968

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:56:10,357600873-07:00] INFO: > Stop system activity collection services[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:56:10,362602298-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:391 - rados_bench() > kill -INT 234180

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:56:10,368425572-07:00] INFO: >> for OSD host[0m
# ./benchmarks/bench-rados:396 - rados_bench() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.2 kill -INT 138196
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:56:10,376256047-07:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.2 kill -INT 138196
[1] 01:56:10 [SUCCESS] ljishen@10.10.2.2

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:56:10,561634758-07:00] INFO: >>> Copy system activity file from OSD host[0m
## ./benchmarks/bench-rados:403 - rados_bench() > basename -- /tmp/bench-rados/sys_activity_16KB_seq.dat.osd_host.1
# ./benchmarks/bench-rados:403 - rados_bench() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.2:/tmp/bench-rados/sys_activity_16KB_seq.dat.osd_host.1 /mnt/sda8/ceph-research/scripts/output/sys_activity_16KB_seq.dat.osd_host.1


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:56:10,716596530-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:56:34,853612479-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 328.00k objects, 5.0 GiB
    usage:   16 GiB used, 84 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:56:34,860799382-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:56:44,362913646-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 328.00k objects, 5.0 GiB
    usage:   16 GiB used, 84 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:56:44,370022221-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:56:53,647056089-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 328.00k objects, 5.0 GiB
    usage:   16 GiB used, 84 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:56:53,654240878-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:57:02,894378878-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 328.00k objects, 5.0 GiB
    usage:   16 GiB used, 84 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:57:02,901566763-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:57:12,135397776-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 328.00k objects, 5.0 GiB
    usage:   16 GiB used, 84 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:57:12,142508156-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:57:12,147814595-07:00] INFO: > The cluster is idle now.[0m


[1;7;39;49m[2021-10-28T01:57:12,149884205-07:00][RUNNING][ROUND 2/2/21] object_size=16KB[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:57:12,153217667-07:00] STAGE: Launch a Ceph cluster (CEPH_HOST: 10.10.2.1, OSD_HOST: 10.10.2.2) ...[0m
# ./benchmarks/bench-rados:183 - launch_ceph_cluster() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.1 bash -euo pipefail
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:57:12,162117017-07:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.1 bash -euo pipefail
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T01:57:12,595894723-07:00] INFO: > Remove existing clusters\x1b[0m\n'
10.10.2.1: b'## bash:17 -  > basename -- /var/lib/ceph/cfe0a75c-37cb-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'# bash:17 -  > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm --host 10.10.2.1 --host 10.10.2.2 rm-cluster --force --fsid cfe0a75c-37cb-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T01:57:12,606532159-07:00] DEBUG: HOST_PARAMS: ['--host', '10.10.2.1', '--host', '10.10.2.2']\x1b[0m\n"
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T01:57:12,609859114-07:00] DEBUG: CEPHADM_PARAMS: ['rm-cluster', '--force', '--fsid', 'cfe0a75c-37cb-11ec-b51d-53e6e728d2d3']\x1b[0m\n"
10.10.2.1: b"# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:63 -  > /tmp/bench-rados/ceph-research/scripts/parallel-ssh --host 10.10.2.1 --host 10.10.2.2 'sudo python3 -u - rm-cluster --force --fsid cfe0a75c-37cb-11ec-b51d-53e6e728d2d3'\n"
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T01:57:12,617952177-07:00] DEBUG: command from stdin\x1b[0m\n'
10.10.2.1: b"# /tmp/bench-rados/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host 10.10.2.1 --host 10.10.2.2 'sudo python3 -u - rm-cluster --force --fsid cfe0a75c-37cb-11ec-b51d-53e6e728d2d3'\n"
10.10.2.1: b'[1] 01:57:18 [SUCCESS] 10.10.2.1\n[2] 01:57:23 [SUCCESS] 10.10.2.2\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T01:57:23,278160569-07:00] INFO: > Deploy a new cluster\x1b[0m\n'
10.10.2.1: b'# bash:24 -  > /tmp/bench-rados/ceph-research/scripts/ceph-deploy -m 10.10.2.1 -o node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/10.10.2.2:/dev/nvme0n1\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T01:57:23,291211853-07:00] STAGE: Verify passwordless SSH to hosts, passwordless sudo via ljishen, and the existence of devices on hosts...\x1b[0m\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T01:57:23,296222102-07:00] INFO: > Check the host for the monitor\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:181 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@10.10.2.1 sudo --non-interactive --validate\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T01:57:23,443912554-07:00] INFO: > Check hosts for OSDs\x1b[0m\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T01:57:23,448668785-07:00] INFO: >> Check host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:200 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us sudo --non-interactive --validate\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T01:57:23,598908640-07:00] INFO: >>> Check device '/dev/nvme0n1'\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:215 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us test -b /dev/nvme0n1\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:226 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us lsblk --nodeps --noheadings --output TYPE /dev/nvme0n1\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:227 - verify_parameters() > grep --word-regexp disk\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T01:57:23,884261105-07:00] STAGE: Reset devices on hosts...\x1b[0m\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T01:57:23,889381089-07:00] INFO: > Resetting device '/dev/nvme0n1' on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:288 - reset_host_devices() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us sudo sh\n'
10.10.2.1: b'  Removing ceph--27c710fb--e5f1--4ef5--9a7e--baeb6786afd0-osd--block--9a583a4e--dcb8--4537--a089--bb02f1073bc8 (253:0)\n'
10.10.2.1: b'  Archiving volume group "ceph-27c710fb-e5f1-4ef5-9a7e-baeb6786afd0" metadata (seqno 5).\n'
10.10.2.1: b'  Releasing logical volume "osd-block-9a583a4e-dcb8-4537-a089-bb02f1073bc8"\n'
10.10.2.1: b'  Creating volume group backup "/etc/lvm/backup/ceph-27c710fb-e5f1-4ef5-9a7e-baeb6786afd0" (seqno 6).\n'
10.10.2.1: b'  Logical volume "osd-block-9a583a4e-dcb8-4537-a089-bb02f1073bc8" successfully removed\n'
10.10.2.1: b'  Removing physical volume "/dev/nvme0n1" from volume group "ceph-27c710fb-e5f1-4ef5-9a7e-baeb6786afd0"\n'
10.10.2.1: b'  Volume group "ceph-27c710fb-e5f1-4ef5-9a7e-baeb6786afd0" successfully removed\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T01:57:24,262067336-07:00] STAGE: Bootstrap a new cluster...\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:317 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.1 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T01:57:24,271813868-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T01:57:24,275203008-07:00] DEBUG: CEPHADM_PARAMS: ['bootstrap', '--config', '/tmp/bench-rados/ceph-research/scripts/ceph.conf', '--mon-ip', '10.10.2.1', '--output-dir', '/tmp/bench-rados/deployment_data_root/etc/ceph', '--ssh-user', 'ljishen', '--skip-dashboard', '--no-minimize-config', '--allow-overwrite', '--allow-fqdn-hostname', '--orphan-initial-daemons', '--skip-monitoring-stack', '--single-host-defaults']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.1 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults\n'
10.10.2.1: b'Verifying podman|docker is present...\nVerifying lvm2 is present...\n'
10.10.2.1: b'Verifying time synchronization is in place...\n'
10.10.2.1: b'Unit ntp.service is enabled and running\n'
10.10.2.1: b'Repeating the final host check...\n'
10.10.2.1: b'podman|docker (/usr/bin/docker) is present\n'
10.10.2.1: b'systemctl is present\n'
10.10.2.1: b'lvcreate is present\n'
10.10.2.1: b'Unit ntp.service is enabled and running\n'
10.10.2.1: b'Host looks OK\n'
10.10.2.1: b'Cluster fsid: 119bb8de-37cd-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Verifying IP 10.10.2.1 port 3300 ...\n'
10.10.2.1: b'Verifying IP 10.10.2.1 port 6789 ...\n'
10.10.2.1: b'Mon IP `10.10.2.1` is in CIDR network `10.10.2.0/24`\n'
10.10.2.1: b'- internal network (--cluster-network) has not been provided, OSD replication will default to the public_network\n'
10.10.2.1: b'Adjusting default settings to suit single-host cluster...\n'
10.10.2.1: b'Pulling container image quay.io/ceph/ceph:v16...\n'
10.10.2.1: b'Ceph version: ceph version 16.2.6 (ee28fb57e47e9f88813e24bbf4c14496ca299d31) pacific (stable)\n'
10.10.2.1: b'Extracting ceph user uid/gid from container image...\n'
10.10.2.1: b'Creating initial keys...\n'
10.10.2.1: b'Creating initial monmap...\n'
10.10.2.1: b'Creating mon...\n'
10.10.2.1: b'Waiting for mon to start...\n'
10.10.2.1: b'Waiting for mon...\n'
10.10.2.1: b'mon is available\nSetting mon public_network to 10.10.2.0/24\n'
10.10.2.1: b'Wrote config to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf\n'
10.10.2.1: b'Wrote keyring to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring\n'
10.10.2.1: b'Creating mgr...\n'
10.10.2.1: b'Verifying port 9283 ...\n'
10.10.2.1: b'Waiting for mgr to start...\nWaiting for mgr...\n'
10.10.2.1: b'mgr not available, waiting (1/15)...\n'
10.10.2.1: b'mgr not available, waiting (2/15)...\n'
10.10.2.1: b'mgr is available\n'
10.10.2.1: b'Enabling cephadm module...\n'
10.10.2.1: b'Waiting for the mgr to restart...\n'
10.10.2.1: b'Waiting for mgr epoch 5...\n'
10.10.2.1: b'mgr epoch 5 is available\nSetting orchestrator backend to cephadm...\n'
10.10.2.1: b'Generating ssh key...\n'
10.10.2.1: b'Wrote public SSH key to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub\n'
10.10.2.1: b'Adding key to ljishen@localhost authorized_keys...\n'
10.10.2.1: b'Adding host node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us...\n'
10.10.2.1: b'Deploying unmanaged mon service...\n'
10.10.2.1: b'Deploying unmanaged mgr service...\n'
10.10.2.1: b'You can access the Ceph CLI with:\n\n\tsudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --fsid 119bb8de-37cd-11ec-b51d-53e6e728d2d3 -c /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf -k /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring\n\n'
10.10.2.1: b'Please consider enabling telemetry to help improve Ceph:\n\n\tceph telemetry on\n\nFor more information see:\n\n\thttps://docs.ceph.com/docs/pacific/mgr/telemetry/\n\n'
10.10.2.1: b'Bootstrap complete.\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T01:58:23,830477726-07:00] INFO: Sleep for 20 seconds\x1b[0m\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T01:58:43,837632154-07:00] INFO: > Set the default number of monitor daemons to 1\x1b[0m\n# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:337 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T01:58:43,848154074-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T01:58:43,851979325-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mon', '1']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1\n'
10.10.2.1: b'Inferring fsid 119bb8de-37cd-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/119bb8de-37cd-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b'Scheduled mon update...\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T01:58:52,851653031-07:00] INFO: > Set the default number of manager daemons to 1\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:342 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T01:58:52,862513818-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T01:58:52,866373644-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mgr', '1']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1\n'
10.10.2.1: b'Inferring fsid 119bb8de-37cd-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/119bb8de-37cd-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b'Scheduled mgr update...\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T01:59:01,587004238-07:00] STAGE: Add hosts to the cluster...\x1b[0m\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T01:59:01,592677543-07:00] INFO: > Adding host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:361 - add_hosts() > ssh-copy-id -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -f -i /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\n'
10.10.2.1: b'/usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: "/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub"\n'
10.10.2.1: b'\nNumber of key(s) added: 1\n\nNow try logging into the machine, with:   "ssh -o \'GlobalKnownHostsFile=/dev/null\' -o \'LogLevel=ERROR\' -o \'PasswordAuthentication=no\' -o \'StrictHostKeyChecking=no\' -o \'UserKnownHostsFile=/dev/null\' \'ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\'"\nand check to make sure that only the key(s) you wanted were added.\n\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:367 - add_hosts() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us 10.10.2.2\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T01:59:01,802346445-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T01:59:01,805961090-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'host', 'add', 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us', '10.10.2.2']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us 10.10.2.2\n'
10.10.2.1: b'Inferring fsid 119bb8de-37cd-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/119bb8de-37cd-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b"Added host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us' with addr '10.10.2.2'\n"
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T01:59:11,106189560-07:00] INFO: Sleep for 20 seconds\x1b[0m\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T01:59:31,111230433-07:00] STAGE: Add OSDs to the cluster...\x1b[0m\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T01:59:31,117213550-07:00] INFO: > Adding devices ['/dev/nvme0n1'] on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us' as OSDs\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:388 - add_host_devices_as_osds() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T01:59:31,126493224-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T01:59:31,130111967-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'daemon', 'add', 'osd', 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1\n'
10.10.2.1: b'Inferring fsid 119bb8de-37cd-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/119bb8de-37cd-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b"Created osd(s) 0 on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'\n"
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T01:59:55,406560957-07:00] INFO: > Sleep for 20 seconds\x1b[0m\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T02:00:15,412283799-07:00] STAGE: Show Cluster Status\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:408 - show_status() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T02:00:15,422419032-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T02:00:15,426154894-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', '--status']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status\n'
10.10.2.1: b'Inferring fsid 119bb8de-37cd-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/119bb8de-37cd-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b'  cluster:\n    id:     119bb8de-37cd-11ec-b51d-53e6e728d2d3\n    health: HEALTH_WARN\n            1 pool(s) have no replicas configured\n \n  services:\n    mon: 1 daemons, quorum node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us (age 2m)\n    mgr: node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us.wuibnj(active, since 2m)\n    osd: 1 osds: 1 up (since 17s), 1 in (since 40s)\n \n  data:\n    pools:   1 pools, 1 pgs\n    objects: 0 objects, 0 B\n    usage:   4.7 MiB used, 100 GiB / 100 GiB avail\n    pgs:     1 active+clean\n \n'
[1] 02:00:24 [SUCCESS] ljishen@10.10.2.1

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:57:12,595894723-07:00] INFO: > Remove existing clusters[0m
## bash:17 -  > basename -- /var/lib/ceph/cfe0a75c-37cb-11ec-b51d-53e6e728d2d3
# bash:17 -  > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm --host 10.10.2.1 --host 10.10.2.2 rm-cluster --force --fsid cfe0a75c-37cb-11ec-b51d-53e6e728d2d3
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:57:12,606532159-07:00] DEBUG: HOST_PARAMS: ['--host', '10.10.2.1', '--host', '10.10.2.2'][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:57:12,609859114-07:00] DEBUG: CEPHADM_PARAMS: ['rm-cluster', '--force', '--fsid', 'cfe0a75c-37cb-11ec-b51d-53e6e728d2d3'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:63 -  > /tmp/bench-rados/ceph-research/scripts/parallel-ssh --host 10.10.2.1 --host 10.10.2.2 'sudo python3 -u - rm-cluster --force --fsid cfe0a75c-37cb-11ec-b51d-53e6e728d2d3'
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:57:12,617952177-07:00] DEBUG: command from stdin[0m
# /tmp/bench-rados/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host 10.10.2.1 --host 10.10.2.2 'sudo python3 -u - rm-cluster --force --fsid cfe0a75c-37cb-11ec-b51d-53e6e728d2d3'
[1] 01:57:18 [SUCCESS] 10.10.2.1
[2] 01:57:23 [SUCCESS] 10.10.2.2

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:57:23,278160569-07:00] INFO: > Deploy a new cluster[0m
# bash:24 -  > /tmp/bench-rados/ceph-research/scripts/ceph-deploy -m 10.10.2.1 -o node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/10.10.2.2:/dev/nvme0n1


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:57:23,291211853-07:00] STAGE: Verify passwordless SSH to hosts, passwordless sudo via ljishen, and the existence of devices on hosts...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:57:23,296222102-07:00] INFO: > Check the host for the monitor[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:181 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@10.10.2.1 sudo --non-interactive --validate

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:57:23,443912554-07:00] INFO: > Check hosts for OSDs[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:57:23,448668785-07:00] INFO: >> Check host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:200 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us sudo --non-interactive --validate

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:57:23,598908640-07:00] INFO: >>> Check device '/dev/nvme0n1'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:215 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us test -b /dev/nvme0n1
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:226 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us lsblk --nodeps --noheadings --output TYPE /dev/nvme0n1
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:227 - verify_parameters() > grep --word-regexp disk


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:57:23,884261105-07:00] STAGE: Reset devices on hosts...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:57:23,889381089-07:00] INFO: > Resetting device '/dev/nvme0n1' on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:288 - reset_host_devices() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us sudo sh
  Removing ceph--27c710fb--e5f1--4ef5--9a7e--baeb6786afd0-osd--block--9a583a4e--dcb8--4537--a089--bb02f1073bc8 (253:0)
  Archiving volume group "ceph-27c710fb-e5f1-4ef5-9a7e-baeb6786afd0" metadata (seqno 5).
  Releasing logical volume "osd-block-9a583a4e-dcb8-4537-a089-bb02f1073bc8"
  Creating volume group backup "/etc/lvm/backup/ceph-27c710fb-e5f1-4ef5-9a7e-baeb6786afd0" (seqno 6).
  Logical volume "osd-block-9a583a4e-dcb8-4537-a089-bb02f1073bc8" successfully removed
  Removing physical volume "/dev/nvme0n1" from volume group "ceph-27c710fb-e5f1-4ef5-9a7e-baeb6786afd0"
  Volume group "ceph-27c710fb-e5f1-4ef5-9a7e-baeb6786afd0" successfully removed


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:57:24,262067336-07:00] STAGE: Bootstrap a new cluster...[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:317 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.1 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:57:24,271813868-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:57:24,275203008-07:00] DEBUG: CEPHADM_PARAMS: ['bootstrap', '--config', '/tmp/bench-rados/ceph-research/scripts/ceph.conf', '--mon-ip', '10.10.2.1', '--output-dir', '/tmp/bench-rados/deployment_data_root/etc/ceph', '--ssh-user', 'ljishen', '--skip-dashboard', '--no-minimize-config', '--allow-overwrite', '--allow-fqdn-hostname', '--orphan-initial-daemons', '--skip-monitoring-stack', '--single-host-defaults'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.1 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults
Verifying podman|docker is present...
Verifying lvm2 is present...
Verifying time synchronization is in place...
Unit ntp.service is enabled and running
Repeating the final host check...
podman|docker (/usr/bin/docker) is present
systemctl is present
lvcreate is present
Unit ntp.service is enabled and running
Host looks OK
Cluster fsid: 119bb8de-37cd-11ec-b51d-53e6e728d2d3
Verifying IP 10.10.2.1 port 3300 ...
Verifying IP 10.10.2.1 port 6789 ...
Mon IP `10.10.2.1` is in CIDR network `10.10.2.0/24`
- internal network (--cluster-network) has not been provided, OSD replication will default to the public_network
Adjusting default settings to suit single-host cluster...
Pulling container image quay.io/ceph/ceph:v16...
Ceph version: ceph version 16.2.6 (ee28fb57e47e9f88813e24bbf4c14496ca299d31) pacific (stable)
Extracting ceph user uid/gid from container image...
Creating initial keys...
Creating initial monmap...
Creating mon...
Waiting for mon to start...
Waiting for mon...
mon is available
Setting mon public_network to 10.10.2.0/24
Wrote config to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf
Wrote keyring to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring
Creating mgr...
Verifying port 9283 ...
Waiting for mgr to start...
Waiting for mgr...
mgr not available, waiting (1/15)...
mgr not available, waiting (2/15)...
mgr is available
Enabling cephadm module...
Waiting for the mgr to restart...
Waiting for mgr epoch 5...
mgr epoch 5 is available
Setting orchestrator backend to cephadm...
Generating ssh key...
Wrote public SSH key to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub
Adding key to ljishen@localhost authorized_keys...
Adding host node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us...
Deploying unmanaged mon service...
Deploying unmanaged mgr service...
You can access the Ceph CLI with:

	sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --fsid 119bb8de-37cd-11ec-b51d-53e6e728d2d3 -c /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf -k /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring

Please consider enabling telemetry to help improve Ceph:

	ceph telemetry on

For more information see:

	https://docs.ceph.com/docs/pacific/mgr/telemetry/

Bootstrap complete.

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:58:23,830477726-07:00] INFO: Sleep for 20 seconds[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:58:43,837632154-07:00] INFO: > Set the default number of monitor daemons to 1[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:337 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:58:43,848154074-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:58:43,851979325-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mon', '1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1
Inferring fsid 119bb8de-37cd-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/119bb8de-37cd-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Scheduled mon update...

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:58:52,851653031-07:00] INFO: > Set the default number of manager daemons to 1[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:342 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:58:52,862513818-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:58:52,866373644-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mgr', '1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1
Inferring fsid 119bb8de-37cd-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/119bb8de-37cd-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Scheduled mgr update...


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:59:01,587004238-07:00] STAGE: Add hosts to the cluster...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:59:01,592677543-07:00] INFO: > Adding host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:361 - add_hosts() > ssh-copy-id -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -f -i /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us
/usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: "/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub"

Number of key(s) added: 1

Now try logging into the machine, with:   "ssh -o 'GlobalKnownHostsFile=/dev/null' -o 'LogLevel=ERROR' -o 'PasswordAuthentication=no' -o 'StrictHostKeyChecking=no' -o 'UserKnownHostsFile=/dev/null' 'ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'"
and check to make sure that only the key(s) you wanted were added.

# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:367 - add_hosts() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us 10.10.2.2
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:59:01,802346445-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:59:01,805961090-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'host', 'add', 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us', '10.10.2.2'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us 10.10.2.2
Inferring fsid 119bb8de-37cd-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/119bb8de-37cd-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Added host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us' with addr '10.10.2.2'

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:59:11,106189560-07:00] INFO: Sleep for 20 seconds[0m


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:59:31,111230433-07:00] STAGE: Add OSDs to the cluster...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:59:31,117213550-07:00] INFO: > Adding devices ['/dev/nvme0n1'] on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us' as OSDs[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:388 - add_host_devices_as_osds() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:59:31,126493224-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:59:31,130111967-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'daemon', 'add', 'osd', 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1
Inferring fsid 119bb8de-37cd-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/119bb8de-37cd-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Created osd(s) 0 on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T01:59:55,406560957-07:00] INFO: > Sleep for 20 seconds[0m


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:00:15,412283799-07:00] STAGE: Show Cluster Status[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:408 - show_status() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:00:15,422419032-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:00:15,426154894-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', '--status'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status
Inferring fsid 119bb8de-37cd-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/119bb8de-37cd-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
  cluster:
    id:     119bb8de-37cd-11ec-b51d-53e6e728d2d3
    health: HEALTH_WARN
            1 pool(s) have no replicas configured
 
  services:
    mon: 1 daemons, quorum node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us (age 2m)
    mgr: node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us.wuibnj(active, since 2m)
    osd: 1 osds: 1 up (since 17s), 1 in (since 40s)
 
  data:
    pools:   1 pools, 1 pgs
    objects: 0 objects, 0 B
    usage:   4.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     1 active+clean
 

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:00:24,109268463-07:00] INFO: > Copy configuration files to local[0m
# ./benchmarks/bench-rados:220 - launch_ceph_cluster() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.1 sudo chown --changes --recursive ljishen /tmp/bench-rados/deployment_data_root/etc/ceph
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:00:24,116749621-07:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.1 sudo chown --changes --recursive ljishen /tmp/bench-rados/deployment_data_root/etc/ceph
[1] 02:00:24 [SUCCESS] ljishen@10.10.2.1
# ./benchmarks/bench-rados:223 - launch_ceph_cluster() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.1:/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf
# ./benchmarks/bench-rados:226 - launch_ceph_cluster() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.1:/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:00:24,593128727-07:00] STAGE: Update local ceph.conf as a client...[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:00:24,596327044-07:00] STAGE: Configure Ceph pool...[0m
# ./benchmarks/bench-rados:257 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_max_backfills 32
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:00:24,618094753-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:00:24,620927391-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '119bb8de-37cd-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'config', 'set', 'osd', 'osd_max_backfills', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 119bb8de-37cd-11ec-b51d-53e6e728d2d3 -- ceph config set osd osd_max_backfills 32
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:258 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_active 32
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:00:29,004943738-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:00:29,008053098-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '119bb8de-37cd-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_active', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 119bb8de-37cd-11ec-b51d-53e6e728d2d3 -- ceph config set osd osd_recovery_max_active 32
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:259 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_single_start 8
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:00:33,330260853-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:00:33,333247732-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '119bb8de-37cd-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_single_start', '8'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 119bb8de-37cd-11ec-b51d-53e6e728d2d3 -- ceph config set osd osd_recovery_max_single_start 8
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:260 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_op_priority 63
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:00:37,565157368-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:00:37,568145458-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '119bb8de-37cd-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_op_priority', '63'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 119bb8de-37cd-11ec-b51d-53e6e728d2d3 -- ceph config set osd osd_recovery_op_priority 63
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:262 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config show-with-defaults osd.0
# ./benchmarks/bench-rados:263 - configure_ceph_pool() > grep 'osd_max_backfills\|osd_recovery'
# ./benchmarks/bench-rados:263 - configure_ceph_pool() > column -t -s ' '
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
osd_max_backfills                        32         mon
osd_recovery_cost                        20971520   default
osd_recovery_delay_start                 0.000000   default
osd_recovery_max_active                  32         mon
osd_recovery_max_active_hdd              3          default
osd_recovery_max_active_ssd              10         default
osd_recovery_max_chunk                   8388608    default
osd_recovery_max_omap_entries_per_chunk  8096       default
osd_recovery_max_single_start            8          mon
osd_recovery_op_priority                 63         mon
osd_recovery_op_warn_multiple            16         default
osd_recovery_priority                    5          default
osd_recovery_retry_interval              30.000000  default
osd_recovery_sleep                       0.000000   default
osd_recovery_sleep_hdd                   0.100000   default
osd_recovery_sleep_hybrid                0.025000   default
osd_recovery_sleep_ssd                   0.000000   default
# ./benchmarks/bench-rados:265 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool create bench_rados 128 128 replicated --size 1 --pg-num-min 128
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:00:46,201665306-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:00:46,204773523-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '119bb8de-37cd-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'create', 'bench_rados', '128', '128', 'replicated', '--size', '1', '--pg-num-min', '128'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 119bb8de-37cd-11ec-b51d-53e6e728d2d3 -- ceph osd pool create bench_rados 128 128 replicated --size 1 --pg-num-min 128
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
pool 'bench_rados' created
# ./benchmarks/bench-rados:267 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados min_size 1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:00:50,566681920-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:00:50,569798814-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '119bb8de-37cd-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'min_size', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 119bb8de-37cd-11ec-b51d-53e6e728d2d3 -- ceph osd pool set bench_rados min_size 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 min_size to 1
# ./benchmarks/bench-rados:268 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados pg_autoscale_mode off
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:00:55,049433104-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:00:55,052682327-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '119bb8de-37cd-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'pg_autoscale_mode', 'off'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 119bb8de-37cd-11ec-b51d-53e6e728d2d3 -- ceph osd pool set bench_rados pg_autoscale_mode off
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 pg_autoscale_mode to off
# ./benchmarks/bench-rados:269 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool application enable bench_rados benchmark
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:00:59,535173351-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:00:59,538139571-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '119bb8de-37cd-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'application', 'enable', 'bench_rados', 'benchmark'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 119bb8de-37cd-11ec-b51d-53e6e728d2d3 -- ceph osd pool application enable bench_rados benchmark
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
enabled application 'benchmark' on pool 'bench_rados'
# ./benchmarks/bench-rados:270 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados noscrub 1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:01:04,169218155-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:01:04,172493947-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '119bb8de-37cd-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'noscrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 119bb8de-37cd-11ec-b51d-53e6e728d2d3 -- ceph osd pool set bench_rados noscrub 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 noscrub to 1
# ./benchmarks/bench-rados:271 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados nodeep-scrub 1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:01:08,669155745-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:01:08,672318685-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '119bb8de-37cd-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'nodeep-scrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 119bb8de-37cd-11ec-b51d-53e6e728d2d3 -- ceph osd pool set bench_rados nodeep-scrub 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 nodeep-scrub to 1
# ./benchmarks/bench-rados:272 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool ls detail
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:01:13,034486570-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:01:13,037411952-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '119bb8de-37cd-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'ls', 'detail'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 119bb8de-37cd-11ec-b51d-53e6e728d2d3 -- ceph osd pool ls detail
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
pool 1 'device_health_metrics' replicated size 1 min_size 1 crush_rule 0 object_hash rjenkins pg_num 64 pgp_num 64 autoscale_mode on last_change 19 lfor 0/0/16 flags hashpspool stripe_width 0 pg_num_min 1 application mgr_devicehealth
pool 2 'bench_rados' replicated size 1 min_size 1 crush_rule 0 object_hash rjenkins pg_num 128 pgp_num 128 autoscale_mode off last_change 20 flags hashpspool,noscrub,nodeep-scrub stripe_width 0 pg_num_min 128 application benchmark

# ./benchmarks/bench-rados:273 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd df tree
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:01:17,159803489-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:01:17,162696641-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '119bb8de-37cd-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'df', 'tree'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 119bb8de-37cd-11ec-b51d-53e6e728d2d3 -- ceph osd df tree
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
ID  CLASS  WEIGHT   REWEIGHT  SIZE     RAW USE  DATA     OMAP  META     AVAIL    %USE  VAR   PGS  STATUS  TYPE NAME      
-1         0.09769         -  100 GiB  5.7 MiB  168 KiB   0 B  5.6 MiB  100 GiB  0.01  1.00    -          root default   
-3         0.09769         -  100 GiB  5.7 MiB  168 KiB   0 B  5.6 MiB  100 GiB  0.01  1.00    -              host node-1
 0    ssd  0.09769   1.00000  100 GiB  5.7 MiB  168 KiB   0 B  5.6 MiB  100 GiB  0.01  1.00  192      up          osd.0  
                       TOTAL  100 GiB  5.7 MiB  168 KiB   0 B  5.6 MiB  100 GiB  0.01                                    
MIN/MAX VAR: 1.00/1.00  STDDEV: 0


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:01:21,412456032-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:01:45,599961800-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:01:54,872145758-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:02:04,294648808-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:02:13,453749399-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:02:13,460768556-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:02:23,008008900-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:02:23,015052313-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:02:32,397741625-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:02:32,404897500-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:02:41,712590367-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:02:41,719769255-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:02:50,994941539-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:02:51,002169820-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:02:51,007351774-07:00] INFO: > The cluster is idle now.[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:02:51,010859324-07:00] STAGE: Start benchmarking of mode write ...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:02:51,017270385-07:00] INFO: > Start system activity collection services[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:02:51,022325040-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:341 - rados_bench() > rm --force /mnt/sda8/ceph-research/scripts/output/sys_activity_16KB_write.dat.rados_bench_host.2
# ./benchmarks/bench-rados:342 - rados_bench() > sar_pid_rados_bench_host=239977
# ./benchmarks/bench-rados:342 - rados_bench() > S_TIME_FORMAT=ISO
# ./benchmarks/bench-rados:342 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/output/sys_activity_16KB_write.dat.rados_bench_host.2 2

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:02:51,028650019-07:00] INFO: >> for OSD host[0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:02:51,037456384-07:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.2 bash
10.10.2.2: b'141803\n'
[1] 02:02:51 [SUCCESS] ljishen@10.10.2.2
141803

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:02:51,231236425-07:00] INFO: > Run rados bench[0m
# ./benchmarks/bench-rados:370 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/output/rados_bench_16KB_write.log.2
## ./benchmarks/bench-rados:361 - rados_bench() > count_cpus 32-63,96-127
# ./benchmarks/bench-rados:361 - rados_bench() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 60 write --pool bench_rados -b 16384 -O 16384 --concurrent-ios 128 --show-time --write-object --write-omap --write-xattr --no-cleanup
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:02:51,250905410-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:02:51,253790537-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '119bb8de-37cd-11ec-b51d-53e6e728d2d3', '--', 'rados', 'bench', '60', 'write', '--pool', 'bench_rados', '-b', '16384', '-O', '16384', '--concurrent-ios', '128', '--show-time', '--write-object', '--write-omap', '--write-xattr', '--no-cleanup'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 119bb8de-37cd-11ec-b51d-53e6e728d2d3 -- rados bench 60 write --pool bench_rados -b 16384 -O 16384 --concurrent-ios 128 --show-time --write-object --write-omap --write-xattr --no-cleanup
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
hints = 1
2021-10-28T09:02:54.622903+0000 Maintaining 128 concurrent writes of 16384 bytes to objects of size 16384 for up to 60 seconds or 0 objects
2021-10-28T09:02:54.622913+0000 Object prefix: benchmark_data_node-2.bluefield2.ucsc-cmps10_7
2021-10-28T09:02:54.623899+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-28T09:02:54.623899+0000     0       0         0         0         0         0           -           0
2021-10-28T09:02:55.624037+0000     1     128     14002     13874   216.761   216.781  0.00909154  0.00918613
2021-10-28T09:02:56.624171+0000     2     128     27505     27377   213.858   210.984  0.00944414   0.0093195
2021-10-28T09:02:57.624278+0000     3     128     35185     35057   182.568       120   0.0259386   0.0108903
2021-10-28T09:02:58.624415+0000     4     128     40945     40817   159.423        90  0.00918951   0.0125283
2021-10-28T09:02:59.624534+0000     5     128     54513     54385   169.933       212  0.00926355   0.0117568
2021-10-28T09:03:00.624637+0000     6     128     63922     63794   166.111   147.016   0.0840464   0.0119948
2021-10-28T09:03:01.624735+0000     7     128     65393     65265   145.664   22.9844   0.0891403   0.0136578
2021-10-28T09:03:02.624857+0000     8     128     67378     67250   131.333   31.0156   0.0224465   0.0152015
2021-10-28T09:03:03.624976+0000     9     128     73585     73457   127.515   96.9844   0.0198875   0.0156602
2021-10-28T09:03:04.625095+0000    10     128     79473     79345   123.962        92   0.0213498   0.0161179
2021-10-28T09:03:05.625210+0000    11     127     92443     92316   131.116   202.672  0.00951029   0.0152429
2021-10-28T09:03:06.625358+0000    12     128     98395     98267   127.937   92.9844    0.024466   0.0156105
2021-10-28T09:03:07.625481+0000    13     128    106358    106230   127.665   124.422  0.00854486   0.0156588
2021-10-28T09:03:08.625604+0000    14     128    120000    119872    133.77   213.156  0.00941594   0.0149457
2021-10-28T09:03:09.625698+0000    15     128    126143    126015    131.25   95.9844   0.0781121   0.0151844
2021-10-28T09:03:10.625818+0000    16     128    127679    127551   124.547        24   0.0930922   0.0160076
2021-10-28T09:03:11.625938+0000    17     127    131185    131058   120.444   54.7969   0.0189628   0.0165962
2021-10-28T09:03:12.626055+0000    18     128    137472    137344   119.208   98.2188   0.0229843   0.0167671
2021-10-28T09:03:13.626151+0000    19     128    143423    143295   117.828   92.9844   0.0210718   0.0169592
2021-10-28T09:03:14.626272+0000 min lat: 0.00526633 max lat: 0.0946606 avg lat: 0.0171428
2021-10-28T09:03:14.626272+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-28T09:03:14.626272+0000    20     128    149376    149248   116.586   93.0156   0.0205995   0.0171428
2021-10-28T09:03:15.626405+0000    21     128    154943    154815   115.176   86.9844   0.0781172   0.0173396
2021-10-28T09:03:16.626531+0000    22     128    159551    159423   113.213        72   0.0227597   0.0176511
2021-10-28T09:03:17.626631+0000    23     127    167620    167493   113.773   126.094  0.00970642   0.0175723
2021-10-28T09:03:18.626758+0000    24     128    179776    179648   116.945   189.922   0.0104254   0.0170975
2021-10-28T09:03:19.626879+0000    25     128    187712    187584   117.226       124   0.0863319   0.0170186
2021-10-28T09:03:20.626998+0000    26     128    189248    189120    113.64        24   0.0850639   0.0175734
2021-10-28T09:03:21.627094+0000    27     128    191552    191424   110.765        36   0.0213763   0.0180493
2021-10-28T09:03:22.627217+0000    28     128    197635    197507   110.203   95.0469   0.0211162   0.0181395
2021-10-28T09:03:23.627349+0000    29     128    203328    203200    109.47   88.9531   0.0230758   0.0182618
2021-10-28T09:03:24.627496+0000    30     127    208910    208783   108.728   87.2344   0.0233788    0.018384
2021-10-28T09:03:25.627615+0000    31     128    214403    214275   107.989   85.8125   0.0218995   0.0185089
2021-10-28T09:03:26.627732+0000    32     128    217536    217408   106.144   48.9531   0.0843248   0.0188024
2021-10-28T09:03:27.627852+0000    33     128    219139    219011   103.686   25.0469   0.0787225   0.0192701
2021-10-28T09:03:28.627972+0000    34     128    221376    221248   101.664   34.9531   0.0233308   0.0196622
2021-10-28T09:03:29.628066+0000    35     128    227264    227136   101.388        92   0.0211143   0.0197185
2021-10-28T09:03:30.628190+0000    36     128    233091    232963     101.1   91.0469   0.0220845   0.0197744
2021-10-28T09:03:31.628319+0000    37     127    238688    238561   100.732   87.4688   0.0248057   0.0198461
2021-10-28T09:03:32.628445+0000    38     127    245300    245173   100.799   103.312  0.00996515   0.0198378
2021-10-28T09:03:33.628539+0000    39     128    249786    249658   100.011   70.0781   0.0844287   0.0199823
2021-10-28T09:03:34.628658+0000 min lat: 0.00507177 max lat: 0.107329 avg lat: 0.0203723
2021-10-28T09:03:34.628658+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-28T09:03:34.628658+0000    40     128    251322    251194   98.1111        24   0.0861951   0.0203723
2021-10-28T09:03:35.628786+0000    41     128    254906    254778   97.0838        56   0.0220986   0.0205956
2021-10-28T09:03:36.628911+0000    42     127    260618    260491   96.8974   89.2656   0.0213736   0.0206324
2021-10-28T09:03:37.629020+0000    43     128    266298    266170   96.7073   88.7344    0.024858   0.0206711
2021-10-28T09:03:38.629146+0000    44     128    271930    271802   96.5092        88   0.0227834   0.0207189
2021-10-28T09:03:39.629265+0000    45     128    277690    277562   96.3643        90    0.022643   0.0207494
2021-10-28T09:03:40.629386+0000    46     128    279994    279866   95.0519        36     0.07215   0.0210145
2021-10-28T09:03:41.629490+0000    47     128    281601    281473   93.5638   25.1094   0.0759421   0.0213655
2021-10-28T09:03:42.629610+0000    48     127    285257    285130   92.8048   57.1406   0.0192038   0.0215437
2021-10-28T09:03:43.629731+0000    49     127    290785    290658   92.6734    86.375   0.0227723   0.0215761
2021-10-28T09:03:44.629850+0000    50     128    296300    296172   92.5428   86.1562   0.0225633   0.0216058
2021-10-28T09:03:45.629945+0000    51     128    301676    301548   92.3752        84    0.024785   0.0216438
2021-10-28T09:03:46.630065+0000    52     128    307130    307002   92.2373   85.2188   0.0221446   0.0216783
2021-10-28T09:03:47.630184+0000    53     128    309996    309868   91.3418   44.7812   0.0864939    0.021866
2021-10-28T09:03:48.630308+0000    54     128    311532    311404   90.0947        24    0.074515   0.0221713
2021-10-28T09:03:49.630410+0000    55     128    313964    313836   89.1475        38   0.0219522   0.0224258
2021-10-28T09:03:50.630533+0000    56     128    319852    319724   89.1982        92   0.0219757   0.0224154
2021-10-28T09:03:51.630661+0000    57     128    325740    325612   89.2472        92    0.021519   0.0224017
2021-10-28T09:03:52.630780+0000    58     128    331756    331628   89.3289        94   0.0215083   0.0223846
2021-10-28T09:03:53.630878+0000    59     128    337516    337388   89.3401        90   0.0219638   0.0223805
2021-10-28T09:03:54.631006+0000 min lat: 0.00507177 max lat: 0.107329 avg lat: 0.0225403
2021-10-28T09:03:54.631006+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-28T09:03:54.631006+0000    60     128    340716    340588   88.6844        50   0.0818308   0.0225403
2021-10-28T09:03:55.631173+0000 Total time run:         60.0588
Total writes made:      340716
Write size:             16384
Object size:            16384
Bandwidth (MB/sec):     88.6412
Stddev Bandwidth:       49.847
Max bandwidth (MB/sec): 216.781
Min bandwidth (MB/sec): 22.9844
Average IOPS:           5673
Stddev IOPS:            3190.21
Max IOPS:               13874
Min IOPS:               1471
Average Latency(s):     0.0225569
Stddev Latency(s):      0.0181762
Max latency(s):         0.107329
Min latency(s):         0.00507177

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:03:56,323275466-07:00] INFO: > Stop system activity collection services[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:03:56,328341663-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:391 - rados_bench() > kill -INT 239977

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:03:56,333455399-07:00] INFO: >> for OSD host[0m
# ./benchmarks/bench-rados:396 - rados_bench() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.2 kill -INT 141803
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:03:56,341332042-07:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.2 kill -INT 141803
[1] 02:03:56 [SUCCESS] ljishen@10.10.2.2

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:03:56,525508146-07:00] INFO: >>> Copy system activity file from OSD host[0m
## ./benchmarks/bench-rados:403 - rados_bench() > basename -- /tmp/bench-rados/sys_activity_16KB_write.dat.osd_host.2
# ./benchmarks/bench-rados:403 - rados_bench() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.2:/tmp/bench-rados/sys_activity_16KB_write.dat.osd_host.2 /mnt/sda8/ceph-research/scripts/output/sys_activity_16KB_write.dat.osd_host.2


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:03:56,699797919-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:04:20,743427669-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 340.72k objects, 5.2 GiB
    usage:   16 GiB used, 84 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:04:20,750479117-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:04:30,118046372-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 340.72k objects, 5.2 GiB
    usage:   16 GiB used, 84 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:04:30,124889587-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:04:39,522772829-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 340.72k objects, 5.2 GiB
    usage:   16 GiB used, 84 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:04:39,529944504-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:04:48,831346094-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 340.72k objects, 5.2 GiB
    usage:   16 GiB used, 84 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:04:48,839069388-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:04:58,162956569-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 340.72k objects, 5.2 GiB
    usage:   16 GiB used, 84 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:04:58,170263418-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:04:58,175798488-07:00] INFO: > The cluster is idle now.[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:04:58,179098477-07:00] STAGE: Start benchmarking of mode seq ...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:04:58,185337163-07:00] INFO: > Start system activity collection services[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:04:58,190550657-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:341 - rados_bench() > rm --force /mnt/sda8/ceph-research/scripts/output/sys_activity_16KB_seq.dat.rados_bench_host.2
# ./benchmarks/bench-rados:342 - rados_bench() > sar_pid_rados_bench_host=241329
# ./benchmarks/bench-rados:342 - rados_bench() > S_TIME_FORMAT=ISO
# ./benchmarks/bench-rados:342 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/output/sys_activity_16KB_seq.dat.rados_bench_host.2 2

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:04:58,197020630-07:00] INFO: >> for OSD host[0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:04:58,205883992-07:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.2 bash
10.10.2.2: b'141902\n'
[1] 02:04:58 [SUCCESS] ljishen@10.10.2.2
141902

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:04:58,398606510-07:00] INFO: > Run rados bench[0m
# ./benchmarks/bench-rados:379 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/output/rados_bench_16KB_seq.log.2
## ./benchmarks/bench-rados:376 - rados_bench() > count_cpus 32-63,96-127
# ./benchmarks/bench-rados:376 - rados_bench() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 128 --show-time
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:04:58,418241661-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:04:58,421213080-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '119bb8de-37cd-11ec-b51d-53e6e728d2d3', '--', 'rados', 'bench', '99999999', 'seq', '--pool', 'bench_rados', '--concurrent-ios', '128', '--show-time'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 119bb8de-37cd-11ec-b51d-53e6e728d2d3 -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 128 --show-time
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
hints = 1
2021-10-28T09:05:01.736446+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-28T09:05:01.736446+0000     0       0         0         0         0         0           -           0
2021-10-28T09:05:02.736548+0000     1     128     33300     33172   518.225   518.312  0.00326479  0.00384326
2021-10-28T09:05:03.736621+0000     2     128     64665     64537   504.134   490.078  0.00271371   0.0039552
2021-10-28T09:05:04.736704+0000     3     128     97699     97571   508.127   516.156  0.00373501  0.00392641
2021-10-28T09:05:05.736778+0000     4     128    130681    130553   509.922   515.344  0.00404957  0.00391298
2021-10-28T09:05:06.736860+0000     5     128    161840    161712   505.301   486.859  0.00341308  0.00394913
2021-10-28T09:05:07.736941+0000     6     128    191666    191538    498.75   466.031  0.00406666  0.00400126
2021-10-28T09:05:08.737013+0000     7     128    221587    221459   494.283   467.516  0.00275227  0.00403788
2021-10-28T09:05:09.737098+0000     8     128    251846    251718   491.593   472.797  0.00374554  0.00406024
2021-10-28T09:05:10.737177+0000     9     128    282027    281899   489.365   471.578  0.00400908  0.00407849
2021-10-28T09:05:11.737266+0000    10     128    312274    312146   487.685   472.609  0.00429597   0.0040926
2021-10-28T09:05:12.737370+0000 Total time run:       10.9639
Total reads made:     340716
Read size:            16384
Object size:          16384
Bandwidth (MB/sec):   485.564
Average IOPS:         31076
Stddev IOPS:          1366.68
Max IOPS:             33172
Min IOPS:             29826
Average Latency(s):   0.00411115
Max latency(s):       0.0230006
Min latency(s):       0.00037699

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:05:13,430851329-07:00] INFO: > Stop system activity collection services[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:05:13,436540490-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:391 - rados_bench() > kill -INT 241329

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:05:13,442189715-07:00] INFO: >> for OSD host[0m
# ./benchmarks/bench-rados:396 - rados_bench() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.2 kill -INT 141902
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:05:13,450030861-07:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.2 kill -INT 141902
[1] 02:05:13 [SUCCESS] ljishen@10.10.2.2

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:05:13,633090160-07:00] INFO: >>> Copy system activity file from OSD host[0m
## ./benchmarks/bench-rados:403 - rados_bench() > basename -- /tmp/bench-rados/sys_activity_16KB_seq.dat.osd_host.2
# ./benchmarks/bench-rados:403 - rados_bench() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.2:/tmp/bench-rados/sys_activity_16KB_seq.dat.osd_host.2 /mnt/sda8/ceph-research/scripts/output/sys_activity_16KB_seq.dat.osd_host.2


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:05:13,788305289-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:05:37,929490726-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 340.72k objects, 5.2 GiB
    usage:   16 GiB used, 84 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:05:37,936267016-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:05:47,257371990-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 340.72k objects, 5.2 GiB
    usage:   16 GiB used, 84 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:05:47,264870440-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:05:56,599657837-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 340.72k objects, 5.2 GiB
    usage:   16 GiB used, 84 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:05:56,606589349-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:06:05,954884800-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 340.72k objects, 5.2 GiB
    usage:   16 GiB used, 84 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:06:05,961988486-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:06:15,155466697-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 340.72k objects, 5.2 GiB
    usage:   16 GiB used, 84 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:06:15,162781340-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:06:15,168463298-07:00] INFO: > The cluster is idle now.[0m


[1;7;39;49m[2021-10-28T02:06:15,170418993-07:00][RUNNING][ROUND 3/2/21] object_size=16KB[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:06:15,173702190-07:00] STAGE: Launch a Ceph cluster (CEPH_HOST: 10.10.2.1, OSD_HOST: 10.10.2.2) ...[0m
# ./benchmarks/bench-rados:183 - launch_ceph_cluster() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.1 bash -euo pipefail
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:06:15,182833387-07:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.1 bash -euo pipefail
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T02:06:15,582179421-07:00] INFO: > Remove existing clusters\x1b[0m\n'
10.10.2.1: b'## bash:17 -  > basename -- /var/lib/ceph/119bb8de-37cd-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'# bash:17 -  > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm --host 10.10.2.1 --host 10.10.2.2 rm-cluster --force --fsid 119bb8de-37cd-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T02:06:15,593567920-07:00] DEBUG: HOST_PARAMS: ['--host', '10.10.2.1', '--host', '10.10.2.2']\x1b[0m\n"
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T02:06:15,597179619-07:00] DEBUG: CEPHADM_PARAMS: ['rm-cluster', '--force', '--fsid', '119bb8de-37cd-11ec-b51d-53e6e728d2d3']\x1b[0m\n"
10.10.2.1: b"# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:63 -  > /tmp/bench-rados/ceph-research/scripts/parallel-ssh --host 10.10.2.1 --host 10.10.2.2 'sudo python3 -u - rm-cluster --force --fsid 119bb8de-37cd-11ec-b51d-53e6e728d2d3'\n"
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T02:06:15,605041850-07:00] DEBUG: command from stdin\x1b[0m\n'
10.10.2.1: b"# /tmp/bench-rados/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host 10.10.2.1 --host 10.10.2.2 'sudo python3 -u - rm-cluster --force --fsid 119bb8de-37cd-11ec-b51d-53e6e728d2d3'\n"
10.10.2.1: b'[1] 02:06:21 [SUCCESS] 10.10.2.1\n[2] 02:06:26 [SUCCESS] 10.10.2.2\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T02:06:26,499732413-07:00] INFO: > Deploy a new cluster\x1b[0m\n'
10.10.2.1: b'# bash:24 -  > /tmp/bench-rados/ceph-research/scripts/ceph-deploy -m 10.10.2.1 -o node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/10.10.2.2:/dev/nvme0n1\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T02:06:26,511742862-07:00] STAGE: Verify passwordless SSH to hosts, passwordless sudo via ljishen, and the existence of devices on hosts...\x1b[0m\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T02:06:26,517131150-07:00] INFO: > Check the host for the monitor\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:181 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@10.10.2.1 sudo --non-interactive --validate\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T02:06:26,667186012-07:00] INFO: > Check hosts for OSDs\x1b[0m\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T02:06:26,672166695-07:00] INFO: >> Check host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:200 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us sudo --non-interactive --validate\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T02:06:26,823398028-07:00] INFO: >>> Check device '/dev/nvme0n1'\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:215 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us test -b /dev/nvme0n1\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:226 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us lsblk --nodeps --noheadings --output TYPE /dev/nvme0n1\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:227 - verify_parameters() > grep --word-regexp disk\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T02:06:27,111389330-07:00] STAGE: Reset devices on hosts...\x1b[0m\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T02:06:27,116028191-07:00] INFO: > Resetting device '/dev/nvme0n1' on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:288 - reset_host_devices() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us sudo sh\n'
10.10.2.1: b'  Removing ceph--d8edfd80--05c2--48b6--8cf9--b35d95e34c21-osd--block--44b9da33--270c--4be0--8b5d--c592a8099a99 (253:0)\n'
10.10.2.1: b'  Archiving volume group "ceph-d8edfd80-05c2-48b6-8cf9-b35d95e34c21" metadata (seqno 5).\n'
10.10.2.1: b'  Releasing logical volume "osd-block-44b9da33-270c-4be0-8b5d-c592a8099a99"\n'
10.10.2.1: b'  Creating volume group backup "/etc/lvm/backup/ceph-d8edfd80-05c2-48b6-8cf9-b35d95e34c21" (seqno 6).\n'
10.10.2.1: b'  Logical volume "osd-block-44b9da33-270c-4be0-8b5d-c592a8099a99" successfully removed\n'
10.10.2.1: b'  Removing physical volume "/dev/nvme0n1" from volume group "ceph-d8edfd80-05c2-48b6-8cf9-b35d95e34c21"\n'
10.10.2.1: b'  Volume group "ceph-d8edfd80-05c2-48b6-8cf9-b35d95e34c21" successfully removed\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T02:06:27,473735469-07:00] STAGE: Bootstrap a new cluster...\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:317 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.1 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T02:06:27,483510745-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T02:06:27,487143885-07:00] DEBUG: CEPHADM_PARAMS: ['bootstrap', '--config', '/tmp/bench-rados/ceph-research/scripts/ceph.conf', '--mon-ip', '10.10.2.1', '--output-dir', '/tmp/bench-rados/deployment_data_root/etc/ceph', '--ssh-user', 'ljishen', '--skip-dashboard', '--no-minimize-config', '--allow-overwrite', '--allow-fqdn-hostname', '--orphan-initial-daemons', '--skip-monitoring-stack', '--single-host-defaults']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.1 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults\n'
10.10.2.1: b'Verifying podman|docker is present...\nVerifying lvm2 is present...\n'
10.10.2.1: b'Verifying time synchronization is in place...\n'
10.10.2.1: b'Unit ntp.service is enabled and running\n'
10.10.2.1: b'Repeating the final host check...\n'
10.10.2.1: b'podman|docker (/usr/bin/docker) is present\n'
10.10.2.1: b'systemctl is present\n'
10.10.2.1: b'lvcreate is present\n'
10.10.2.1: b'Unit ntp.service is enabled and running\nHost looks OK\n'
10.10.2.1: b'Cluster fsid: 55639040-37ce-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Verifying IP 10.10.2.1 port 3300 ...\n'
10.10.2.1: b'Verifying IP 10.10.2.1 port 6789 ...\n'
10.10.2.1: b'Mon IP `10.10.2.1` is in CIDR network `10.10.2.0/24`\n'
10.10.2.1: b'- internal network (--cluster-network) has not been provided, OSD replication will default to the public_network\n'
10.10.2.1: b'Adjusting default settings to suit single-host cluster...\n'
10.10.2.1: b'Pulling container image quay.io/ceph/ceph:v16...\n'
10.10.2.1: b'Ceph version: ceph version 16.2.6 (ee28fb57e47e9f88813e24bbf4c14496ca299d31) pacific (stable)\n'
10.10.2.1: b'Extracting ceph user uid/gid from container image...\n'
10.10.2.1: b'Creating initial keys...\n'
10.10.2.1: b'Creating initial monmap...\n'
10.10.2.1: b'Creating mon...\n'
10.10.2.1: b'Waiting for mon to start...\n'
10.10.2.1: b'Waiting for mon...\n'
10.10.2.1: b'mon is available\nSetting mon public_network to 10.10.2.0/24\n'
10.10.2.1: b'Wrote config to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf\n'
10.10.2.1: b'Wrote keyring to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring\n'
10.10.2.1: b'Creating mgr...\n'
10.10.2.1: b'Verifying port 9283 ...\n'
10.10.2.1: b'Waiting for mgr to start...\n'
10.10.2.1: b'Waiting for mgr...\n'
10.10.2.1: b'mgr not available, waiting (1/15)...\n'
10.10.2.1: b'mgr not available, waiting (2/15)...\n'
10.10.2.1: b'mgr is available\n'
10.10.2.1: b'Enabling cephadm module...\n'
10.10.2.1: b'Waiting for the mgr to restart...\n'
10.10.2.1: b'Waiting for mgr epoch 5...\n'
10.10.2.1: b'mgr epoch 5 is available\n'
10.10.2.1: b'Setting orchestrator backend to cephadm...\n'
10.10.2.1: b'Generating ssh key...\n'
10.10.2.1: b'Wrote public SSH key to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub\n'
10.10.2.1: b'Adding key to ljishen@localhost authorized_keys...\n'
10.10.2.1: b'Adding host node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us...\n'
10.10.2.1: b'Deploying unmanaged mon service...\n'
10.10.2.1: b'Deploying unmanaged mgr service...\n'
10.10.2.1: b'You can access the Ceph CLI with:\n\n\tsudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --fsid 55639040-37ce-11ec-b51d-53e6e728d2d3 -c /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf -k /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring\n\n'
10.10.2.1: b'Please consider enabling telemetry to help improve Ceph:\n\n\tceph telemetry on\n\nFor more information see:\n\n\thttps://docs.ceph.com/docs/pacific/mgr/telemetry/\n\n'
10.10.2.1: b'Bootstrap complete.\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T02:07:25,160416679-07:00] INFO: Sleep for 20 seconds\x1b[0m\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T02:07:45,168106979-07:00] INFO: > Set the default number of monitor daemons to 1\x1b[0m\n# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:337 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T02:07:45,178723416-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T02:07:45,182257800-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mon', '1']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1\n'
10.10.2.1: b'Inferring fsid 55639040-37ce-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/55639040-37ce-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b'Scheduled mon update...\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T02:07:53,832499870-07:00] INFO: > Set the default number of manager daemons to 1\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:342 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T02:07:53,842777570-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T02:07:53,846789712-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mgr', '1']\x1b[0m\n# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1\n"
10.10.2.1: b'Inferring fsid 55639040-37ce-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/55639040-37ce-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b'Scheduled mgr update...\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T02:08:02,893234436-07:00] STAGE: Add hosts to the cluster...\x1b[0m\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T02:08:02,900022045-07:00] INFO: > Adding host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:361 - add_hosts() > ssh-copy-id -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -f -i /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\n'
10.10.2.1: b'/usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: "/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub"\n'
10.10.2.1: b'\nNumber of key(s) added: 1\n\nNow try logging into the machine, with:   "ssh -o \'GlobalKnownHostsFile=/dev/null\' -o \'LogLevel=ERROR\' -o \'PasswordAuthentication=no\' -o \'StrictHostKeyChecking=no\' -o \'UserKnownHostsFile=/dev/null\' \'ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\'"\nand check to make sure that only the key(s) you wanted were added.\n\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:367 - add_hosts() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us 10.10.2.2\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T02:08:03,114000765-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T02:08:03,117863356-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'host', 'add', 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us', '10.10.2.2']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us 10.10.2.2\n'
10.10.2.1: b'Inferring fsid 55639040-37ce-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/55639040-37ce-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b"Added host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us' with addr '10.10.2.2'\n"
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T02:08:12,329384389-07:00] INFO: Sleep for 20 seconds\x1b[0m\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T02:08:32,334350803-07:00] STAGE: Add OSDs to the cluster...\x1b[0m\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T02:08:32,340612905-07:00] INFO: > Adding devices ['/dev/nvme0n1'] on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us' as OSDs\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:388 - add_host_devices_as_osds() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T02:08:32,350715987-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T02:08:32,354587886-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'daemon', 'add', 'osd', 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1\n'
10.10.2.1: b'Inferring fsid 55639040-37ce-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/55639040-37ce-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b"Created osd(s) 0 on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'\n"
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T02:08:57,607642728-07:00] INFO: > Sleep for 20 seconds\x1b[0m\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T02:09:17,612821529-07:00] STAGE: Show Cluster Status\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:408 - show_status() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T02:09:17,622675413-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T02:09:17,626644975-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', '--status']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status\n'
10.10.2.1: b'Inferring fsid 55639040-37ce-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/55639040-37ce-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b'  cluster:\n    id:     55639040-37ce-11ec-b51d-53e6e728d2d3\n    health: HEALTH_WARN\n            1 pool(s) have no replicas configured\n \n  services:\n    mon: 1 daemons, quorum node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us (age 2m)\n    mgr: node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us.owtvji(active, since 2m)\n    osd: 1 osds: 1 up (since 16s), 1 in (since 40s)\n \n  data:\n    pools:   1 pools, 1 pgs\n    objects: 0 objects, 0 B\n    usage:   4.7 MiB used, 100 GiB / 100 GiB avail\n    pgs:     1 active+clean\n \n'
[1] 02:09:25 [SUCCESS] ljishen@10.10.2.1

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:06:15,582179421-07:00] INFO: > Remove existing clusters[0m
## bash:17 -  > basename -- /var/lib/ceph/119bb8de-37cd-11ec-b51d-53e6e728d2d3
# bash:17 -  > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm --host 10.10.2.1 --host 10.10.2.2 rm-cluster --force --fsid 119bb8de-37cd-11ec-b51d-53e6e728d2d3
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:06:15,593567920-07:00] DEBUG: HOST_PARAMS: ['--host', '10.10.2.1', '--host', '10.10.2.2'][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:06:15,597179619-07:00] DEBUG: CEPHADM_PARAMS: ['rm-cluster', '--force', '--fsid', '119bb8de-37cd-11ec-b51d-53e6e728d2d3'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:63 -  > /tmp/bench-rados/ceph-research/scripts/parallel-ssh --host 10.10.2.1 --host 10.10.2.2 'sudo python3 -u - rm-cluster --force --fsid 119bb8de-37cd-11ec-b51d-53e6e728d2d3'
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:06:15,605041850-07:00] DEBUG: command from stdin[0m
# /tmp/bench-rados/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host 10.10.2.1 --host 10.10.2.2 'sudo python3 -u - rm-cluster --force --fsid 119bb8de-37cd-11ec-b51d-53e6e728d2d3'
[1] 02:06:21 [SUCCESS] 10.10.2.1
[2] 02:06:26 [SUCCESS] 10.10.2.2

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:06:26,499732413-07:00] INFO: > Deploy a new cluster[0m
# bash:24 -  > /tmp/bench-rados/ceph-research/scripts/ceph-deploy -m 10.10.2.1 -o node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/10.10.2.2:/dev/nvme0n1


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:06:26,511742862-07:00] STAGE: Verify passwordless SSH to hosts, passwordless sudo via ljishen, and the existence of devices on hosts...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:06:26,517131150-07:00] INFO: > Check the host for the monitor[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:181 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@10.10.2.1 sudo --non-interactive --validate

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:06:26,667186012-07:00] INFO: > Check hosts for OSDs[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:06:26,672166695-07:00] INFO: >> Check host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:200 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us sudo --non-interactive --validate

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:06:26,823398028-07:00] INFO: >>> Check device '/dev/nvme0n1'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:215 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us test -b /dev/nvme0n1
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:226 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us lsblk --nodeps --noheadings --output TYPE /dev/nvme0n1
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:227 - verify_parameters() > grep --word-regexp disk


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:06:27,111389330-07:00] STAGE: Reset devices on hosts...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:06:27,116028191-07:00] INFO: > Resetting device '/dev/nvme0n1' on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:288 - reset_host_devices() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us sudo sh
  Removing ceph--d8edfd80--05c2--48b6--8cf9--b35d95e34c21-osd--block--44b9da33--270c--4be0--8b5d--c592a8099a99 (253:0)
  Archiving volume group "ceph-d8edfd80-05c2-48b6-8cf9-b35d95e34c21" metadata (seqno 5).
  Releasing logical volume "osd-block-44b9da33-270c-4be0-8b5d-c592a8099a99"
  Creating volume group backup "/etc/lvm/backup/ceph-d8edfd80-05c2-48b6-8cf9-b35d95e34c21" (seqno 6).
  Logical volume "osd-block-44b9da33-270c-4be0-8b5d-c592a8099a99" successfully removed
  Removing physical volume "/dev/nvme0n1" from volume group "ceph-d8edfd80-05c2-48b6-8cf9-b35d95e34c21"
  Volume group "ceph-d8edfd80-05c2-48b6-8cf9-b35d95e34c21" successfully removed


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:06:27,473735469-07:00] STAGE: Bootstrap a new cluster...[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:317 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.1 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:06:27,483510745-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:06:27,487143885-07:00] DEBUG: CEPHADM_PARAMS: ['bootstrap', '--config', '/tmp/bench-rados/ceph-research/scripts/ceph.conf', '--mon-ip', '10.10.2.1', '--output-dir', '/tmp/bench-rados/deployment_data_root/etc/ceph', '--ssh-user', 'ljishen', '--skip-dashboard', '--no-minimize-config', '--allow-overwrite', '--allow-fqdn-hostname', '--orphan-initial-daemons', '--skip-monitoring-stack', '--single-host-defaults'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.1 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults
Verifying podman|docker is present...
Verifying lvm2 is present...
Verifying time synchronization is in place...
Unit ntp.service is enabled and running
Repeating the final host check...
podman|docker (/usr/bin/docker) is present
systemctl is present
lvcreate is present
Unit ntp.service is enabled and running
Host looks OK
Cluster fsid: 55639040-37ce-11ec-b51d-53e6e728d2d3
Verifying IP 10.10.2.1 port 3300 ...
Verifying IP 10.10.2.1 port 6789 ...
Mon IP `10.10.2.1` is in CIDR network `10.10.2.0/24`
- internal network (--cluster-network) has not been provided, OSD replication will default to the public_network
Adjusting default settings to suit single-host cluster...
Pulling container image quay.io/ceph/ceph:v16...
Ceph version: ceph version 16.2.6 (ee28fb57e47e9f88813e24bbf4c14496ca299d31) pacific (stable)
Extracting ceph user uid/gid from container image...
Creating initial keys...
Creating initial monmap...
Creating mon...
Waiting for mon to start...
Waiting for mon...
mon is available
Setting mon public_network to 10.10.2.0/24
Wrote config to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf
Wrote keyring to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring
Creating mgr...
Verifying port 9283 ...
Waiting for mgr to start...
Waiting for mgr...
mgr not available, waiting (1/15)...
mgr not available, waiting (2/15)...
mgr is available
Enabling cephadm module...
Waiting for the mgr to restart...
Waiting for mgr epoch 5...
mgr epoch 5 is available
Setting orchestrator backend to cephadm...
Generating ssh key...
Wrote public SSH key to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub
Adding key to ljishen@localhost authorized_keys...
Adding host node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us...
Deploying unmanaged mon service...
Deploying unmanaged mgr service...
You can access the Ceph CLI with:

	sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --fsid 55639040-37ce-11ec-b51d-53e6e728d2d3 -c /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf -k /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring

Please consider enabling telemetry to help improve Ceph:

	ceph telemetry on

For more information see:

	https://docs.ceph.com/docs/pacific/mgr/telemetry/

Bootstrap complete.

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:07:25,160416679-07:00] INFO: Sleep for 20 seconds[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:07:45,168106979-07:00] INFO: > Set the default number of monitor daemons to 1[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:337 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:07:45,178723416-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:07:45,182257800-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mon', '1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1
Inferring fsid 55639040-37ce-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/55639040-37ce-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Scheduled mon update...

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:07:53,832499870-07:00] INFO: > Set the default number of manager daemons to 1[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:342 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:07:53,842777570-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:07:53,846789712-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mgr', '1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1
Inferring fsid 55639040-37ce-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/55639040-37ce-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Scheduled mgr update...


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:08:02,893234436-07:00] STAGE: Add hosts to the cluster...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:08:02,900022045-07:00] INFO: > Adding host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:361 - add_hosts() > ssh-copy-id -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -f -i /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us
/usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: "/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub"

Number of key(s) added: 1

Now try logging into the machine, with:   "ssh -o 'GlobalKnownHostsFile=/dev/null' -o 'LogLevel=ERROR' -o 'PasswordAuthentication=no' -o 'StrictHostKeyChecking=no' -o 'UserKnownHostsFile=/dev/null' 'ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'"
and check to make sure that only the key(s) you wanted were added.

# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:367 - add_hosts() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us 10.10.2.2
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:08:03,114000765-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:08:03,117863356-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'host', 'add', 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us', '10.10.2.2'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us 10.10.2.2
Inferring fsid 55639040-37ce-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/55639040-37ce-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Added host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us' with addr '10.10.2.2'

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:08:12,329384389-07:00] INFO: Sleep for 20 seconds[0m


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:08:32,334350803-07:00] STAGE: Add OSDs to the cluster...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:08:32,340612905-07:00] INFO: > Adding devices ['/dev/nvme0n1'] on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us' as OSDs[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:388 - add_host_devices_as_osds() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:08:32,350715987-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:08:32,354587886-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'daemon', 'add', 'osd', 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1
Inferring fsid 55639040-37ce-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/55639040-37ce-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Created osd(s) 0 on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:08:57,607642728-07:00] INFO: > Sleep for 20 seconds[0m


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:09:17,612821529-07:00] STAGE: Show Cluster Status[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:408 - show_status() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:09:17,622675413-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:09:17,626644975-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', '--status'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status
Inferring fsid 55639040-37ce-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/55639040-37ce-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
  cluster:
    id:     55639040-37ce-11ec-b51d-53e6e728d2d3
    health: HEALTH_WARN
            1 pool(s) have no replicas configured
 
  services:
    mon: 1 daemons, quorum node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us (age 2m)
    mgr: node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us.owtvji(active, since 2m)
    osd: 1 osds: 1 up (since 16s), 1 in (since 40s)
 
  data:
    pools:   1 pools, 1 pgs
    objects: 0 objects, 0 B
    usage:   4.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     1 active+clean
 

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:09:25,945015405-07:00] INFO: > Copy configuration files to local[0m
# ./benchmarks/bench-rados:220 - launch_ceph_cluster() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.1 sudo chown --changes --recursive ljishen /tmp/bench-rados/deployment_data_root/etc/ceph
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:09:25,952502725-07:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.1 sudo chown --changes --recursive ljishen /tmp/bench-rados/deployment_data_root/etc/ceph
[1] 02:09:26 [SUCCESS] ljishen@10.10.2.1
# ./benchmarks/bench-rados:223 - launch_ceph_cluster() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.1:/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf
# ./benchmarks/bench-rados:226 - launch_ceph_cluster() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.1:/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:09:26,429047229-07:00] STAGE: Update local ceph.conf as a client...[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:09:26,432732173-07:00] STAGE: Configure Ceph pool...[0m
# ./benchmarks/bench-rados:257 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_max_backfills 32
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:09:26,454980527-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:09:26,457815379-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '55639040-37ce-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'config', 'set', 'osd', 'osd_max_backfills', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 55639040-37ce-11ec-b51d-53e6e728d2d3 -- ceph config set osd osd_max_backfills 32
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:258 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_active 32
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:09:30,640383265-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:09:30,643224419-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '55639040-37ce-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_active', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 55639040-37ce-11ec-b51d-53e6e728d2d3 -- ceph config set osd osd_recovery_max_active 32
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:259 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_single_start 8
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:09:35,138648612-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:09:35,141751529-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '55639040-37ce-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_single_start', '8'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 55639040-37ce-11ec-b51d-53e6e728d2d3 -- ceph config set osd osd_recovery_max_single_start 8
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:260 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_op_priority 63
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:09:39,404689765-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:09:39,407590040-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '55639040-37ce-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_op_priority', '63'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 55639040-37ce-11ec-b51d-53e6e728d2d3 -- ceph config set osd osd_recovery_op_priority 63
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:262 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config show-with-defaults osd.0
# ./benchmarks/bench-rados:263 - configure_ceph_pool() > grep 'osd_max_backfills\|osd_recovery'
# ./benchmarks/bench-rados:263 - configure_ceph_pool() > column -t -s ' '
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
osd_max_backfills                        32         mon
osd_recovery_cost                        20971520   default
osd_recovery_delay_start                 0.000000   default
osd_recovery_max_active                  32         mon
osd_recovery_max_active_hdd              3          default
osd_recovery_max_active_ssd              10         default
osd_recovery_max_chunk                   8388608    default
osd_recovery_max_omap_entries_per_chunk  8096       default
osd_recovery_max_single_start            8          mon
osd_recovery_op_priority                 63         mon
osd_recovery_op_warn_multiple            16         default
osd_recovery_priority                    5          default
osd_recovery_retry_interval              30.000000  default
osd_recovery_sleep                       0.000000   default
osd_recovery_sleep_hdd                   0.100000   default
osd_recovery_sleep_hybrid                0.025000   default
osd_recovery_sleep_ssd                   0.000000   default
# ./benchmarks/bench-rados:265 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool create bench_rados 128 128 replicated --size 1 --pg-num-min 128
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:09:48,297160858-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:09:48,300561056-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '55639040-37ce-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'create', 'bench_rados', '128', '128', 'replicated', '--size', '1', '--pg-num-min', '128'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 55639040-37ce-11ec-b51d-53e6e728d2d3 -- ceph osd pool create bench_rados 128 128 replicated --size 1 --pg-num-min 128
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
pool 'bench_rados' created
# ./benchmarks/bench-rados:267 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados min_size 1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:09:52,988317511-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:09:52,991426921-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '55639040-37ce-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'min_size', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 55639040-37ce-11ec-b51d-53e6e728d2d3 -- ceph osd pool set bench_rados min_size 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 min_size to 1
# ./benchmarks/bench-rados:268 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados pg_autoscale_mode off
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:09:57,239152017-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:09:57,242091907-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '55639040-37ce-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'pg_autoscale_mode', 'off'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 55639040-37ce-11ec-b51d-53e6e728d2d3 -- ceph osd pool set bench_rados pg_autoscale_mode off
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 pg_autoscale_mode to off
# ./benchmarks/bench-rados:269 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool application enable bench_rados benchmark
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:10:01,711778741-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:10:01,714595739-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '55639040-37ce-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'application', 'enable', 'bench_rados', 'benchmark'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 55639040-37ce-11ec-b51d-53e6e728d2d3 -- ceph osd pool application enable bench_rados benchmark
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
enabled application 'benchmark' on pool 'bench_rados'
# ./benchmarks/bench-rados:270 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados noscrub 1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:10:07,098441370-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:10:07,101539829-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '55639040-37ce-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'noscrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 55639040-37ce-11ec-b51d-53e6e728d2d3 -- ceph osd pool set bench_rados noscrub 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 noscrub to 1
# ./benchmarks/bench-rados:271 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados nodeep-scrub 1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:10:11,852302382-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:10:11,855319919-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '55639040-37ce-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'nodeep-scrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 55639040-37ce-11ec-b51d-53e6e728d2d3 -- ceph osd pool set bench_rados nodeep-scrub 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 nodeep-scrub to 1
# ./benchmarks/bench-rados:272 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool ls detail
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:10:17,236342018-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:10:17,239305974-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '55639040-37ce-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'ls', 'detail'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 55639040-37ce-11ec-b51d-53e6e728d2d3 -- ceph osd pool ls detail
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
pool 1 'device_health_metrics' replicated size 1 min_size 1 crush_rule 0 object_hash rjenkins pg_num 64 pgp_num 64 autoscale_mode on last_change 19 lfor 0/0/16 flags hashpspool stripe_width 0 pg_num_min 1 application mgr_devicehealth
pool 2 'bench_rados' replicated size 1 min_size 1 crush_rule 0 object_hash rjenkins pg_num 128 pgp_num 128 autoscale_mode off last_change 20 flags hashpspool,noscrub,nodeep-scrub stripe_width 0 pg_num_min 128 application benchmark

# ./benchmarks/bench-rados:273 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd df tree
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:10:21,491581655-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:10:21,494619189-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '55639040-37ce-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'df', 'tree'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 55639040-37ce-11ec-b51d-53e6e728d2d3 -- ceph osd df tree
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
ID  CLASS  WEIGHT   REWEIGHT  SIZE     RAW USE  DATA     OMAP  META     AVAIL    %USE  VAR   PGS  STATUS  TYPE NAME      
-1         0.09769         -  100 GiB  5.7 MiB  168 KiB   0 B  5.6 MiB  100 GiB  0.01  1.00    -          root default   
-3         0.09769         -  100 GiB  5.7 MiB  168 KiB   0 B  5.6 MiB  100 GiB  0.01  1.00    -              host node-1
 0    ssd  0.09769   1.00000  100 GiB  5.7 MiB  168 KiB   0 B  5.6 MiB  100 GiB  0.01  1.00  192      up          osd.0  
                       TOTAL  100 GiB  5.7 MiB  168 KiB   0 B  5.6 MiB  100 GiB  0.01                                    
MIN/MAX VAR: 1.00/1.00  STDDEV: 0


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:10:25,810502452-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:10:50,199915407-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:10:59,313000713-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:11:08,602031622-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:11:08,609469387-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:11:17,910132265-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:11:17,917754187-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:11:27,199499666-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:11:27,206770687-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:11:36,475237352-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:11:36,482825441-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:11:45,947694287-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:11:45,954995916-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:11:45,960416050-07:00] INFO: > The cluster is idle now.[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:11:45,964101515-07:00] STAGE: Start benchmarking of mode write ...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:11:45,970194988-07:00] INFO: > Start system activity collection services[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:11:45,975538036-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:341 - rados_bench() > rm --force /mnt/sda8/ceph-research/scripts/output/sys_activity_16KB_write.dat.rados_bench_host.3
# ./benchmarks/bench-rados:342 - rados_bench() > sar_pid_rados_bench_host=246938
# ./benchmarks/bench-rados:342 - rados_bench() > S_TIME_FORMAT=ISO
# ./benchmarks/bench-rados:342 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/output/sys_activity_16KB_write.dat.rados_bench_host.3 2

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:11:45,982139225-07:00] INFO: >> for OSD host[0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:11:45,990774007-07:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.2 bash
10.10.2.2: b'145538\n'
[1] 02:11:46 [SUCCESS] ljishen@10.10.2.2
145538

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:11:46,183172113-07:00] INFO: > Run rados bench[0m
# ./benchmarks/bench-rados:370 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/output/rados_bench_16KB_write.log.3
## ./benchmarks/bench-rados:361 - rados_bench() > count_cpus 32-63,96-127
# ./benchmarks/bench-rados:361 - rados_bench() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 60 write --pool bench_rados -b 16384 -O 16384 --concurrent-ios 128 --show-time --write-object --write-omap --write-xattr --no-cleanup
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:11:46,203033110-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:11:46,205917465-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '55639040-37ce-11ec-b51d-53e6e728d2d3', '--', 'rados', 'bench', '60', 'write', '--pool', 'bench_rados', '-b', '16384', '-O', '16384', '--concurrent-ios', '128', '--show-time', '--write-object', '--write-omap', '--write-xattr', '--no-cleanup'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 55639040-37ce-11ec-b51d-53e6e728d2d3 -- rados bench 60 write --pool bench_rados -b 16384 -O 16384 --concurrent-ios 128 --show-time --write-object --write-omap --write-xattr --no-cleanup
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
hints = 1
2021-10-28T09:11:49.537689+0000 Maintaining 128 concurrent writes of 16384 bytes to objects of size 16384 for up to 60 seconds or 0 objects
2021-10-28T09:11:49.537699+0000 Object prefix: benchmark_data_node-2.bluefield2.ucsc-cmps10_7
2021-10-28T09:11:49.538705+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-28T09:11:49.538705+0000     0       0         0         0         0         0           -           0
2021-10-28T09:11:50.538846+0000     1     128     13916     13788   215.421   215.438  0.00947321  0.00923907
2021-10-28T09:11:51.538939+0000     2     128     27713     27585    215.49   215.578  0.00822485  0.00924504
2021-10-28T09:11:52.539013+0000     3     128     35264     35136   182.985   117.984    0.023355   0.0108664
2021-10-28T09:11:53.539081+0000     4     128     41466     41338   161.464   96.9062  0.00826987   0.0123675
2021-10-28T09:11:54.539148+0000     5     128     54764     54636   170.725   207.781  0.00938821   0.0117022
2021-10-28T09:11:55.539217+0000     6     128     64004     63876   166.331   144.375   0.0907632   0.0119573
2021-10-28T09:11:56.539282+0000     7     128     65493     65365   145.893   23.2656   0.0863071   0.0136532
2021-10-28T09:11:57.539350+0000     8     128     67541     67413   131.656        32   0.0216184   0.0151755
2021-10-28T09:11:58.539424+0000     9     128     73802     73674   127.897   97.8281     0.02113   0.0156211
2021-10-28T09:11:59.539526+0000    10     128     79818     79690   124.506        94   0.0225354   0.0160499
2021-10-28T09:12:00.539630+0000    11     128     92694     92566   131.476   201.188  0.00949609   0.0152023
2021-10-28T09:12:01.539716+0000    12     128     97784     97656   127.146   79.5312   0.0257555   0.0157011
2021-10-28T09:12:02.539805+0000    13     127    104432    104305   125.357   103.891  0.00979675   0.0159458
2021-10-28T09:12:03.539895+0000    14     128    116938    116810   130.358   195.391   0.0102728   0.0153336
2021-10-28T09:12:04.539992+0000    15     128    125894    125766   130.996   139.938   0.0869311   0.0152266
2021-10-28T09:12:05.540088+0000    16     128    127302    127174   124.183        22   0.0941155   0.0160567
2021-10-28T09:12:06.540168+0000    17     128    128776    128648   118.233   23.0312   0.0871547   0.0168767
2021-10-28T09:12:07.540237+0000    18     128    133702    133574    115.94   76.9688   0.0213255   0.0172414
2021-10-28T09:12:08.540309+0000    19     128    139206    139078   114.364        86   0.0258031   0.0174721
2021-10-28T09:12:09.540377+0000 min lat: 0.00531062 max lat: 0.100136 avg lat: 0.0176991
2021-10-28T09:12:09.540377+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-28T09:12:09.540377+0000    20     127    144680    144553   112.923   85.5469    0.022805   0.0176991
2021-10-28T09:12:10.540462+0000    21     128    150280    150152   111.711   87.4844   0.0211263   0.0178936
2021-10-28T09:12:11.540543+0000    22     127    156706    156579   111.198   100.422   0.0178999    0.017977
2021-10-28T09:12:12.540615+0000    23     127    162874    162747   110.553    96.375   0.0238346   0.0180768
2021-10-28T09:12:13.540687+0000    24     128    171905    171777   111.825   141.094  0.00929247   0.0178799
2021-10-28T09:12:14.540758+0000    25     127    184611    184484   115.293   198.547  0.00954076   0.0173418
2021-10-28T09:12:15.540830+0000    26     128    188247    188119   113.043   56.7969   0.0913643   0.0176591
2021-10-28T09:12:16.540901+0000    27     127    189687    189560    109.69   22.5156   0.0921708   0.0181968
2021-10-28T09:12:17.540968+0000    28     128    193751    193623    108.04   63.4844   0.0174553   0.0185052
2021-10-28T09:12:18.541037+0000    29     128    199895    199767   107.625        96    0.017358   0.0185752
2021-10-28T09:12:19.541107+0000    30     128    205655    205527   107.037        90   0.0230697   0.0186731
2021-10-28T09:12:20.541176+0000    31     128    211287    211159   106.423        88   0.0224327   0.0187862
2021-10-28T09:12:21.541241+0000    32     128    216732    216604   105.756   85.0781   0.0596516   0.0188958
2021-10-28T09:12:22.541305+0000    33     128    218268    218140   103.278        24   0.0852545    0.019343
2021-10-28T09:12:23.541370+0000    34     128    219804    219676   100.946        24   0.0776154   0.0197943
2021-10-28T09:12:24.541439+0000    35     128    223575    223447   99.7455   58.9219   0.0220345   0.0200423
2021-10-28T09:12:25.541507+0000    36     128    229404    229276   99.5046   91.0781   0.0220617   0.0200902
2021-10-28T09:12:26.541581+0000    37     128    235223    235095   99.2724   90.9219   0.0225345   0.0201398
2021-10-28T09:12:27.541647+0000    38     128    240855    240727   98.9757        88   0.0225905   0.0201969
2021-10-28T09:12:28.541714+0000    39     128    247818    247690   99.2273   108.797   0.0240514   0.0201478
2021-10-28T09:12:29.541787+0000 min lat: 0.00531062 max lat: 0.130781 avg lat: 0.0204588
2021-10-28T09:12:29.541787+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-28T09:12:29.541787+0000    40     128    250122    249994   97.6465        36    0.092609   0.0204588
2021-10-28T09:12:30.541856+0000    41     128    251593    251465   95.8255   22.9844   0.0902038   0.0208408
2021-10-28T09:12:31.541923+0000    42     128    256351    256223   95.3139   74.3438   0.0189426    0.020977
2021-10-28T09:12:32.541989+0000    43     128    262495    262367   95.3297        96   0.0225693    0.020974
2021-10-28T09:12:33.542053+0000    44     128    268196    268068   95.1875   89.0781   0.0219057   0.0210032
2021-10-28T09:12:34.542122+0000    45     128    273828    273700   95.0276        88   0.0226633   0.0210381
2021-10-28T09:12:35.542190+0000    46     128    278564    278436   94.5704        74   0.0852741   0.0211194
2021-10-28T09:12:36.542266+0000    47     128    280484    280356   93.1965        30   0.0727091   0.0214437
2021-10-28T09:12:37.542339+0000    48     128    282020    281892   91.7549        24    0.079289    0.021785
2021-10-28T09:12:38.542404+0000    49     128    287268    287140   91.5557        82   0.0202233   0.0218383
2021-10-28T09:12:39.542487+0000    50     128    293343    293215   91.6229   94.9219   0.0231316   0.0218236
2021-10-28T09:12:40.542585+0000    51     128    298788    298660   91.4944   85.0781   0.0227658   0.0218519
2021-10-28T09:12:41.542662+0000    52     128    304223    304095   91.3679   84.9219   0.0223237   0.0218831
2021-10-28T09:12:42.542746+0000    53     128    309215    309087   91.1155        78   0.0843319   0.0219385
2021-10-28T09:12:43.542815+0000    54     128    310692    310564   89.8555   23.0781   0.0838207    0.022234
2021-10-28T09:12:44.542888+0000    55     128    312159    312031   88.6385   22.9219   0.0823735   0.0225325
2021-10-28T09:12:45.542984+0000    56     127    316825    316698   88.3578   72.9219   0.0182677   0.0226309
2021-10-28T09:12:46.543061+0000    57     128    322995    322867   88.4985   96.3906   0.0229187   0.0225929
2021-10-28T09:12:47.543131+0000    58     128    328678    328550   88.5036   88.7969   0.0221726   0.0225904
2021-10-28T09:12:48.543199+0000    59     128    334438    334310   88.5288        90   0.0228396   0.0225865
2021-10-28T09:12:49.543291+0000 min lat: 0.00531062 max lat: 0.130781 avg lat: 0.0225938
2021-10-28T09:12:49.543291+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-28T09:12:49.543291+0000    60     128    339880    339752   88.4704   85.0312   0.0463042   0.0225938
2021-10-28T09:12:50.543423+0000 Total time run:         60.0801
Total writes made:      339880
Write size:             16384
Object size:            16384
Bandwidth (MB/sec):     88.3924
Stddev Bandwidth:       50.2593
Max bandwidth (MB/sec): 215.578
Min bandwidth (MB/sec): 22
Average IOPS:           5657
Stddev IOPS:            3216.6
Max IOPS:               13797
Min IOPS:               1408
Average Latency(s):     0.022616
Stddev Latency(s):      0.0183935
Max latency(s):         0.130781
Min latency(s):         0.00531062

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:12:51,278619958-07:00] INFO: > Stop system activity collection services[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:12:51,283675174-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:391 - rados_bench() > kill -INT 246938

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:12:51,288819247-07:00] INFO: >> for OSD host[0m
# ./benchmarks/bench-rados:396 - rados_bench() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.2 kill -INT 145538
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:12:51,296479793-07:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.2 kill -INT 145538
[1] 02:12:51 [SUCCESS] ljishen@10.10.2.2

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:12:51,481620193-07:00] INFO: >>> Copy system activity file from OSD host[0m
## ./benchmarks/bench-rados:403 - rados_bench() > basename -- /tmp/bench-rados/sys_activity_16KB_write.dat.osd_host.3
# ./benchmarks/bench-rados:403 - rados_bench() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.2:/tmp/bench-rados/sys_activity_16KB_write.dat.osd_host.3 /mnt/sda8/ceph-research/scripts/output/sys_activity_16KB_write.dat.osd_host.3


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:12:51,655842559-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:13:15,971160691-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 339.88k objects, 5.2 GiB
    usage:   16 GiB used, 84 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:13:15,978546930-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:13:25,286883648-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 339.88k objects, 5.2 GiB
    usage:   16 GiB used, 84 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:13:25,294762265-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:13:34,482961765-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 339.88k objects, 5.2 GiB
    usage:   16 GiB used, 84 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:13:34,490155431-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:13:43,798323641-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 339.88k objects, 5.2 GiB
    usage:   16 GiB used, 84 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:13:43,805824586-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:13:52,980925022-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 339.88k objects, 5.2 GiB
    usage:   16 GiB used, 84 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:13:52,988248212-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:13:52,993815933-07:00] INFO: > The cluster is idle now.[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:13:52,997191875-07:00] STAGE: Start benchmarking of mode seq ...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:13:53,003698476-07:00] INFO: > Start system activity collection services[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:13:53,008782707-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:341 - rados_bench() > rm --force /mnt/sda8/ceph-research/scripts/output/sys_activity_16KB_seq.dat.rados_bench_host.3
# ./benchmarks/bench-rados:342 - rados_bench() > sar_pid_rados_bench_host=248292
# ./benchmarks/bench-rados:342 - rados_bench() > S_TIME_FORMAT=ISO
# ./benchmarks/bench-rados:342 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/output/sys_activity_16KB_seq.dat.rados_bench_host.3 2

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:13:53,015464148-07:00] INFO: >> for OSD host[0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:13:53,024320176-07:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.2 bash
10.10.2.2: b'145631\n'
[1] 02:13:53 [SUCCESS] ljishen@10.10.2.2
145631

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:13:53,214821477-07:00] INFO: > Run rados bench[0m
# ./benchmarks/bench-rados:379 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/output/rados_bench_16KB_seq.log.3
## ./benchmarks/bench-rados:376 - rados_bench() > count_cpus 32-63,96-127
# ./benchmarks/bench-rados:376 - rados_bench() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 128 --show-time
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:13:53,234534996-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:13:53,237463234-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '55639040-37ce-11ec-b51d-53e6e728d2d3', '--', 'rados', 'bench', '99999999', 'seq', '--pool', 'bench_rados', '--concurrent-ios', '128', '--show-time'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 55639040-37ce-11ec-b51d-53e6e728d2d3 -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 128 --show-time
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
hints = 1
2021-10-28T09:13:56.505285+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-28T09:13:56.505285+0000     0       0         0         0         0         0           -           0
2021-10-28T09:13:57.505378+0000     1     127     31416     31289   488.813   488.891  0.00348809   0.0040735
2021-10-28T09:13:58.505467+0000     2     128     62938     62810   490.642   492.516  0.00431248  0.00406321
2021-10-28T09:13:59.505557+0000     3     128     93815     93687   487.898   482.453   0.0045368  0.00408812
2021-10-28T09:14:00.505634+0000     4     128    124131    124003   484.336   473.688  0.00389252  0.00411896
2021-10-28T09:14:01.505706+0000     5     128    154622    154494   482.747   476.422  0.00411262  0.00413257
2021-10-28T09:14:02.505782+0000     6     128    185229    185101   481.989   478.234   0.0042732  0.00414007
2021-10-28T09:14:03.505852+0000     7     128    215821    215693   481.414       478  0.00406729   0.0041449
2021-10-28T09:14:04.505942+0000     8     127    246083    245956   480.339   472.859  0.00425247  0.00415434
2021-10-28T09:14:05.506017+0000     9     128    276405    276277   479.605   473.766  0.00411215  0.00416131
2021-10-28T09:14:06.506103+0000    10     128    306627    306499   478.862   472.219  0.00426905  0.00416756
2021-10-28T09:14:07.506188+0000    11     127    336829    336702   478.228   471.922  0.00454142  0.00417336
2021-10-28T09:14:08.506296+0000 Total time run:       11.1063
Total reads made:     339880
Read size:            16384
Object size:          16384
Bandwidth (MB/sec):   478.165
Average IOPS:         30602
Stddev IOPS:          445.342
Max IOPS:             31521
Min IOPS:             30203
Average Latency(s):   0.00417398
Max latency(s):       0.0106648
Min latency(s):       0.000824754

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:14:09,167402057-07:00] INFO: > Stop system activity collection services[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:14:09,172873989-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:391 - rados_bench() > kill -INT 248292

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:14:09,178413948-07:00] INFO: >> for OSD host[0m
# ./benchmarks/bench-rados:396 - rados_bench() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.2 kill -INT 145631
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:14:09,186146970-07:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.2 kill -INT 145631
[1] 02:14:09 [SUCCESS] ljishen@10.10.2.2

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:14:09,369551709-07:00] INFO: >>> Copy system activity file from OSD host[0m
## ./benchmarks/bench-rados:403 - rados_bench() > basename -- /tmp/bench-rados/sys_activity_16KB_seq.dat.osd_host.3
# ./benchmarks/bench-rados:403 - rados_bench() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.2:/tmp/bench-rados/sys_activity_16KB_seq.dat.osd_host.3 /mnt/sda8/ceph-research/scripts/output/sys_activity_16KB_seq.dat.osd_host.3


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:14:09,524962807-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:14:33,711988300-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 339.88k objects, 5.2 GiB
    usage:   16 GiB used, 84 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:14:33,719300630-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:14:43,002777167-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 339.88k objects, 5.2 GiB
    usage:   16 GiB used, 84 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:14:43,010087021-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:14:52,199459788-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 339.88k objects, 5.2 GiB
    usage:   16 GiB used, 84 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:14:52,206798998-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:15:01,430355636-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 339.88k objects, 5.2 GiB
    usage:   16 GiB used, 84 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:15:01,437500680-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:15:10,702848664-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 339.88k objects, 5.2 GiB
    usage:   16 GiB used, 84 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:15:10,710512015-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:15:10,715934774-07:00] INFO: > The cluster is idle now.[0m


[1;7;39;49m[2021-10-28T02:15:10,719524779-07:00][RUNNING][ROUND 1/3/21] object_size=64KB[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:15:10,722813927-07:00] STAGE: Launch a Ceph cluster (CEPH_HOST: 10.10.2.1, OSD_HOST: 10.10.2.2) ...[0m
# ./benchmarks/bench-rados:183 - launch_ceph_cluster() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.1 bash -euo pipefail
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:15:10,732118951-07:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.1 bash -euo pipefail
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T02:15:11,195946396-07:00] INFO: > Remove existing clusters\x1b[0m\n'
10.10.2.1: b'## bash:17 -  > basename -- /var/lib/ceph/55639040-37ce-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'# bash:17 -  > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm --host 10.10.2.1 --host 10.10.2.2 rm-cluster --force --fsid 55639040-37ce-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T02:15:11,206745106-07:00] DEBUG: HOST_PARAMS: ['--host', '10.10.2.1', '--host', '10.10.2.2']\x1b[0m\n"
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T02:15:11,209984225-07:00] DEBUG: CEPHADM_PARAMS: ['rm-cluster', '--force', '--fsid', '55639040-37ce-11ec-b51d-53e6e728d2d3']\x1b[0m\n"
10.10.2.1: b"# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:63 -  > /tmp/bench-rados/ceph-research/scripts/parallel-ssh --host 10.10.2.1 --host 10.10.2.2 'sudo python3 -u - rm-cluster --force --fsid 55639040-37ce-11ec-b51d-53e6e728d2d3'\n"
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T02:15:11,218328431-07:00] DEBUG: command from stdin\x1b[0m\n'
10.10.2.1: b"# /tmp/bench-rados/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host 10.10.2.1 --host 10.10.2.2 'sudo python3 -u - rm-cluster --force --fsid 55639040-37ce-11ec-b51d-53e6e728d2d3'\n"
10.10.2.1: b'[1] 02:15:16 [SUCCESS] 10.10.2.1\n[2] 02:15:21 [SUCCESS] 10.10.2.2\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T02:15:21,330812428-07:00] INFO: > Deploy a new cluster\x1b[0m\n'
10.10.2.1: b'# bash:24 -  > /tmp/bench-rados/ceph-research/scripts/ceph-deploy -m 10.10.2.1 -o node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/10.10.2.2:/dev/nvme0n1\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T02:15:21,342089708-07:00] STAGE: Verify passwordless SSH to hosts, passwordless sudo via ljishen, and the existence of devices on hosts...\x1b[0m\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T02:15:21,347109044-07:00] INFO: > Check the host for the monitor\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:181 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@10.10.2.1 sudo --non-interactive --validate\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T02:15:21,499354012-07:00] INFO: > Check hosts for OSDs\x1b[0m\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T02:15:21,503450152-07:00] INFO: >> Check host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:200 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us sudo --non-interactive --validate\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T02:15:21,654643253-07:00] INFO: >>> Check device '/dev/nvme0n1'\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:215 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us test -b /dev/nvme0n1\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:226 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us lsblk --nodeps --noheadings --output TYPE /dev/nvme0n1\n# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:227 - verify_parameters() > grep --word-regexp disk\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T02:15:21,943727649-07:00] STAGE: Reset devices on hosts...\x1b[0m\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T02:15:21,948782021-07:00] INFO: > Resetting device '/dev/nvme0n1' on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:288 - reset_host_devices() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us sudo sh\n'
10.10.2.1: b'  Removing ceph--c55298f5--1ef5--4c0d--bfa4--81947069c979-osd--block--54edde89--5e43--4f20--b530--9a5b11d7090f (253:0)\n'
10.10.2.1: b'  Archiving volume group "ceph-c55298f5-1ef5-4c0d-bfa4-81947069c979" metadata (seqno 5).\n'
10.10.2.1: b'  Releasing logical volume "osd-block-54edde89-5e43-4f20-b530-9a5b11d7090f"\n'
10.10.2.1: b'  Creating volume group backup "/etc/lvm/backup/ceph-c55298f5-1ef5-4c0d-bfa4-81947069c979" (seqno 6).\n'
10.10.2.1: b'  Logical volume "osd-block-54edde89-5e43-4f20-b530-9a5b11d7090f" successfully removed\n'
10.10.2.1: b'  Removing physical volume "/dev/nvme0n1" from volume group "ceph-c55298f5-1ef5-4c0d-bfa4-81947069c979"\n'
10.10.2.1: b'  Volume group "ceph-c55298f5-1ef5-4c0d-bfa4-81947069c979" successfully removed\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T02:15:22,317391243-07:00] STAGE: Bootstrap a new cluster...\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:317 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.1 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T02:15:22,327077571-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T02:15:22,330372185-07:00] DEBUG: CEPHADM_PARAMS: ['bootstrap', '--config', '/tmp/bench-rados/ceph-research/scripts/ceph.conf', '--mon-ip', '10.10.2.1', '--output-dir', '/tmp/bench-rados/deployment_data_root/etc/ceph', '--ssh-user', 'ljishen', '--skip-dashboard', '--no-minimize-config', '--allow-overwrite', '--allow-fqdn-hostname', '--orphan-initial-daemons', '--skip-monitoring-stack', '--single-host-defaults']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.1 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults\n'
10.10.2.1: b'Verifying podman|docker is present...\nVerifying lvm2 is present...\n'
10.10.2.1: b'Verifying time synchronization is in place...\n'
10.10.2.1: b'Unit ntp.service is enabled and running\n'
10.10.2.1: b'Repeating the final host check...\n'
10.10.2.1: b'podman|docker (/usr/bin/docker) is present\n'
10.10.2.1: b'systemctl is present\nlvcreate is present\n'
10.10.2.1: b'Unit ntp.service is enabled and running\nHost looks OK\n'
10.10.2.1: b'Cluster fsid: 942e792e-37cf-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Verifying IP 10.10.2.1 port 3300 ...\n'
10.10.2.1: b'Verifying IP 10.10.2.1 port 6789 ...\n'
10.10.2.1: b'Mon IP `10.10.2.1` is in CIDR network `10.10.2.0/24`\n'
10.10.2.1: b'- internal network (--cluster-network) has not been provided, OSD replication will default to the public_network\n'
10.10.2.1: b'Adjusting default settings to suit single-host cluster...\n'
10.10.2.1: b'Pulling container image quay.io/ceph/ceph:v16...\n'
10.10.2.1: b'Ceph version: ceph version 16.2.6 (ee28fb57e47e9f88813e24bbf4c14496ca299d31) pacific (stable)\n'
10.10.2.1: b'Extracting ceph user uid/gid from container image...\n'
10.10.2.1: b'Creating initial keys...\n'
10.10.2.1: b'Creating initial monmap...\n'
10.10.2.1: b'Creating mon...\n'
10.10.2.1: b'Waiting for mon to start...\n'
10.10.2.1: b'Waiting for mon...\n'
10.10.2.1: b'mon is available\nSetting mon public_network to 10.10.2.0/24\n'
10.10.2.1: b'Wrote config to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf\n'
10.10.2.1: b'Wrote keyring to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring\n'
10.10.2.1: b'Creating mgr...\n'
10.10.2.1: b'Verifying port 9283 ...\n'
10.10.2.1: b'Waiting for mgr to start...\n'
10.10.2.1: b'Waiting for mgr...\n'
10.10.2.1: b'mgr not available, waiting (1/15)...\n'
10.10.2.1: b'mgr not available, waiting (2/15)...\n'
10.10.2.1: b'mgr is available\n'
10.10.2.1: b'Enabling cephadm module...\n'
10.10.2.1: b'Waiting for the mgr to restart...\n'
10.10.2.1: b'Waiting for mgr epoch 5...\n'
10.10.2.1: b'mgr epoch 5 is available\nSetting orchestrator backend to cephadm...\n'
10.10.2.1: b'Generating ssh key...\n'
10.10.2.1: b'Wrote public SSH key to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub\n'
10.10.2.1: b'Adding key to ljishen@localhost authorized_keys...\n'
10.10.2.1: b'Adding host node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us...\n'
10.10.2.1: b'Deploying unmanaged mon service...\n'
10.10.2.1: b'Deploying unmanaged mgr service...\n'
10.10.2.1: b'You can access the Ceph CLI with:\n\n\tsudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --fsid 942e792e-37cf-11ec-b51d-53e6e728d2d3 -c /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf -k /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring\n\n'
10.10.2.1: b'Please consider enabling telemetry to help improve Ceph:\n\n\tceph telemetry on\n\nFor more information see:\n\n\thttps://docs.ceph.com/docs/pacific/mgr/telemetry/\n\nBootstrap complete.\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T02:16:22,386377083-07:00] INFO: Sleep for 20 seconds\x1b[0m\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T02:16:42,394076799-07:00] INFO: > Set the default number of monitor daemons to 1\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:337 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T02:16:42,404587989-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T02:16:42,408763699-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mon', '1']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1\n'
10.10.2.1: b'Inferring fsid 942e792e-37cf-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/942e792e-37cf-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b'Scheduled mon update...\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T02:16:51,309793175-07:00] INFO: > Set the default number of manager daemons to 1\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:342 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T02:16:51,319572870-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T02:16:51,323018787-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mgr', '1']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1\n'
10.10.2.1: b'Inferring fsid 942e792e-37cf-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/942e792e-37cf-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b'Scheduled mgr update...\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T02:17:00,041577416-07:00] STAGE: Add hosts to the cluster...\x1b[0m\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T02:17:00,047936359-07:00] INFO: > Adding host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:361 - add_hosts() > ssh-copy-id -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -f -i /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\n'
10.10.2.1: b'/usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: "/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub"\n'
10.10.2.1: b'\nNumber of key(s) added: 1\n\nNow try logging into the machine, with:   "ssh -o \'GlobalKnownHostsFile=/dev/null\' -o \'LogLevel=ERROR\' -o \'PasswordAuthentication=no\' -o \'StrictHostKeyChecking=no\' -o \'UserKnownHostsFile=/dev/null\' \'ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\'"\nand check to make sure that only the key(s) you wanted were added.\n\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:367 - add_hosts() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us 10.10.2.2\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T02:17:00,265702348-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T02:17:00,269464329-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'host', 'add', 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us', '10.10.2.2']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us 10.10.2.2\n'
10.10.2.1: b'Inferring fsid 942e792e-37cf-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/942e792e-37cf-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b"Added host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us' with addr '10.10.2.2'\n"
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T02:17:09,656381824-07:00] INFO: Sleep for 20 seconds\x1b[0m\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T02:17:29,660765214-07:00] STAGE: Add OSDs to the cluster...\x1b[0m\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T02:17:29,667719958-07:00] INFO: > Adding devices ['/dev/nvme0n1'] on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us' as OSDs\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:388 - add_host_devices_as_osds() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T02:17:29,677643784-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T02:17:29,681028266-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'daemon', 'add', 'osd', 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1\n'
10.10.2.1: b'Inferring fsid 942e792e-37cf-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/942e792e-37cf-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b"Created osd(s) 0 on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'\n"
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T02:17:54,173396640-07:00] INFO: > Sleep for 20 seconds\x1b[0m\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T02:18:14,178837916-07:00] STAGE: Show Cluster Status\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:408 - show_status() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T02:18:14,189480813-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T02:18:14,193186850-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', '--status']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status\n'
10.10.2.1: b'Inferring fsid 942e792e-37cf-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/942e792e-37cf-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b'  cluster:\n    id:     942e792e-37cf-11ec-b51d-53e6e728d2d3\n    health: HEALTH_WARN\n            1 pool(s) have no replicas configured\n \n  services:\n    mon: 1 daemons, quorum node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us (age 2m)\n    mgr: node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us.ecegtf(active, since 2m)\n    osd: 1 osds: 1 up (since 18s), 1 in (since 40s)\n \n  data:\n    pools:   1 pools, 1 pgs\n    objects: 0 objects, 0 B\n    usage:   4.7 MiB used, 100 GiB / 100 GiB avail\n    pgs:     1 active+clean\n \n'
[1] 02:18:22 [SUCCESS] ljishen@10.10.2.1

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:15:11,195946396-07:00] INFO: > Remove existing clusters[0m
## bash:17 -  > basename -- /var/lib/ceph/55639040-37ce-11ec-b51d-53e6e728d2d3
# bash:17 -  > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm --host 10.10.2.1 --host 10.10.2.2 rm-cluster --force --fsid 55639040-37ce-11ec-b51d-53e6e728d2d3
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:15:11,206745106-07:00] DEBUG: HOST_PARAMS: ['--host', '10.10.2.1', '--host', '10.10.2.2'][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:15:11,209984225-07:00] DEBUG: CEPHADM_PARAMS: ['rm-cluster', '--force', '--fsid', '55639040-37ce-11ec-b51d-53e6e728d2d3'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:63 -  > /tmp/bench-rados/ceph-research/scripts/parallel-ssh --host 10.10.2.1 --host 10.10.2.2 'sudo python3 -u - rm-cluster --force --fsid 55639040-37ce-11ec-b51d-53e6e728d2d3'
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:15:11,218328431-07:00] DEBUG: command from stdin[0m
# /tmp/bench-rados/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host 10.10.2.1 --host 10.10.2.2 'sudo python3 -u - rm-cluster --force --fsid 55639040-37ce-11ec-b51d-53e6e728d2d3'
[1] 02:15:16 [SUCCESS] 10.10.2.1
[2] 02:15:21 [SUCCESS] 10.10.2.2

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:15:21,330812428-07:00] INFO: > Deploy a new cluster[0m
# bash:24 -  > /tmp/bench-rados/ceph-research/scripts/ceph-deploy -m 10.10.2.1 -o node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/10.10.2.2:/dev/nvme0n1


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:15:21,342089708-07:00] STAGE: Verify passwordless SSH to hosts, passwordless sudo via ljishen, and the existence of devices on hosts...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:15:21,347109044-07:00] INFO: > Check the host for the monitor[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:181 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@10.10.2.1 sudo --non-interactive --validate

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:15:21,499354012-07:00] INFO: > Check hosts for OSDs[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:15:21,503450152-07:00] INFO: >> Check host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:200 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us sudo --non-interactive --validate

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:15:21,654643253-07:00] INFO: >>> Check device '/dev/nvme0n1'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:215 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us test -b /dev/nvme0n1
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:226 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us lsblk --nodeps --noheadings --output TYPE /dev/nvme0n1
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:227 - verify_parameters() > grep --word-regexp disk


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:15:21,943727649-07:00] STAGE: Reset devices on hosts...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:15:21,948782021-07:00] INFO: > Resetting device '/dev/nvme0n1' on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:288 - reset_host_devices() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us sudo sh
  Removing ceph--c55298f5--1ef5--4c0d--bfa4--81947069c979-osd--block--54edde89--5e43--4f20--b530--9a5b11d7090f (253:0)
  Archiving volume group "ceph-c55298f5-1ef5-4c0d-bfa4-81947069c979" metadata (seqno 5).
  Releasing logical volume "osd-block-54edde89-5e43-4f20-b530-9a5b11d7090f"
  Creating volume group backup "/etc/lvm/backup/ceph-c55298f5-1ef5-4c0d-bfa4-81947069c979" (seqno 6).
  Logical volume "osd-block-54edde89-5e43-4f20-b530-9a5b11d7090f" successfully removed
  Removing physical volume "/dev/nvme0n1" from volume group "ceph-c55298f5-1ef5-4c0d-bfa4-81947069c979"
  Volume group "ceph-c55298f5-1ef5-4c0d-bfa4-81947069c979" successfully removed


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:15:22,317391243-07:00] STAGE: Bootstrap a new cluster...[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:317 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.1 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:15:22,327077571-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:15:22,330372185-07:00] DEBUG: CEPHADM_PARAMS: ['bootstrap', '--config', '/tmp/bench-rados/ceph-research/scripts/ceph.conf', '--mon-ip', '10.10.2.1', '--output-dir', '/tmp/bench-rados/deployment_data_root/etc/ceph', '--ssh-user', 'ljishen', '--skip-dashboard', '--no-minimize-config', '--allow-overwrite', '--allow-fqdn-hostname', '--orphan-initial-daemons', '--skip-monitoring-stack', '--single-host-defaults'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.1 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults
Verifying podman|docker is present...
Verifying lvm2 is present...
Verifying time synchronization is in place...
Unit ntp.service is enabled and running
Repeating the final host check...
podman|docker (/usr/bin/docker) is present
systemctl is present
lvcreate is present
Unit ntp.service is enabled and running
Host looks OK
Cluster fsid: 942e792e-37cf-11ec-b51d-53e6e728d2d3
Verifying IP 10.10.2.1 port 3300 ...
Verifying IP 10.10.2.1 port 6789 ...
Mon IP `10.10.2.1` is in CIDR network `10.10.2.0/24`
- internal network (--cluster-network) has not been provided, OSD replication will default to the public_network
Adjusting default settings to suit single-host cluster...
Pulling container image quay.io/ceph/ceph:v16...
Ceph version: ceph version 16.2.6 (ee28fb57e47e9f88813e24bbf4c14496ca299d31) pacific (stable)
Extracting ceph user uid/gid from container image...
Creating initial keys...
Creating initial monmap...
Creating mon...
Waiting for mon to start...
Waiting for mon...
mon is available
Setting mon public_network to 10.10.2.0/24
Wrote config to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf
Wrote keyring to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring
Creating mgr...
Verifying port 9283 ...
Waiting for mgr to start...
Waiting for mgr...
mgr not available, waiting (1/15)...
mgr not available, waiting (2/15)...
mgr is available
Enabling cephadm module...
Waiting for the mgr to restart...
Waiting for mgr epoch 5...
mgr epoch 5 is available
Setting orchestrator backend to cephadm...
Generating ssh key...
Wrote public SSH key to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub
Adding key to ljishen@localhost authorized_keys...
Adding host node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us...
Deploying unmanaged mon service...
Deploying unmanaged mgr service...
You can access the Ceph CLI with:

	sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --fsid 942e792e-37cf-11ec-b51d-53e6e728d2d3 -c /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf -k /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring

Please consider enabling telemetry to help improve Ceph:

	ceph telemetry on

For more information see:

	https://docs.ceph.com/docs/pacific/mgr/telemetry/

Bootstrap complete.

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:16:22,386377083-07:00] INFO: Sleep for 20 seconds[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:16:42,394076799-07:00] INFO: > Set the default number of monitor daemons to 1[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:337 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:16:42,404587989-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:16:42,408763699-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mon', '1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1
Inferring fsid 942e792e-37cf-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/942e792e-37cf-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Scheduled mon update...

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:16:51,309793175-07:00] INFO: > Set the default number of manager daemons to 1[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:342 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:16:51,319572870-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:16:51,323018787-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mgr', '1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1
Inferring fsid 942e792e-37cf-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/942e792e-37cf-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Scheduled mgr update...


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:17:00,041577416-07:00] STAGE: Add hosts to the cluster...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:17:00,047936359-07:00] INFO: > Adding host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:361 - add_hosts() > ssh-copy-id -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -f -i /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us
/usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: "/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub"

Number of key(s) added: 1

Now try logging into the machine, with:   "ssh -o 'GlobalKnownHostsFile=/dev/null' -o 'LogLevel=ERROR' -o 'PasswordAuthentication=no' -o 'StrictHostKeyChecking=no' -o 'UserKnownHostsFile=/dev/null' 'ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'"
and check to make sure that only the key(s) you wanted were added.

# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:367 - add_hosts() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us 10.10.2.2
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:17:00,265702348-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:17:00,269464329-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'host', 'add', 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us', '10.10.2.2'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us 10.10.2.2
Inferring fsid 942e792e-37cf-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/942e792e-37cf-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Added host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us' with addr '10.10.2.2'

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:17:09,656381824-07:00] INFO: Sleep for 20 seconds[0m


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:17:29,660765214-07:00] STAGE: Add OSDs to the cluster...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:17:29,667719958-07:00] INFO: > Adding devices ['/dev/nvme0n1'] on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us' as OSDs[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:388 - add_host_devices_as_osds() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:17:29,677643784-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:17:29,681028266-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'daemon', 'add', 'osd', 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1
Inferring fsid 942e792e-37cf-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/942e792e-37cf-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Created osd(s) 0 on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:17:54,173396640-07:00] INFO: > Sleep for 20 seconds[0m


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:18:14,178837916-07:00] STAGE: Show Cluster Status[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:408 - show_status() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:18:14,189480813-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:18:14,193186850-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', '--status'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status
Inferring fsid 942e792e-37cf-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/942e792e-37cf-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
  cluster:
    id:     942e792e-37cf-11ec-b51d-53e6e728d2d3
    health: HEALTH_WARN
            1 pool(s) have no replicas configured
 
  services:
    mon: 1 daemons, quorum node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us (age 2m)
    mgr: node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us.ecegtf(active, since 2m)
    osd: 1 osds: 1 up (since 18s), 1 in (since 40s)
 
  data:
    pools:   1 pools, 1 pgs
    objects: 0 objects, 0 B
    usage:   4.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     1 active+clean
 

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:18:22,920381503-07:00] INFO: > Copy configuration files to local[0m
# ./benchmarks/bench-rados:220 - launch_ceph_cluster() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.1 sudo chown --changes --recursive ljishen /tmp/bench-rados/deployment_data_root/etc/ceph
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:18:22,927980221-07:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.1 sudo chown --changes --recursive ljishen /tmp/bench-rados/deployment_data_root/etc/ceph
[1] 02:18:23 [SUCCESS] ljishen@10.10.2.1
# ./benchmarks/bench-rados:223 - launch_ceph_cluster() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.1:/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf
# ./benchmarks/bench-rados:226 - launch_ceph_cluster() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.1:/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:18:23,405033750-07:00] STAGE: Update local ceph.conf as a client...[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:18:23,408524237-07:00] STAGE: Configure Ceph pool...[0m
# ./benchmarks/bench-rados:257 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_max_backfills 32
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:18:23,430369633-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:18:23,433164420-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '942e792e-37cf-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'config', 'set', 'osd', 'osd_max_backfills', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 942e792e-37cf-11ec-b51d-53e6e728d2d3 -- ceph config set osd osd_max_backfills 32
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:258 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_active 32
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:18:27,625995691-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:18:27,628949006-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '942e792e-37cf-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_active', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 942e792e-37cf-11ec-b51d-53e6e728d2d3 -- ceph config set osd osd_recovery_max_active 32
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:259 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_single_start 8
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:18:31,909986857-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:18:31,912811219-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '942e792e-37cf-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_single_start', '8'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 942e792e-37cf-11ec-b51d-53e6e728d2d3 -- ceph config set osd osd_recovery_max_single_start 8
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:260 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_op_priority 63
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:18:36,267845261-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:18:36,270759242-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '942e792e-37cf-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_op_priority', '63'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 942e792e-37cf-11ec-b51d-53e6e728d2d3 -- ceph config set osd osd_recovery_op_priority 63
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:262 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config show-with-defaults osd.0
# ./benchmarks/bench-rados:263 - configure_ceph_pool() > grep 'osd_max_backfills\|osd_recovery'
# ./benchmarks/bench-rados:263 - configure_ceph_pool() > column -t -s ' '
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
osd_max_backfills                        32         mon
osd_recovery_cost                        20971520   default
osd_recovery_delay_start                 0.000000   default
osd_recovery_max_active                  32         mon
osd_recovery_max_active_hdd              3          default
osd_recovery_max_active_ssd              10         default
osd_recovery_max_chunk                   8388608    default
osd_recovery_max_omap_entries_per_chunk  8096       default
osd_recovery_max_single_start            8          mon
osd_recovery_op_priority                 63         mon
osd_recovery_op_warn_multiple            16         default
osd_recovery_priority                    5          default
osd_recovery_retry_interval              30.000000  default
osd_recovery_sleep                       0.000000   default
osd_recovery_sleep_hdd                   0.100000   default
osd_recovery_sleep_hybrid                0.025000   default
osd_recovery_sleep_ssd                   0.000000   default
# ./benchmarks/bench-rados:265 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool create bench_rados 128 128 replicated --size 1 --pg-num-min 128
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:18:44,827156551-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:18:44,830125957-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '942e792e-37cf-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'create', 'bench_rados', '128', '128', 'replicated', '--size', '1', '--pg-num-min', '128'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 942e792e-37cf-11ec-b51d-53e6e728d2d3 -- ceph osd pool create bench_rados 128 128 replicated --size 1 --pg-num-min 128
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
pool 'bench_rados' created
# ./benchmarks/bench-rados:267 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados min_size 1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:18:49,967674177-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:18:49,970695009-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '942e792e-37cf-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'min_size', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 942e792e-37cf-11ec-b51d-53e6e728d2d3 -- ceph osd pool set bench_rados min_size 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 min_size to 1
# ./benchmarks/bench-rados:268 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados pg_autoscale_mode off
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:18:55,421283383-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:18:55,424271154-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '942e792e-37cf-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'pg_autoscale_mode', 'off'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 942e792e-37cf-11ec-b51d-53e6e728d2d3 -- ceph osd pool set bench_rados pg_autoscale_mode off
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 pg_autoscale_mode to off
# ./benchmarks/bench-rados:269 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool application enable bench_rados benchmark
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:19:00,718627929-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:19:00,721700749-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '942e792e-37cf-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'application', 'enable', 'bench_rados', 'benchmark'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 942e792e-37cf-11ec-b51d-53e6e728d2d3 -- ceph osd pool application enable bench_rados benchmark
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
enabled application 'benchmark' on pool 'bench_rados'
# ./benchmarks/bench-rados:270 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados noscrub 1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:19:06,261525232-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:19:06,264379651-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '942e792e-37cf-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'noscrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 942e792e-37cf-11ec-b51d-53e6e728d2d3 -- ceph osd pool set bench_rados noscrub 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 noscrub to 1
# ./benchmarks/bench-rados:271 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados nodeep-scrub 1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:19:11,618574646-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:19:11,621531188-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '942e792e-37cf-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'nodeep-scrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 942e792e-37cf-11ec-b51d-53e6e728d2d3 -- ceph osd pool set bench_rados nodeep-scrub 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 nodeep-scrub to 1
# ./benchmarks/bench-rados:272 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool ls detail
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:19:17,009964513-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:19:17,012880137-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '942e792e-37cf-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'ls', 'detail'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 942e792e-37cf-11ec-b51d-53e6e728d2d3 -- ceph osd pool ls detail
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
pool 1 'device_health_metrics' replicated size 1 min_size 1 crush_rule 0 object_hash rjenkins pg_num 64 pgp_num 64 autoscale_mode on last_change 19 lfor 0/0/16 flags hashpspool stripe_width 0 pg_num_min 1 application mgr_devicehealth
pool 2 'bench_rados' replicated size 1 min_size 1 crush_rule 0 object_hash rjenkins pg_num 128 pgp_num 128 autoscale_mode off last_change 21 flags hashpspool,noscrub,nodeep-scrub stripe_width 0 pg_num_min 128 application benchmark

# ./benchmarks/bench-rados:273 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd df tree
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:19:21,230180906-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:19:21,233321814-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '942e792e-37cf-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'df', 'tree'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 942e792e-37cf-11ec-b51d-53e6e728d2d3 -- ceph osd df tree
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
ID  CLASS  WEIGHT   REWEIGHT  SIZE     RAW USE  DATA     OMAP  META     AVAIL    %USE  VAR   PGS  STATUS  TYPE NAME      
-1         0.09769         -  100 GiB  5.7 MiB  176 KiB   0 B  5.6 MiB  100 GiB  0.01  1.00    -          root default   
-3         0.09769         -  100 GiB  5.7 MiB  176 KiB   0 B  5.6 MiB  100 GiB  0.01  1.00    -              host node-1
 0    ssd  0.09769   1.00000  100 GiB  5.7 MiB  176 KiB   0 B  5.6 MiB  100 GiB  0.01  1.00  192      up          osd.0  
                       TOTAL  100 GiB  5.7 MiB  176 KiB   0 B  5.6 MiB  100 GiB  0.01                                    
MIN/MAX VAR: 1.00/1.00  STDDEV: 0


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:19:25,538306056-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:19:49,677494424-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:19:58,960311765-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:20:08,287785246-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:20:08,295322970-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:20:17,694001842-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:20:17,701633133-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:20:26,928805851-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:20:26,936142005-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:20:36,168069302-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:20:36,175501988-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:20:45,448871679-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:20:45,456408612-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:20:45,461882887-07:00] INFO: > The cluster is idle now.[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:20:45,465468133-07:00] STAGE: Start benchmarking of mode write ...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:20:45,471984944-07:00] INFO: > Start system activity collection services[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:20:45,477323164-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:341 - rados_bench() > rm --force /mnt/sda8/ceph-research/scripts/output/sys_activity_64KB_write.dat.rados_bench_host.1
# ./benchmarks/bench-rados:342 - rados_bench() > sar_pid_rados_bench_host=253893
# ./benchmarks/bench-rados:342 - rados_bench() > S_TIME_FORMAT=ISO
# ./benchmarks/bench-rados:342 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/output/sys_activity_64KB_write.dat.rados_bench_host.1 2

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:20:45,483836418-07:00] INFO: >> for OSD host[0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:20:45,493037286-07:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.2 bash
10.10.2.2: b'149250\n'
[1] 02:20:45 [SUCCESS] ljishen@10.10.2.2
149250

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:20:45,682889230-07:00] INFO: > Run rados bench[0m
# ./benchmarks/bench-rados:370 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/output/rados_bench_64KB_write.log.1
## ./benchmarks/bench-rados:361 - rados_bench() > count_cpus 32-63,96-127
# ./benchmarks/bench-rados:361 - rados_bench() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 60 write --pool bench_rados -b 65536 -O 65536 --concurrent-ios 128 --show-time --write-object --write-omap --write-xattr --no-cleanup
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:20:45,703197289-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:20:45,706090531-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '942e792e-37cf-11ec-b51d-53e6e728d2d3', '--', 'rados', 'bench', '60', 'write', '--pool', 'bench_rados', '-b', '65536', '-O', '65536', '--concurrent-ios', '128', '--show-time', '--write-object', '--write-omap', '--write-xattr', '--no-cleanup'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 942e792e-37cf-11ec-b51d-53e6e728d2d3 -- rados bench 60 write --pool bench_rados -b 65536 -O 65536 --concurrent-ios 128 --show-time --write-object --write-omap --write-xattr --no-cleanup
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
hints = 1
2021-10-28T09:20:49.000711+0000 Maintaining 128 concurrent writes of 65536 bytes to objects of size 65536 for up to 60 seconds or 0 objects
2021-10-28T09:20:49.000724+0000 Object prefix: benchmark_data_node-2.bluefield2.ucsc-cmps10_7
2021-10-28T09:20:49.003470+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-28T09:20:49.003470+0000     0       0         0         0         0         0           -           0
2021-10-28T09:20:50.003596+0000     1     128      4513      4385   274.044   274.062   0.0307286   0.0286782
2021-10-28T09:20:51.003702+0000     2     128      8481      8353   261.009       248   0.0774857   0.0300143
2021-10-28T09:20:52.003796+0000     3     128     10197     10069   209.752    107.25   0.0732281   0.0373951
2021-10-28T09:20:53.003898+0000     4     128     13473     13345   208.496    204.75   0.0253052   0.0381034
2021-10-28T09:20:54.003988+0000     5     128     16853     16725   209.043    211.25   0.0987197   0.0377576
2021-10-28T09:20:55.004088+0000     6     128     17809     17681    184.16     59.75    0.129501   0.0428372
2021-10-28T09:20:56.004189+0000     7     128     18773     18645   166.458     60.25    0.130385    0.047418
2021-10-28T09:20:57.004287+0000     8     128     20753     20625   161.118    123.75   0.0581323   0.0494285
2021-10-28T09:20:58.004384+0000     9     128     22673     22545   156.548       120   0.0691666   0.0508015
2021-10-28T09:20:59.004480+0000    10     128     25045     24917   155.716    148.25   0.0724486   0.0512479
2021-10-28T09:21:00.004577+0000    11     128     26769     26641   151.355    107.75   0.0714441   0.0526741
2021-10-28T09:21:01.004669+0000    12     128     30225     30097    156.74       216   0.0400636   0.0509504
2021-10-28T09:21:02.004766+0000    13     128     33109     32981   158.547    180.25   0.0707672    0.050347
2021-10-28T09:21:03.004864+0000    14     128     34065     33937    151.49     59.75    0.134424   0.0526714
2021-10-28T09:21:04.004958+0000    15     128     34961     34833   145.124        56    0.138501   0.0547934
2021-10-28T09:21:05.005061+0000    16     128     36693     36565   142.818    108.25   0.0611742   0.0559244
2021-10-28T09:21:06.005159+0000    17     128     38545     38417   141.225    115.75   0.0692969   0.0565291
2021-10-28T09:21:07.005256+0000    18     128     40465     40337   140.046       120    0.071747   0.0570559
2021-10-28T09:21:08.005352+0000    19     128     41429     41301   135.846     60.25    0.127639   0.0586734
2021-10-28T09:21:09.005452+0000 min lat: 0.0219067 max lat: 0.143077 avg lat: 0.0603775
2021-10-28T09:21:09.005452+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-28T09:21:09.005452+0000    20     128     42453     42325   132.253        64    0.130864   0.0603775
2021-10-28T09:21:10.005560+0000    21     127     44147     44020   130.999   105.938     0.02515   0.0610308
2021-10-28T09:21:11.005651+0000    22     128     48529     48401    137.49   273.812    0.028903   0.0581188
2021-10-28T09:21:12.005746+0000    23     128     49809     49681    134.99        80     0.13057   0.0590175
2021-10-28T09:21:13.005850+0000    24     128     50833     50705   132.032        64    0.130146   0.0604208
2021-10-28T09:21:14.005950+0000    25     127     52328     52201    130.49      93.5   0.0576629   0.0612254
2021-10-28T09:21:15.006049+0000    26     128     54355     54227   130.341   126.625   0.0656571   0.0613308
2021-10-28T09:21:16.006141+0000    27     128     56275     56147   129.957       120   0.0682906   0.0615053
2021-10-28T09:21:17.006234+0000    28     128     57427     57299   127.887        72    0.128042   0.0624027
2021-10-28T09:21:18.006326+0000    29     128     58451     58323   125.684        64    0.124964   0.0635429
2021-10-28T09:21:19.006420+0000    30     128     59537     59409   123.757    67.875   0.0654016   0.0645543
2021-10-28T09:21:20.006518+0000    31     128     61651     61523   124.026   132.125   0.0692132   0.0644448
2021-10-28T09:21:21.006618+0000    32     128     63571     63443     123.9       120   0.0655436   0.0645078
2021-10-28T09:21:22.006713+0000    33     128     65107     64979   123.054        96    0.127674   0.0648995
2021-10-28T09:21:23.006806+0000    34     128     66065     65937   121.196    59.875    0.125195    0.065833
2021-10-28T09:21:24.006901+0000    35     128     67089     66961   119.562        64    0.126248   0.0668297
2021-10-28T09:21:25.006997+0000    36     128     68819     68691   119.244   108.125    0.059825   0.0670363
2021-10-28T09:21:26.007096+0000    37     128     70929     70801   119.585   131.875   0.0655643   0.0668191
2021-10-28T09:21:27.007191+0000    38     128     72787     72659   119.493   116.125   0.0693676   0.0668358
2021-10-28T09:21:28.007284+0000    39     128     73811     73683    118.07        64    0.123215   0.0676829
2021-10-28T09:21:29.007381+0000 min lat: 0.0219067 max lat: 0.144576 avg lat: 0.068498
2021-10-28T09:21:29.007381+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-28T09:21:29.007381+0000    40     128     74769     74641   116.615    59.875    0.130444    0.068498
2021-10-28T09:21:30.007466+0000    41     128     76049     75921   115.722        80   0.0556291    0.069073
2021-10-28T09:21:31.007547+0000    42     128     78097     77969   116.014       128   0.0662125    0.068898
2021-10-28T09:21:32.007626+0000    43     128     80017     79889   116.107       120   0.0687725   0.0688378
2021-10-28T09:21:33.007711+0000    44     128     81425     81297   115.468        88    0.126514   0.0691988
2021-10-28T09:21:34.007790+0000    45     128     82387     82259   114.238    60.125    0.128135   0.0699015
2021-10-28T09:21:35.007877+0000    46     128     83411     83283   113.146        64    0.131838   0.0706047
2021-10-28T09:21:36.007962+0000    47     128     85265     85137   113.203   115.875   0.0753448   0.0706241
2021-10-28T09:21:37.008048+0000    48     128     87123     86995   113.264   116.125    0.069181   0.0705799
2021-10-28T09:21:38.008127+0000    49     128     88977     88849   113.317   115.875   0.0904362   0.0705323
2021-10-28T09:21:39.008205+0000    50     128     89939     89811   112.253    60.125    0.127653   0.0711747
2021-10-28T09:21:40.008287+0000    51     128     90963     90835   111.307        64    0.129719   0.0718225
2021-10-28T09:21:41.008401+0000    52     128     92243     92115   110.705        80   0.0575962   0.0722053
2021-10-28T09:21:42.008503+0000    53     128     94291     94163   111.031       128   0.0671443    0.071989
2021-10-28T09:21:43.008605+0000    54     128     96273     96145   111.268   123.875   0.0657925   0.0718538
2021-10-28T09:21:44.008697+0000    55     128     97619     97491   110.775    84.125    0.131101   0.0721656
2021-10-28T09:21:45.008794+0000    56     128     98577     98449   109.866    59.875     0.13079   0.0727052
2021-10-28T09:21:46.008897+0000    57     128     99601     99473   109.061        64    0.131436   0.0732699
2021-10-28T09:21:47.008978+0000    58     128    101649    101521   109.387       128   0.0617333   0.0731072
2021-10-28T09:21:48.009075+0000    59     128    103569    103441   109.567       120   0.0647914   0.0729782
2021-10-28T09:21:49.009155+0000 min lat: 0.0219067 max lat: 0.144576 avg lat: 0.0729569
2021-10-28T09:21:49.009155+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-28T09:21:49.009155+0000    60     121    105233    105112   109.481   104.438    0.131993   0.0729569
2021-10-28T09:21:50.009290+0000 Total time run:         60.0689
Total writes made:      105233
Write size:             65536
Object size:            65536
Bandwidth (MB/sec):     109.492
Stddev Bandwidth:       52.5585
Max bandwidth (MB/sec): 274.062
Min bandwidth (MB/sec): 56
Average IOPS:           1751
Stddev IOPS:            840.937
Max IOPS:               4385
Min IOPS:               896
Average Latency(s):     0.0730228
Stddev Latency(s):      0.0357531
Max latency(s):         0.144576
Min latency(s):         0.0219067

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:21:50,788343523-07:00] INFO: > Stop system activity collection services[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:21:50,794165926-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:391 - rados_bench() > kill -INT 253893

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:21:50,799769956-07:00] INFO: >> for OSD host[0m
# ./benchmarks/bench-rados:396 - rados_bench() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.2 kill -INT 149250
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:21:50,807371761-07:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.2 kill -INT 149250
[1] 02:21:50 [SUCCESS] ljishen@10.10.2.2

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:21:50,993690697-07:00] INFO: >>> Copy system activity file from OSD host[0m
## ./benchmarks/bench-rados:403 - rados_bench() > basename -- /tmp/bench-rados/sys_activity_64KB_write.dat.osd_host.1
# ./benchmarks/bench-rados:403 - rados_bench() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.2:/tmp/bench-rados/sys_activity_64KB_write.dat.osd_host.1 /mnt/sda8/ceph-research/scripts/output/sys_activity_64KB_write.dat.osd_host.1


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:21:51,167324689-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:22:15,409240148-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 105.23k objects, 6.4 GiB
    usage:   20 GiB used, 80 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:22:15,416351750-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:22:24,743893682-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 105.23k objects, 6.4 GiB
    usage:   20 GiB used, 80 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:22:24,750927587-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:22:34,033461030-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 105.23k objects, 6.4 GiB
    usage:   20 GiB used, 80 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:22:34,040125268-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:22:43,307020872-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 105.23k objects, 6.4 GiB
    usage:   20 GiB used, 80 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:22:43,314121793-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:22:52,631904088-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 105.23k objects, 6.4 GiB
    usage:   20 GiB used, 80 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:22:52,639418037-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:22:52,644618387-07:00] INFO: > The cluster is idle now.[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:22:52,648258056-07:00] STAGE: Start benchmarking of mode seq ...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:22:52,654557497-07:00] INFO: > Start system activity collection services[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:22:52,660236358-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:341 - rados_bench() > rm --force /mnt/sda8/ceph-research/scripts/output/sys_activity_64KB_seq.dat.rados_bench_host.1
# ./benchmarks/bench-rados:342 - rados_bench() > sar_pid_rados_bench_host=255251
# ./benchmarks/bench-rados:342 - rados_bench() > S_TIME_FORMAT=ISO
# ./benchmarks/bench-rados:342 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/output/sys_activity_64KB_seq.dat.rados_bench_host.1 2

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:22:52,666657118-07:00] INFO: >> for OSD host[0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:22:52,675418478-07:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.2 bash
10.10.2.2: b'149346\n'
[1] 02:22:52 [SUCCESS] ljishen@10.10.2.2
149346

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:22:52,862145803-07:00] INFO: > Run rados bench[0m
# ./benchmarks/bench-rados:379 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/output/rados_bench_64KB_seq.log.1
## ./benchmarks/bench-rados:376 - rados_bench() > count_cpus 32-63,96-127
# ./benchmarks/bench-rados:376 - rados_bench() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 128 --show-time
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:22:52,882352722-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:22:52,885203033-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '942e792e-37cf-11ec-b51d-53e6e728d2d3', '--', 'rados', 'bench', '99999999', 'seq', '--pool', 'bench_rados', '--concurrent-ios', '128', '--show-time'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 942e792e-37cf-11ec-b51d-53e6e728d2d3 -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 128 --show-time
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
hints = 1
2021-10-28T09:22:56.216072+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-28T09:22:56.216072+0000     0       0         0         0         0         0           -           0
2021-10-28T09:22:57.216187+0000     1     127     19301     19174   1198.16   1198.38  0.00758338  0.00664146
2021-10-28T09:22:58.216313+0000     2     128     38605     38477   1202.22   1206.44  0.00616839  0.00663177
2021-10-28T09:22:59.216400+0000     3     128     57790     57662   1201.13   1199.06   0.0065112   0.0066395
2021-10-28T09:23:00.216498+0000     4     127     77152     77025   1203.37   1210.19  0.00685781  0.00663091
2021-10-28T09:23:01.216576+0000     5     127     96554     96427    1205.2   1212.62  0.00520021  0.00662176
2021-10-28T09:23:02.216677+0000 Total time run:       5.45288
Total reads made:     105233
Read size:            65536
Object size:          65536
Bandwidth (MB/sec):   1206.16
Average IOPS:         19298
Stddev IOPS:          102.977
Max IOPS:             19402
Min IOPS:             19174
Average Latency(s):   0.00661756
Max latency(s):       0.0103442
Min latency(s):       0.00224524

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:23:02,932040422-07:00] INFO: > Stop system activity collection services[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:23:02,937697112-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:391 - rados_bench() > kill -INT 255251

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:23:02,943384509-07:00] INFO: >> for OSD host[0m
# ./benchmarks/bench-rados:396 - rados_bench() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.2 kill -INT 149346
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:23:02,951089870-07:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.2 kill -INT 149346
[1] 02:23:03 [SUCCESS] ljishen@10.10.2.2

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:23:03,137720442-07:00] INFO: >>> Copy system activity file from OSD host[0m
## ./benchmarks/bench-rados:403 - rados_bench() > basename -- /tmp/bench-rados/sys_activity_64KB_seq.dat.osd_host.1
# ./benchmarks/bench-rados:403 - rados_bench() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.2:/tmp/bench-rados/sys_activity_64KB_seq.dat.osd_host.1 /mnt/sda8/ceph-research/scripts/output/sys_activity_64KB_seq.dat.osd_host.1


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:23:03,290005924-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:23:27,483476189-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 105.23k objects, 6.4 GiB
    usage:   20 GiB used, 80 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:23:27,490820529-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:23:36,849386632-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 105.23k objects, 6.4 GiB
    usage:   20 GiB used, 80 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:23:36,857245452-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:23:46,216426632-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 105.23k objects, 6.4 GiB
    usage:   20 GiB used, 80 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:23:46,223956492-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:23:55,531358317-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 105.23k objects, 6.4 GiB
    usage:   20 GiB used, 80 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:23:55,538784160-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:24:04,946718791-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 105.23k objects, 6.4 GiB
    usage:   20 GiB used, 80 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:24:04,954111612-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:24:04,959911271-07:00] INFO: > The cluster is idle now.[0m


[1;7;39;49m[2021-10-28T02:24:04,962012281-07:00][RUNNING][ROUND 2/3/21] object_size=64KB[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:24:04,965336765-07:00] STAGE: Launch a Ceph cluster (CEPH_HOST: 10.10.2.1, OSD_HOST: 10.10.2.2) ...[0m
# ./benchmarks/bench-rados:183 - launch_ceph_cluster() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.1 bash -euo pipefail
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:24:04,974598639-07:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.1 bash -euo pipefail
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T02:24:05,398502416-07:00] INFO: > Remove existing clusters\x1b[0m\n'
10.10.2.1: b'## bash:17 -  > basename -- /var/lib/ceph/942e792e-37cf-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'# bash:17 -  > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm --host 10.10.2.1 --host 10.10.2.2 rm-cluster --force --fsid 942e792e-37cf-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T02:24:05,409136917-07:00] DEBUG: HOST_PARAMS: ['--host', '10.10.2.1', '--host', '10.10.2.2']\x1b[0m\n"
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T02:24:05,412561454-07:00] DEBUG: CEPHADM_PARAMS: ['rm-cluster', '--force', '--fsid', '942e792e-37cf-11ec-b51d-53e6e728d2d3']\x1b[0m\n"
10.10.2.1: b"# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:63 -  > /tmp/bench-rados/ceph-research/scripts/parallel-ssh --host 10.10.2.1 --host 10.10.2.2 'sudo python3 -u - rm-cluster --force --fsid 942e792e-37cf-11ec-b51d-53e6e728d2d3'\n"
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T02:24:05,421308738-07:00] DEBUG: command from stdin\x1b[0m\n'
10.10.2.1: b"# /tmp/bench-rados/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host 10.10.2.1 --host 10.10.2.2 'sudo python3 -u - rm-cluster --force --fsid 942e792e-37cf-11ec-b51d-53e6e728d2d3'\n"
10.10.2.1: b'[1] 02:24:11 [SUCCESS] 10.10.2.1\n[2] 02:24:16 [SUCCESS] 10.10.2.2\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T02:24:16,629732996-07:00] INFO: > Deploy a new cluster\x1b[0m\n'
10.10.2.1: b'# bash:24 -  > /tmp/bench-rados/ceph-research/scripts/ceph-deploy -m 10.10.2.1 -o node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/10.10.2.2:/dev/nvme0n1\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T02:24:16,641805922-07:00] STAGE: Verify passwordless SSH to hosts, passwordless sudo via ljishen, and the existence of devices on hosts...\x1b[0m\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T02:24:16,646590436-07:00] INFO: > Check the host for the monitor\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:181 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@10.10.2.1 sudo --non-interactive --validate\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T02:24:16,799089041-07:00] INFO: > Check hosts for OSDs\x1b[0m\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T02:24:16,804113446-07:00] INFO: >> Check host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:200 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us sudo --non-interactive --validate\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T02:24:16,955341362-07:00] INFO: >>> Check device '/dev/nvme0n1'\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:215 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us test -b /dev/nvme0n1\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:226 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us lsblk --nodeps --noheadings --output TYPE /dev/nvme0n1\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:227 - verify_parameters() > grep --word-regexp disk\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T02:24:17,239109214-07:00] STAGE: Reset devices on hosts...\x1b[0m\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T02:24:17,244241502-07:00] INFO: > Resetting device '/dev/nvme0n1' on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:288 - reset_host_devices() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us sudo sh\n'
10.10.2.1: b'  Removing ceph--947ead37--f44e--433e--a5fc--ca7648181ab2-osd--block--a004da5b--b9aa--4c53--8d84--15ac70482c1e (253:0)\n'
10.10.2.1: b'  Archiving volume group "ceph-947ead37-f44e-433e-a5fc-ca7648181ab2" metadata (seqno 5).\n'
10.10.2.1: b'  Releasing logical volume "osd-block-a004da5b-b9aa-4c53-8d84-15ac70482c1e"\n'
10.10.2.1: b'  Creating volume group backup "/etc/lvm/backup/ceph-947ead37-f44e-433e-a5fc-ca7648181ab2" (seqno 6).\n'
10.10.2.1: b'  Logical volume "osd-block-a004da5b-b9aa-4c53-8d84-15ac70482c1e" successfully removed\n'
10.10.2.1: b'  Removing physical volume "/dev/nvme0n1" from volume group "ceph-947ead37-f44e-433e-a5fc-ca7648181ab2"\n'
10.10.2.1: b'  Volume group "ceph-947ead37-f44e-433e-a5fc-ca7648181ab2" successfully removed\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T02:24:17,566015460-07:00] STAGE: Bootstrap a new cluster...\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:317 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.1 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T02:24:17,575698973-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T02:24:17,579355346-07:00] DEBUG: CEPHADM_PARAMS: ['bootstrap', '--config', '/tmp/bench-rados/ceph-research/scripts/ceph.conf', '--mon-ip', '10.10.2.1', '--output-dir', '/tmp/bench-rados/deployment_data_root/etc/ceph', '--ssh-user', 'ljishen', '--skip-dashboard', '--no-minimize-config', '--allow-overwrite', '--allow-fqdn-hostname', '--orphan-initial-daemons', '--skip-monitoring-stack', '--single-host-defaults']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.1 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults\n'
10.10.2.1: b'Verifying podman|docker is present...\nVerifying lvm2 is present...\n'
10.10.2.1: b'Verifying time synchronization is in place...\n'
10.10.2.1: b'Unit ntp.service is enabled and running\n'
10.10.2.1: b'Repeating the final host check...\n'
10.10.2.1: b'podman|docker (/usr/bin/docker) is present\n'
10.10.2.1: b'systemctl is present\n'
10.10.2.1: b'lvcreate is present\n'
10.10.2.1: b'Unit ntp.service is enabled and running\nHost looks OK\n'
10.10.2.1: b'Cluster fsid: d336c45e-37d0-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Verifying IP 10.10.2.1 port 3300 ...\n'
10.10.2.1: b'Verifying IP 10.10.2.1 port 6789 ...\n'
10.10.2.1: b'Mon IP `10.10.2.1` is in CIDR network `10.10.2.0/24`\n- internal network (--cluster-network) has not been provided, OSD replication will default to the public_network\n'
10.10.2.1: b'Adjusting default settings to suit single-host cluster...\n'
10.10.2.1: b'Pulling container image quay.io/ceph/ceph:v16...\n'
10.10.2.1: b'Ceph version: ceph version 16.2.6 (ee28fb57e47e9f88813e24bbf4c14496ca299d31) pacific (stable)\nExtracting ceph user uid/gid from container image...\n'
10.10.2.1: b'Creating initial keys...\n'
10.10.2.1: b'Creating initial monmap...\n'
10.10.2.1: b'Creating mon...\n'
10.10.2.1: b'Waiting for mon to start...\n'
10.10.2.1: b'Waiting for mon...\n'
10.10.2.1: b'mon is available\nSetting mon public_network to 10.10.2.0/24\n'
10.10.2.1: b'Wrote config to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf\n'
10.10.2.1: b'Wrote keyring to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring\n'
10.10.2.1: b'Creating mgr...\n'
10.10.2.1: b'Verifying port 9283 ...\n'
10.10.2.1: b'Waiting for mgr to start...\nWaiting for mgr...\n'
10.10.2.1: b'mgr not available, waiting (1/15)...\n'
10.10.2.1: b'mgr not available, waiting (2/15)...\n'
10.10.2.1: b'mgr is available\n'
10.10.2.1: b'Enabling cephadm module...\n'
10.10.2.1: b'Waiting for the mgr to restart...\n'
10.10.2.1: b'Waiting for mgr epoch 5...\n'
10.10.2.1: b'mgr epoch 5 is available\nSetting orchestrator backend to cephadm...\n'
10.10.2.1: b'Generating ssh key...\n'
10.10.2.1: b'Wrote public SSH key to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub\n'
10.10.2.1: b'Adding key to ljishen@localhost authorized_keys...\n'
10.10.2.1: b'Adding host node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us...\n'
10.10.2.1: b'Deploying unmanaged mon service...\n'
10.10.2.1: b'Deploying unmanaged mgr service...\n'
10.10.2.1: b'You can access the Ceph CLI with:\n\n\tsudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --fsid d336c45e-37d0-11ec-b51d-53e6e728d2d3 -c /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf -k /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring\n\n'
10.10.2.1: b'Please consider enabling telemetry to help improve Ceph:\n\n\tceph telemetry on\n\nFor more information see:\n\n\thttps://docs.ceph.com/docs/pacific/mgr/telemetry/\n\n'
10.10.2.1: b'Bootstrap complete.\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T02:25:15,910796701-07:00] INFO: Sleep for 20 seconds\x1b[0m\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T02:25:35,918340782-07:00] INFO: > Set the default number of monitor daemons to 1\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:337 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T02:25:35,929130095-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T02:25:35,932893229-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mon', '1']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1\n'
10.10.2.1: b'Inferring fsid d336c45e-37d0-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/d336c45e-37d0-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b'Scheduled mon update...\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T02:25:44,570214279-07:00] INFO: > Set the default number of manager daemons to 1\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:342 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T02:25:44,580118788-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T02:25:44,583842378-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mgr', '1']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1\n'
10.10.2.1: b'Inferring fsid d336c45e-37d0-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/d336c45e-37d0-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b'Scheduled mgr update...\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T02:25:53,571097432-07:00] STAGE: Add hosts to the cluster...\x1b[0m\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T02:25:53,577138468-07:00] INFO: > Adding host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:361 - add_hosts() > ssh-copy-id -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -f -i /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\n'
10.10.2.1: b'/usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: "/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub"\n'
10.10.2.1: b'\nNumber of key(s) added: 1\n\nNow try logging into the machine, with:   "ssh -o \'GlobalKnownHostsFile=/dev/null\' -o \'LogLevel=ERROR\' -o \'PasswordAuthentication=no\' -o \'StrictHostKeyChecking=no\' -o \'UserKnownHostsFile=/dev/null\' \'ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\'"\nand check to make sure that only the key(s) you wanted were added.\n\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:367 - add_hosts() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us 10.10.2.2\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T02:25:53,789177500-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T02:25:53,792710682-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'host', 'add', 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us', '10.10.2.2']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us 10.10.2.2\n'
10.10.2.1: b'Inferring fsid d336c45e-37d0-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/d336c45e-37d0-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b"Added host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us' with addr '10.10.2.2'\n"
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T02:26:03,035706712-07:00] INFO: Sleep for 20 seconds\x1b[0m\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T02:26:23,040437275-07:00] STAGE: Add OSDs to the cluster...\x1b[0m\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T02:26:23,047364357-07:00] INFO: > Adding devices ['/dev/nvme0n1'] on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us' as OSDs\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:388 - add_host_devices_as_osds() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T02:26:23,057001944-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T02:26:23,060885064-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'daemon', 'add', 'osd', 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1\n'
10.10.2.1: b'Inferring fsid d336c45e-37d0-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/d336c45e-37d0-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b"Created osd(s) 0 on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'\n"
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T02:26:47,114344930-07:00] INFO: > Sleep for 20 seconds\x1b[0m\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T02:27:07,119791838-07:00] STAGE: Show Cluster Status\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:408 - show_status() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T02:27:07,129967817-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T02:27:07,133746260-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', '--status']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status\n'
10.10.2.1: b'Inferring fsid d336c45e-37d0-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/d336c45e-37d0-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b'  cluster:\n    id:     d336c45e-37d0-11ec-b51d-53e6e728d2d3\n    health: HEALTH_WARN\n            1 pool(s) have no replicas configured\n \n  services:\n    mon: 1 daemons, quorum node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us (age 2m)\n    mgr: node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us.yndyyy(active, since 2m)\n    osd: 1 osds: 1 up (since 19s), 1 in (since 40s)\n \n  data:\n    pools:   1 pools, 1 pgs\n    objects: 0 objects, 0 B\n    usage:   4.7 MiB used, 100 GiB / 100 GiB avail\n    pgs:     1 active+clean\n \n'
[1] 02:27:15 [SUCCESS] ljishen@10.10.2.1

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:24:05,398502416-07:00] INFO: > Remove existing clusters[0m
## bash:17 -  > basename -- /var/lib/ceph/942e792e-37cf-11ec-b51d-53e6e728d2d3
# bash:17 -  > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm --host 10.10.2.1 --host 10.10.2.2 rm-cluster --force --fsid 942e792e-37cf-11ec-b51d-53e6e728d2d3
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:24:05,409136917-07:00] DEBUG: HOST_PARAMS: ['--host', '10.10.2.1', '--host', '10.10.2.2'][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:24:05,412561454-07:00] DEBUG: CEPHADM_PARAMS: ['rm-cluster', '--force', '--fsid', '942e792e-37cf-11ec-b51d-53e6e728d2d3'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:63 -  > /tmp/bench-rados/ceph-research/scripts/parallel-ssh --host 10.10.2.1 --host 10.10.2.2 'sudo python3 -u - rm-cluster --force --fsid 942e792e-37cf-11ec-b51d-53e6e728d2d3'
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:24:05,421308738-07:00] DEBUG: command from stdin[0m
# /tmp/bench-rados/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host 10.10.2.1 --host 10.10.2.2 'sudo python3 -u - rm-cluster --force --fsid 942e792e-37cf-11ec-b51d-53e6e728d2d3'
[1] 02:24:11 [SUCCESS] 10.10.2.1
[2] 02:24:16 [SUCCESS] 10.10.2.2

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:24:16,629732996-07:00] INFO: > Deploy a new cluster[0m
# bash:24 -  > /tmp/bench-rados/ceph-research/scripts/ceph-deploy -m 10.10.2.1 -o node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/10.10.2.2:/dev/nvme0n1


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:24:16,641805922-07:00] STAGE: Verify passwordless SSH to hosts, passwordless sudo via ljishen, and the existence of devices on hosts...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:24:16,646590436-07:00] INFO: > Check the host for the monitor[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:181 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@10.10.2.1 sudo --non-interactive --validate

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:24:16,799089041-07:00] INFO: > Check hosts for OSDs[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:24:16,804113446-07:00] INFO: >> Check host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:200 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us sudo --non-interactive --validate

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:24:16,955341362-07:00] INFO: >>> Check device '/dev/nvme0n1'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:215 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us test -b /dev/nvme0n1
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:226 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us lsblk --nodeps --noheadings --output TYPE /dev/nvme0n1
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:227 - verify_parameters() > grep --word-regexp disk


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:24:17,239109214-07:00] STAGE: Reset devices on hosts...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:24:17,244241502-07:00] INFO: > Resetting device '/dev/nvme0n1' on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:288 - reset_host_devices() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us sudo sh
  Removing ceph--947ead37--f44e--433e--a5fc--ca7648181ab2-osd--block--a004da5b--b9aa--4c53--8d84--15ac70482c1e (253:0)
  Archiving volume group "ceph-947ead37-f44e-433e-a5fc-ca7648181ab2" metadata (seqno 5).
  Releasing logical volume "osd-block-a004da5b-b9aa-4c53-8d84-15ac70482c1e"
  Creating volume group backup "/etc/lvm/backup/ceph-947ead37-f44e-433e-a5fc-ca7648181ab2" (seqno 6).
  Logical volume "osd-block-a004da5b-b9aa-4c53-8d84-15ac70482c1e" successfully removed
  Removing physical volume "/dev/nvme0n1" from volume group "ceph-947ead37-f44e-433e-a5fc-ca7648181ab2"
  Volume group "ceph-947ead37-f44e-433e-a5fc-ca7648181ab2" successfully removed


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:24:17,566015460-07:00] STAGE: Bootstrap a new cluster...[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:317 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.1 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:24:17,575698973-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:24:17,579355346-07:00] DEBUG: CEPHADM_PARAMS: ['bootstrap', '--config', '/tmp/bench-rados/ceph-research/scripts/ceph.conf', '--mon-ip', '10.10.2.1', '--output-dir', '/tmp/bench-rados/deployment_data_root/etc/ceph', '--ssh-user', 'ljishen', '--skip-dashboard', '--no-minimize-config', '--allow-overwrite', '--allow-fqdn-hostname', '--orphan-initial-daemons', '--skip-monitoring-stack', '--single-host-defaults'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.1 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults
Verifying podman|docker is present...
Verifying lvm2 is present...
Verifying time synchronization is in place...
Unit ntp.service is enabled and running
Repeating the final host check...
podman|docker (/usr/bin/docker) is present
systemctl is present
lvcreate is present
Unit ntp.service is enabled and running
Host looks OK
Cluster fsid: d336c45e-37d0-11ec-b51d-53e6e728d2d3
Verifying IP 10.10.2.1 port 3300 ...
Verifying IP 10.10.2.1 port 6789 ...
Mon IP `10.10.2.1` is in CIDR network `10.10.2.0/24`
- internal network (--cluster-network) has not been provided, OSD replication will default to the public_network
Adjusting default settings to suit single-host cluster...
Pulling container image quay.io/ceph/ceph:v16...
Ceph version: ceph version 16.2.6 (ee28fb57e47e9f88813e24bbf4c14496ca299d31) pacific (stable)
Extracting ceph user uid/gid from container image...
Creating initial keys...
Creating initial monmap...
Creating mon...
Waiting for mon to start...
Waiting for mon...
mon is available
Setting mon public_network to 10.10.2.0/24
Wrote config to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf
Wrote keyring to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring
Creating mgr...
Verifying port 9283 ...
Waiting for mgr to start...
Waiting for mgr...
mgr not available, waiting (1/15)...
mgr not available, waiting (2/15)...
mgr is available
Enabling cephadm module...
Waiting for the mgr to restart...
Waiting for mgr epoch 5...
mgr epoch 5 is available
Setting orchestrator backend to cephadm...
Generating ssh key...
Wrote public SSH key to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub
Adding key to ljishen@localhost authorized_keys...
Adding host node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us...
Deploying unmanaged mon service...
Deploying unmanaged mgr service...
You can access the Ceph CLI with:

	sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --fsid d336c45e-37d0-11ec-b51d-53e6e728d2d3 -c /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf -k /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring

Please consider enabling telemetry to help improve Ceph:

	ceph telemetry on

For more information see:

	https://docs.ceph.com/docs/pacific/mgr/telemetry/

Bootstrap complete.

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:25:15,910796701-07:00] INFO: Sleep for 20 seconds[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:25:35,918340782-07:00] INFO: > Set the default number of monitor daemons to 1[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:337 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:25:35,929130095-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:25:35,932893229-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mon', '1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1
Inferring fsid d336c45e-37d0-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/d336c45e-37d0-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Scheduled mon update...

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:25:44,570214279-07:00] INFO: > Set the default number of manager daemons to 1[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:342 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:25:44,580118788-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:25:44,583842378-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mgr', '1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1
Inferring fsid d336c45e-37d0-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/d336c45e-37d0-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Scheduled mgr update...


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:25:53,571097432-07:00] STAGE: Add hosts to the cluster...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:25:53,577138468-07:00] INFO: > Adding host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:361 - add_hosts() > ssh-copy-id -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -f -i /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us
/usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: "/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub"

Number of key(s) added: 1

Now try logging into the machine, with:   "ssh -o 'GlobalKnownHostsFile=/dev/null' -o 'LogLevel=ERROR' -o 'PasswordAuthentication=no' -o 'StrictHostKeyChecking=no' -o 'UserKnownHostsFile=/dev/null' 'ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'"
and check to make sure that only the key(s) you wanted were added.

# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:367 - add_hosts() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us 10.10.2.2
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:25:53,789177500-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:25:53,792710682-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'host', 'add', 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us', '10.10.2.2'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us 10.10.2.2
Inferring fsid d336c45e-37d0-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/d336c45e-37d0-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Added host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us' with addr '10.10.2.2'

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:26:03,035706712-07:00] INFO: Sleep for 20 seconds[0m


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:26:23,040437275-07:00] STAGE: Add OSDs to the cluster...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:26:23,047364357-07:00] INFO: > Adding devices ['/dev/nvme0n1'] on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us' as OSDs[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:388 - add_host_devices_as_osds() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:26:23,057001944-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:26:23,060885064-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'daemon', 'add', 'osd', 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1
Inferring fsid d336c45e-37d0-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/d336c45e-37d0-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Created osd(s) 0 on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:26:47,114344930-07:00] INFO: > Sleep for 20 seconds[0m


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:27:07,119791838-07:00] STAGE: Show Cluster Status[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:408 - show_status() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:27:07,129967817-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:27:07,133746260-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', '--status'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status
Inferring fsid d336c45e-37d0-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/d336c45e-37d0-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
  cluster:
    id:     d336c45e-37d0-11ec-b51d-53e6e728d2d3
    health: HEALTH_WARN
            1 pool(s) have no replicas configured
 
  services:
    mon: 1 daemons, quorum node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us (age 2m)
    mgr: node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us.yndyyy(active, since 2m)
    osd: 1 osds: 1 up (since 19s), 1 in (since 40s)
 
  data:
    pools:   1 pools, 1 pgs
    objects: 0 objects, 0 B
    usage:   4.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     1 active+clean
 

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:27:15,980636372-07:00] INFO: > Copy configuration files to local[0m
# ./benchmarks/bench-rados:220 - launch_ceph_cluster() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.1 sudo chown --changes --recursive ljishen /tmp/bench-rados/deployment_data_root/etc/ceph
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:27:15,988390614-07:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.1 sudo chown --changes --recursive ljishen /tmp/bench-rados/deployment_data_root/etc/ceph
[1] 02:27:16 [SUCCESS] ljishen@10.10.2.1
# ./benchmarks/bench-rados:223 - launch_ceph_cluster() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.1:/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf
# ./benchmarks/bench-rados:226 - launch_ceph_cluster() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.1:/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:27:16,464780211-07:00] STAGE: Update local ceph.conf as a client...[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:27:16,468017332-07:00] STAGE: Configure Ceph pool...[0m
# ./benchmarks/bench-rados:257 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_max_backfills 32
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:27:16,489533086-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:27:16,492475491-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'd336c45e-37d0-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'config', 'set', 'osd', 'osd_max_backfills', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid d336c45e-37d0-11ec-b51d-53e6e728d2d3 -- ceph config set osd osd_max_backfills 32
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:258 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_active 32
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:27:20,782873165-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:27:20,785957848-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'd336c45e-37d0-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_active', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid d336c45e-37d0-11ec-b51d-53e6e728d2d3 -- ceph config set osd osd_recovery_max_active 32
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:259 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_single_start 8
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:27:25,257864179-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:27:25,260998936-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'd336c45e-37d0-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_single_start', '8'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid d336c45e-37d0-11ec-b51d-53e6e728d2d3 -- ceph config set osd osd_recovery_max_single_start 8
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:260 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_op_priority 63
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:27:29,384403245-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:27:29,387453963-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'd336c45e-37d0-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_op_priority', '63'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid d336c45e-37d0-11ec-b51d-53e6e728d2d3 -- ceph config set osd osd_recovery_op_priority 63
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:262 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config show-with-defaults osd.0
# ./benchmarks/bench-rados:263 - configure_ceph_pool() > grep 'osd_max_backfills\|osd_recovery'
# ./benchmarks/bench-rados:263 - configure_ceph_pool() > column -t -s ' '
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
osd_max_backfills                        32         mon
osd_recovery_cost                        20971520   default
osd_recovery_delay_start                 0.000000   default
osd_recovery_max_active                  32         mon
osd_recovery_max_active_hdd              3          default
osd_recovery_max_active_ssd              10         default
osd_recovery_max_chunk                   8388608    default
osd_recovery_max_omap_entries_per_chunk  8096       default
osd_recovery_max_single_start            8          mon
osd_recovery_op_priority                 63         mon
osd_recovery_op_warn_multiple            16         default
osd_recovery_priority                    5          default
osd_recovery_retry_interval              30.000000  default
osd_recovery_sleep                       0.000000   default
osd_recovery_sleep_hdd                   0.100000   default
osd_recovery_sleep_hybrid                0.025000   default
osd_recovery_sleep_ssd                   0.000000   default
# ./benchmarks/bench-rados:265 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool create bench_rados 128 128 replicated --size 1 --pg-num-min 128
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:27:37,890475782-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:27:37,893945571-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'd336c45e-37d0-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'create', 'bench_rados', '128', '128', 'replicated', '--size', '1', '--pg-num-min', '128'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid d336c45e-37d0-11ec-b51d-53e6e728d2d3 -- ceph osd pool create bench_rados 128 128 replicated --size 1 --pg-num-min 128
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
pool 'bench_rados' created
# ./benchmarks/bench-rados:267 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados min_size 1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:27:43,066009131-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:27:43,069285435-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'd336c45e-37d0-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'min_size', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid d336c45e-37d0-11ec-b51d-53e6e728d2d3 -- ceph osd pool set bench_rados min_size 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 min_size to 1
# ./benchmarks/bench-rados:268 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados pg_autoscale_mode off
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:27:47,449199785-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:27:47,452552022-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'd336c45e-37d0-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'pg_autoscale_mode', 'off'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid d336c45e-37d0-11ec-b51d-53e6e728d2d3 -- ceph osd pool set bench_rados pg_autoscale_mode off
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 pg_autoscale_mode to off
# ./benchmarks/bench-rados:269 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool application enable bench_rados benchmark
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:27:52,214837980-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:27:52,217849404-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'd336c45e-37d0-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'application', 'enable', 'bench_rados', 'benchmark'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid d336c45e-37d0-11ec-b51d-53e6e728d2d3 -- ceph osd pool application enable bench_rados benchmark
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
enabled application 'benchmark' on pool 'bench_rados'
# ./benchmarks/bench-rados:270 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados noscrub 1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:27:57,003740494-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:27:57,006632624-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'd336c45e-37d0-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'noscrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid d336c45e-37d0-11ec-b51d-53e6e728d2d3 -- ceph osd pool set bench_rados noscrub 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 noscrub to 1
# ./benchmarks/bench-rados:271 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados nodeep-scrub 1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:28:01,653167198-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:28:01,656356148-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'd336c45e-37d0-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'nodeep-scrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid d336c45e-37d0-11ec-b51d-53e6e728d2d3 -- ceph osd pool set bench_rados nodeep-scrub 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 nodeep-scrub to 1
# ./benchmarks/bench-rados:272 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool ls detail
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:28:06,061798292-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:28:06,064746548-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'd336c45e-37d0-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'ls', 'detail'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid d336c45e-37d0-11ec-b51d-53e6e728d2d3 -- ceph osd pool ls detail
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
pool 1 'device_health_metrics' replicated size 1 min_size 1 crush_rule 0 object_hash rjenkins pg_num 64 pgp_num 64 autoscale_mode on last_change 19 lfor 0/0/16 flags hashpspool stripe_width 0 pg_num_min 1 application mgr_devicehealth
pool 2 'bench_rados' replicated size 1 min_size 1 crush_rule 0 object_hash rjenkins pg_num 128 pgp_num 128 autoscale_mode off last_change 20 flags hashpspool,noscrub,nodeep-scrub stripe_width 0 pg_num_min 128 application benchmark

# ./benchmarks/bench-rados:273 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd df tree
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:28:10,295594895-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:28:10,298469772-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'd336c45e-37d0-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'df', 'tree'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid d336c45e-37d0-11ec-b51d-53e6e728d2d3 -- ceph osd df tree
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
ID  CLASS  WEIGHT   REWEIGHT  SIZE     RAW USE  DATA     OMAP  META     AVAIL    %USE  VAR   PGS  STATUS  TYPE NAME      
-1         0.09769         -  100 GiB  5.7 MiB  168 KiB   0 B  5.6 MiB  100 GiB  0.01  1.00    -          root default   
-3         0.09769         -  100 GiB  5.7 MiB  168 KiB   0 B  5.6 MiB  100 GiB  0.01  1.00    -              host node-1
 0    ssd  0.09769   1.00000  100 GiB  5.7 MiB  168 KiB   0 B  5.6 MiB  100 GiB  0.01  1.00  192      up          osd.0  
                       TOTAL  100 GiB  5.7 MiB  168 KiB   0 B  5.6 MiB  100 GiB  0.01                                    
MIN/MAX VAR: 1.00/1.00  STDDEV: 0


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:28:14,465065660-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:28:38,684596570-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:28:47,984478584-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:28:57,440579801-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:29:06,696859930-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:29:06,704129098-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:29:16,079987883-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:29:16,087203270-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:29:25,302539844-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:29:25,309983281-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:29:34,624069914-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:29:34,631658284-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:29:44,014589847-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:29:44,021305612-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:29:44,027084191-07:00] INFO: > The cluster is idle now.[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:29:44,030476864-07:00] STAGE: Start benchmarking of mode write ...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:29:44,036831930-07:00] INFO: > Start system activity collection services[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:29:44,042141435-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:341 - rados_bench() > rm --force /mnt/sda8/ceph-research/scripts/output/sys_activity_64KB_write.dat.rados_bench_host.2
# ./benchmarks/bench-rados:342 - rados_bench() > sar_pid_rados_bench_host=261036
# ./benchmarks/bench-rados:342 - rados_bench() > S_TIME_FORMAT=ISO
# ./benchmarks/bench-rados:342 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/output/sys_activity_64KB_write.dat.rados_bench_host.2 2

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:29:44,048840299-07:00] INFO: >> for OSD host[0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:29:44,057679726-07:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.2 bash
10.10.2.2: b'152964\n'
[1] 02:29:44 [SUCCESS] ljishen@10.10.2.2
152964

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:29:44,247152201-07:00] INFO: > Run rados bench[0m
# ./benchmarks/bench-rados:370 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/output/rados_bench_64KB_write.log.2
## ./benchmarks/bench-rados:361 - rados_bench() > count_cpus 32-63,96-127
# ./benchmarks/bench-rados:361 - rados_bench() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 60 write --pool bench_rados -b 65536 -O 65536 --concurrent-ios 128 --show-time --write-object --write-omap --write-xattr --no-cleanup
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:29:44,267503642-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:29:44,270348553-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'd336c45e-37d0-11ec-b51d-53e6e728d2d3', '--', 'rados', 'bench', '60', 'write', '--pool', 'bench_rados', '-b', '65536', '-O', '65536', '--concurrent-ios', '128', '--show-time', '--write-object', '--write-omap', '--write-xattr', '--no-cleanup'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid d336c45e-37d0-11ec-b51d-53e6e728d2d3 -- rados bench 60 write --pool bench_rados -b 65536 -O 65536 --concurrent-ios 128 --show-time --write-object --write-omap --write-xattr --no-cleanup
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
hints = 1
2021-10-28T09:29:47.596630+0000 Maintaining 128 concurrent writes of 65536 bytes to objects of size 65536 for up to 60 seconds or 0 objects
2021-10-28T09:29:47.596642+0000 Object prefix: benchmark_data_node-2.bluefield2.ucsc-cmps10_7
2021-10-28T09:29:47.599647+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-28T09:29:47.599647+0000     0       0         0         0         0         0           -           0
2021-10-28T09:29:48.599739+0000     1     128      4546      4418   276.111   276.125   0.0284201   0.0286479
2021-10-28T09:29:49.599813+0000     2     128      8514      8386   262.046       248   0.0715469   0.0300077
2021-10-28T09:29:50.599880+0000     3     128     10243     10115   210.716   108.062    0.069239   0.0373258
2021-10-28T09:29:51.599947+0000     4     128     13634     13506   211.018   211.938   0.0246227   0.0377614
2021-10-28T09:29:52.600015+0000     5     128     16899     16771   209.624   204.062    0.126176   0.0377093
2021-10-28T09:29:53.600089+0000     6     128     17858     17730   184.675   59.9375    0.134508   0.0426953
2021-10-28T09:29:54.600155+0000     7     128     18882     18754   167.435        64    0.135636   0.0474278
2021-10-28T09:29:55.600216+0000     8     128     20546     20418   159.505       104   0.0688799   0.0498704
2021-10-28T09:29:56.600283+0000     9     128     22466     22338   155.115       120   0.0754014   0.0513245
2021-10-28T09:29:57.600348+0000    10     128     24963     24835   155.209   156.062   0.0723409   0.0513782
2021-10-28T09:29:58.600415+0000    11     128     26690     26562    150.91   107.938   0.0770071   0.0528114
2021-10-28T09:29:59.600483+0000    12     128     30595     30467   158.672   244.062   0.0261877   0.0503803
2021-10-28T09:30:00.600552+0000    13     128     33218     33090   159.076   163.938    0.129395   0.0501256
2021-10-28T09:30:01.600628+0000    14     128     34179     34051   152.003   60.0625    0.129083    0.052366
2021-10-28T09:30:02.600703+0000    15     128     35203     35075   146.136        64    0.117802   0.0546175
2021-10-28T09:30:03.600814+0000    16     128     37314     37186   145.248   131.938   0.0651451   0.0550068
2021-10-28T09:30:04.600924+0000    17     128     39234     39106   143.762       120   0.0647247   0.0555049
2021-10-28T09:30:05.601003+0000    18     128     40835     40707   141.333   100.062    0.133805    0.056376
2021-10-28T09:30:06.601114+0000    19     128     41794     41666   137.049   59.9375      0.1364   0.0581143
2021-10-28T09:30:07.601178+0000 min lat: 0.00477523 max lat: 0.142994 avg lat: 0.0598257
2021-10-28T09:30:07.601178+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-28T09:30:07.601178+0000    20     128     42818     42690   133.396        64    0.129515   0.0598257
2021-10-28T09:30:08.601305+0000    21     128     45827     45699   135.998   188.062   0.0268186   0.0587988
2021-10-28T09:30:09.601393+0000    22     128     49155     49027    139.27       208    0.115224   0.0572828
2021-10-28T09:30:10.601508+0000    23     128     50114     49986   135.821   59.9375    0.133565   0.0586837
2021-10-28T09:30:11.601618+0000    24     128     51138     51010   132.828        64    0.132329   0.0600963
2021-10-28T09:30:12.601684+0000    25     128     52867     52739   131.837   108.062   0.0617458   0.0606251
2021-10-28T09:30:13.601773+0000    26     128     54722     54594   131.225   115.938   0.0693672   0.0608542
2021-10-28T09:30:14.601888+0000    27     128     56642     56514   130.809       120   0.0687453   0.0610821
2021-10-28T09:30:15.602001+0000    28     128     57603     57475   128.282   60.0625    0.118221   0.0622133
2021-10-28T09:30:16.602114+0000    29     128     58627     58499   126.065        64    0.131702   0.0633517
2021-10-28T09:30:17.602180+0000    30     128     59970     59842    124.66   83.9375   0.0565829   0.0641281
2021-10-28T09:30:18.602289+0000    31     128     61955     61827   124.641   124.062   0.0674324   0.0641098
2021-10-28T09:30:19.602399+0000    32     128     63810     63682   124.368   115.938   0.0717457   0.0642612
2021-10-28T09:30:20.602512+0000    33     128     65218     65090   123.266        88    0.129713   0.0647958
2021-10-28T09:30:21.602598+0000    34     128     66179     66051   121.407   60.0625    0.126346   0.0657329
2021-10-28T09:30:22.602663+0000    35     128     67138     67010   119.651   59.9375    0.129195    0.066693
2021-10-28T09:30:23.602779+0000    36     128     68930     68802   119.438       112   0.0652946   0.0669129
2021-10-28T09:30:24.602899+0000    37     128     70850     70722   119.452       120    0.064735   0.0668944
2021-10-28T09:30:25.602984+0000    38     128     72770     72642   119.467       120   0.0677105   0.0668913
2021-10-28T09:30:26.603095+0000    39     128     73731     73603   117.943   60.0625    0.128346   0.0676722
2021-10-28T09:30:27.603160+0000 min lat: 0.00477523 max lat: 0.149503 avg lat: 0.0685279
2021-10-28T09:30:27.603160+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-28T09:30:27.603160+0000    40     128     74755     74627   116.595        64    0.129419   0.0685279
2021-10-28T09:30:28.603284+0000    41     128     76035     75907   115.702        80   0.0606166    0.069081
2021-10-28T09:30:29.603370+0000    42     128     78083     77955   115.994       128   0.0681802     0.06891
2021-10-28T09:30:30.603482+0000    43     128     80003     79875   116.087       120   0.0755632   0.0688542
2021-10-28T09:30:31.603596+0000    44     128     81411     81283   115.449        88    0.131101    0.069171
2021-10-28T09:30:32.603658+0000    45     128     82435     82307   114.305        64    0.131261   0.0698568
2021-10-28T09:30:33.603737+0000    46     128     83459     83331   113.212        64    0.135859   0.0705769
2021-10-28T09:30:34.603846+0000    47     128     85442     85314   113.439   123.938   0.0608399   0.0704766
2021-10-28T09:30:35.603962+0000    48     128     87362     87234   113.576       120   0.0672769   0.0704077
2021-10-28T09:30:36.604075+0000    49     128     89091     88963   113.463   108.062    0.124075   0.0704168
2021-10-28T09:30:37.604137+0000    50     128     90050     89922   112.392   59.9375     0.12775   0.0710496
2021-10-28T09:30:38.604248+0000    51     128     91074     90946   111.443        64    0.124499   0.0717043
2021-10-28T09:30:39.604358+0000    52     128     92547     92419   111.071   92.0625   0.0571802   0.0719719
2021-10-28T09:30:40.604467+0000    53     128     94786     94658   111.615   139.938   0.0574367   0.0716424
2021-10-28T09:30:41.604547+0000    54     128     96706     96578    111.77       120   0.0669635   0.0715071
2021-10-28T09:30:42.604616+0000    55     128     97858     97730   111.047        72    0.127779   0.0719794
2021-10-28T09:30:43.604727+0000    56     128     98819     98691   110.136   60.0625    0.123516   0.0725232
2021-10-28T09:30:44.604836+0000    57     127     99987     99860   109.486   73.0625   0.0600903   0.0730206
2021-10-28T09:30:45.604916+0000    58     128    102082    101954   109.854   130.875   0.0658361   0.0727889
2021-10-28T09:30:46.605026+0000    59     128    104002    103874   110.026       120   0.0664659   0.0726698
2021-10-28T09:30:47.605093+0000 min lat: 0.00477523 max lat: 0.149503 avg lat: 0.072818
2021-10-28T09:30:47.605093+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-28T09:30:47.605093+0000    60     128    105475    105347   109.727   92.0625    0.128234    0.072818
2021-10-28T09:30:48.605247+0000 Total time run:         60.0924
Total writes made:      105475
Write size:             65536
Object size:            65536
Bandwidth (MB/sec):     109.701
Stddev Bandwidth:       51.7941
Max bandwidth (MB/sec): 276.125
Min bandwidth (MB/sec): 59.9375
Average IOPS:           1755
Stddev IOPS:            828.706
Max IOPS:               4418
Min IOPS:               959
Average Latency(s):     0.0728833
Stddev Latency(s):      0.0360738
Max latency(s):         0.149503
Min latency(s):         0.00477523

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:30:49,218680341-07:00] INFO: > Stop system activity collection services[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:30:49,224154336-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:391 - rados_bench() > kill -INT 261036

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:30:49,229622991-07:00] INFO: >> for OSD host[0m
# ./benchmarks/bench-rados:396 - rados_bench() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.2 kill -INT 152964
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:30:49,237323091-07:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.2 kill -INT 152964
[1] 02:30:49 [SUCCESS] ljishen@10.10.2.2

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:30:49,421788531-07:00] INFO: >>> Copy system activity file from OSD host[0m
## ./benchmarks/bench-rados:403 - rados_bench() > basename -- /tmp/bench-rados/sys_activity_64KB_write.dat.osd_host.2
# ./benchmarks/bench-rados:403 - rados_bench() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.2:/tmp/bench-rados/sys_activity_64KB_write.dat.osd_host.2 /mnt/sda8/ceph-research/scripts/output/sys_activity_64KB_write.dat.osd_host.2


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:30:49,599119928-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:31:13,649852287-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 105.48k objects, 6.4 GiB
    usage:   20 GiB used, 80 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:31:13,656991380-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:31:23,022767230-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 105.48k objects, 6.4 GiB
    usage:   20 GiB used, 80 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:31:23,030309944-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:31:32,202145219-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 105.48k objects, 6.4 GiB
    usage:   20 GiB used, 80 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:31:32,209753957-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:31:41,542564878-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 105.48k objects, 6.4 GiB
    usage:   20 GiB used, 80 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:31:41,549698731-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:31:50,950931927-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 105.48k objects, 6.4 GiB
    usage:   20 GiB used, 80 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:31:50,958388548-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:31:50,964017115-07:00] INFO: > The cluster is idle now.[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:31:50,967789524-07:00] STAGE: Start benchmarking of mode seq ...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:31:50,974128478-07:00] INFO: > Start system activity collection services[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:31:50,979583548-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:341 - rados_bench() > rm --force /mnt/sda8/ceph-research/scripts/output/sys_activity_64KB_seq.dat.rados_bench_host.2
# ./benchmarks/bench-rados:342 - rados_bench() > sar_pid_rados_bench_host=262389
# ./benchmarks/bench-rados:342 - rados_bench() > S_TIME_FORMAT=ISO
# ./benchmarks/bench-rados:342 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/output/sys_activity_64KB_seq.dat.rados_bench_host.2 2

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:31:50,986723312-07:00] INFO: >> for OSD host[0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:31:50,995518537-07:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.2 bash
10.10.2.2: b'153060\n'
[1] 02:31:51 [SUCCESS] ljishen@10.10.2.2
153060

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:31:51,182698602-07:00] INFO: > Run rados bench[0m
# ./benchmarks/bench-rados:379 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/output/rados_bench_64KB_seq.log.2
## ./benchmarks/bench-rados:376 - rados_bench() > count_cpus 32-63,96-127
# ./benchmarks/bench-rados:376 - rados_bench() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 128 --show-time
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:31:51,202593352-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:31:51,205517993-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'd336c45e-37d0-11ec-b51d-53e6e728d2d3', '--', 'rados', 'bench', '99999999', 'seq', '--pool', 'bench_rados', '--concurrent-ios', '128', '--show-time'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid d336c45e-37d0-11ec-b51d-53e6e728d2d3 -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 128 --show-time
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
hints = 1
2021-10-28T09:31:54.518358+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-28T09:31:54.518358+0000     0       0         0         0         0         0           -           0
2021-10-28T09:31:55.518506+0000     1     128     17728     17600   1099.75      1100  0.00698584  0.00723466
2021-10-28T09:31:56.518590+0000     2     128     35278     35150   1098.26   1096.88  0.00787581  0.00725877
2021-10-28T09:31:57.518698+0000     3     127     52939     52812   1100.09   1103.88  0.00754201  0.00725458
2021-10-28T09:31:58.518813+0000     4     128     70670     70542   1102.07   1108.12  0.00693367  0.00724306
2021-10-28T09:31:59.518935+0000     5     127     88430     88303   1103.64   1110.06  0.00894019  0.00723474
2021-10-28T09:32:00.519084+0000 Total time run:       5.96899
Total reads made:     105475
Read size:            65536
Object size:          65536
Bandwidth (MB/sec):   1104.41
Average IOPS:         17670
Stddev IOPS:          87.7542
Max IOPS:             17761
Min IOPS:             17550
Average Latency(s):   0.00723129
Max latency(s):       0.0121413
Min latency(s):       0.00238089

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:32:01,168550463-07:00] INFO: > Stop system activity collection services[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:32:01,174170474-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:391 - rados_bench() > kill -INT 262389

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:32:01,179929876-07:00] INFO: >> for OSD host[0m
# ./benchmarks/bench-rados:396 - rados_bench() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.2 kill -INT 153060
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:32:01,188069736-07:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.2 kill -INT 153060
[1] 02:32:01 [SUCCESS] ljishen@10.10.2.2

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:32:01,372969855-07:00] INFO: >>> Copy system activity file from OSD host[0m
## ./benchmarks/bench-rados:403 - rados_bench() > basename -- /tmp/bench-rados/sys_activity_64KB_seq.dat.osd_host.2
# ./benchmarks/bench-rados:403 - rados_bench() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.2:/tmp/bench-rados/sys_activity_64KB_seq.dat.osd_host.2 /mnt/sda8/ceph-research/scripts/output/sys_activity_64KB_seq.dat.osd_host.2


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:32:01,526379402-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:32:25,676899106-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 105.48k objects, 6.4 GiB
    usage:   20 GiB used, 80 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:32:25,684841773-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:32:34,999923413-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 105.48k objects, 6.4 GiB
    usage:   20 GiB used, 80 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:32:35,007730584-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:32:44,430489683-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 105.48k objects, 6.4 GiB
    usage:   20 GiB used, 80 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:32:44,438005926-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:32:53,795825601-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 105.48k objects, 6.4 GiB
    usage:   20 GiB used, 80 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:32:53,803123793-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:33:03,093944955-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 105.48k objects, 6.4 GiB
    usage:   20 GiB used, 80 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:33:03,101162816-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:33:03,106956234-07:00] INFO: > The cluster is idle now.[0m


[1;7;39;49m[2021-10-28T02:33:03,109226943-07:00][RUNNING][ROUND 3/3/21] object_size=64KB[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:33:03,112523755-07:00] STAGE: Launch a Ceph cluster (CEPH_HOST: 10.10.2.1, OSD_HOST: 10.10.2.2) ...[0m
# ./benchmarks/bench-rados:183 - launch_ceph_cluster() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.1 bash -euo pipefail
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:33:03,121430820-07:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.1 bash -euo pipefail
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T02:33:03,573153246-07:00] INFO: > Remove existing clusters\x1b[0m\n'
10.10.2.1: b'## bash:17 -  > basename -- /var/lib/ceph/d336c45e-37d0-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'# bash:17 -  > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm --host 10.10.2.1 --host 10.10.2.2 rm-cluster --force --fsid d336c45e-37d0-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T02:33:03,584843753-07:00] DEBUG: HOST_PARAMS: ['--host', '10.10.2.1', '--host', '10.10.2.2']\x1b[0m\n"
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T02:33:03,588874039-07:00] DEBUG: CEPHADM_PARAMS: ['rm-cluster', '--force', '--fsid', 'd336c45e-37d0-11ec-b51d-53e6e728d2d3']\x1b[0m\n"
10.10.2.1: b"# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:63 -  > /tmp/bench-rados/ceph-research/scripts/parallel-ssh --host 10.10.2.1 --host 10.10.2.2 'sudo python3 -u - rm-cluster --force --fsid d336c45e-37d0-11ec-b51d-53e6e728d2d3'\n"
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T02:33:03,597171517-07:00] DEBUG: command from stdin\x1b[0m\n'
10.10.2.1: b"# /tmp/bench-rados/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host 10.10.2.1 --host 10.10.2.2 'sudo python3 -u - rm-cluster --force --fsid d336c45e-37d0-11ec-b51d-53e6e728d2d3'\n"
10.10.2.1: b'[1] 02:33:09 [SUCCESS] 10.10.2.1\n[2] 02:33:14 [SUCCESS] 10.10.2.2\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T02:33:14,562841399-07:00] INFO: > Deploy a new cluster\x1b[0m\n'
10.10.2.1: b'# bash:24 -  > /tmp/bench-rados/ceph-research/scripts/ceph-deploy -m 10.10.2.1 -o node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/10.10.2.2:/dev/nvme0n1\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T02:33:14,574681837-07:00] STAGE: Verify passwordless SSH to hosts, passwordless sudo via ljishen, and the existence of devices on hosts...\x1b[0m\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T02:33:14,579751768-07:00] INFO: > Check the host for the monitor\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:181 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@10.10.2.1 sudo --non-interactive --validate\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T02:33:14,731607975-07:00] INFO: > Check hosts for OSDs\x1b[0m\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T02:33:14,735648170-07:00] INFO: >> Check host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:200 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us sudo --non-interactive --validate\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T02:33:14,886904610-07:00] INFO: >>> Check device '/dev/nvme0n1'\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:215 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us test -b /dev/nvme0n1\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:226 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us lsblk --nodeps --noheadings --output TYPE /dev/nvme0n1\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:227 - verify_parameters() > grep --word-regexp disk\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T02:33:15,175780223-07:00] STAGE: Reset devices on hosts...\x1b[0m\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T02:33:15,181007309-07:00] INFO: > Resetting device '/dev/nvme0n1' on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:288 - reset_host_devices() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us sudo sh\n'
10.10.2.1: b'  Removing ceph--8780ea92--b68e--4026--84ad--bbb3203bee0d-osd--block--2e92a9a8--d7e8--4269--8e8a--8216cad4e3ca (253:0)\n'
10.10.2.1: b'  Archiving volume group "ceph-8780ea92-b68e-4026-84ad-bbb3203bee0d" metadata (seqno 5).\n'
10.10.2.1: b'  Releasing logical volume "osd-block-2e92a9a8-d7e8-4269-8e8a-8216cad4e3ca"\n'
10.10.2.1: b'  Creating volume group backup "/etc/lvm/backup/ceph-8780ea92-b68e-4026-84ad-bbb3203bee0d" (seqno 6).\n'
10.10.2.1: b'  Logical volume "osd-block-2e92a9a8-d7e8-4269-8e8a-8216cad4e3ca" successfully removed\n'
10.10.2.1: b'  Removing physical volume "/dev/nvme0n1" from volume group "ceph-8780ea92-b68e-4026-84ad-bbb3203bee0d"\n'
10.10.2.1: b'  Volume group "ceph-8780ea92-b68e-4026-84ad-bbb3203bee0d" successfully removed\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T02:33:15,505461575-07:00] STAGE: Bootstrap a new cluster...\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:317 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.1 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T02:33:15,515470700-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T02:33:15,519147693-07:00] DEBUG: CEPHADM_PARAMS: ['bootstrap', '--config', '/tmp/bench-rados/ceph-research/scripts/ceph.conf', '--mon-ip', '10.10.2.1', '--output-dir', '/tmp/bench-rados/deployment_data_root/etc/ceph', '--ssh-user', 'ljishen', '--skip-dashboard', '--no-minimize-config', '--allow-overwrite', '--allow-fqdn-hostname', '--orphan-initial-daemons', '--skip-monitoring-stack', '--single-host-defaults']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.1 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults\n'
10.10.2.1: b'Verifying podman|docker is present...\nVerifying lvm2 is present...\n'
10.10.2.1: b'Verifying time synchronization is in place...\n'
10.10.2.1: b'Unit ntp.service is enabled and running\n'
10.10.2.1: b'Repeating the final host check...\n'
10.10.2.1: b'podman|docker (/usr/bin/docker) is present\n'
10.10.2.1: b'systemctl is present\n'
10.10.2.1: b'lvcreate is present\n'
10.10.2.1: b'Unit ntp.service is enabled and running\n'
10.10.2.1: b'Host looks OK\n'
10.10.2.1: b'Cluster fsid: 13da2572-37d2-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Verifying IP 10.10.2.1 port 3300 ...\n'
10.10.2.1: b'Verifying IP 10.10.2.1 port 6789 ...\n'
10.10.2.1: b'Mon IP `10.10.2.1` is in CIDR network `10.10.2.0/24`\n- internal network (--cluster-network) has not been provided, OSD replication will default to the public_network\n'
10.10.2.1: b'Adjusting default settings to suit single-host cluster...\n'
10.10.2.1: b'Pulling container image quay.io/ceph/ceph:v16...\n'
10.10.2.1: b'Ceph version: ceph version 16.2.6 (ee28fb57e47e9f88813e24bbf4c14496ca299d31) pacific (stable)\n'
10.10.2.1: b'Extracting ceph user uid/gid from container image...\n'
10.10.2.1: b'Creating initial keys...\n'
10.10.2.1: b'Creating initial monmap...\n'
10.10.2.1: b'Creating mon...\n'
10.10.2.1: b'Waiting for mon to start...\nWaiting for mon...\n'
10.10.2.1: b'mon is available\nSetting mon public_network to 10.10.2.0/24\n'
10.10.2.1: b'Wrote config to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf\n'
10.10.2.1: b'Wrote keyring to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring\n'
10.10.2.1: b'Creating mgr...\n'
10.10.2.1: b'Verifying port 9283 ...\n'
10.10.2.1: b'Waiting for mgr to start...\nWaiting for mgr...\n'
10.10.2.1: b'mgr not available, waiting (1/15)...\n'
10.10.2.1: b'mgr not available, waiting (2/15)...\n'
10.10.2.1: b'mgr is available\n'
10.10.2.1: b'Enabling cephadm module...\n'
10.10.2.1: b'Waiting for the mgr to restart...\nWaiting for mgr epoch 5...\n'
10.10.2.1: b'mgr epoch 5 is available\nSetting orchestrator backend to cephadm...\n'
10.10.2.1: b'Generating ssh key...\n'
10.10.2.1: b'Wrote public SSH key to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub\n'
10.10.2.1: b'Adding key to ljishen@localhost authorized_keys...\n'
10.10.2.1: b'Adding host node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us...\n'
10.10.2.1: b'Deploying unmanaged mon service...\n'
10.10.2.1: b'Deploying unmanaged mgr service...\n'
10.10.2.1: b'You can access the Ceph CLI with:\n\n\tsudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --fsid 13da2572-37d2-11ec-b51d-53e6e728d2d3 -c /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf -k /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring\n\n'
10.10.2.1: b'Please consider enabling telemetry to help improve Ceph:\n\n\tceph telemetry on\n\nFor more information see:\n\n\thttps://docs.ceph.com/docs/pacific/mgr/telemetry/\n\nBootstrap complete.\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T02:34:14,820490293-07:00] INFO: Sleep for 20 seconds\x1b[0m\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T02:34:34,827837234-07:00] INFO: > Set the default number of monitor daemons to 1\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:337 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T02:34:34,838003816-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T02:34:34,841717457-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mon', '1']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1\n'
10.10.2.1: b'Inferring fsid 13da2572-37d2-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/13da2572-37d2-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b'Scheduled mon update...\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T02:34:43,643363173-07:00] INFO: > Set the default number of manager daemons to 1\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:342 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T02:34:43,653659779-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T02:34:43,657675498-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mgr', '1']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1\n'
10.10.2.1: b'Inferring fsid 13da2572-37d2-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/13da2572-37d2-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b'Scheduled mgr update...\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T02:34:52,701246929-07:00] STAGE: Add hosts to the cluster...\x1b[0m\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T02:34:52,707241478-07:00] INFO: > Adding host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:361 - add_hosts() > ssh-copy-id -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -f -i /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\n'
10.10.2.1: b'/usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: "/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub"\n'
10.10.2.1: b'\nNumber of key(s) added: 1\n\nNow try logging into the machine, with:   "ssh -o \'GlobalKnownHostsFile=/dev/null\' -o \'LogLevel=ERROR\' -o \'PasswordAuthentication=no\' -o \'StrictHostKeyChecking=no\' -o \'UserKnownHostsFile=/dev/null\' \'ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\'"\nand check to make sure that only the key(s) you wanted were added.\n\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:367 - add_hosts() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us 10.10.2.2\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T02:34:52,922148050-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T02:34:52,925815975-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'host', 'add', 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us', '10.10.2.2']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us 10.10.2.2\n'
10.10.2.1: b'Inferring fsid 13da2572-37d2-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/13da2572-37d2-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b"Added host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us' with addr '10.10.2.2'\n"
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T02:35:02,106786807-07:00] INFO: Sleep for 20 seconds\x1b[0m\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T02:35:22,111648924-07:00] STAGE: Add OSDs to the cluster...\x1b[0m\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T02:35:22,118132983-07:00] INFO: > Adding devices ['/dev/nvme0n1'] on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us' as OSDs\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:388 - add_host_devices_as_osds() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T02:35:22,128930040-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T02:35:22,132755171-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'daemon', 'add', 'osd', 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1\n'
10.10.2.1: b'Inferring fsid 13da2572-37d2-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/13da2572-37d2-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b"Created osd(s) 0 on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'\n"
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T02:35:45,733468704-07:00] INFO: > Sleep for 20 seconds\x1b[0m\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T02:36:05,738977135-07:00] STAGE: Show Cluster Status\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:408 - show_status() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T02:36:05,748840266-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T02:36:05,752602418-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', '--status']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status\n'
10.10.2.1: b'Inferring fsid 13da2572-37d2-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/13da2572-37d2-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b'  cluster:\n    id:     13da2572-37d2-11ec-b51d-53e6e728d2d3\n    health: HEALTH_WARN\n            1 pool(s) have no replicas configured\n \n  services:\n    mon: 1 daemons, quorum node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us (age 2m)\n    mgr: node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us.qmcwqj(active, since 2m)\n    osd: 1 osds: 1 up (since 18s), 1 in (since 39s)\n \n  data:\n    pools:   1 pools, 1 pgs\n    objects: 0 objects, 0 B\n    usage:   4.7 MiB used, 100 GiB / 100 GiB avail\n    pgs:     1 active+clean\n \n'
[1] 02:36:14 [SUCCESS] ljishen@10.10.2.1

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:33:03,573153246-07:00] INFO: > Remove existing clusters[0m
## bash:17 -  > basename -- /var/lib/ceph/d336c45e-37d0-11ec-b51d-53e6e728d2d3
# bash:17 -  > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm --host 10.10.2.1 --host 10.10.2.2 rm-cluster --force --fsid d336c45e-37d0-11ec-b51d-53e6e728d2d3
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:33:03,584843753-07:00] DEBUG: HOST_PARAMS: ['--host', '10.10.2.1', '--host', '10.10.2.2'][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:33:03,588874039-07:00] DEBUG: CEPHADM_PARAMS: ['rm-cluster', '--force', '--fsid', 'd336c45e-37d0-11ec-b51d-53e6e728d2d3'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:63 -  > /tmp/bench-rados/ceph-research/scripts/parallel-ssh --host 10.10.2.1 --host 10.10.2.2 'sudo python3 -u - rm-cluster --force --fsid d336c45e-37d0-11ec-b51d-53e6e728d2d3'
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:33:03,597171517-07:00] DEBUG: command from stdin[0m
# /tmp/bench-rados/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host 10.10.2.1 --host 10.10.2.2 'sudo python3 -u - rm-cluster --force --fsid d336c45e-37d0-11ec-b51d-53e6e728d2d3'
[1] 02:33:09 [SUCCESS] 10.10.2.1
[2] 02:33:14 [SUCCESS] 10.10.2.2

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:33:14,562841399-07:00] INFO: > Deploy a new cluster[0m
# bash:24 -  > /tmp/bench-rados/ceph-research/scripts/ceph-deploy -m 10.10.2.1 -o node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/10.10.2.2:/dev/nvme0n1


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:33:14,574681837-07:00] STAGE: Verify passwordless SSH to hosts, passwordless sudo via ljishen, and the existence of devices on hosts...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:33:14,579751768-07:00] INFO: > Check the host for the monitor[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:181 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@10.10.2.1 sudo --non-interactive --validate

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:33:14,731607975-07:00] INFO: > Check hosts for OSDs[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:33:14,735648170-07:00] INFO: >> Check host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:200 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us sudo --non-interactive --validate

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:33:14,886904610-07:00] INFO: >>> Check device '/dev/nvme0n1'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:215 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us test -b /dev/nvme0n1
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:226 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us lsblk --nodeps --noheadings --output TYPE /dev/nvme0n1
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:227 - verify_parameters() > grep --word-regexp disk


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:33:15,175780223-07:00] STAGE: Reset devices on hosts...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:33:15,181007309-07:00] INFO: > Resetting device '/dev/nvme0n1' on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:288 - reset_host_devices() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us sudo sh
  Removing ceph--8780ea92--b68e--4026--84ad--bbb3203bee0d-osd--block--2e92a9a8--d7e8--4269--8e8a--8216cad4e3ca (253:0)
  Archiving volume group "ceph-8780ea92-b68e-4026-84ad-bbb3203bee0d" metadata (seqno 5).
  Releasing logical volume "osd-block-2e92a9a8-d7e8-4269-8e8a-8216cad4e3ca"
  Creating volume group backup "/etc/lvm/backup/ceph-8780ea92-b68e-4026-84ad-bbb3203bee0d" (seqno 6).
  Logical volume "osd-block-2e92a9a8-d7e8-4269-8e8a-8216cad4e3ca" successfully removed
  Removing physical volume "/dev/nvme0n1" from volume group "ceph-8780ea92-b68e-4026-84ad-bbb3203bee0d"
  Volume group "ceph-8780ea92-b68e-4026-84ad-bbb3203bee0d" successfully removed


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:33:15,505461575-07:00] STAGE: Bootstrap a new cluster...[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:317 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.1 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:33:15,515470700-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:33:15,519147693-07:00] DEBUG: CEPHADM_PARAMS: ['bootstrap', '--config', '/tmp/bench-rados/ceph-research/scripts/ceph.conf', '--mon-ip', '10.10.2.1', '--output-dir', '/tmp/bench-rados/deployment_data_root/etc/ceph', '--ssh-user', 'ljishen', '--skip-dashboard', '--no-minimize-config', '--allow-overwrite', '--allow-fqdn-hostname', '--orphan-initial-daemons', '--skip-monitoring-stack', '--single-host-defaults'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.1 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults
Verifying podman|docker is present...
Verifying lvm2 is present...
Verifying time synchronization is in place...
Unit ntp.service is enabled and running
Repeating the final host check...
podman|docker (/usr/bin/docker) is present
systemctl is present
lvcreate is present
Unit ntp.service is enabled and running
Host looks OK
Cluster fsid: 13da2572-37d2-11ec-b51d-53e6e728d2d3
Verifying IP 10.10.2.1 port 3300 ...
Verifying IP 10.10.2.1 port 6789 ...
Mon IP `10.10.2.1` is in CIDR network `10.10.2.0/24`
- internal network (--cluster-network) has not been provided, OSD replication will default to the public_network
Adjusting default settings to suit single-host cluster...
Pulling container image quay.io/ceph/ceph:v16...
Ceph version: ceph version 16.2.6 (ee28fb57e47e9f88813e24bbf4c14496ca299d31) pacific (stable)
Extracting ceph user uid/gid from container image...
Creating initial keys...
Creating initial monmap...
Creating mon...
Waiting for mon to start...
Waiting for mon...
mon is available
Setting mon public_network to 10.10.2.0/24
Wrote config to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf
Wrote keyring to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring
Creating mgr...
Verifying port 9283 ...
Waiting for mgr to start...
Waiting for mgr...
mgr not available, waiting (1/15)...
mgr not available, waiting (2/15)...
mgr is available
Enabling cephadm module...
Waiting for the mgr to restart...
Waiting for mgr epoch 5...
mgr epoch 5 is available
Setting orchestrator backend to cephadm...
Generating ssh key...
Wrote public SSH key to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub
Adding key to ljishen@localhost authorized_keys...
Adding host node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us...
Deploying unmanaged mon service...
Deploying unmanaged mgr service...
You can access the Ceph CLI with:

	sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --fsid 13da2572-37d2-11ec-b51d-53e6e728d2d3 -c /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf -k /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring

Please consider enabling telemetry to help improve Ceph:

	ceph telemetry on

For more information see:

	https://docs.ceph.com/docs/pacific/mgr/telemetry/

Bootstrap complete.

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:34:14,820490293-07:00] INFO: Sleep for 20 seconds[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:34:34,827837234-07:00] INFO: > Set the default number of monitor daemons to 1[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:337 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:34:34,838003816-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:34:34,841717457-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mon', '1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1
Inferring fsid 13da2572-37d2-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/13da2572-37d2-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Scheduled mon update...

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:34:43,643363173-07:00] INFO: > Set the default number of manager daemons to 1[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:342 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:34:43,653659779-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:34:43,657675498-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mgr', '1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1
Inferring fsid 13da2572-37d2-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/13da2572-37d2-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Scheduled mgr update...


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:34:52,701246929-07:00] STAGE: Add hosts to the cluster...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:34:52,707241478-07:00] INFO: > Adding host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:361 - add_hosts() > ssh-copy-id -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -f -i /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us
/usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: "/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub"

Number of key(s) added: 1

Now try logging into the machine, with:   "ssh -o 'GlobalKnownHostsFile=/dev/null' -o 'LogLevel=ERROR' -o 'PasswordAuthentication=no' -o 'StrictHostKeyChecking=no' -o 'UserKnownHostsFile=/dev/null' 'ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'"
and check to make sure that only the key(s) you wanted were added.

# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:367 - add_hosts() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us 10.10.2.2
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:34:52,922148050-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:34:52,925815975-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'host', 'add', 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us', '10.10.2.2'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us 10.10.2.2
Inferring fsid 13da2572-37d2-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/13da2572-37d2-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Added host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us' with addr '10.10.2.2'

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:35:02,106786807-07:00] INFO: Sleep for 20 seconds[0m


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:35:22,111648924-07:00] STAGE: Add OSDs to the cluster...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:35:22,118132983-07:00] INFO: > Adding devices ['/dev/nvme0n1'] on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us' as OSDs[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:388 - add_host_devices_as_osds() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:35:22,128930040-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:35:22,132755171-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'daemon', 'add', 'osd', 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1
Inferring fsid 13da2572-37d2-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/13da2572-37d2-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Created osd(s) 0 on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:35:45,733468704-07:00] INFO: > Sleep for 20 seconds[0m


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:36:05,738977135-07:00] STAGE: Show Cluster Status[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:408 - show_status() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:36:05,748840266-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:36:05,752602418-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', '--status'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status
Inferring fsid 13da2572-37d2-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/13da2572-37d2-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
  cluster:
    id:     13da2572-37d2-11ec-b51d-53e6e728d2d3
    health: HEALTH_WARN
            1 pool(s) have no replicas configured
 
  services:
    mon: 1 daemons, quorum node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us (age 2m)
    mgr: node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us.qmcwqj(active, since 2m)
    osd: 1 osds: 1 up (since 18s), 1 in (since 39s)
 
  data:
    pools:   1 pools, 1 pgs
    objects: 0 objects, 0 B
    usage:   4.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     1 active+clean
 

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:36:14,482002444-07:00] INFO: > Copy configuration files to local[0m
# ./benchmarks/bench-rados:220 - launch_ceph_cluster() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.1 sudo chown --changes --recursive ljishen /tmp/bench-rados/deployment_data_root/etc/ceph
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:36:14,489753110-07:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.1 sudo chown --changes --recursive ljishen /tmp/bench-rados/deployment_data_root/etc/ceph
[1] 02:36:14 [SUCCESS] ljishen@10.10.2.1
# ./benchmarks/bench-rados:223 - launch_ceph_cluster() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.1:/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf
# ./benchmarks/bench-rados:226 - launch_ceph_cluster() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.1:/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:36:14,965568584-07:00] STAGE: Update local ceph.conf as a client...[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:36:14,969126178-07:00] STAGE: Configure Ceph pool...[0m
# ./benchmarks/bench-rados:257 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_max_backfills 32
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:36:14,990463948-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:36:14,993377758-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '13da2572-37d2-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'config', 'set', 'osd', 'osd_max_backfills', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 13da2572-37d2-11ec-b51d-53e6e728d2d3 -- ceph config set osd osd_max_backfills 32
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:258 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_active 32
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:36:19,357392068-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:36:19,360436636-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '13da2572-37d2-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_active', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 13da2572-37d2-11ec-b51d-53e6e728d2d3 -- ceph config set osd osd_recovery_max_active 32
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:259 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_single_start 8
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:36:23,654601941-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:36:23,657681394-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '13da2572-37d2-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_single_start', '8'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 13da2572-37d2-11ec-b51d-53e6e728d2d3 -- ceph config set osd osd_recovery_max_single_start 8
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:260 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_op_priority 63
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:36:27,862710459-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:36:27,866131887-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '13da2572-37d2-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_op_priority', '63'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 13da2572-37d2-11ec-b51d-53e6e728d2d3 -- ceph config set osd osd_recovery_op_priority 63
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:262 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config show-with-defaults osd.0
# ./benchmarks/bench-rados:263 - configure_ceph_pool() > grep 'osd_max_backfills\|osd_recovery'
# ./benchmarks/bench-rados:263 - configure_ceph_pool() > column -t -s ' '
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
osd_max_backfills                        32         mon
osd_recovery_cost                        20971520   default
osd_recovery_delay_start                 0.000000   default
osd_recovery_max_active                  32         mon
osd_recovery_max_active_hdd              3          default
osd_recovery_max_active_ssd              10         default
osd_recovery_max_chunk                   8388608    default
osd_recovery_max_omap_entries_per_chunk  8096       default
osd_recovery_max_single_start            8          mon
osd_recovery_op_priority                 63         mon
osd_recovery_op_warn_multiple            16         default
osd_recovery_priority                    5          default
osd_recovery_retry_interval              30.000000  default
osd_recovery_sleep                       0.000000   default
osd_recovery_sleep_hdd                   0.100000   default
osd_recovery_sleep_hybrid                0.025000   default
osd_recovery_sleep_ssd                   0.000000   default
# ./benchmarks/bench-rados:265 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool create bench_rados 128 128 replicated --size 1 --pg-num-min 128
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:36:36,538117498-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:36:36,541217008-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '13da2572-37d2-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'create', 'bench_rados', '128', '128', 'replicated', '--size', '1', '--pg-num-min', '128'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 13da2572-37d2-11ec-b51d-53e6e728d2d3 -- ceph osd pool create bench_rados 128 128 replicated --size 1 --pg-num-min 128
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
pool 'bench_rados' created
# ./benchmarks/bench-rados:267 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados min_size 1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:36:40,902843839-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:36:40,906040593-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '13da2572-37d2-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'min_size', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 13da2572-37d2-11ec-b51d-53e6e728d2d3 -- ceph osd pool set bench_rados min_size 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 min_size to 1
# ./benchmarks/bench-rados:268 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados pg_autoscale_mode off
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:36:45,228450234-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:36:45,231379444-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '13da2572-37d2-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'pg_autoscale_mode', 'off'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 13da2572-37d2-11ec-b51d-53e6e728d2d3 -- ceph osd pool set bench_rados pg_autoscale_mode off
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 pg_autoscale_mode to off
# ./benchmarks/bench-rados:269 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool application enable bench_rados benchmark
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:36:49,994549069-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:36:49,997558501-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '13da2572-37d2-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'application', 'enable', 'bench_rados', 'benchmark'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 13da2572-37d2-11ec-b51d-53e6e728d2d3 -- ceph osd pool application enable bench_rados benchmark
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
enabled application 'benchmark' on pool 'bench_rados'
# ./benchmarks/bench-rados:270 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados noscrub 1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:36:54,336885248-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:36:54,339799169-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '13da2572-37d2-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'noscrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 13da2572-37d2-11ec-b51d-53e6e728d2d3 -- ceph osd pool set bench_rados noscrub 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 noscrub to 1
# ./benchmarks/bench-rados:271 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados nodeep-scrub 1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:36:58,643921056-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:36:58,646801855-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '13da2572-37d2-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'nodeep-scrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 13da2572-37d2-11ec-b51d-53e6e728d2d3 -- ceph osd pool set bench_rados nodeep-scrub 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 nodeep-scrub to 1
# ./benchmarks/bench-rados:272 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool ls detail
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:37:03,351931488-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:37:03,354981165-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '13da2572-37d2-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'ls', 'detail'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 13da2572-37d2-11ec-b51d-53e6e728d2d3 -- ceph osd pool ls detail
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
pool 1 'device_health_metrics' replicated size 1 min_size 1 crush_rule 0 object_hash rjenkins pg_num 64 pgp_num 64 autoscale_mode on last_change 19 lfor 0/0/17 flags hashpspool stripe_width 0 pg_num_min 1 application mgr_devicehealth
pool 2 'bench_rados' replicated size 1 min_size 1 crush_rule 0 object_hash rjenkins pg_num 128 pgp_num 128 autoscale_mode off last_change 19 flags hashpspool,noscrub,nodeep-scrub stripe_width 0 pg_num_min 128 application benchmark

# ./benchmarks/bench-rados:273 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd df tree
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:37:07,465828953-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:37:07,468886344-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '13da2572-37d2-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'df', 'tree'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 13da2572-37d2-11ec-b51d-53e6e728d2d3 -- ceph osd df tree
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
ID  CLASS  WEIGHT   REWEIGHT  SIZE     RAW USE  DATA     OMAP  META     AVAIL    %USE  VAR   PGS  STATUS  TYPE NAME      
-1         0.09769         -  100 GiB  5.7 MiB  160 KiB   0 B  5.6 MiB  100 GiB  0.01  1.00    -          root default   
-3         0.09769         -  100 GiB  5.7 MiB  160 KiB   0 B  5.6 MiB  100 GiB  0.01  1.00    -              host node-1
 0    ssd  0.09769   1.00000  100 GiB  5.7 MiB  160 KiB   0 B  5.6 MiB  100 GiB  0.01  1.00  192      up          osd.0  
                       TOTAL  100 GiB  5.7 MiB  160 KiB   0 B  5.6 MiB  100 GiB  0.01                                    
MIN/MAX VAR: 1.00/1.00  STDDEV: 0


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:37:11,611077997-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:37:35,955915481-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:37:45,164939836-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:37:54,396258059-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:38:03,726216442-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:38:03,733801896-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:38:13,082341444-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:38:13,089883146-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:38:22,431458690-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:38:22,438946129-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:38:31,620878258-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:38:31,628526581-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:38:40,878779956-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:38:40,886401057-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:38:40,892236884-07:00] INFO: > The cluster is idle now.[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:38:40,895806430-07:00] STAGE: Start benchmarking of mode write ...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:38:40,902242108-07:00] INFO: > Start system activity collection services[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:38:40,907708299-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:341 - rados_bench() > rm --force /mnt/sda8/ceph-research/scripts/output/sys_activity_64KB_write.dat.rados_bench_host.3
# ./benchmarks/bench-rados:342 - rados_bench() > sar_pid_rados_bench_host=268193
# ./benchmarks/bench-rados:342 - rados_bench() > S_TIME_FORMAT=ISO
# ./benchmarks/bench-rados:342 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/output/sys_activity_64KB_write.dat.rados_bench_host.3 2

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:38:40,914327121-07:00] INFO: >> for OSD host[0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:38:40,923138475-07:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.2 bash
10.10.2.2: b'156668\n'
[1] 02:38:41 [SUCCESS] ljishen@10.10.2.2
156668

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:38:41,115675275-07:00] INFO: > Run rados bench[0m
# ./benchmarks/bench-rados:370 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/output/rados_bench_64KB_write.log.3
## ./benchmarks/bench-rados:361 - rados_bench() > count_cpus 32-63,96-127
# ./benchmarks/bench-rados:361 - rados_bench() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 60 write --pool bench_rados -b 65536 -O 65536 --concurrent-ios 128 --show-time --write-object --write-omap --write-xattr --no-cleanup
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:38:41,135716410-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:38:41,138537076-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '13da2572-37d2-11ec-b51d-53e6e728d2d3', '--', 'rados', 'bench', '60', 'write', '--pool', 'bench_rados', '-b', '65536', '-O', '65536', '--concurrent-ios', '128', '--show-time', '--write-object', '--write-omap', '--write-xattr', '--no-cleanup'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 13da2572-37d2-11ec-b51d-53e6e728d2d3 -- rados bench 60 write --pool bench_rados -b 65536 -O 65536 --concurrent-ios 128 --show-time --write-object --write-omap --write-xattr --no-cleanup
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
hints = 1
2021-10-28T09:38:44.277058+0000 Maintaining 128 concurrent writes of 65536 bytes to objects of size 65536 for up to 60 seconds or 0 objects
2021-10-28T09:38:44.277070+0000 Object prefix: benchmark_data_node-2.bluefield2.ucsc-cmps10_7
2021-10-28T09:38:44.280250+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-28T09:38:44.280250+0000     0       0         0         0         0         0           -           0
2021-10-28T09:38:45.280358+0000     1     128      4319      4191   261.923   261.938   0.0305152   0.0301432
2021-10-28T09:38:46.280473+0000     2     128      8287      8159   254.947       248   0.0543473   0.0304063
2021-10-28T09:38:47.280586+0000     3     128     10024      9896   206.147   108.562   0.0778042   0.0384267
2021-10-28T09:38:48.280701+0000     4     128     12639     12511   195.465   163.438   0.0291041    0.040832
2021-10-28T09:38:49.280785+0000     5     128     16479     16351   204.368       240   0.0763149   0.0387026
2021-10-28T09:38:50.280914+0000     6     128     17576     17448   181.731   68.5625    0.131137   0.0435469
2021-10-28T09:38:51.281032+0000     7     128     18527     18399    164.26   59.4375    0.131379   0.0481345
2021-10-28T09:38:52.281148+0000     8     128     20136     20008   156.296   100.562     0.06448   0.0509467
2021-10-28T09:38:53.281222+0000     9     128     22099     21971   152.561   122.688   0.0678168   0.0523037
2021-10-28T09:38:54.281337+0000    10     128     24543     24415   152.578    152.75   0.0764891   0.0522784
2021-10-28T09:38:55.281451+0000    11     128     26207     26079   148.161       104   0.0744151   0.0538578
2021-10-28T09:38:56.281565+0000    12     128     29023     28895   150.479       176   0.0296335   0.0530772
2021-10-28T09:38:57.281642+0000    13     128     32735     32607   156.748       232   0.0751092   0.0508471
2021-10-28T09:38:58.281756+0000    14     128     33759     33631   150.123        64    0.131694   0.0530695
2021-10-28T09:38:59.281867+0000    15     128     34728     34600   144.152   60.5625    0.129743   0.0552389
2021-10-28T09:39:00.281980+0000    16     128     36319     36191   141.356   99.4375   0.0839262   0.0564438
2021-10-28T09:39:01.282052+0000    17     128     38239     38111     140.1       120   0.0653394   0.0569443
2021-10-28T09:39:02.282163+0000    18     128     40104     39976   138.791   116.562    0.064677   0.0575209
2021-10-28T09:39:03.282276+0000    19     128     41256     41128   135.275        72    0.131968   0.0590085
2021-10-28T09:39:04.282392+0000 min lat: 0.0218283 max lat: 0.146234 avg lat: 0.0606754
2021-10-28T09:39:04.282392+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-28T09:39:04.282392+0000    20     128     42207     42079   131.483   59.4375    0.136544   0.0606754
2021-10-28T09:39:05.282479+0000    21     128     43359     43231    128.65        72   0.0674983   0.0621319
2021-10-28T09:39:06.282598+0000    22     128     47583     47455   134.801       264   0.0298439   0.0592925
2021-10-28T09:39:07.282717+0000    23     128     49503     49375   134.157       120    0.133014     0.05939
2021-10-28T09:39:08.282832+0000    24     128     50527     50399   131.234        64    0.126816   0.0608385
2021-10-28T09:39:09.282902+0000    25     128     51624     51496   128.727   68.5625   0.0652573     0.06199
2021-10-28T09:39:10.283016+0000    26     128     53544     53416    128.39       120   0.0769987    0.062217
2021-10-28T09:39:11.283132+0000    27     128     55391     55263    127.91   115.438   0.0668881    0.062492
2021-10-28T09:39:12.283245+0000    28     128     56927     56799    126.77        96    0.135731   0.0629404
2021-10-28T09:39:13.283318+0000    29     128     57951     57823   124.606        64    0.132605   0.0641219
2021-10-28T09:39:14.283395+0000    30     128     58847     58719   122.319        56    0.133231    0.065207
2021-10-28T09:39:15.283521+0000    31     128     60456     60328   121.616   100.562   0.0619644   0.0656973
2021-10-28T09:39:16.283634+0000    32     128     62303     62175   121.423   115.438   0.0722905   0.0658178
2021-10-28T09:39:17.283710+0000    33     128     64095     63967   121.137       112   0.0716569   0.0659704
2021-10-28T09:39:18.283831+0000    34     128     65324     65196   119.833   76.8125    0.129118   0.0665961
2021-10-28T09:39:19.283945+0000    35     128     66344     66216   118.231     63.75    0.133121   0.0675525
2021-10-28T09:39:20.284061+0000    36     128     67295     67167   116.597   59.4375    0.134107   0.0684599
2021-10-28T09:39:21.284132+0000    37     128     69087     68959   116.473       112   0.0740813   0.0686248
2021-10-28T09:39:22.284257+0000    38     128     70952     70824   116.475   116.562   0.0706604   0.0686465
2021-10-28T09:39:23.284382+0000    39     128     72744     72616    116.36       112   0.0637188   0.0686705
2021-10-28T09:39:24.284499+0000 min lat: 0.0218283 max lat: 0.146234 avg lat: 0.0694457
2021-10-28T09:39:24.284499+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-28T09:39:24.284499+0000    40     128     73695     73567   114.936   59.4375    0.131205   0.0694457
2021-10-28T09:39:25.284585+0000    41     128     74719     74591   113.694        64    0.131425    0.070302
2021-10-28T09:39:26.284699+0000    42     128     75999     75871   112.891        80   0.0620073   0.0708317
2021-10-28T09:39:27.284818+0000    43     128     78047     77919   113.242       128   0.0667343   0.0705855
2021-10-28T09:39:28.284934+0000    44     128     79967     79839   113.396       120   0.0692605   0.0705161
2021-10-28T09:39:29.285004+0000    45     128     81375     81247   112.831        88    0.127748   0.0708333
2021-10-28T09:39:30.285114+0000    46     128     82344     82216   111.695   60.5625    0.128056   0.0715461
2021-10-28T09:39:31.285227+0000    47     128     83295     83167   110.583   59.4375    0.135746   0.0722316
2021-10-28T09:39:32.285343+0000    48     128     84959     84831   110.445       104   0.0692159   0.0723968
2021-10-28T09:39:33.285417+0000    49     127     86755     86628   110.483   112.312   0.0686543   0.0723286
2021-10-28T09:39:34.285536+0000    50     128     88671     88543   110.667   119.688   0.0691738    0.072261
2021-10-28T09:39:35.285654+0000    51     128     89768     89640   109.841   68.5625    0.127165   0.0727658
2021-10-28T09:39:36.285770+0000    52     128     90719     90591   108.872   59.4375     0.12749   0.0733409
2021-10-28T09:39:37.285842+0000    53     128     91871     91743   108.176        72   0.0643109   0.0738952
2021-10-28T09:39:38.285960+0000    54     128     93919     93791   108.543       128    0.065783    0.073633
2021-10-28T09:39:39.286086+0000    55     128     95839     95711   108.751       120   0.0672085   0.0735296
2021-10-28T09:39:40.286211+0000    56     128     97375     97247   108.523        96    0.132193   0.0736467
2021-10-28T09:39:41.286285+0000    57     128     98344     98216   107.682   60.5625    0.130085   0.0741969
2021-10-28T09:39:42.286396+0000    58     128     99368     99240   106.928        64    0.127258   0.0747619
2021-10-28T09:39:43.286509+0000    59     128    101087    100959   106.937   107.438   0.0574693   0.0747507
2021-10-28T09:39:44.286591+0000 min lat: 0.0218283 max lat: 0.146234 avg lat: 0.0744773
2021-10-28T09:39:44.286591+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-28T09:39:44.286591+0000    60     128    103208    103080   107.364   132.562   0.0664702   0.0744773
2021-10-28T09:39:45.286712+0000 Total time run:         60.0648
Total writes made:      103208
Write size:             65536
Object size:            65536
Bandwidth (MB/sec):     107.392
Stddev Bandwidth:       51.9724
Max bandwidth (MB/sec): 264
Min bandwidth (MB/sec): 56
Average IOPS:           1718
Stddev IOPS:            831.559
Max IOPS:               4224
Min IOPS:               896
Average Latency(s):     0.0744684
Stddev Latency(s):      0.0358412
Max latency(s):         0.146234
Min latency(s):         0.0218283

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:39:45,868367791-07:00] INFO: > Stop system activity collection services[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:39:45,874161048-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:391 - rados_bench() > kill -INT 268193

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:39:45,879846482-07:00] INFO: >> for OSD host[0m
# ./benchmarks/bench-rados:396 - rados_bench() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.2 kill -INT 156668
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:39:45,887585174-07:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.2 kill -INT 156668
[1] 02:39:46 [SUCCESS] ljishen@10.10.2.2

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:39:46,073345084-07:00] INFO: >>> Copy system activity file from OSD host[0m
## ./benchmarks/bench-rados:403 - rados_bench() > basename -- /tmp/bench-rados/sys_activity_64KB_write.dat.osd_host.3
# ./benchmarks/bench-rados:403 - rados_bench() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.2:/tmp/bench-rados/sys_activity_64KB_write.dat.osd_host.3 /mnt/sda8/ceph-research/scripts/output/sys_activity_64KB_write.dat.osd_host.3


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:39:46,247546221-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:40:10,647947398-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 103.21k objects, 6.3 GiB
    usage:   19 GiB used, 81 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:40:10,655187682-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:40:20,123300882-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 103.21k objects, 6.3 GiB
    usage:   19 GiB used, 81 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:40:20,130719452-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:40:29,388745921-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 103.21k objects, 6.3 GiB
    usage:   19 GiB used, 81 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:40:29,396429941-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:40:38,714027567-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 103.21k objects, 6.3 GiB
    usage:   19 GiB used, 81 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:40:38,721448461-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:40:48,005226516-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 103.21k objects, 6.3 GiB
    usage:   19 GiB used, 81 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:40:48,012604399-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:40:48,018569330-07:00] INFO: > The cluster is idle now.[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:40:48,022441777-07:00] STAGE: Start benchmarking of mode seq ...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:40:48,029033238-07:00] INFO: > Start system activity collection services[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:40:48,034624164-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:341 - rados_bench() > rm --force /mnt/sda8/ceph-research/scripts/output/sys_activity_64KB_seq.dat.rados_bench_host.3
# ./benchmarks/bench-rados:342 - rados_bench() > sar_pid_rados_bench_host=269561
# ./benchmarks/bench-rados:342 - rados_bench() > S_TIME_FORMAT=ISO
# ./benchmarks/bench-rados:342 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/output/sys_activity_64KB_seq.dat.rados_bench_host.3 2

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:40:48,041411374-07:00] INFO: >> for OSD host[0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:40:48,050359045-07:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.2 bash
10.10.2.2: b'156768\n'
[1] 02:40:48 [SUCCESS] ljishen@10.10.2.2
156768

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:40:48,242649520-07:00] INFO: > Run rados bench[0m
# ./benchmarks/bench-rados:379 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/output/rados_bench_64KB_seq.log.3
## ./benchmarks/bench-rados:376 - rados_bench() > count_cpus 32-63,96-127
# ./benchmarks/bench-rados:376 - rados_bench() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 128 --show-time
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:40:48,262614623-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:40:48,265594147-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '13da2572-37d2-11ec-b51d-53e6e728d2d3', '--', 'rados', 'bench', '99999999', 'seq', '--pool', 'bench_rados', '--concurrent-ios', '128', '--show-time'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 13da2572-37d2-11ec-b51d-53e6e728d2d3 -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 128 --show-time
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
hints = 1
2021-10-28T09:40:51.533275+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-28T09:40:51.533275+0000     0       0         0         0         0         0           -           0
2021-10-28T09:40:52.533391+0000     1     128     17126     16998   1062.18   1062.38   0.0075152  0.00748895
2021-10-28T09:40:53.533501+0000     2     127     34124     33997   1062.25   1062.44  0.00857799  0.00750712
2021-10-28T09:40:54.533588+0000     3     128     51213     51085   1064.13      1068  0.00795297  0.00749967
2021-10-28T09:40:55.533668+0000     4     128     68329     68201   1065.52   1069.75  0.00782824  0.00749204
2021-10-28T09:40:56.533752+0000     5     128     85422     85294   1066.06   1068.31  0.00744963  0.00749015
2021-10-28T09:40:57.533828+0000     6     128    102434    102306   1065.58   1063.25  0.00813472  0.00749392
2021-10-28T09:40:58.533931+0000 Total time run:       6.05298
Total reads made:     103208
Read size:            65536
Object size:          65536
Bandwidth (MB/sec):   1065.67
Average IOPS:         17050
Stddev IOPS:          53.6507
Max IOPS:             17116
Min IOPS:             16998
Average Latency(s):   0.00749416
Max latency(s):       0.0124584
Min latency(s):       0.00296886

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:40:59,168917929-07:00] INFO: > Stop system activity collection services[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:40:59,174689225-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:391 - rados_bench() > kill -INT 269561

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:40:59,180292384-07:00] INFO: >> for OSD host[0m
# ./benchmarks/bench-rados:396 - rados_bench() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.2 kill -INT 156768
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:40:59,188570132-07:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.2 kill -INT 156768
[1] 02:40:59 [SUCCESS] ljishen@10.10.2.2

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:40:59,373507381-07:00] INFO: >>> Copy system activity file from OSD host[0m
## ./benchmarks/bench-rados:403 - rados_bench() > basename -- /tmp/bench-rados/sys_activity_64KB_seq.dat.osd_host.3
# ./benchmarks/bench-rados:403 - rados_bench() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.2:/tmp/bench-rados/sys_activity_64KB_seq.dat.osd_host.3 /mnt/sda8/ceph-research/scripts/output/sys_activity_64KB_seq.dat.osd_host.3


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:40:59,521868715-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:41:23,704570832-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 103.21k objects, 6.3 GiB
    usage:   19 GiB used, 81 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:41:23,711679728-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:41:32,946600365-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 103.21k objects, 6.3 GiB
    usage:   19 GiB used, 81 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:41:32,954428426-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:41:42,286782858-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 103.21k objects, 6.3 GiB
    usage:   19 GiB used, 81 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:41:42,294315452-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:41:51,506534499-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 103.21k objects, 6.3 GiB
    usage:   19 GiB used, 81 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:41:51,513673442-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:42:00,867441094-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 103.21k objects, 6.3 GiB
    usage:   19 GiB used, 81 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:42:00,874770506-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:42:00,880558222-07:00] INFO: > The cluster is idle now.[0m


[1;7;39;49m[2021-10-28T02:42:00,884322816-07:00][RUNNING][ROUND 1/4/21] object_size=256KB[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:42:00,887515633-07:00] STAGE: Launch a Ceph cluster (CEPH_HOST: 10.10.2.1, OSD_HOST: 10.10.2.2) ...[0m
# ./benchmarks/bench-rados:183 - launch_ceph_cluster() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.1 bash -euo pipefail
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:42:00,897028630-07:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.1 bash -euo pipefail
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T02:42:01,445749055-07:00] INFO: > Remove existing clusters\x1b[0m\n'
10.10.2.1: b'## bash:17 -  > basename -- /var/lib/ceph/13da2572-37d2-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'# bash:17 -  > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm --host 10.10.2.1 --host 10.10.2.2 rm-cluster --force --fsid 13da2572-37d2-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T02:42:01,456780472-07:00] DEBUG: HOST_PARAMS: ['--host', '10.10.2.1', '--host', '10.10.2.2']\x1b[0m\n"
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T02:42:01,460503170-07:00] DEBUG: CEPHADM_PARAMS: ['rm-cluster', '--force', '--fsid', '13da2572-37d2-11ec-b51d-53e6e728d2d3']\x1b[0m\n"
10.10.2.1: b"# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:63 -  > /tmp/bench-rados/ceph-research/scripts/parallel-ssh --host 10.10.2.1 --host 10.10.2.2 'sudo python3 -u - rm-cluster --force --fsid 13da2572-37d2-11ec-b51d-53e6e728d2d3'\n"
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T02:42:01,468241016-07:00] DEBUG: command from stdin\x1b[0m\n'
10.10.2.1: b"# /tmp/bench-rados/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host 10.10.2.1 --host 10.10.2.2 'sudo python3 -u - rm-cluster --force --fsid 13da2572-37d2-11ec-b51d-53e6e728d2d3'\n"
10.10.2.1: b'[1] 02:42:07 [SUCCESS] 10.10.2.1\n[2] 02:42:11 [SUCCESS] 10.10.2.2\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T02:42:11,587774929-07:00] INFO: > Deploy a new cluster\x1b[0m\n'
10.10.2.1: b'# bash:24 -  > /tmp/bench-rados/ceph-research/scripts/ceph-deploy -m 10.10.2.1 -o node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/10.10.2.2:/dev/nvme0n1\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T02:42:11,600298732-07:00] STAGE: Verify passwordless SSH to hosts, passwordless sudo via ljishen, and the existence of devices on hosts...\x1b[0m\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T02:42:11,604704935-07:00] INFO: > Check the host for the monitor\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:181 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@10.10.2.1 sudo --non-interactive --validate\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T02:42:11,755693200-07:00] INFO: > Check hosts for OSDs\x1b[0m\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T02:42:11,759699822-07:00] INFO: >> Check host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:200 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us sudo --non-interactive --validate\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T02:42:11,910876061-07:00] INFO: >>> Check device '/dev/nvme0n1'\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:215 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us test -b /dev/nvme0n1\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:226 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us lsblk --nodeps --noheadings --output TYPE /dev/nvme0n1\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:227 - verify_parameters() > grep --word-regexp disk\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T02:42:12,199771631-07:00] STAGE: Reset devices on hosts...\x1b[0m\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T02:42:12,205210355-07:00] INFO: > Resetting device '/dev/nvme0n1' on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:288 - reset_host_devices() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us sudo sh\n'
10.10.2.1: b'  Removing ceph--11d477a6--ab39--4ac4--90fe--4bda52647d94-osd--block--de284b0d--42f1--4d97--a923--c4d26752b9e6 (253:0)\n'
10.10.2.1: b'  Archiving volume group "ceph-11d477a6-ab39-4ac4-90fe-4bda52647d94" metadata (seqno 5).\n'
10.10.2.1: b'  Releasing logical volume "osd-block-de284b0d-42f1-4d97-a923-c4d26752b9e6"\n'
10.10.2.1: b'  Creating volume group backup "/etc/lvm/backup/ceph-11d477a6-ab39-4ac4-90fe-4bda52647d94" (seqno 6).\n'
10.10.2.1: b'  Logical volume "osd-block-de284b0d-42f1-4d97-a923-c4d26752b9e6" successfully removed\n'
10.10.2.1: b'  Removing physical volume "/dev/nvme0n1" from volume group "ceph-11d477a6-ab39-4ac4-90fe-4bda52647d94"\n'
10.10.2.1: b'  Volume group "ceph-11d477a6-ab39-4ac4-90fe-4bda52647d94" successfully removed\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T02:42:12,545943172-07:00] STAGE: Bootstrap a new cluster...\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:317 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.1 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T02:42:12,556192719-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T02:42:12,560121134-07:00] DEBUG: CEPHADM_PARAMS: ['bootstrap', '--config', '/tmp/bench-rados/ceph-research/scripts/ceph.conf', '--mon-ip', '10.10.2.1', '--output-dir', '/tmp/bench-rados/deployment_data_root/etc/ceph', '--ssh-user', 'ljishen', '--skip-dashboard', '--no-minimize-config', '--allow-overwrite', '--allow-fqdn-hostname', '--orphan-initial-daemons', '--skip-monitoring-stack', '--single-host-defaults']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.1 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults\n'
10.10.2.1: b'Verifying podman|docker is present...\nVerifying lvm2 is present...\n'
10.10.2.1: b'Verifying time synchronization is in place...\n'
10.10.2.1: b'Unit ntp.service is enabled and running\n'
10.10.2.1: b'Repeating the final host check...\n'
10.10.2.1: b'podman|docker (/usr/bin/docker) is present\n'
10.10.2.1: b'systemctl is present\n'
10.10.2.1: b'lvcreate is present\n'
10.10.2.1: b'Unit ntp.service is enabled and running\n'
10.10.2.1: b'Host looks OK\n'
10.10.2.1: b'Cluster fsid: 53f3c108-37d3-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Verifying IP 10.10.2.1 port 3300 ...\n'
10.10.2.1: b'Verifying IP 10.10.2.1 port 6789 ...\n'
10.10.2.1: b'Mon IP `10.10.2.1` is in CIDR network `10.10.2.0/24`\n'
10.10.2.1: b'- internal network (--cluster-network) has not been provided, OSD replication will default to the public_network\n'
10.10.2.1: b'Adjusting default settings to suit single-host cluster...\n'
10.10.2.1: b'Pulling container image quay.io/ceph/ceph:v16...\n'
10.10.2.1: b'Ceph version: ceph version 16.2.6 (ee28fb57e47e9f88813e24bbf4c14496ca299d31) pacific (stable)\n'
10.10.2.1: b'Extracting ceph user uid/gid from container image...\n'
10.10.2.1: b'Creating initial keys...\n'
10.10.2.1: b'Creating initial monmap...\n'
10.10.2.1: b'Creating mon...\n'
10.10.2.1: b'Waiting for mon to start...\n'
10.10.2.1: b'Waiting for mon...\n'
10.10.2.1: b'mon is available\n'
10.10.2.1: b'Setting mon public_network to 10.10.2.0/24\n'
10.10.2.1: b'Wrote config to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf\n'
10.10.2.1: b'Wrote keyring to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring\n'
10.10.2.1: b'Creating mgr...\n'
10.10.2.1: b'Verifying port 9283 ...\n'
10.10.2.1: b'Waiting for mgr to start...\n'
10.10.2.1: b'Waiting for mgr...\n'
10.10.2.1: b'mgr not available, waiting (1/15)...\n'
10.10.2.1: b'mgr not available, waiting (2/15)...\n'
10.10.2.1: b'mgr is available\n'
10.10.2.1: b'Enabling cephadm module...\n'
10.10.2.1: b'Waiting for the mgr to restart...\n'
10.10.2.1: b'Waiting for mgr epoch 5...\n'
10.10.2.1: b'mgr epoch 5 is available\n'
10.10.2.1: b'Setting orchestrator backend to cephadm...\n'
10.10.2.1: b'Generating ssh key...\n'
10.10.2.1: b'Wrote public SSH key to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub\n'
10.10.2.1: b'Adding key to ljishen@localhost authorized_keys...\n'
10.10.2.1: b'Adding host node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us...\n'
10.10.2.1: b'Deploying unmanaged mon service...\n'
10.10.2.1: b'Deploying unmanaged mgr service...\n'
10.10.2.1: b'You can access the Ceph CLI with:\n\n\tsudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --fsid 53f3c108-37d3-11ec-b51d-53e6e728d2d3 -c /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf -k /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring\n\n'
10.10.2.1: b'Please consider enabling telemetry to help improve Ceph:\n\n\tceph telemetry on\n\nFor more information see:\n\n\thttps://docs.ceph.com/docs/pacific/mgr/telemetry/\n\nBootstrap complete.\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T02:43:13,493629027-07:00] INFO: Sleep for 20 seconds\x1b[0m\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T02:43:33,501079434-07:00] INFO: > Set the default number of monitor daemons to 1\x1b[0m\n# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:337 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T02:43:33,511557411-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T02:43:33,515438446-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mon', '1']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1\n'
10.10.2.1: b'Inferring fsid 53f3c108-37d3-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/53f3c108-37d3-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b'Scheduled mon update...\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T02:43:42,152044876-07:00] INFO: > Set the default number of manager daemons to 1\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:342 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T02:43:42,162535437-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T02:43:42,166163337-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mgr', '1']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1\n'
10.10.2.1: b'Inferring fsid 53f3c108-37d3-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/53f3c108-37d3-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b'Scheduled mgr update...\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T02:43:50,997488518-07:00] STAGE: Add hosts to the cluster...\x1b[0m\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T02:43:51,003774925-07:00] INFO: > Adding host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:361 - add_hosts() > ssh-copy-id -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -f -i /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\n'
10.10.2.1: b'/usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: "/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub"\n'
10.10.2.1: b'\nNumber of key(s) added: 1\n\nNow try logging into the machine, with:   "ssh -o \'GlobalKnownHostsFile=/dev/null\' -o \'LogLevel=ERROR\' -o \'PasswordAuthentication=no\' -o \'StrictHostKeyChecking=no\' -o \'UserKnownHostsFile=/dev/null\' \'ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\'"\nand check to make sure that only the key(s) you wanted were added.\n\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:367 - add_hosts() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us 10.10.2.2\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T02:43:51,218402433-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T02:43:51,222200052-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'host', 'add', 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us', '10.10.2.2']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us 10.10.2.2\n'
10.10.2.1: b'Inferring fsid 53f3c108-37d3-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/53f3c108-37d3-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b"Added host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us' with addr '10.10.2.2'\n"
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T02:44:00,546932455-07:00] INFO: Sleep for 20 seconds\x1b[0m\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T02:44:20,551792813-07:00] STAGE: Add OSDs to the cluster...\x1b[0m\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T02:44:20,558712741-07:00] INFO: > Adding devices ['/dev/nvme0n1'] on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us' as OSDs\x1b[0m\n# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:388 - add_host_devices_as_osds() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1\n"
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T02:44:20,568449785-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T02:44:20,572006381-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'daemon', 'add', 'osd', 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1\n'
10.10.2.1: b'Inferring fsid 53f3c108-37d3-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/53f3c108-37d3-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b"Created osd(s) 0 on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'\n"
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T02:44:43,816877763-07:00] INFO: > Sleep for 20 seconds\x1b[0m\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T02:45:03,822576834-07:00] STAGE: Show Cluster Status\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:408 - show_status() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T02:45:03,832937140-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T02:45:03,836827283-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', '--status']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status\n'
10.10.2.1: b'Inferring fsid 53f3c108-37d3-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/53f3c108-37d3-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b'  cluster:\n    id:     53f3c108-37d3-11ec-b51d-53e6e728d2d3\n    health: HEALTH_WARN\n            1 pool(s) have no replicas configured\n \n  services:\n    mon: 1 daemons, quorum node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us (age 2m)\n    mgr: node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us.nbkcdu(active, since 2m)\n    osd: 1 osds: 1 up (since 19s), 1 in (since 39s)\n \n  data:\n    pools:   1 pools, 1 pgs\n    objects: 0 objects, 0 B\n    usage:   4.7 MiB used, 100 GiB / 100 GiB avail\n    pgs:     1 active+clean\n \n'
[1] 02:45:12 [SUCCESS] ljishen@10.10.2.1

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:42:01,445749055-07:00] INFO: > Remove existing clusters[0m
## bash:17 -  > basename -- /var/lib/ceph/13da2572-37d2-11ec-b51d-53e6e728d2d3
# bash:17 -  > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm --host 10.10.2.1 --host 10.10.2.2 rm-cluster --force --fsid 13da2572-37d2-11ec-b51d-53e6e728d2d3
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:42:01,456780472-07:00] DEBUG: HOST_PARAMS: ['--host', '10.10.2.1', '--host', '10.10.2.2'][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:42:01,460503170-07:00] DEBUG: CEPHADM_PARAMS: ['rm-cluster', '--force', '--fsid', '13da2572-37d2-11ec-b51d-53e6e728d2d3'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:63 -  > /tmp/bench-rados/ceph-research/scripts/parallel-ssh --host 10.10.2.1 --host 10.10.2.2 'sudo python3 -u - rm-cluster --force --fsid 13da2572-37d2-11ec-b51d-53e6e728d2d3'
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:42:01,468241016-07:00] DEBUG: command from stdin[0m
# /tmp/bench-rados/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host 10.10.2.1 --host 10.10.2.2 'sudo python3 -u - rm-cluster --force --fsid 13da2572-37d2-11ec-b51d-53e6e728d2d3'
[1] 02:42:07 [SUCCESS] 10.10.2.1
[2] 02:42:11 [SUCCESS] 10.10.2.2

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:42:11,587774929-07:00] INFO: > Deploy a new cluster[0m
# bash:24 -  > /tmp/bench-rados/ceph-research/scripts/ceph-deploy -m 10.10.2.1 -o node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/10.10.2.2:/dev/nvme0n1


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:42:11,600298732-07:00] STAGE: Verify passwordless SSH to hosts, passwordless sudo via ljishen, and the existence of devices on hosts...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:42:11,604704935-07:00] INFO: > Check the host for the monitor[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:181 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@10.10.2.1 sudo --non-interactive --validate

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:42:11,755693200-07:00] INFO: > Check hosts for OSDs[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:42:11,759699822-07:00] INFO: >> Check host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:200 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us sudo --non-interactive --validate

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:42:11,910876061-07:00] INFO: >>> Check device '/dev/nvme0n1'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:215 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us test -b /dev/nvme0n1
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:226 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us lsblk --nodeps --noheadings --output TYPE /dev/nvme0n1
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:227 - verify_parameters() > grep --word-regexp disk


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:42:12,199771631-07:00] STAGE: Reset devices on hosts...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:42:12,205210355-07:00] INFO: > Resetting device '/dev/nvme0n1' on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:288 - reset_host_devices() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us sudo sh
  Removing ceph--11d477a6--ab39--4ac4--90fe--4bda52647d94-osd--block--de284b0d--42f1--4d97--a923--c4d26752b9e6 (253:0)
  Archiving volume group "ceph-11d477a6-ab39-4ac4-90fe-4bda52647d94" metadata (seqno 5).
  Releasing logical volume "osd-block-de284b0d-42f1-4d97-a923-c4d26752b9e6"
  Creating volume group backup "/etc/lvm/backup/ceph-11d477a6-ab39-4ac4-90fe-4bda52647d94" (seqno 6).
  Logical volume "osd-block-de284b0d-42f1-4d97-a923-c4d26752b9e6" successfully removed
  Removing physical volume "/dev/nvme0n1" from volume group "ceph-11d477a6-ab39-4ac4-90fe-4bda52647d94"
  Volume group "ceph-11d477a6-ab39-4ac4-90fe-4bda52647d94" successfully removed


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:42:12,545943172-07:00] STAGE: Bootstrap a new cluster...[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:317 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.1 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:42:12,556192719-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:42:12,560121134-07:00] DEBUG: CEPHADM_PARAMS: ['bootstrap', '--config', '/tmp/bench-rados/ceph-research/scripts/ceph.conf', '--mon-ip', '10.10.2.1', '--output-dir', '/tmp/bench-rados/deployment_data_root/etc/ceph', '--ssh-user', 'ljishen', '--skip-dashboard', '--no-minimize-config', '--allow-overwrite', '--allow-fqdn-hostname', '--orphan-initial-daemons', '--skip-monitoring-stack', '--single-host-defaults'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.1 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults
Verifying podman|docker is present...
Verifying lvm2 is present...
Verifying time synchronization is in place...
Unit ntp.service is enabled and running
Repeating the final host check...
podman|docker (/usr/bin/docker) is present
systemctl is present
lvcreate is present
Unit ntp.service is enabled and running
Host looks OK
Cluster fsid: 53f3c108-37d3-11ec-b51d-53e6e728d2d3
Verifying IP 10.10.2.1 port 3300 ...
Verifying IP 10.10.2.1 port 6789 ...
Mon IP `10.10.2.1` is in CIDR network `10.10.2.0/24`
- internal network (--cluster-network) has not been provided, OSD replication will default to the public_network
Adjusting default settings to suit single-host cluster...
Pulling container image quay.io/ceph/ceph:v16...
Ceph version: ceph version 16.2.6 (ee28fb57e47e9f88813e24bbf4c14496ca299d31) pacific (stable)
Extracting ceph user uid/gid from container image...
Creating initial keys...
Creating initial monmap...
Creating mon...
Waiting for mon to start...
Waiting for mon...
mon is available
Setting mon public_network to 10.10.2.0/24
Wrote config to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf
Wrote keyring to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring
Creating mgr...
Verifying port 9283 ...
Waiting for mgr to start...
Waiting for mgr...
mgr not available, waiting (1/15)...
mgr not available, waiting (2/15)...
mgr is available
Enabling cephadm module...
Waiting for the mgr to restart...
Waiting for mgr epoch 5...
mgr epoch 5 is available
Setting orchestrator backend to cephadm...
Generating ssh key...
Wrote public SSH key to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub
Adding key to ljishen@localhost authorized_keys...
Adding host node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us...
Deploying unmanaged mon service...
Deploying unmanaged mgr service...
You can access the Ceph CLI with:

	sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --fsid 53f3c108-37d3-11ec-b51d-53e6e728d2d3 -c /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf -k /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring

Please consider enabling telemetry to help improve Ceph:

	ceph telemetry on

For more information see:

	https://docs.ceph.com/docs/pacific/mgr/telemetry/

Bootstrap complete.

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:43:13,493629027-07:00] INFO: Sleep for 20 seconds[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:43:33,501079434-07:00] INFO: > Set the default number of monitor daemons to 1[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:337 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:43:33,511557411-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:43:33,515438446-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mon', '1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1
Inferring fsid 53f3c108-37d3-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/53f3c108-37d3-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Scheduled mon update...

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:43:42,152044876-07:00] INFO: > Set the default number of manager daemons to 1[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:342 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:43:42,162535437-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:43:42,166163337-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mgr', '1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1
Inferring fsid 53f3c108-37d3-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/53f3c108-37d3-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Scheduled mgr update...


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:43:50,997488518-07:00] STAGE: Add hosts to the cluster...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:43:51,003774925-07:00] INFO: > Adding host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:361 - add_hosts() > ssh-copy-id -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -f -i /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us
/usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: "/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub"

Number of key(s) added: 1

Now try logging into the machine, with:   "ssh -o 'GlobalKnownHostsFile=/dev/null' -o 'LogLevel=ERROR' -o 'PasswordAuthentication=no' -o 'StrictHostKeyChecking=no' -o 'UserKnownHostsFile=/dev/null' 'ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'"
and check to make sure that only the key(s) you wanted were added.

# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:367 - add_hosts() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us 10.10.2.2
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:43:51,218402433-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:43:51,222200052-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'host', 'add', 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us', '10.10.2.2'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us 10.10.2.2
Inferring fsid 53f3c108-37d3-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/53f3c108-37d3-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Added host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us' with addr '10.10.2.2'

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:44:00,546932455-07:00] INFO: Sleep for 20 seconds[0m


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:44:20,551792813-07:00] STAGE: Add OSDs to the cluster...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:44:20,558712741-07:00] INFO: > Adding devices ['/dev/nvme0n1'] on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us' as OSDs[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:388 - add_host_devices_as_osds() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:44:20,568449785-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:44:20,572006381-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'daemon', 'add', 'osd', 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1
Inferring fsid 53f3c108-37d3-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/53f3c108-37d3-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Created osd(s) 0 on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:44:43,816877763-07:00] INFO: > Sleep for 20 seconds[0m


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:45:03,822576834-07:00] STAGE: Show Cluster Status[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:408 - show_status() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:45:03,832937140-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:45:03,836827283-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', '--status'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status
Inferring fsid 53f3c108-37d3-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/53f3c108-37d3-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
  cluster:
    id:     53f3c108-37d3-11ec-b51d-53e6e728d2d3
    health: HEALTH_WARN
            1 pool(s) have no replicas configured
 
  services:
    mon: 1 daemons, quorum node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us (age 2m)
    mgr: node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us.nbkcdu(active, since 2m)
    osd: 1 osds: 1 up (since 19s), 1 in (since 39s)
 
  data:
    pools:   1 pools, 1 pgs
    objects: 0 objects, 0 B
    usage:   4.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     1 active+clean
 

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:45:12,397967596-07:00] INFO: > Copy configuration files to local[0m
# ./benchmarks/bench-rados:220 - launch_ceph_cluster() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.1 sudo chown --changes --recursive ljishen /tmp/bench-rados/deployment_data_root/etc/ceph
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:45:12,405668678-07:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.1 sudo chown --changes --recursive ljishen /tmp/bench-rados/deployment_data_root/etc/ceph
[1] 02:45:12 [SUCCESS] ljishen@10.10.2.1
# ./benchmarks/bench-rados:223 - launch_ceph_cluster() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.1:/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf
# ./benchmarks/bench-rados:226 - launch_ceph_cluster() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.1:/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:45:12,880984369-07:00] STAGE: Update local ceph.conf as a client...[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:45:12,884203295-07:00] STAGE: Configure Ceph pool...[0m
# ./benchmarks/bench-rados:257 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_max_backfills 32
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:45:12,905870706-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:45:12,908684137-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '53f3c108-37d3-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'config', 'set', 'osd', 'osd_max_backfills', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 53f3c108-37d3-11ec-b51d-53e6e728d2d3 -- ceph config set osd osd_max_backfills 32
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:258 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_active 32
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:45:17,165091283-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:45:17,168033869-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '53f3c108-37d3-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_active', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 53f3c108-37d3-11ec-b51d-53e6e728d2d3 -- ceph config set osd osd_recovery_max_active 32
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:259 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_single_start 8
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:45:21,582293945-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:45:21,585393175-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '53f3c108-37d3-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_single_start', '8'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 53f3c108-37d3-11ec-b51d-53e6e728d2d3 -- ceph config set osd osd_recovery_max_single_start 8
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:260 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_op_priority 63
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:45:25,799409560-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:45:25,802384105-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '53f3c108-37d3-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_op_priority', '63'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 53f3c108-37d3-11ec-b51d-53e6e728d2d3 -- ceph config set osd osd_recovery_op_priority 63
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:262 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config show-with-defaults osd.0
# ./benchmarks/bench-rados:263 - configure_ceph_pool() > grep 'osd_max_backfills\|osd_recovery'
# ./benchmarks/bench-rados:263 - configure_ceph_pool() > column -t -s ' '
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
osd_max_backfills                        32         mon
osd_recovery_cost                        20971520   default
osd_recovery_delay_start                 0.000000   default
osd_recovery_max_active                  32         mon
osd_recovery_max_active_hdd              3          default
osd_recovery_max_active_ssd              10         default
osd_recovery_max_chunk                   8388608    default
osd_recovery_max_omap_entries_per_chunk  8096       default
osd_recovery_max_single_start            8          mon
osd_recovery_op_priority                 63         mon
osd_recovery_op_warn_multiple            16         default
osd_recovery_priority                    5          default
osd_recovery_retry_interval              30.000000  default
osd_recovery_sleep                       0.000000   default
osd_recovery_sleep_hdd                   0.100000   default
osd_recovery_sleep_hybrid                0.025000   default
osd_recovery_sleep_ssd                   0.000000   default
# ./benchmarks/bench-rados:265 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool create bench_rados 128 128 replicated --size 1 --pg-num-min 128
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:45:34,367827362-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:45:34,370777531-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '53f3c108-37d3-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'create', 'bench_rados', '128', '128', 'replicated', '--size', '1', '--pg-num-min', '128'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 53f3c108-37d3-11ec-b51d-53e6e728d2d3 -- ceph osd pool create bench_rados 128 128 replicated --size 1 --pg-num-min 128
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
pool 'bench_rados' created
# ./benchmarks/bench-rados:267 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados min_size 1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:45:38,768360230-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:45:38,771210862-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '53f3c108-37d3-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'min_size', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 53f3c108-37d3-11ec-b51d-53e6e728d2d3 -- ceph osd pool set bench_rados min_size 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 min_size to 1
# ./benchmarks/bench-rados:268 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados pg_autoscale_mode off
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:45:43,510711506-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:45:43,513713163-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '53f3c108-37d3-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'pg_autoscale_mode', 'off'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 53f3c108-37d3-11ec-b51d-53e6e728d2d3 -- ceph osd pool set bench_rados pg_autoscale_mode off
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 pg_autoscale_mode to off
# ./benchmarks/bench-rados:269 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool application enable bench_rados benchmark
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:45:47,975552899-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:45:47,978486006-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '53f3c108-37d3-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'application', 'enable', 'bench_rados', 'benchmark'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 53f3c108-37d3-11ec-b51d-53e6e728d2d3 -- ceph osd pool application enable bench_rados benchmark
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
enabled application 'benchmark' on pool 'bench_rados'
# ./benchmarks/bench-rados:270 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados noscrub 1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:45:52,644056546-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:45:52,647047261-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '53f3c108-37d3-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'noscrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 53f3c108-37d3-11ec-b51d-53e6e728d2d3 -- ceph osd pool set bench_rados noscrub 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 noscrub to 1
# ./benchmarks/bench-rados:271 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados nodeep-scrub 1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:45:57,330138817-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:45:57,333202821-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '53f3c108-37d3-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'nodeep-scrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 53f3c108-37d3-11ec-b51d-53e6e728d2d3 -- ceph osd pool set bench_rados nodeep-scrub 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 nodeep-scrub to 1
# ./benchmarks/bench-rados:272 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool ls detail
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:46:02,719443321-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:46:02,722326354-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '53f3c108-37d3-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'ls', 'detail'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 53f3c108-37d3-11ec-b51d-53e6e728d2d3 -- ceph osd pool ls detail
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
pool 1 'device_health_metrics' replicated size 1 min_size 1 crush_rule 0 object_hash rjenkins pg_num 64 pgp_num 64 autoscale_mode on last_change 20 lfor 0/0/17 flags hashpspool stripe_width 0 pg_num_min 1 application mgr_devicehealth
pool 2 'bench_rados' replicated size 1 min_size 1 crush_rule 0 object_hash rjenkins pg_num 128 pgp_num 128 autoscale_mode off last_change 21 flags hashpspool,noscrub,nodeep-scrub stripe_width 0 pg_num_min 128 application benchmark

# ./benchmarks/bench-rados:273 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd df tree
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:46:06,778439262-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:46:06,781485643-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '53f3c108-37d3-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'df', 'tree'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 53f3c108-37d3-11ec-b51d-53e6e728d2d3 -- ceph osd df tree
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
ID  CLASS  WEIGHT   REWEIGHT  SIZE     RAW USE  DATA     OMAP  META     AVAIL    %USE  VAR   PGS  STATUS  TYPE NAME      
-1         0.09769         -  100 GiB  5.7 MiB  176 KiB   0 B  5.6 MiB  100 GiB  0.01  1.00    -          root default   
-3         0.09769         -  100 GiB  5.7 MiB  176 KiB   0 B  5.6 MiB  100 GiB  0.01  1.00    -              host node-1
 0    ssd  0.09769   1.00000  100 GiB  5.7 MiB  176 KiB   0 B  5.6 MiB  100 GiB  0.01  1.00  192      up          osd.0  
                       TOTAL  100 GiB  5.7 MiB  176 KiB   0 B  5.6 MiB  100 GiB  0.01                                    
MIN/MAX VAR: 1.00/1.00  STDDEV: 0


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:46:11,080666772-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:46:35,430682666-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:46:44,747143509-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:46:53,995334614-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:47:03,343240182-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:47:03,351468067-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:47:12,616019541-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:47:12,623540995-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:47:21,989771834-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:47:21,997293268-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:47:31,256765660-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:47:31,264609020-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:47:40,563642773-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:47:40,571036125-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:47:40,576744161-07:00] INFO: > The cluster is idle now.[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:47:40,580440176-07:00] STAGE: Start benchmarking of mode write ...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:47:40,587075309-07:00] INFO: > Start system activity collection services[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:47:40,592760212-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:341 - rados_bench() > rm --force /mnt/sda8/ceph-research/scripts/output/sys_activity_256KB_write.dat.rados_bench_host.1
# ./benchmarks/bench-rados:342 - rados_bench() > sar_pid_rados_bench_host=275343
# ./benchmarks/bench-rados:342 - rados_bench() > S_TIME_FORMAT=ISO
# ./benchmarks/bench-rados:342 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/output/sys_activity_256KB_write.dat.rados_bench_host.1 2

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:47:40,599724726-07:00] INFO: >> for OSD host[0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:47:40,608507787-07:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.2 bash
10.10.2.2: b'160369\n'
[1] 02:47:40 [SUCCESS] ljishen@10.10.2.2
160369

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:47:40,802692832-07:00] INFO: > Run rados bench[0m
# ./benchmarks/bench-rados:370 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/output/rados_bench_256KB_write.log.1
## ./benchmarks/bench-rados:361 - rados_bench() > count_cpus 32-63,96-127
# ./benchmarks/bench-rados:361 - rados_bench() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 60 write --pool bench_rados -b 262144 -O 262144 --concurrent-ios 128 --show-time --write-object --write-omap --write-xattr --no-cleanup
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:47:40,823022401-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:47:40,825853547-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '53f3c108-37d3-11ec-b51d-53e6e728d2d3', '--', 'rados', 'bench', '60', 'write', '--pool', 'bench_rados', '-b', '262144', '-O', '262144', '--concurrent-ios', '128', '--show-time', '--write-object', '--write-omap', '--write-xattr', '--no-cleanup'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 53f3c108-37d3-11ec-b51d-53e6e728d2d3 -- rados bench 60 write --pool bench_rados -b 262144 -O 262144 --concurrent-ios 128 --show-time --write-object --write-omap --write-xattr --no-cleanup
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
hints = 1
2021-10-28T09:47:44.303630+0000 Maintaining 128 concurrent writes of 262144 bytes to objects of size 262144 for up to 60 seconds or 0 objects
2021-10-28T09:47:44.303642+0000 Object prefix: benchmark_data_node-2.bluefield2.ucsc-cmps10_7
2021-10-28T09:47:44.312671+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-28T09:47:44.312671+0000     0       0         0         0         0         0           -           0
2021-10-28T09:47:45.312779+0000     1     128      1334      1206   301.483     301.5   0.0802143     0.10166
2021-10-28T09:47:46.312882+0000     2     128      2340      2212   276.478     251.5    0.164032    0.108179
2021-10-28T09:47:47.312950+0000     3     128      2984      2856   237.982       161    0.228411    0.128317
2021-10-28T09:47:48.313036+0000     4     128      3880      3752   234.482       224    0.119043    0.133294
2021-10-28T09:47:49.313105+0000     5     128      4547      4419   220.933    166.75    0.318837     0.14127
2021-10-28T09:47:50.313192+0000     6     128      4921      4793   199.693      93.5    0.327771     0.15611
2021-10-28T09:47:51.313264+0000     7     128      5289      5161   184.307        92    0.302357    0.168202
2021-10-28T09:47:52.313387+0000     8     128      5880      5752   179.735    147.75    0.182284    0.175119
2021-10-28T09:47:53.313465+0000     9     128      6417      6289    174.68    134.25     0.30143    0.180353
2021-10-28T09:47:54.313530+0000    10     128      6822      6694   167.336    101.25    0.214491    0.188605
2021-10-28T09:47:55.313606+0000    11     128      7436      7308   166.078     153.5    0.157449    0.191798
2021-10-28T09:47:56.313718+0000    12     128      8080      7952   165.653       161    0.137453    0.191879
2021-10-28T09:47:57.313785+0000    13     128      8648      8520   163.833       142   0.0884919    0.192831
2021-10-28T09:47:58.313900+0000    14     128      8999      8871   158.397     87.75    0.494922    0.198035
2021-10-28T09:47:59.313969+0000    15     128      9431      9303   155.037       108    0.308611    0.203176
2021-10-28T09:48:00.314061+0000    16     128     10024      9896   154.612    148.25     0.19636     0.20504
2021-10-28T09:48:01.314136+0000    17     128     10522     10394    152.84     124.5    0.321648    0.206998
2021-10-28T09:48:02.314254+0000    18     128     10888     10760   149.432      91.5    0.307612    0.211069
2021-10-28T09:48:03.314323+0000    19     128     11288     11160    146.83       100   0.0794561    0.216064
2021-10-28T09:48:04.314391+0000 min lat: 0.00762992 max lat: 0.624534 avg lat: 0.218359
2021-10-28T09:48:04.314391+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-28T09:48:04.314391+0000    20     128     11807     11679   145.975    129.75    0.138363    0.218359
2021-10-28T09:48:05.314471+0000    21     128     12393     12265       146     146.5    0.153754    0.218122
2021-10-28T09:48:06.314588+0000    22     128     12726     12598   143.147     83.25    0.117498     0.22072
2021-10-28T09:48:07.314655+0000    23     128     13132     13004   141.336     101.5     0.34291    0.224829
2021-10-28T09:48:08.314764+0000    24     128     13552     13424   139.821       105    0.295758     0.22792
2021-10-28T09:48:09.314828+0000    25     128     14139     14011   140.098    146.75    0.140887    0.227619
2021-10-28T09:48:10.314919+0000    26     128     14633     14505   139.459     123.5    0.315167    0.228694
2021-10-28T09:48:11.314988+0000    27     128     14982     14854   137.525     87.25   0.0514876    0.230966
2021-10-28T09:48:12.315062+0000    28     128     15427     15299   136.587    111.25    0.341691    0.233343
2021-10-28T09:48:13.315130+0000    29     128     15930     15802   136.213    125.75     0.27318    0.233742
2021-10-28T09:48:14.315199+0000    30     128     16553     16425   136.864    155.75    0.238662    0.232662
2021-10-28T09:48:15.315272+0000    31     128     16884     16756   135.118     82.75    0.230311    0.234333
2021-10-28T09:48:16.315383+0000    32     128     17315     17187   134.262    107.75   0.0688087     0.23657
2021-10-28T09:48:17.315453+0000    33     128     17863     17735   134.345       137    0.153187    0.237839
2021-10-28T09:48:18.315574+0000    34     128     18467     18339   134.834       151   0.0386655    0.236532
2021-10-28T09:48:19.315646+0000    35     128     18908     18780   134.132    110.25    0.270743    0.238163
2021-10-28T09:48:20.315740+0000    36     128     19249     19121   132.774     85.25    0.334286    0.239729
2021-10-28T09:48:21.315819+0000    37     128     19697     19569   132.212       112    0.225097    0.241381
2021-10-28T09:48:22.315922+0000    38     127     20375     20248   133.199    169.75    0.157674    0.239895
2021-10-28T09:48:23.316037+0000    39     128     20837     20709   132.739    115.25    0.307733    0.240409
2021-10-28T09:48:24.316103+0000 min lat: 0.00762992 max lat: 0.624534 avg lat: 0.24179
2021-10-28T09:48:24.316103+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-28T09:48:24.316103+0000    40     128     21258     21130   132.051    105.25    0.325052     0.24179
2021-10-28T09:48:25.316181+0000    41     128     21632     21504   131.111      93.5   0.0668905    0.243162
2021-10-28T09:48:26.316294+0000    42     128     22222     22094   131.501     147.5     0.21054     0.24259
2021-10-28T09:48:27.316409+0000    43     128     22719     22591   131.332    124.25   0.0951177    0.242286
2021-10-28T09:48:28.316524+0000    44     128     23156     23028    130.83    109.25    0.255415    0.243736
2021-10-28T09:48:29.316589+0000    45     128     23545     23417   130.083     97.25   0.0815312    0.244986
2021-10-28T09:48:30.316678+0000    46     128     24081     23953   130.168       134    0.141397    0.244992
2021-10-28T09:48:31.316763+0000    47     128     24692     24564   130.648    152.75   0.0747932    0.244134
2021-10-28T09:48:32.316877+0000    48     128     25125     24997   130.181    108.25    0.268062    0.245328
2021-10-28T09:48:33.316954+0000    49     128     25477     25349    129.32        88    0.251748    0.246231
2021-10-28T09:48:34.317025+0000    50     128     26017     25889   129.434       135    0.153235    0.246857
2021-10-28T09:48:35.317138+0000    51     128     26667     26539   130.082     162.5    0.211346    0.245506
2021-10-28T09:48:36.317262+0000    52     128     27021     26893   129.282      88.5    0.056877    0.246501
2021-10-28T09:48:37.317343+0000    53     128     27464     27336   128.932    110.75    0.272581    0.247535
2021-10-28T09:48:38.317461+0000    54     127     27885     27758   128.498     105.5    0.168581    0.248517
2021-10-28T09:48:39.317534+0000    55     128     28558     28430   129.216       168    0.142524    0.247264
2021-10-28T09:48:40.317604+0000    56     128     29013     28885    128.94    113.75    0.307975    0.247557
2021-10-28T09:48:41.317677+0000    57     128     29394     29266   128.348     95.25    0.317489    0.248578
2021-10-28T09:48:42.317758+0000    58     128     29830     29702   128.015       109    0.321807    0.249567
2021-10-28T09:48:43.317873+0000    59     128     30438     30310   128.421       152    0.220437    0.248883
2021-10-28T09:48:44.317940+0000 min lat: 0.00762992 max lat: 0.624534 avg lat: 0.248518
2021-10-28T09:48:44.317940+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-28T09:48:44.317940+0000    60     128     30899     30771   128.201    115.25    0.305897    0.248518
2021-10-28T09:48:45.318062+0000 Total time run:         60.1366
Total writes made:      30899
Write size:             262144
Object size:            262144
Bandwidth (MB/sec):     128.453
Stddev Bandwidth:       40.1139
Max bandwidth (MB/sec): 301.5
Min bandwidth (MB/sec): 82.75
Average IOPS:           513
Stddev IOPS:            160.456
Max IOPS:               1206
Min IOPS:               331
Average Latency(s):     0.248809
Stddev Latency(s):      0.0989439
Max latency(s):         0.624534
Min latency(s):         0.00762992

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:48:46,058244892-07:00] INFO: > Stop system activity collection services[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:48:46,064001540-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:391 - rados_bench() > kill -INT 275343

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:48:46,069582447-07:00] INFO: >> for OSD host[0m
# ./benchmarks/bench-rados:396 - rados_bench() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.2 kill -INT 160369
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:48:46,077459801-07:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.2 kill -INT 160369
[1] 02:48:46 [SUCCESS] ljishen@10.10.2.2

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:48:46,265815451-07:00] INFO: >>> Copy system activity file from OSD host[0m
## ./benchmarks/bench-rados:403 - rados_bench() > basename -- /tmp/bench-rados/sys_activity_256KB_write.dat.osd_host.1
# ./benchmarks/bench-rados:403 - rados_bench() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.2:/tmp/bench-rados/sys_activity_256KB_write.dat.osd_host.1 /mnt/sda8/ceph-research/scripts/output/sys_activity_256KB_write.dat.osd_host.1


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:48:46,439669363-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:49:10,723448008-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 30.90k objects, 7.5 GiB
    usage:   23 GiB used, 77 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:49:10,731250060-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:49:19,951065079-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 30.90k objects, 7.5 GiB
    usage:   23 GiB used, 77 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:49:19,958823339-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:49:29,292096590-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 30.90k objects, 7.5 GiB
    usage:   23 GiB used, 77 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:49:29,299632311-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:49:38,647680514-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 30.90k objects, 7.5 GiB
    usage:   23 GiB used, 77 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:49:38,655230490-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:49:47,988243140-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 30.90k objects, 7.5 GiB
    usage:   23 GiB used, 77 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:49:47,995910729-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:49:48,002285001-07:00] INFO: > The cluster is idle now.[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:49:48,006147139-07:00] STAGE: Start benchmarking of mode seq ...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:49:48,012737297-07:00] INFO: > Start system activity collection services[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:49:48,018032275-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:341 - rados_bench() > rm --force /mnt/sda8/ceph-research/scripts/output/sys_activity_256KB_seq.dat.rados_bench_host.1
# ./benchmarks/bench-rados:342 - rados_bench() > sar_pid_rados_bench_host=276700
# ./benchmarks/bench-rados:342 - rados_bench() > S_TIME_FORMAT=ISO
# ./benchmarks/bench-rados:342 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/output/sys_activity_256KB_seq.dat.rados_bench_host.1 2

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:49:48,024969288-07:00] INFO: >> for OSD host[0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:49:48,033864670-07:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.2 bash
10.10.2.2: b'160461\n'
[1] 02:49:48 [SUCCESS] ljishen@10.10.2.2
160461

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:49:48,226451634-07:00] INFO: > Run rados bench[0m
# ./benchmarks/bench-rados:379 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/output/rados_bench_256KB_seq.log.1
## ./benchmarks/bench-rados:376 - rados_bench() > count_cpus 32-63,96-127
# ./benchmarks/bench-rados:376 - rados_bench() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 128 --show-time
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:49:48,246311347-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:49:48,249165205-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '53f3c108-37d3-11ec-b51d-53e6e728d2d3', '--', 'rados', 'bench', '99999999', 'seq', '--pool', 'bench_rados', '--concurrent-ios', '128', '--show-time'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 53f3c108-37d3-11ec-b51d-53e6e728d2d3 -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 128 --show-time
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
hints = 1
2021-10-28T09:49:51.463416+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-28T09:49:51.463416+0000     0       0         0         0         0         0           -           0
2021-10-28T09:49:52.463536+0000     1     128      8434      8306   2076.11    2076.5   0.0145569   0.0152589
2021-10-28T09:49:53.463640+0000     2     127     16757     16630   2078.45      2081   0.0143982   0.0153056
2021-10-28T09:49:54.463805+0000     3     127     25098     24971    2080.6   2085.25   0.0149598   0.0153074
2021-10-28T09:49:55.463922+0000 Total time run:       3.71666
Total reads made:     30899
Read size:            262144
Object size:          262144
Bandwidth (MB/sec):   2078.41
Average IOPS:         8313
Stddev IOPS:          17.5024
Max IOPS:             8341
Min IOPS:             8306
Average Latency(s):   0.0153358
Max latency(s):       0.0260014
Min latency(s):       0.00213244

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:49:56,261413869-07:00] INFO: > Stop system activity collection services[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:49:56,267735321-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:391 - rados_bench() > kill -INT 276700

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:49:56,273989236-07:00] INFO: >> for OSD host[0m
# ./benchmarks/bench-rados:396 - rados_bench() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.2 kill -INT 160461
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:49:56,281906505-07:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.2 kill -INT 160461
[1] 02:49:56 [SUCCESS] ljishen@10.10.2.2

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:49:56,473896874-07:00] INFO: >>> Copy system activity file from OSD host[0m
## ./benchmarks/bench-rados:403 - rados_bench() > basename -- /tmp/bench-rados/sys_activity_256KB_seq.dat.osd_host.1
# ./benchmarks/bench-rados:403 - rados_bench() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.2:/tmp/bench-rados/sys_activity_256KB_seq.dat.osd_host.1 /mnt/sda8/ceph-research/scripts/output/sys_activity_256KB_seq.dat.osd_host.1


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:49:56,629932480-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:50:20,854889987-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 30.90k objects, 7.5 GiB
    usage:   23 GiB used, 77 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:50:20,862512261-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:50:30,136746897-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 30.90k objects, 7.5 GiB
    usage:   23 GiB used, 77 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:50:30,144650300-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:50:39,417308936-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 30.90k objects, 7.5 GiB
    usage:   23 GiB used, 77 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:50:39,424788531-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:50:48,758134247-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 30.90k objects, 7.5 GiB
    usage:   23 GiB used, 77 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:50:48,766160301-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:50:58,005458652-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 30.90k objects, 7.5 GiB
    usage:   23 GiB used, 77 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:50:58,013219838-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:50:58,018816774-07:00] INFO: > The cluster is idle now.[0m


[1;7;39;49m[2021-10-28T02:50:58,021176341-07:00][RUNNING][ROUND 2/4/21] object_size=256KB[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:50:58,024910939-07:00] STAGE: Launch a Ceph cluster (CEPH_HOST: 10.10.2.1, OSD_HOST: 10.10.2.2) ...[0m
# ./benchmarks/bench-rados:183 - launch_ceph_cluster() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.1 bash -euo pipefail
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:50:58,034045522-07:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.1 bash -euo pipefail
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T02:50:58,529547335-07:00] INFO: > Remove existing clusters\x1b[0m\n'
10.10.2.1: b'## bash:17 -  > basename -- /var/lib/ceph/53f3c108-37d3-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'# bash:17 -  > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm --host 10.10.2.1 --host 10.10.2.2 rm-cluster --force --fsid 53f3c108-37d3-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T02:50:58,540607006-07:00] DEBUG: HOST_PARAMS: ['--host', '10.10.2.1', '--host', '10.10.2.2']\x1b[0m\n"
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T02:50:58,544462003-07:00] DEBUG: CEPHADM_PARAMS: ['rm-cluster', '--force', '--fsid', '53f3c108-37d3-11ec-b51d-53e6e728d2d3']\x1b[0m\n"
10.10.2.1: b"# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:63 -  > /tmp/bench-rados/ceph-research/scripts/parallel-ssh --host 10.10.2.1 --host 10.10.2.2 'sudo python3 -u - rm-cluster --force --fsid 53f3c108-37d3-11ec-b51d-53e6e728d2d3'\n"
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T02:50:58,552897831-07:00] DEBUG: command from stdin\x1b[0m\n'
10.10.2.1: b"# /tmp/bench-rados/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host 10.10.2.1 --host 10.10.2.2 'sudo python3 -u - rm-cluster --force --fsid 53f3c108-37d3-11ec-b51d-53e6e728d2d3'\n"
10.10.2.1: b'[1] 02:51:04 [SUCCESS] 10.10.2.1\n[2] 02:51:10 [SUCCESS] 10.10.2.2\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T02:51:10,145813220-07:00] INFO: > Deploy a new cluster\x1b[0m\n'
10.10.2.1: b'# bash:24 -  > /tmp/bench-rados/ceph-research/scripts/ceph-deploy -m 10.10.2.1 -o node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/10.10.2.2:/dev/nvme0n1\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T02:51:10,158024967-07:00] STAGE: Verify passwordless SSH to hosts, passwordless sudo via ljishen, and the existence of devices on hosts...\x1b[0m\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T02:51:10,163196188-07:00] INFO: > Check the host for the monitor\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:181 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@10.10.2.1 sudo --non-interactive --validate\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T02:51:10,311354525-07:00] INFO: > Check hosts for OSDs\x1b[0m\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T02:51:10,316750238-07:00] INFO: >> Check host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:200 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us sudo --non-interactive --validate\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T02:51:10,467020525-07:00] INFO: >>> Check device '/dev/nvme0n1'\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:215 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us test -b /dev/nvme0n1\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:226 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us lsblk --nodeps --noheadings --output TYPE /dev/nvme0n1\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:227 - verify_parameters() > grep --word-regexp disk\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T02:51:10,759648475-07:00] STAGE: Reset devices on hosts...\x1b[0m\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T02:51:10,764709289-07:00] INFO: > Resetting device '/dev/nvme0n1' on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:288 - reset_host_devices() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us sudo sh\n'
10.10.2.1: b'  Removing ceph--d995e478--5608--41ba--ac5b--33d028c0e81d-osd--block--6dcd04b2--9637--4ca4--8a83--4faa072c71f6 (253:0)\n'
10.10.2.1: b'  Archiving volume group "ceph-d995e478-5608-41ba-ac5b-33d028c0e81d" metadata (seqno 5).\n'
10.10.2.1: b'  Releasing logical volume "osd-block-6dcd04b2-9637-4ca4-8a83-4faa072c71f6"\n'
10.10.2.1: b'  Creating volume group backup "/etc/lvm/backup/ceph-d995e478-5608-41ba-ac5b-33d028c0e81d" (seqno 6).\n'
10.10.2.1: b'  Logical volume "osd-block-6dcd04b2-9637-4ca4-8a83-4faa072c71f6" successfully removed\n'
10.10.2.1: b'  Removing physical volume "/dev/nvme0n1" from volume group "ceph-d995e478-5608-41ba-ac5b-33d028c0e81d"\n'
10.10.2.1: b'  Volume group "ceph-d995e478-5608-41ba-ac5b-33d028c0e81d" successfully removed\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T02:51:11,057982241-07:00] STAGE: Bootstrap a new cluster...\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:317 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.1 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T02:51:11,067913951-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T02:51:11,071413280-07:00] DEBUG: CEPHADM_PARAMS: ['bootstrap', '--config', '/tmp/bench-rados/ceph-research/scripts/ceph.conf', '--mon-ip', '10.10.2.1', '--output-dir', '/tmp/bench-rados/deployment_data_root/etc/ceph', '--ssh-user', 'ljishen', '--skip-dashboard', '--no-minimize-config', '--allow-overwrite', '--allow-fqdn-hostname', '--orphan-initial-daemons', '--skip-monitoring-stack', '--single-host-defaults']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.1 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults\n'
10.10.2.1: b'Verifying podman|docker is present...\n'
10.10.2.1: b'Verifying lvm2 is present...\n'
10.10.2.1: b'Verifying time synchronization is in place...\n'
10.10.2.1: b'Unit ntp.service is enabled and running\n'
10.10.2.1: b'Repeating the final host check...\n'
10.10.2.1: b'podman|docker (/usr/bin/docker) is present\n'
10.10.2.1: b'systemctl is present\n'
10.10.2.1: b'lvcreate is present\n'
10.10.2.1: b'Unit ntp.service is enabled and running\n'
10.10.2.1: b'Host looks OK\n'
10.10.2.1: b'Cluster fsid: 94ee3b2e-37d4-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Verifying IP 10.10.2.1 port 3300 ...\n'
10.10.2.1: b'Verifying IP 10.10.2.1 port 6789 ...\n'
10.10.2.1: b'Mon IP `10.10.2.1` is in CIDR network `10.10.2.0/24`\n'
10.10.2.1: b'- internal network (--cluster-network) has not been provided, OSD replication will default to the public_network\n'
10.10.2.1: b'Adjusting default settings to suit single-host cluster...\n'
10.10.2.1: b'Pulling container image quay.io/ceph/ceph:v16...\n'
10.10.2.1: b'Ceph version: ceph version 16.2.6 (ee28fb57e47e9f88813e24bbf4c14496ca299d31) pacific (stable)\n'
10.10.2.1: b'Extracting ceph user uid/gid from container image...\n'
10.10.2.1: b'Creating initial keys...\n'
10.10.2.1: b'Creating initial monmap...\n'
10.10.2.1: b'Creating mon...\n'
10.10.2.1: b'Waiting for mon to start...\n'
10.10.2.1: b'Waiting for mon...\n'
10.10.2.1: b'mon is available\n'
10.10.2.1: b'Setting mon public_network to 10.10.2.0/24\n'
10.10.2.1: b'Wrote config to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf\n'
10.10.2.1: b'Wrote keyring to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring\n'
10.10.2.1: b'Creating mgr...\n'
10.10.2.1: b'Verifying port 9283 ...\n'
10.10.2.1: b'Waiting for mgr to start...\n'
10.10.2.1: b'Waiting for mgr...\n'
10.10.2.1: b'mgr not available, waiting (1/15)...\n'
10.10.2.1: b'mgr not available, waiting (2/15)...\n'
10.10.2.1: b'mgr is available\n'
10.10.2.1: b'Enabling cephadm module...\n'
10.10.2.1: b'Waiting for the mgr to restart...\n'
10.10.2.1: b'Waiting for mgr epoch 5...\n'
10.10.2.1: b'mgr epoch 5 is available\nSetting orchestrator backend to cephadm...\n'
10.10.2.1: b'Generating ssh key...\n'
10.10.2.1: b'Wrote public SSH key to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub\n'
10.10.2.1: b'Adding key to ljishen@localhost authorized_keys...\n'
10.10.2.1: b'Adding host node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us...\n'
10.10.2.1: b'Deploying unmanaged mon service...\n'
10.10.2.1: b'Deploying unmanaged mgr service...\n'
10.10.2.1: b'You can access the Ceph CLI with:\n\n\tsudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --fsid 94ee3b2e-37d4-11ec-b51d-53e6e728d2d3 -c /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf -k /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring\n\n'
10.10.2.1: b'Please consider enabling telemetry to help improve Ceph:\n\n\tceph telemetry on\n\nFor more information see:\n\n\thttps://docs.ceph.com/docs/pacific/mgr/telemetry/\n\n'
10.10.2.1: b'Bootstrap complete.\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T02:52:09,353874322-07:00] INFO: Sleep for 20 seconds\x1b[0m\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T02:52:29,360863738-07:00] INFO: > Set the default number of monitor daemons to 1\x1b[0m\n# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:337 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T02:52:29,370788615-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T02:52:29,374360259-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mon', '1']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1\n'
10.10.2.1: b'Inferring fsid 94ee3b2e-37d4-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/94ee3b2e-37d4-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b'Scheduled mon update...\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T02:52:37,999963243-07:00] INFO: > Set the default number of manager daemons to 1\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:342 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T02:52:38,010085000-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T02:52:38,014101570-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mgr', '1']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1\n'
10.10.2.1: b'Inferring fsid 94ee3b2e-37d4-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/94ee3b2e-37d4-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b'Scheduled mgr update...\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T02:52:47,084370483-07:00] STAGE: Add hosts to the cluster...\x1b[0m\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T02:52:47,090454340-07:00] INFO: > Adding host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:361 - add_hosts() > ssh-copy-id -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -f -i /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\n'
10.10.2.1: b'/usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: "/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub"\n'
10.10.2.1: b'\nNumber of key(s) added: 1\n\nNow try logging into the machine, with:   "ssh -o \'GlobalKnownHostsFile=/dev/null\' -o \'LogLevel=ERROR\' -o \'PasswordAuthentication=no\' -o \'StrictHostKeyChecking=no\' -o \'UserKnownHostsFile=/dev/null\' \'ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\'"\nand check to make sure that only the key(s) you wanted were added.\n\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:367 - add_hosts() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us 10.10.2.2\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T02:52:47,305459570-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T02:52:47,309151481-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'host', 'add', 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us', '10.10.2.2']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us 10.10.2.2\n'
10.10.2.1: b'Inferring fsid 94ee3b2e-37d4-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/94ee3b2e-37d4-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b"Added host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us' with addr '10.10.2.2'\n"
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T02:52:56,630200565-07:00] INFO: Sleep for 20 seconds\x1b[0m\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T02:53:16,635160805-07:00] STAGE: Add OSDs to the cluster...\x1b[0m\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T02:53:16,641820975-07:00] INFO: > Adding devices ['/dev/nvme0n1'] on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us' as OSDs\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:388 - add_host_devices_as_osds() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T02:53:16,652488669-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T02:53:16,656491654-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'daemon', 'add', 'osd', 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1\n'
10.10.2.1: b'Inferring fsid 94ee3b2e-37d4-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/94ee3b2e-37d4-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b"Created osd(s) 0 on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'\n"
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T02:53:38,964483902-07:00] INFO: > Sleep for 20 seconds\x1b[0m\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T02:53:58,969876630-07:00] STAGE: Show Cluster Status\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:408 - show_status() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T02:53:58,980323760-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T02:53:58,984192843-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', '--status']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status\n'
10.10.2.1: b'Inferring fsid 94ee3b2e-37d4-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/94ee3b2e-37d4-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b'  cluster:\n    id:     94ee3b2e-37d4-11ec-b51d-53e6e728d2d3\n    health: HEALTH_WARN\n            1 pool(s) have no replicas configured\n \n  services:\n    mon: 1 daemons, quorum node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us (age 2m)\n    mgr: node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us.ltgyeb(active, since 2m)\n    osd: 1 osds: 1 up (since 18s), 1 in (since 38s)\n \n  data:\n    pools:   1 pools, 1 pgs\n    objects: 0 objects, 0 B\n    usage:   4.7 MiB used, 100 GiB / 100 GiB avail\n    pgs:     1 active+clean\n \n'
[1] 02:54:07 [SUCCESS] ljishen@10.10.2.1

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:50:58,529547335-07:00] INFO: > Remove existing clusters[0m
## bash:17 -  > basename -- /var/lib/ceph/53f3c108-37d3-11ec-b51d-53e6e728d2d3
# bash:17 -  > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm --host 10.10.2.1 --host 10.10.2.2 rm-cluster --force --fsid 53f3c108-37d3-11ec-b51d-53e6e728d2d3
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:50:58,540607006-07:00] DEBUG: HOST_PARAMS: ['--host', '10.10.2.1', '--host', '10.10.2.2'][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:50:58,544462003-07:00] DEBUG: CEPHADM_PARAMS: ['rm-cluster', '--force', '--fsid', '53f3c108-37d3-11ec-b51d-53e6e728d2d3'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:63 -  > /tmp/bench-rados/ceph-research/scripts/parallel-ssh --host 10.10.2.1 --host 10.10.2.2 'sudo python3 -u - rm-cluster --force --fsid 53f3c108-37d3-11ec-b51d-53e6e728d2d3'
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:50:58,552897831-07:00] DEBUG: command from stdin[0m
# /tmp/bench-rados/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host 10.10.2.1 --host 10.10.2.2 'sudo python3 -u - rm-cluster --force --fsid 53f3c108-37d3-11ec-b51d-53e6e728d2d3'
[1] 02:51:04 [SUCCESS] 10.10.2.1
[2] 02:51:10 [SUCCESS] 10.10.2.2

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:51:10,145813220-07:00] INFO: > Deploy a new cluster[0m
# bash:24 -  > /tmp/bench-rados/ceph-research/scripts/ceph-deploy -m 10.10.2.1 -o node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/10.10.2.2:/dev/nvme0n1


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:51:10,158024967-07:00] STAGE: Verify passwordless SSH to hosts, passwordless sudo via ljishen, and the existence of devices on hosts...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:51:10,163196188-07:00] INFO: > Check the host for the monitor[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:181 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@10.10.2.1 sudo --non-interactive --validate

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:51:10,311354525-07:00] INFO: > Check hosts for OSDs[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:51:10,316750238-07:00] INFO: >> Check host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:200 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us sudo --non-interactive --validate

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:51:10,467020525-07:00] INFO: >>> Check device '/dev/nvme0n1'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:215 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us test -b /dev/nvme0n1
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:226 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us lsblk --nodeps --noheadings --output TYPE /dev/nvme0n1
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:227 - verify_parameters() > grep --word-regexp disk


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:51:10,759648475-07:00] STAGE: Reset devices on hosts...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:51:10,764709289-07:00] INFO: > Resetting device '/dev/nvme0n1' on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:288 - reset_host_devices() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us sudo sh
  Removing ceph--d995e478--5608--41ba--ac5b--33d028c0e81d-osd--block--6dcd04b2--9637--4ca4--8a83--4faa072c71f6 (253:0)
  Archiving volume group "ceph-d995e478-5608-41ba-ac5b-33d028c0e81d" metadata (seqno 5).
  Releasing logical volume "osd-block-6dcd04b2-9637-4ca4-8a83-4faa072c71f6"
  Creating volume group backup "/etc/lvm/backup/ceph-d995e478-5608-41ba-ac5b-33d028c0e81d" (seqno 6).
  Logical volume "osd-block-6dcd04b2-9637-4ca4-8a83-4faa072c71f6" successfully removed
  Removing physical volume "/dev/nvme0n1" from volume group "ceph-d995e478-5608-41ba-ac5b-33d028c0e81d"
  Volume group "ceph-d995e478-5608-41ba-ac5b-33d028c0e81d" successfully removed


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:51:11,057982241-07:00] STAGE: Bootstrap a new cluster...[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:317 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.1 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:51:11,067913951-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:51:11,071413280-07:00] DEBUG: CEPHADM_PARAMS: ['bootstrap', '--config', '/tmp/bench-rados/ceph-research/scripts/ceph.conf', '--mon-ip', '10.10.2.1', '--output-dir', '/tmp/bench-rados/deployment_data_root/etc/ceph', '--ssh-user', 'ljishen', '--skip-dashboard', '--no-minimize-config', '--allow-overwrite', '--allow-fqdn-hostname', '--orphan-initial-daemons', '--skip-monitoring-stack', '--single-host-defaults'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.1 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults
Verifying podman|docker is present...
Verifying lvm2 is present...
Verifying time synchronization is in place...
Unit ntp.service is enabled and running
Repeating the final host check...
podman|docker (/usr/bin/docker) is present
systemctl is present
lvcreate is present
Unit ntp.service is enabled and running
Host looks OK
Cluster fsid: 94ee3b2e-37d4-11ec-b51d-53e6e728d2d3
Verifying IP 10.10.2.1 port 3300 ...
Verifying IP 10.10.2.1 port 6789 ...
Mon IP `10.10.2.1` is in CIDR network `10.10.2.0/24`
- internal network (--cluster-network) has not been provided, OSD replication will default to the public_network
Adjusting default settings to suit single-host cluster...
Pulling container image quay.io/ceph/ceph:v16...
Ceph version: ceph version 16.2.6 (ee28fb57e47e9f88813e24bbf4c14496ca299d31) pacific (stable)
Extracting ceph user uid/gid from container image...
Creating initial keys...
Creating initial monmap...
Creating mon...
Waiting for mon to start...
Waiting for mon...
mon is available
Setting mon public_network to 10.10.2.0/24
Wrote config to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf
Wrote keyring to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring
Creating mgr...
Verifying port 9283 ...
Waiting for mgr to start...
Waiting for mgr...
mgr not available, waiting (1/15)...
mgr not available, waiting (2/15)...
mgr is available
Enabling cephadm module...
Waiting for the mgr to restart...
Waiting for mgr epoch 5...
mgr epoch 5 is available
Setting orchestrator backend to cephadm...
Generating ssh key...
Wrote public SSH key to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub
Adding key to ljishen@localhost authorized_keys...
Adding host node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us...
Deploying unmanaged mon service...
Deploying unmanaged mgr service...
You can access the Ceph CLI with:

	sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --fsid 94ee3b2e-37d4-11ec-b51d-53e6e728d2d3 -c /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf -k /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring

Please consider enabling telemetry to help improve Ceph:

	ceph telemetry on

For more information see:

	https://docs.ceph.com/docs/pacific/mgr/telemetry/

Bootstrap complete.

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:52:09,353874322-07:00] INFO: Sleep for 20 seconds[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:52:29,360863738-07:00] INFO: > Set the default number of monitor daemons to 1[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:337 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:52:29,370788615-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:52:29,374360259-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mon', '1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1
Inferring fsid 94ee3b2e-37d4-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/94ee3b2e-37d4-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Scheduled mon update...

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:52:37,999963243-07:00] INFO: > Set the default number of manager daemons to 1[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:342 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:52:38,010085000-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:52:38,014101570-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mgr', '1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1
Inferring fsid 94ee3b2e-37d4-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/94ee3b2e-37d4-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Scheduled mgr update...


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:52:47,084370483-07:00] STAGE: Add hosts to the cluster...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:52:47,090454340-07:00] INFO: > Adding host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:361 - add_hosts() > ssh-copy-id -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -f -i /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us
/usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: "/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub"

Number of key(s) added: 1

Now try logging into the machine, with:   "ssh -o 'GlobalKnownHostsFile=/dev/null' -o 'LogLevel=ERROR' -o 'PasswordAuthentication=no' -o 'StrictHostKeyChecking=no' -o 'UserKnownHostsFile=/dev/null' 'ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'"
and check to make sure that only the key(s) you wanted were added.

# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:367 - add_hosts() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us 10.10.2.2
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:52:47,305459570-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:52:47,309151481-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'host', 'add', 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us', '10.10.2.2'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us 10.10.2.2
Inferring fsid 94ee3b2e-37d4-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/94ee3b2e-37d4-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Added host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us' with addr '10.10.2.2'

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:52:56,630200565-07:00] INFO: Sleep for 20 seconds[0m


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:53:16,635160805-07:00] STAGE: Add OSDs to the cluster...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:53:16,641820975-07:00] INFO: > Adding devices ['/dev/nvme0n1'] on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us' as OSDs[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:388 - add_host_devices_as_osds() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:53:16,652488669-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:53:16,656491654-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'daemon', 'add', 'osd', 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1
Inferring fsid 94ee3b2e-37d4-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/94ee3b2e-37d4-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Created osd(s) 0 on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:53:38,964483902-07:00] INFO: > Sleep for 20 seconds[0m


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:53:58,969876630-07:00] STAGE: Show Cluster Status[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:408 - show_status() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:53:58,980323760-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:53:58,984192843-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', '--status'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status
Inferring fsid 94ee3b2e-37d4-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/94ee3b2e-37d4-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
  cluster:
    id:     94ee3b2e-37d4-11ec-b51d-53e6e728d2d3
    health: HEALTH_WARN
            1 pool(s) have no replicas configured
 
  services:
    mon: 1 daemons, quorum node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us (age 2m)
    mgr: node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us.ltgyeb(active, since 2m)
    osd: 1 osds: 1 up (since 18s), 1 in (since 38s)
 
  data:
    pools:   1 pools, 1 pgs
    objects: 0 objects, 0 B
    usage:   4.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     1 active+clean
 

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:54:07,416647480-07:00] INFO: > Copy configuration files to local[0m
# ./benchmarks/bench-rados:220 - launch_ceph_cluster() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.1 sudo chown --changes --recursive ljishen /tmp/bench-rados/deployment_data_root/etc/ceph
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:54:07,424633409-07:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.1 sudo chown --changes --recursive ljishen /tmp/bench-rados/deployment_data_root/etc/ceph
[1] 02:54:07 [SUCCESS] ljishen@10.10.2.1
# ./benchmarks/bench-rados:223 - launch_ceph_cluster() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.1:/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf
# ./benchmarks/bench-rados:226 - launch_ceph_cluster() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.1:/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:54:07,901219815-07:00] STAGE: Update local ceph.conf as a client...[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:54:07,904520855-07:00] STAGE: Configure Ceph pool...[0m
# ./benchmarks/bench-rados:257 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_max_backfills 32
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:54:07,926431153-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:54:07,929251047-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '94ee3b2e-37d4-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'config', 'set', 'osd', 'osd_max_backfills', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 94ee3b2e-37d4-11ec-b51d-53e6e728d2d3 -- ceph config set osd osd_max_backfills 32
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:258 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_active 32
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:54:12,101169005-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:54:12,104279626-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '94ee3b2e-37d4-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_active', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 94ee3b2e-37d4-11ec-b51d-53e6e728d2d3 -- ceph config set osd osd_recovery_max_active 32
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:259 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_single_start 8
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:54:16,585422601-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:54:16,588787342-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '94ee3b2e-37d4-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_single_start', '8'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 94ee3b2e-37d4-11ec-b51d-53e6e728d2d3 -- ceph config set osd osd_recovery_max_single_start 8
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:260 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_op_priority 63
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:54:20,966794477-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:54:20,969809428-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '94ee3b2e-37d4-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_op_priority', '63'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 94ee3b2e-37d4-11ec-b51d-53e6e728d2d3 -- ceph config set osd osd_recovery_op_priority 63
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:262 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config show-with-defaults osd.0
# ./benchmarks/bench-rados:263 - configure_ceph_pool() > grep 'osd_max_backfills\|osd_recovery'
# ./benchmarks/bench-rados:263 - configure_ceph_pool() > column -t -s ' '
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
osd_max_backfills                        32         mon
osd_recovery_cost                        20971520   default
osd_recovery_delay_start                 0.000000   default
osd_recovery_max_active                  32         mon
osd_recovery_max_active_hdd              3          default
osd_recovery_max_active_ssd              10         default
osd_recovery_max_chunk                   8388608    default
osd_recovery_max_omap_entries_per_chunk  8096       default
osd_recovery_max_single_start            8          mon
osd_recovery_op_priority                 3          default
osd_recovery_op_warn_multiple            16         default
osd_recovery_priority                    5          default
osd_recovery_retry_interval              30.000000  default
osd_recovery_sleep                       0.000000   default
osd_recovery_sleep_hdd                   0.100000   default
osd_recovery_sleep_hybrid                0.025000   default
osd_recovery_sleep_ssd                   0.000000   default
# ./benchmarks/bench-rados:265 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool create bench_rados 128 128 replicated --size 1 --pg-num-min 128
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:54:29,610238788-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:54:29,613200479-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '94ee3b2e-37d4-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'create', 'bench_rados', '128', '128', 'replicated', '--size', '1', '--pg-num-min', '128'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 94ee3b2e-37d4-11ec-b51d-53e6e728d2d3 -- ceph osd pool create bench_rados 128 128 replicated --size 1 --pg-num-min 128
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
pool 'bench_rados' created
# ./benchmarks/bench-rados:267 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados min_size 1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:54:34,732873358-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:54:34,736084409-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '94ee3b2e-37d4-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'min_size', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 94ee3b2e-37d4-11ec-b51d-53e6e728d2d3 -- ceph osd pool set bench_rados min_size 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 min_size to 1
# ./benchmarks/bench-rados:268 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados pg_autoscale_mode off
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:54:39,673413417-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:54:39,676793988-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '94ee3b2e-37d4-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'pg_autoscale_mode', 'off'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 94ee3b2e-37d4-11ec-b51d-53e6e728d2d3 -- ceph osd pool set bench_rados pg_autoscale_mode off
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 pg_autoscale_mode to off
# ./benchmarks/bench-rados:269 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool application enable bench_rados benchmark
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:54:44,907055622-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:54:44,910046378-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '94ee3b2e-37d4-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'application', 'enable', 'bench_rados', 'benchmark'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 94ee3b2e-37d4-11ec-b51d-53e6e728d2d3 -- ceph osd pool application enable bench_rados benchmark
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
enabled application 'benchmark' on pool 'bench_rados'
# ./benchmarks/bench-rados:270 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados noscrub 1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:54:49,799215733-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:54:49,802159250-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '94ee3b2e-37d4-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'noscrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 94ee3b2e-37d4-11ec-b51d-53e6e728d2d3 -- ceph osd pool set bench_rados noscrub 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 noscrub to 1
# ./benchmarks/bench-rados:271 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados nodeep-scrub 1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:54:55,298948933-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:54:55,302009580-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '94ee3b2e-37d4-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'nodeep-scrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 94ee3b2e-37d4-11ec-b51d-53e6e728d2d3 -- ceph osd pool set bench_rados nodeep-scrub 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 nodeep-scrub to 1
# ./benchmarks/bench-rados:272 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool ls detail
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:54:59,696793760-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:54:59,699729001-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '94ee3b2e-37d4-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'ls', 'detail'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 94ee3b2e-37d4-11ec-b51d-53e6e728d2d3 -- ceph osd pool ls detail
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
pool 1 'device_health_metrics' replicated size 1 min_size 1 crush_rule 0 object_hash rjenkins pg_num 64 pgp_num 64 autoscale_mode on last_change 20 lfor 0/0/17 flags hashpspool stripe_width 0 pg_num_min 1 application mgr_devicehealth
pool 2 'bench_rados' replicated size 1 min_size 1 crush_rule 0 object_hash rjenkins pg_num 128 pgp_num 128 autoscale_mode off last_change 21 flags hashpspool,noscrub,nodeep-scrub stripe_width 0 pg_num_min 128 application benchmark

# ./benchmarks/bench-rados:273 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd df tree
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:55:04,129238176-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:55:04,132417397-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '94ee3b2e-37d4-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'df', 'tree'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 94ee3b2e-37d4-11ec-b51d-53e6e728d2d3 -- ceph osd df tree
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
ID  CLASS  WEIGHT   REWEIGHT  SIZE     RAW USE  DATA     OMAP  META     AVAIL    %USE  VAR   PGS  STATUS  TYPE NAME      
-1         0.09769         -  100 GiB  5.7 MiB  176 KiB   0 B  5.6 MiB  100 GiB  0.01  1.00    -          root default   
-3         0.09769         -  100 GiB  5.7 MiB  176 KiB   0 B  5.6 MiB  100 GiB  0.01  1.00    -              host node-1
 0    ssd  0.09769   1.00000  100 GiB  5.7 MiB  176 KiB   0 B  5.6 MiB  100 GiB  0.01  1.00  192      up          osd.0  
                       TOTAL  100 GiB  5.7 MiB  176 KiB   0 B  5.6 MiB  100 GiB  0.01                                    
MIN/MAX VAR: 1.00/1.00  STDDEV: 0


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:55:08,373349305-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:55:32,770691136-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:55:42,069291880-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:55:51,359836474-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:56:00,555964137-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:56:00,563428312-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:56:09,839553062-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:56:09,847282277-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:56:18,938505155-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:56:18,945834326-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:56:28,226269088-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:56:28,234320530-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:56:37,557730555-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:56:37,565676378-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:56:37,571432665-07:00] INFO: > The cluster is idle now.[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:56:37,574942740-07:00] STAGE: Start benchmarking of mode write ...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:56:37,581384218-07:00] INFO: > Start system activity collection services[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:56:37,586918987-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:341 - rados_bench() > rm --force /mnt/sda8/ceph-research/scripts/output/sys_activity_256KB_write.dat.rados_bench_host.2
# ./benchmarks/bench-rados:342 - rados_bench() > sar_pid_rados_bench_host=282541
# ./benchmarks/bench-rados:342 - rados_bench() > S_TIME_FORMAT=ISO
# ./benchmarks/bench-rados:342 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/output/sys_activity_256KB_write.dat.rados_bench_host.2 2

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:56:37,593573087-07:00] INFO: >> for OSD host[0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:56:37,602708662-07:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.2 bash
10.10.2.2: b'164072\n'
[1] 02:56:37 [SUCCESS] ljishen@10.10.2.2
164072

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:56:37,791392971-07:00] INFO: > Run rados bench[0m
# ./benchmarks/bench-rados:370 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/output/rados_bench_256KB_write.log.2
## ./benchmarks/bench-rados:361 - rados_bench() > count_cpus 32-63,96-127
# ./benchmarks/bench-rados:361 - rados_bench() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 60 write --pool bench_rados -b 262144 -O 262144 --concurrent-ios 128 --show-time --write-object --write-omap --write-xattr --no-cleanup
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:56:37,811638452-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:56:37,814731410-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '94ee3b2e-37d4-11ec-b51d-53e6e728d2d3', '--', 'rados', 'bench', '60', 'write', '--pool', 'bench_rados', '-b', '262144', '-O', '262144', '--concurrent-ios', '128', '--show-time', '--write-object', '--write-omap', '--write-xattr', '--no-cleanup'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 94ee3b2e-37d4-11ec-b51d-53e6e728d2d3 -- rados bench 60 write --pool bench_rados -b 262144 -O 262144 --concurrent-ios 128 --show-time --write-object --write-omap --write-xattr --no-cleanup
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
hints = 1
2021-10-28T09:56:41.099917+0000 Maintaining 128 concurrent writes of 262144 bytes to objects of size 262144 for up to 60 seconds or 0 objects
2021-10-28T09:56:41.099930+0000 Object prefix: benchmark_data_node-2.bluefield2.ucsc-cmps10_7
2021-10-28T09:56:41.108546+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-28T09:56:41.108546+0000     0       0         0         0         0         0           -           0
2021-10-28T09:56:42.108647+0000     1     128      1292      1164   290.984       291    0.101925     0.10371
2021-10-28T09:56:43.108740+0000     2     128      2344      2216   276.979       263    0.147736    0.109643
2021-10-28T09:56:44.108823+0000     3     128      2985      2857   238.065    160.25    0.147373    0.129201
2021-10-28T09:56:45.108900+0000     4     128      3958      3830   239.357    243.25   0.0922252    0.132254
2021-10-28T09:56:46.108975+0000     5     128      4597      4469   223.433    159.75    0.310803    0.139157
2021-10-28T09:56:47.109051+0000     6     128      5023      4895   203.943     106.5    0.433491    0.153979
2021-10-28T09:56:48.109130+0000     7     128      5406      5278   188.486     95.75     0.28619    0.165502
2021-10-28T09:56:49.109202+0000     8     128      6089      5961   186.267    170.75    0.149498    0.170052
2021-10-28T09:56:50.109283+0000     9     128      6566      6438    178.82    119.25    0.403918    0.177499
2021-10-28T09:56:51.109370+0000    10     128      6984      6856   171.387     104.5    0.221266    0.184967
2021-10-28T09:56:52.109449+0000    11     127      7811      7684   174.623       207    0.132891    0.182587
2021-10-28T09:56:53.109528+0000    12     128      8588      8460   176.236       194    0.262434    0.178728
2021-10-28T09:56:54.109609+0000    13     128      8967      8839   169.967     94.75    0.326605    0.185006
2021-10-28T09:56:55.109697+0000    14     128      9398      9270   165.523    107.75    0.318857    0.191364
2021-10-28T09:56:56.109779+0000    15     128     10005      9877   164.604    151.75    0.122171    0.193499
2021-10-28T09:56:57.109854+0000    16     128     10513     10385   162.253       127    0.280463    0.194738
2021-10-28T09:56:58.109937+0000    17     128     10939     10811   158.973     106.5    0.321777    0.199421
2021-10-28T09:56:59.110022+0000    18     128     11372     11244   156.154    108.25    0.320136    0.203499
2021-10-28T09:57:00.110094+0000    19     128     11887     11759   154.711    128.75    0.139264    0.205702
2021-10-28T09:57:01.110168+0000 min lat: 0.00703791 max lat: 0.561222 avg lat: 0.20575
2021-10-28T09:57:01.110168+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-28T09:57:01.110168+0000    20     128     12438     12310   153.863    137.75    0.236934     0.20575
2021-10-28T09:57:02.110248+0000    21     128     12884     12756   151.845     111.5    0.257082    0.209815
2021-10-28T09:57:03.110330+0000    22     128     13236     13108   148.943        88    0.374495    0.213104
2021-10-28T09:57:04.110417+0000    23     128     13750     13622   148.053     128.5    0.176917    0.215418
2021-10-28T09:57:05.110491+0000    24     128     14328     14200   147.905     144.5    0.189965    0.215009
2021-10-28T09:57:06.110564+0000    25     128     14788     14660   146.588       115    0.355745    0.217353
2021-10-28T09:57:07.110643+0000    26     128     15174     15046   144.662      96.5     0.27757    0.219919
2021-10-28T09:57:08.110722+0000    27     128     15610     15482   143.341       109    0.268834    0.222631
2021-10-28T09:57:09.110797+0000    28     128     16280     16152   144.203     167.5    0.142592    0.221455
2021-10-28T09:57:10.110930+0000    29     128     16722     16594    143.04     110.5    0.304604    0.222479
2021-10-28T09:57:11.111009+0000    30     128     17104     16976   141.455      95.5    0.310961    0.224882
2021-10-28T09:57:12.111097+0000    31     128     17464     17336   139.795        90    0.317715    0.226825
2021-10-28T09:57:13.111173+0000    32     128     18072     17944   140.176       152    0.159918    0.227507
2021-10-28T09:57:14.111258+0000    33     128     18596     18468   139.898       131    0.240966    0.227114
2021-10-28T09:57:15.111346+0000    34     128     19029     18901   138.967    108.25    0.291114    0.229659
2021-10-28T09:57:16.111427+0000    35     128     19379     19251   137.496      87.5    0.326453    0.231203
2021-10-28T09:57:17.111501+0000    36     128     19911     19783   137.371       133      0.1984    0.232542
2021-10-28T09:57:18.111579+0000    37     128     20542     20414   137.921    157.75    0.206423     0.23152
2021-10-28T09:57:19.111665+0000    38     128     20920     20792   136.778      94.5    0.257458    0.233133
2021-10-28T09:57:20.111762+0000    39     128     21256     21128   135.425        84    0.371088     0.23481
2021-10-28T09:57:21.111847+0000 min lat: 0.00703791 max lat: 0.61075 avg lat: 0.236414
2021-10-28T09:57:21.111847+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-28T09:57:21.111847+0000    40     128     21711     21583   134.883    113.75    0.233314    0.236414
2021-10-28T09:57:22.111941+0000    41     128     22374     22246   135.635    165.75    0.180194    0.235469
2021-10-28T09:57:23.112020+0000    42     128     22828     22700   135.108     113.5    0.317819    0.235784
2021-10-28T09:57:24.112106+0000    43     128     23205     23077   134.158     94.25    0.323836    0.237279
2021-10-28T09:57:25.112206+0000    44     128     23637     23509   133.563       108    0.331142    0.239057
2021-10-28T09:57:26.112284+0000    45     128     24180     24052   133.611    135.75    0.222107     0.23878
2021-10-28T09:57:27.112367+0000    46     128     24754     24626   133.826     143.5    0.322652    0.238407
2021-10-28T09:57:28.112442+0000    47     128     25131     25003   132.984     94.25    0.078183    0.239831
2021-10-28T09:57:29.112518+0000    48     128     25478     25350    132.02     86.75   0.0919193    0.240811
2021-10-28T09:57:30.112592+0000    49     128     26017     25889   132.076    134.75    0.161663    0.241621
2021-10-28T09:57:31.112669+0000    50     128     26668     26540   132.689    162.75    0.140088    0.240643
2021-10-28T09:57:32.112745+0000    51     128     27069     26941   132.053    100.25    0.110803    0.241702
2021-10-28T09:57:33.112820+0000    52     128     27455     27327   131.369      96.5    0.318444    0.242671
2021-10-28T09:57:34.112897+0000    53     128     27909     27781   131.032     113.5    0.183529     0.24363
2021-10-28T09:57:35.112975+0000    54     128     28517     28389    131.42       152    0.185095    0.242813
2021-10-28T09:57:36.113060+0000    55     128     28939     28811   130.948     105.5   0.0910089    0.243263
2021-10-28T09:57:37.113145+0000    56     128     29380     29252   130.579    110.25    0.314467    0.244439
2021-10-28T09:57:38.113229+0000    57     128     29713     29585   129.748     83.25   0.0817696     0.24541
2021-10-28T09:57:39.113312+0000    58     128     30376     30248   130.369    165.75    0.141502    0.244972
2021-10-28T09:57:40.113390+0000    59     128     30926     30798   130.489     137.5    0.302667     0.24467
2021-10-28T09:57:41.113482+0000 min lat: 0.00703791 max lat: 0.613741 avg lat: 0.245545
2021-10-28T09:57:41.113482+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-28T09:57:41.113482+0000    60     128     31344     31216   130.056     104.5    0.296259    0.245545
2021-10-28T09:57:42.113623+0000 Total time run:         60.2659
Total writes made:      31344
Write size:             262144
Object size:            262144
Bandwidth (MB/sec):     130.024
Stddev Bandwidth:       42.6419
Max bandwidth (MB/sec): 291
Min bandwidth (MB/sec): 83.25
Average IOPS:           520
Stddev IOPS:            170.568
Max IOPS:               1164
Min IOPS:               333
Average Latency(s):     0.245768
Stddev Latency(s):      0.0975738
Max latency(s):         0.613741
Min latency(s):         0.00703791

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:57:42,812714434-07:00] INFO: > Stop system activity collection services[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:57:42,818636935-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:391 - rados_bench() > kill -INT 282541

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:57:42,824415754-07:00] INFO: >> for OSD host[0m
# ./benchmarks/bench-rados:396 - rados_bench() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.2 kill -INT 164072
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:57:42,832213558-07:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.2 kill -INT 164072
[1] 02:57:43 [SUCCESS] ljishen@10.10.2.2

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:57:43,017392252-07:00] INFO: >>> Copy system activity file from OSD host[0m
## ./benchmarks/bench-rados:403 - rados_bench() > basename -- /tmp/bench-rados/sys_activity_256KB_write.dat.osd_host.2
# ./benchmarks/bench-rados:403 - rados_bench() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.2:/tmp/bench-rados/sys_activity_256KB_write.dat.osd_host.2 /mnt/sda8/ceph-research/scripts/output/sys_activity_256KB_write.dat.osd_host.2


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:57:43,191885779-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:58:07,432976780-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 31.34k objects, 7.7 GiB
    usage:   23 GiB used, 77 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:58:07,440484347-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:58:16,839293933-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 31.34k objects, 7.7 GiB
    usage:   23 GiB used, 77 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:58:16,846673009-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:58:26,070193337-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 31.34k objects, 7.7 GiB
    usage:   23 GiB used, 77 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:58:26,078269817-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:58:35,128599000-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 31.34k objects, 7.7 GiB
    usage:   23 GiB used, 77 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:58:35,136553169-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:58:44,362307658-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 31.34k objects, 7.7 GiB
    usage:   23 GiB used, 77 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:58:44,369764860-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:58:44,375785606-07:00] INFO: > The cluster is idle now.[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:58:44,379649777-07:00] STAGE: Start benchmarking of mode seq ...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:58:44,386501248-07:00] INFO: > Start system activity collection services[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:58:44,392374516-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:341 - rados_bench() > rm --force /mnt/sda8/ceph-research/scripts/output/sys_activity_256KB_seq.dat.rados_bench_host.2
# ./benchmarks/bench-rados:342 - rados_bench() > sar_pid_rados_bench_host=283899
# ./benchmarks/bench-rados:342 - rados_bench() > S_TIME_FORMAT=ISO
# ./benchmarks/bench-rados:342 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/output/sys_activity_256KB_seq.dat.rados_bench_host.2 2

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:58:44,399622334-07:00] INFO: >> for OSD host[0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:58:44,408810238-07:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.2 bash
10.10.2.2: b'164166\n'
[1] 02:58:44 [SUCCESS] ljishen@10.10.2.2
164166

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:58:44,598911266-07:00] INFO: > Run rados bench[0m
# ./benchmarks/bench-rados:379 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/output/rados_bench_256KB_seq.log.2
## ./benchmarks/bench-rados:376 - rados_bench() > count_cpus 32-63,96-127
# ./benchmarks/bench-rados:376 - rados_bench() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 128 --show-time
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:58:44,619009029-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:58:44,622124930-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '94ee3b2e-37d4-11ec-b51d-53e6e728d2d3', '--', 'rados', 'bench', '99999999', 'seq', '--pool', 'bench_rados', '--concurrent-ios', '128', '--show-time'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 94ee3b2e-37d4-11ec-b51d-53e6e728d2d3 -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 128 --show-time
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
hints = 1
2021-10-28T09:58:47.974789+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-28T09:58:47.974789+0000     0       0         0         0         0         0           -           0
2021-10-28T09:58:48.974929+0000     1     127      6793      6666   1666.15    1666.5   0.0193021   0.0189976
2021-10-28T09:58:49.975052+0000     2     128     13500     13372   1671.22    1676.5   0.0211345   0.0190226
2021-10-28T09:58:50.975172+0000     3     128     20174     20046   1670.25    1668.5   0.0222352   0.0190649
2021-10-28T09:58:51.975311+0000     4     127     26890     26763   1672.44   1679.25   0.0188238   0.0190613
2021-10-28T09:58:52.975462+0000 Total time run:       4.68438
Total reads made:     31344
Read size:            262144
Object size:          262144
Bandwidth (MB/sec):   1672.79
Average IOPS:         6691
Stddev IOPS:          24.5951
Max IOPS:             6717
Min IOPS:             6666
Average Latency(s):   0.019063
Max latency(s):       0.0303899
Min latency(s):       0.00271137

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:58:53,614851879-07:00] INFO: > Stop system activity collection services[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:58:53,620808333-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:391 - rados_bench() > kill -INT 283899

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:58:53,626857332-07:00] INFO: >> for OSD host[0m
# ./benchmarks/bench-rados:396 - rados_bench() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.2 kill -INT 164166
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:58:53,634727993-07:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.2 kill -INT 164166
[1] 02:58:53 [SUCCESS] ljishen@10.10.2.2

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:58:53,822161435-07:00] INFO: >>> Copy system activity file from OSD host[0m
## ./benchmarks/bench-rados:403 - rados_bench() > basename -- /tmp/bench-rados/sys_activity_256KB_seq.dat.osd_host.2
# ./benchmarks/bench-rados:403 - rados_bench() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.2:/tmp/bench-rados/sys_activity_256KB_seq.dat.osd_host.2 /mnt/sda8/ceph-research/scripts/output/sys_activity_256KB_seq.dat.osd_host.2


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:58:53,977749878-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:59:18,093382970-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 31.34k objects, 7.7 GiB
    usage:   23 GiB used, 77 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:59:18,101051340-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:59:27,390874690-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 31.34k objects, 7.7 GiB
    usage:   23 GiB used, 77 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:59:27,398892359-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:59:36,631075730-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 31.34k objects, 7.7 GiB
    usage:   23 GiB used, 77 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:59:36,638903872-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:59:45,961363511-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 31.34k objects, 7.7 GiB
    usage:   23 GiB used, 77 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:59:45,968596060-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:59:55,159832310-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 31.34k objects, 7.7 GiB
    usage:   23 GiB used, 77 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:59:55,167555253-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:59:55,173344833-07:00] INFO: > The cluster is idle now.[0m


[1;7;39;49m[2021-10-28T02:59:55,175949382-07:00][RUNNING][ROUND 3/4/21] object_size=256KB[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:59:55,179379335-07:00] STAGE: Launch a Ceph cluster (CEPH_HOST: 10.10.2.1, OSD_HOST: 10.10.2.2) ...[0m
# ./benchmarks/bench-rados:183 - launch_ceph_cluster() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.1 bash -euo pipefail
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:59:55,188972713-07:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.1 bash -euo pipefail
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T02:59:55,591331207-07:00] INFO: > Remove existing clusters\x1b[0m\n'
10.10.2.1: b'## bash:17 -  > basename -- /var/lib/ceph/94ee3b2e-37d4-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'# bash:17 -  > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm --host 10.10.2.1 --host 10.10.2.2 rm-cluster --force --fsid 94ee3b2e-37d4-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T02:59:55,601964887-07:00] DEBUG: HOST_PARAMS: ['--host', '10.10.2.1', '--host', '10.10.2.2']\x1b[0m\n"
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T02:59:55,605597326-07:00] DEBUG: CEPHADM_PARAMS: ['rm-cluster', '--force', '--fsid', '94ee3b2e-37d4-11ec-b51d-53e6e728d2d3']\x1b[0m\n"
10.10.2.1: b"# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:63 -  > /tmp/bench-rados/ceph-research/scripts/parallel-ssh --host 10.10.2.1 --host 10.10.2.2 'sudo python3 -u - rm-cluster --force --fsid 94ee3b2e-37d4-11ec-b51d-53e6e728d2d3'\n"
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T02:59:55,613329150-07:00] DEBUG: command from stdin\x1b[0m\n'
10.10.2.1: b"# /tmp/bench-rados/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host 10.10.2.1 --host 10.10.2.2 'sudo python3 -u - rm-cluster --force --fsid 94ee3b2e-37d4-11ec-b51d-53e6e728d2d3'\n"
10.10.2.1: b'[1] 03:00:01 [SUCCESS] 10.10.2.1\n[2] 03:00:06 [SUCCESS] 10.10.2.2\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T03:00:06,936507529-07:00] INFO: > Deploy a new cluster\x1b[0m\n'
10.10.2.1: b'# bash:24 -  > /tmp/bench-rados/ceph-research/scripts/ceph-deploy -m 10.10.2.1 -o node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/10.10.2.2:/dev/nvme0n1\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T03:00:06,948577289-07:00] STAGE: Verify passwordless SSH to hosts, passwordless sudo via ljishen, and the existence of devices on hosts...\x1b[0m\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T03:00:06,953170223-07:00] INFO: > Check the host for the monitor\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:181 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@10.10.2.1 sudo --non-interactive --validate\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T03:00:07,103726979-07:00] INFO: > Check hosts for OSDs\x1b[0m\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T03:00:07,108403650-07:00] INFO: >> Check host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:200 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us sudo --non-interactive --validate\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T03:00:07,258840320-07:00] INFO: >>> Check device '/dev/nvme0n1'\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:215 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us test -b /dev/nvme0n1\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:226 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us lsblk --nodeps --noheadings --output TYPE /dev/nvme0n1\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:227 - verify_parameters() > grep --word-regexp disk\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T03:00:07,547504748-07:00] STAGE: Reset devices on hosts...\x1b[0m\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T03:00:07,552573868-07:00] INFO: > Resetting device '/dev/nvme0n1' on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:288 - reset_host_devices() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us sudo sh\n'
10.10.2.1: b'  Removing ceph--bba8ed3b--5a94--4b9b--b725--d3b80cb11a1e-osd--block--42c2e4a0--2a85--4381--8eba--6283efcf5f48 (253:0)\n'
10.10.2.1: b'  Archiving volume group "ceph-bba8ed3b-5a94-4b9b-b725-d3b80cb11a1e" metadata (seqno 5).\n'
10.10.2.1: b'  Releasing logical volume "osd-block-42c2e4a0-2a85-4381-8eba-6283efcf5f48"\n'
10.10.2.1: b'  Creating volume group backup "/etc/lvm/backup/ceph-bba8ed3b-5a94-4b9b-b725-d3b80cb11a1e" (seqno 6).\n'
10.10.2.1: b'  Logical volume "osd-block-42c2e4a0-2a85-4381-8eba-6283efcf5f48" successfully removed\n'
10.10.2.1: b'  Removing physical volume "/dev/nvme0n1" from volume group "ceph-bba8ed3b-5a94-4b9b-b725-d3b80cb11a1e"\n'
10.10.2.1: b'  Volume group "ceph-bba8ed3b-5a94-4b9b-b725-d3b80cb11a1e" successfully removed\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T03:00:07,866161228-07:00] STAGE: Bootstrap a new cluster...\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:317 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.1 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T03:00:07,876385348-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T03:00:07,880272836-07:00] DEBUG: CEPHADM_PARAMS: ['bootstrap', '--config', '/tmp/bench-rados/ceph-research/scripts/ceph.conf', '--mon-ip', '10.10.2.1', '--output-dir', '/tmp/bench-rados/deployment_data_root/etc/ceph', '--ssh-user', 'ljishen', '--skip-dashboard', '--no-minimize-config', '--allow-overwrite', '--allow-fqdn-hostname', '--orphan-initial-daemons', '--skip-monitoring-stack', '--single-host-defaults']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.1 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults\n'
10.10.2.1: b'Verifying podman|docker is present...\n'
10.10.2.1: b'Verifying lvm2 is present...\nVerifying time synchronization is in place...\n'
10.10.2.1: b'Unit ntp.service is enabled and running\n'
10.10.2.1: b'Repeating the final host check...\n'
10.10.2.1: b'podman|docker (/usr/bin/docker) is present\n'
10.10.2.1: b'systemctl is present\n'
10.10.2.1: b'lvcreate is present\n'
10.10.2.1: b'Unit ntp.service is enabled and running\n'
10.10.2.1: b'Host looks OK\n'
10.10.2.1: b'Cluster fsid: d4e4a370-37d5-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Verifying IP 10.10.2.1 port 3300 ...\n'
10.10.2.1: b'Verifying IP 10.10.2.1 port 6789 ...\n'
10.10.2.1: b'Mon IP `10.10.2.1` is in CIDR network `10.10.2.0/24`\n'
10.10.2.1: b'- internal network (--cluster-network) has not been provided, OSD replication will default to the public_network\n'
10.10.2.1: b'Adjusting default settings to suit single-host cluster...\n'
10.10.2.1: b'Pulling container image quay.io/ceph/ceph:v16...\n'
10.10.2.1: b'Ceph version: ceph version 16.2.6 (ee28fb57e47e9f88813e24bbf4c14496ca299d31) pacific (stable)\n'
10.10.2.1: b'Extracting ceph user uid/gid from container image...\n'
10.10.2.1: b'Creating initial keys...\n'
10.10.2.1: b'Creating initial monmap...\n'
10.10.2.1: b'Creating mon...\n'
10.10.2.1: b'Waiting for mon to start...\n'
10.10.2.1: b'Waiting for mon...\n'
10.10.2.1: b'mon is available\n'
10.10.2.1: b'Setting mon public_network to 10.10.2.0/24\n'
10.10.2.1: b'Wrote config to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf\n'
10.10.2.1: b'Wrote keyring to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring\n'
10.10.2.1: b'Creating mgr...\n'
10.10.2.1: b'Verifying port 9283 ...\n'
10.10.2.1: b'Waiting for mgr to start...\nWaiting for mgr...\n'
10.10.2.1: b'mgr not available, waiting (1/15)...\n'
10.10.2.1: b'mgr not available, waiting (2/15)...\n'
10.10.2.1: b'mgr is available\n'
10.10.2.1: b'Enabling cephadm module...\n'
10.10.2.1: b'Waiting for the mgr to restart...\n'
10.10.2.1: b'Waiting for mgr epoch 5...\n'
10.10.2.1: b'mgr epoch 5 is available\n'
10.10.2.1: b'Setting orchestrator backend to cephadm...\n'
10.10.2.1: b'Generating ssh key...\n'
10.10.2.1: b'Wrote public SSH key to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub\n'
10.10.2.1: b'Adding key to ljishen@localhost authorized_keys...\n'
10.10.2.1: b'Adding host node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us...\n'
10.10.2.1: b'Deploying unmanaged mon service...\n'
10.10.2.1: b'Deploying unmanaged mgr service...\n'
10.10.2.1: b'You can access the Ceph CLI with:\n\n\tsudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --fsid d4e4a370-37d5-11ec-b51d-53e6e728d2d3 -c /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf -k /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring\n\n'
10.10.2.1: b'Please consider enabling telemetry to help improve Ceph:\n\n\tceph telemetry on\n\nFor more information see:\n\n\thttps://docs.ceph.com/docs/pacific/mgr/telemetry/\n\n'
10.10.2.1: b'Bootstrap complete.\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T03:01:06,282155628-07:00] INFO: Sleep for 20 seconds\x1b[0m\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T03:01:26,289181039-07:00] INFO: > Set the default number of monitor daemons to 1\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:337 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T03:01:26,299421300-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T03:01:26,302839896-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mon', '1']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1\n'
10.10.2.1: b'Inferring fsid d4e4a370-37d5-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/d4e4a370-37d5-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b'Scheduled mon update...\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T03:01:34,868906249-07:00] INFO: > Set the default number of manager daemons to 1\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:342 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T03:01:34,878569805-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T03:01:34,882442205-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mgr', '1']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1\n'
10.10.2.1: b'Inferring fsid d4e4a370-37d5-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/d4e4a370-37d5-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b'Scheduled mgr update...\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T03:01:43,917371644-07:00] STAGE: Add hosts to the cluster...\x1b[0m\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T03:01:43,923419283-07:00] INFO: > Adding host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:361 - add_hosts() > ssh-copy-id -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -f -i /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\n'
10.10.2.1: b'/usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: "/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub"\n'
10.10.2.1: b'\nNumber of key(s) added: 1\n\nNow try logging into the machine, with:   "ssh -o \'GlobalKnownHostsFile=/dev/null\' -o \'LogLevel=ERROR\' -o \'PasswordAuthentication=no\' -o \'StrictHostKeyChecking=no\' -o \'UserKnownHostsFile=/dev/null\' \'ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\'"\nand check to make sure that only the key(s) you wanted were added.\n\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:367 - add_hosts() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us 10.10.2.2\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T03:01:44,134007820-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T03:01:44,137685624-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'host', 'add', 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us', '10.10.2.2']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us 10.10.2.2\n'
10.10.2.1: b'Inferring fsid d4e4a370-37d5-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/d4e4a370-37d5-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b"Added host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us' with addr '10.10.2.2'\n"
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T03:01:53,144679368-07:00] INFO: Sleep for 20 seconds\x1b[0m\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T03:02:13,149036080-07:00] STAGE: Add OSDs to the cluster...\x1b[0m\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T03:02:13,155424680-07:00] INFO: > Adding devices ['/dev/nvme0n1'] on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us' as OSDs\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:388 - add_host_devices_as_osds() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T03:02:13,164747546-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T03:02:13,168562838-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'daemon', 'add', 'osd', 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1\n'
10.10.2.1: b'Inferring fsid d4e4a370-37d5-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/d4e4a370-37d5-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b"Created osd(s) 0 on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'\n"
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T03:02:35,852690062-07:00] INFO: > Sleep for 20 seconds\x1b[0m\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T03:02:55,858394326-07:00] STAGE: Show Cluster Status\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:408 - show_status() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T03:02:55,868790780-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T03:02:55,872864769-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', '--status']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status\n'
10.10.2.1: b'Inferring fsid d4e4a370-37d5-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/d4e4a370-37d5-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b'  cluster:\n    id:     d4e4a370-37d5-11ec-b51d-53e6e728d2d3\n    health: HEALTH_WARN\n            1 pool(s) have no replicas configured\n \n  services:\n    mon: 1 daemons, quorum node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us (age 2m)\n    mgr: node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us.zscixv(active, since 2m)\n    osd: 1 osds: 1 up (since 19s), 1 in (since 38s)\n \n  data:\n    pools:   1 pools, 128 pgs\n    objects: 0 objects, 0 B\n    usage:   5.3 MiB used, 100 GiB / 100 GiB avail\n    pgs:     128 active+clean\n \n  progress:\n \n'
[1] 03:03:04 [SUCCESS] ljishen@10.10.2.1

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:59:55,591331207-07:00] INFO: > Remove existing clusters[0m
## bash:17 -  > basename -- /var/lib/ceph/94ee3b2e-37d4-11ec-b51d-53e6e728d2d3
# bash:17 -  > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm --host 10.10.2.1 --host 10.10.2.2 rm-cluster --force --fsid 94ee3b2e-37d4-11ec-b51d-53e6e728d2d3
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:59:55,601964887-07:00] DEBUG: HOST_PARAMS: ['--host', '10.10.2.1', '--host', '10.10.2.2'][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:59:55,605597326-07:00] DEBUG: CEPHADM_PARAMS: ['rm-cluster', '--force', '--fsid', '94ee3b2e-37d4-11ec-b51d-53e6e728d2d3'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:63 -  > /tmp/bench-rados/ceph-research/scripts/parallel-ssh --host 10.10.2.1 --host 10.10.2.2 'sudo python3 -u - rm-cluster --force --fsid 94ee3b2e-37d4-11ec-b51d-53e6e728d2d3'
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T02:59:55,613329150-07:00] DEBUG: command from stdin[0m
# /tmp/bench-rados/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host 10.10.2.1 --host 10.10.2.2 'sudo python3 -u - rm-cluster --force --fsid 94ee3b2e-37d4-11ec-b51d-53e6e728d2d3'
[1] 03:00:01 [SUCCESS] 10.10.2.1
[2] 03:00:06 [SUCCESS] 10.10.2.2

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:00:06,936507529-07:00] INFO: > Deploy a new cluster[0m
# bash:24 -  > /tmp/bench-rados/ceph-research/scripts/ceph-deploy -m 10.10.2.1 -o node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/10.10.2.2:/dev/nvme0n1


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:00:06,948577289-07:00] STAGE: Verify passwordless SSH to hosts, passwordless sudo via ljishen, and the existence of devices on hosts...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:00:06,953170223-07:00] INFO: > Check the host for the monitor[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:181 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@10.10.2.1 sudo --non-interactive --validate

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:00:07,103726979-07:00] INFO: > Check hosts for OSDs[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:00:07,108403650-07:00] INFO: >> Check host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:200 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us sudo --non-interactive --validate

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:00:07,258840320-07:00] INFO: >>> Check device '/dev/nvme0n1'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:215 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us test -b /dev/nvme0n1
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:226 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us lsblk --nodeps --noheadings --output TYPE /dev/nvme0n1
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:227 - verify_parameters() > grep --word-regexp disk


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:00:07,547504748-07:00] STAGE: Reset devices on hosts...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:00:07,552573868-07:00] INFO: > Resetting device '/dev/nvme0n1' on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:288 - reset_host_devices() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us sudo sh
  Removing ceph--bba8ed3b--5a94--4b9b--b725--d3b80cb11a1e-osd--block--42c2e4a0--2a85--4381--8eba--6283efcf5f48 (253:0)
  Archiving volume group "ceph-bba8ed3b-5a94-4b9b-b725-d3b80cb11a1e" metadata (seqno 5).
  Releasing logical volume "osd-block-42c2e4a0-2a85-4381-8eba-6283efcf5f48"
  Creating volume group backup "/etc/lvm/backup/ceph-bba8ed3b-5a94-4b9b-b725-d3b80cb11a1e" (seqno 6).
  Logical volume "osd-block-42c2e4a0-2a85-4381-8eba-6283efcf5f48" successfully removed
  Removing physical volume "/dev/nvme0n1" from volume group "ceph-bba8ed3b-5a94-4b9b-b725-d3b80cb11a1e"
  Volume group "ceph-bba8ed3b-5a94-4b9b-b725-d3b80cb11a1e" successfully removed


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:00:07,866161228-07:00] STAGE: Bootstrap a new cluster...[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:317 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.1 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:00:07,876385348-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:00:07,880272836-07:00] DEBUG: CEPHADM_PARAMS: ['bootstrap', '--config', '/tmp/bench-rados/ceph-research/scripts/ceph.conf', '--mon-ip', '10.10.2.1', '--output-dir', '/tmp/bench-rados/deployment_data_root/etc/ceph', '--ssh-user', 'ljishen', '--skip-dashboard', '--no-minimize-config', '--allow-overwrite', '--allow-fqdn-hostname', '--orphan-initial-daemons', '--skip-monitoring-stack', '--single-host-defaults'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.1 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults
Verifying podman|docker is present...
Verifying lvm2 is present...
Verifying time synchronization is in place...
Unit ntp.service is enabled and running
Repeating the final host check...
podman|docker (/usr/bin/docker) is present
systemctl is present
lvcreate is present
Unit ntp.service is enabled and running
Host looks OK
Cluster fsid: d4e4a370-37d5-11ec-b51d-53e6e728d2d3
Verifying IP 10.10.2.1 port 3300 ...
Verifying IP 10.10.2.1 port 6789 ...
Mon IP `10.10.2.1` is in CIDR network `10.10.2.0/24`
- internal network (--cluster-network) has not been provided, OSD replication will default to the public_network
Adjusting default settings to suit single-host cluster...
Pulling container image quay.io/ceph/ceph:v16...
Ceph version: ceph version 16.2.6 (ee28fb57e47e9f88813e24bbf4c14496ca299d31) pacific (stable)
Extracting ceph user uid/gid from container image...
Creating initial keys...
Creating initial monmap...
Creating mon...
Waiting for mon to start...
Waiting for mon...
mon is available
Setting mon public_network to 10.10.2.0/24
Wrote config to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf
Wrote keyring to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring
Creating mgr...
Verifying port 9283 ...
Waiting for mgr to start...
Waiting for mgr...
mgr not available, waiting (1/15)...
mgr not available, waiting (2/15)...
mgr is available
Enabling cephadm module...
Waiting for the mgr to restart...
Waiting for mgr epoch 5...
mgr epoch 5 is available
Setting orchestrator backend to cephadm...
Generating ssh key...
Wrote public SSH key to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub
Adding key to ljishen@localhost authorized_keys...
Adding host node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us...
Deploying unmanaged mon service...
Deploying unmanaged mgr service...
You can access the Ceph CLI with:

	sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --fsid d4e4a370-37d5-11ec-b51d-53e6e728d2d3 -c /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf -k /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring

Please consider enabling telemetry to help improve Ceph:

	ceph telemetry on

For more information see:

	https://docs.ceph.com/docs/pacific/mgr/telemetry/

Bootstrap complete.

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:01:06,282155628-07:00] INFO: Sleep for 20 seconds[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:01:26,289181039-07:00] INFO: > Set the default number of monitor daemons to 1[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:337 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:01:26,299421300-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:01:26,302839896-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mon', '1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1
Inferring fsid d4e4a370-37d5-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/d4e4a370-37d5-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Scheduled mon update...

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:01:34,868906249-07:00] INFO: > Set the default number of manager daemons to 1[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:342 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:01:34,878569805-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:01:34,882442205-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mgr', '1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1
Inferring fsid d4e4a370-37d5-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/d4e4a370-37d5-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Scheduled mgr update...


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:01:43,917371644-07:00] STAGE: Add hosts to the cluster...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:01:43,923419283-07:00] INFO: > Adding host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:361 - add_hosts() > ssh-copy-id -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -f -i /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us
/usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: "/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub"

Number of key(s) added: 1

Now try logging into the machine, with:   "ssh -o 'GlobalKnownHostsFile=/dev/null' -o 'LogLevel=ERROR' -o 'PasswordAuthentication=no' -o 'StrictHostKeyChecking=no' -o 'UserKnownHostsFile=/dev/null' 'ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'"
and check to make sure that only the key(s) you wanted were added.

# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:367 - add_hosts() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us 10.10.2.2
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:01:44,134007820-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:01:44,137685624-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'host', 'add', 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us', '10.10.2.2'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us 10.10.2.2
Inferring fsid d4e4a370-37d5-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/d4e4a370-37d5-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Added host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us' with addr '10.10.2.2'

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:01:53,144679368-07:00] INFO: Sleep for 20 seconds[0m


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:02:13,149036080-07:00] STAGE: Add OSDs to the cluster...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:02:13,155424680-07:00] INFO: > Adding devices ['/dev/nvme0n1'] on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us' as OSDs[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:388 - add_host_devices_as_osds() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:02:13,164747546-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:02:13,168562838-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'daemon', 'add', 'osd', 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1
Inferring fsid d4e4a370-37d5-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/d4e4a370-37d5-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Created osd(s) 0 on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:02:35,852690062-07:00] INFO: > Sleep for 20 seconds[0m


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:02:55,858394326-07:00] STAGE: Show Cluster Status[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:408 - show_status() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:02:55,868790780-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:02:55,872864769-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', '--status'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status
Inferring fsid d4e4a370-37d5-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/d4e4a370-37d5-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
  cluster:
    id:     d4e4a370-37d5-11ec-b51d-53e6e728d2d3
    health: HEALTH_WARN
            1 pool(s) have no replicas configured
 
  services:
    mon: 1 daemons, quorum node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us (age 2m)
    mgr: node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us.zscixv(active, since 2m)
    osd: 1 osds: 1 up (since 19s), 1 in (since 38s)
 
  data:
    pools:   1 pools, 128 pgs
    objects: 0 objects, 0 B
    usage:   5.3 MiB used, 100 GiB / 100 GiB avail
    pgs:     128 active+clean
 
  progress:
 

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:03:04,101219564-07:00] INFO: > Copy configuration files to local[0m
# ./benchmarks/bench-rados:220 - launch_ceph_cluster() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.1 sudo chown --changes --recursive ljishen /tmp/bench-rados/deployment_data_root/etc/ceph
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:03:04,109144978-07:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.1 sudo chown --changes --recursive ljishen /tmp/bench-rados/deployment_data_root/etc/ceph
[1] 03:03:04 [SUCCESS] ljishen@10.10.2.1
# ./benchmarks/bench-rados:223 - launch_ceph_cluster() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.1:/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf
# ./benchmarks/bench-rados:226 - launch_ceph_cluster() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.1:/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:03:04,588824239-07:00] STAGE: Update local ceph.conf as a client...[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:03:04,592509123-07:00] STAGE: Configure Ceph pool...[0m
# ./benchmarks/bench-rados:257 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_max_backfills 32
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:03:04,614325885-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:03:04,617199571-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'd4e4a370-37d5-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'config', 'set', 'osd', 'osd_max_backfills', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid d4e4a370-37d5-11ec-b51d-53e6e728d2d3 -- ceph config set osd osd_max_backfills 32
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:258 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_active 32
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:03:08,908586750-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:03:08,911725434-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'd4e4a370-37d5-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_active', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid d4e4a370-37d5-11ec-b51d-53e6e728d2d3 -- ceph config set osd osd_recovery_max_active 32
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:259 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_single_start 8
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:03:13,190398083-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:03:13,193487275-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'd4e4a370-37d5-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_single_start', '8'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid d4e4a370-37d5-11ec-b51d-53e6e728d2d3 -- ceph config set osd osd_recovery_max_single_start 8
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:260 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_op_priority 63
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:03:17,480928628-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:03:17,484187139-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'd4e4a370-37d5-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_op_priority', '63'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid d4e4a370-37d5-11ec-b51d-53e6e728d2d3 -- ceph config set osd osd_recovery_op_priority 63
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:262 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config show-with-defaults osd.0
# ./benchmarks/bench-rados:263 - configure_ceph_pool() > grep 'osd_max_backfills\|osd_recovery'
# ./benchmarks/bench-rados:263 - configure_ceph_pool() > column -t -s ' '
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
osd_max_backfills                        32         mon
osd_recovery_cost                        20971520   default
osd_recovery_delay_start                 0.000000   default
osd_recovery_max_active                  32         mon
osd_recovery_max_active_hdd              3          default
osd_recovery_max_active_ssd              10         default
osd_recovery_max_chunk                   8388608    default
osd_recovery_max_omap_entries_per_chunk  8096       default
osd_recovery_max_single_start            8          mon
osd_recovery_op_priority                 3          default
osd_recovery_op_warn_multiple            16         default
osd_recovery_priority                    5          default
osd_recovery_retry_interval              30.000000  default
osd_recovery_sleep                       0.000000   default
osd_recovery_sleep_hdd                   0.100000   default
osd_recovery_sleep_hybrid                0.025000   default
osd_recovery_sleep_ssd                   0.000000   default
# ./benchmarks/bench-rados:265 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool create bench_rados 128 128 replicated --size 1 --pg-num-min 128
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:03:26,015452421-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:03:26,018383725-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'd4e4a370-37d5-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'create', 'bench_rados', '128', '128', 'replicated', '--size', '1', '--pg-num-min', '128'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid d4e4a370-37d5-11ec-b51d-53e6e728d2d3 -- ceph osd pool create bench_rados 128 128 replicated --size 1 --pg-num-min 128
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
pool 'bench_rados' created
# ./benchmarks/bench-rados:267 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados min_size 1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:03:30,830406263-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:03:30,833713144-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'd4e4a370-37d5-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'min_size', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid d4e4a370-37d5-11ec-b51d-53e6e728d2d3 -- ceph osd pool set bench_rados min_size 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 min_size to 1
# ./benchmarks/bench-rados:268 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados pg_autoscale_mode off
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:03:35,156581617-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:03:35,159991924-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'd4e4a370-37d5-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'pg_autoscale_mode', 'off'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid d4e4a370-37d5-11ec-b51d-53e6e728d2d3 -- ceph osd pool set bench_rados pg_autoscale_mode off
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 pg_autoscale_mode to off
# ./benchmarks/bench-rados:269 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool application enable bench_rados benchmark
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:03:39,857162932-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:03:39,860260950-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'd4e4a370-37d5-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'application', 'enable', 'bench_rados', 'benchmark'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid d4e4a370-37d5-11ec-b51d-53e6e728d2d3 -- ceph osd pool application enable bench_rados benchmark
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
enabled application 'benchmark' on pool 'bench_rados'
# ./benchmarks/bench-rados:270 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados noscrub 1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:03:44,899519347-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:03:44,902563684-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'd4e4a370-37d5-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'noscrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid d4e4a370-37d5-11ec-b51d-53e6e728d2d3 -- ceph osd pool set bench_rados noscrub 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 noscrub to 1
# ./benchmarks/bench-rados:271 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados nodeep-scrub 1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:03:49,993373186-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:03:49,996658447-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'd4e4a370-37d5-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'nodeep-scrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid d4e4a370-37d5-11ec-b51d-53e6e728d2d3 -- ceph osd pool set bench_rados nodeep-scrub 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 nodeep-scrub to 1
# ./benchmarks/bench-rados:272 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool ls detail
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:03:54,992588485-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:03:54,995486075-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'd4e4a370-37d5-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'ls', 'detail'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid d4e4a370-37d5-11ec-b51d-53e6e728d2d3 -- ceph osd pool ls detail
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
pool 1 'device_health_metrics' replicated size 1 min_size 1 crush_rule 0 object_hash rjenkins pg_num 128 pgp_num 128 autoscale_mode on last_change 12 lfor 0/0/10 flags hashpspool stripe_width 0 pg_num_min 1 application mgr_devicehealth
pool 2 'bench_rados' replicated size 1 min_size 1 crush_rule 0 object_hash rjenkins pg_num 128 pgp_num 128 autoscale_mode off last_change 20 flags hashpspool,noscrub,nodeep-scrub stripe_width 0 pg_num_min 128 application benchmark

# ./benchmarks/bench-rados:273 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd df tree
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:03:59,233281884-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:03:59,236516680-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'd4e4a370-37d5-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'df', 'tree'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid d4e4a370-37d5-11ec-b51d-53e6e728d2d3 -- ceph osd df tree
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
ID  CLASS  WEIGHT   REWEIGHT  SIZE     RAW USE  DATA     OMAP  META     AVAIL    %USE  VAR   PGS  STATUS  TYPE NAME      
-1         0.09769         -  100 GiB  6.0 MiB  168 KiB   0 B  5.8 MiB  100 GiB  0.01  1.00    -          root default   
-3         0.09769         -  100 GiB  6.0 MiB  168 KiB   0 B  5.8 MiB  100 GiB  0.01  1.00    -              host node-1
 0    ssd  0.09769   1.00000  100 GiB  6.0 MiB  168 KiB   0 B  5.8 MiB  100 GiB  0.01  1.00  256      up          osd.0  
                       TOTAL  100 GiB  6.0 MiB  168 KiB   0 B  5.8 MiB  100 GiB  0.01                                    
MIN/MAX VAR: 1.00/1.00  STDDEV: 0


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:04:03,443454589-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:04:27,675242376-07:00] INFO: >>   data:
    pools:   2 pools, 256 pgs
    objects: 0 objects, 0 B
    usage:   6.0 MiB used, 100 GiB / 100 GiB avail
    pgs:     256 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:04:27,683110242-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:04:36,909476660-07:00] INFO: >>   data:
    pools:   2 pools, 256 pgs
    objects: 0 objects, 0 B
    usage:   6.0 MiB used, 100 GiB / 100 GiB avail
    pgs:     256 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:04:36,917184144-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:04:46,124251122-07:00] INFO: >>   data:
    pools:   2 pools, 256 pgs
    objects: 0 objects, 0 B
    usage:   6.0 MiB used, 100 GiB / 100 GiB avail
    pgs:     256 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:04:46,132090225-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:04:55,390351154-07:00] INFO: >>   data:
    pools:   2 pools, 256 pgs
    objects: 0 objects, 0 B
    usage:   6.0 MiB used, 100 GiB / 100 GiB avail
    pgs:     256 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:04:55,398153848-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:05:04,797577478-07:00] INFO: >>   data:
    pools:   2 pools, 256 pgs
    objects: 0 objects, 0 B
    usage:   6.0 MiB used, 100 GiB / 100 GiB avail
    pgs:     256 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:05:04,805522290-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:05:04,811173760-07:00] INFO: > The cluster is idle now.[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:05:04,814910010-07:00] STAGE: Start benchmarking of mode write ...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:05:04,821886998-07:00] INFO: > Start system activity collection services[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:05:04,827849774-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:341 - rados_bench() > rm --force /mnt/sda8/ceph-research/scripts/output/sys_activity_256KB_write.dat.rados_bench_host.3
# ./benchmarks/bench-rados:342 - rados_bench() > sar_pid_rados_bench_host=289057
# ./benchmarks/bench-rados:342 - rados_bench() > S_TIME_FORMAT=ISO
# ./benchmarks/bench-rados:342 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/output/sys_activity_256KB_write.dat.rados_bench_host.3 2

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:05:04,834876385-07:00] INFO: >> for OSD host[0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:05:04,844046405-07:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.2 bash
10.10.2.2: b'167750\n'
[1] 03:05:05 [SUCCESS] ljishen@10.10.2.2
167750

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:05:05,035354322-07:00] INFO: > Run rados bench[0m
# ./benchmarks/bench-rados:370 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/output/rados_bench_256KB_write.log.3
## ./benchmarks/bench-rados:361 - rados_bench() > count_cpus 32-63,96-127
# ./benchmarks/bench-rados:361 - rados_bench() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 60 write --pool bench_rados -b 262144 -O 262144 --concurrent-ios 128 --show-time --write-object --write-omap --write-xattr --no-cleanup
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:05:05,055551381-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:05:05,058427391-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'd4e4a370-37d5-11ec-b51d-53e6e728d2d3', '--', 'rados', 'bench', '60', 'write', '--pool', 'bench_rados', '-b', '262144', '-O', '262144', '--concurrent-ios', '128', '--show-time', '--write-object', '--write-omap', '--write-xattr', '--no-cleanup'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid d4e4a370-37d5-11ec-b51d-53e6e728d2d3 -- rados bench 60 write --pool bench_rados -b 262144 -O 262144 --concurrent-ios 128 --show-time --write-object --write-omap --write-xattr --no-cleanup
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
hints = 1
2021-10-28T10:05:08.410530+0000 Maintaining 128 concurrent writes of 262144 bytes to objects of size 262144 for up to 60 seconds or 0 objects
2021-10-28T10:05:08.410547+0000 Object prefix: benchmark_data_node-2.bluefield2.ucsc-cmps10_7
2021-10-28T10:05:08.419447+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-28T10:05:08.419447+0000     0       0         0         0         0         0           -           0
2021-10-28T10:05:09.419557+0000     1     128      1262      1134   283.485     283.5   0.0723918    0.107431
2021-10-28T10:05:10.419645+0000     2     128      2315      2187   273.356    263.25    0.239032    0.113061
2021-10-28T10:05:11.419729+0000     3     128      2931      2803   233.566       154    0.168512    0.133288
2021-10-28T10:05:12.419804+0000     4     128      3574      3446   215.359    160.75    0.154913    0.146545
2021-10-28T10:05:13.419881+0000     5     128      4366      4238   211.884       198    0.166131    0.147427
2021-10-28T10:05:14.419954+0000     6     128      4679      4551   189.611     78.25    0.265358    0.159362
2021-10-28T10:05:15.420030+0000     7     128      5102      4974    177.63    105.75    0.241642    0.174748
2021-10-28T10:05:16.420111+0000     8     128      5595      5467   170.831    123.25    0.218442    0.184354
2021-10-28T10:05:17.420195+0000     9     128      6218      6090   169.154    155.75    0.147848    0.186115
2021-10-28T10:05:18.420294+0000    10     128      6632      6504   162.587     103.5    0.264313    0.194764
2021-10-28T10:05:19.420363+0000    11     128      7135      7007   159.238    125.75    0.162625    0.199234
2021-10-28T10:05:20.420450+0000    12     128      7971      7843   163.383       209    0.104384    0.194699
2021-10-28T10:05:21.420532+0000    13     128      8590      8462   162.718    154.75    0.251776    0.192591
2021-10-28T10:05:22.420640+0000    14     128      9011      8883   158.612    105.25    0.253119    0.198707
2021-10-28T10:05:23.420766+0000    15     128      9399      9271   154.504        97      0.2547    0.204074
2021-10-28T10:05:24.420848+0000    16     128     10031      9903   154.721       158    0.157992    0.205471
2021-10-28T10:05:25.420924+0000    17     128     10533     10405   153.002     125.5    0.255653    0.207224
2021-10-28T10:05:26.421055+0000    18     128     10928     10800   149.987     98.75    0.251017     0.21123
2021-10-28T10:05:27.421146+0000    19     128     11288     11160   146.829        90    0.271658    0.215032
2021-10-28T10:05:28.421243+0000 min lat: 0.0512056 max lat: 0.636975 avg lat: 0.217891
2021-10-28T10:05:28.421243+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-28T10:05:28.421243+0000    20     128     11815     11687   146.075    131.75    0.161362    0.217891
2021-10-28T10:05:29.421320+0000    21     128     12381     12253   145.856     141.5    0.216315    0.216992
2021-10-28T10:05:30.421403+0000    22     128     12829     12701   144.317       112    0.203872    0.220598
2021-10-28T10:05:31.421523+0000    23     128     13209     13081   142.172        95    0.265067    0.223449
2021-10-28T10:05:32.421608+0000    24     128     13644     13516   140.779    108.75    0.168561    0.226286
2021-10-28T10:05:33.421687+0000    25     128     14257     14129   141.278    153.25    0.200926    0.225395
2021-10-28T10:05:34.421792+0000    26     128     14685     14557   139.959       107    0.285147    0.226859
2021-10-28T10:05:35.421905+0000    27     128     15066     14938   138.303     95.25    0.287595    0.229886
2021-10-28T10:05:36.422034+0000    28     128     15406     15278   136.398        85    0.309449    0.232218
2021-10-28T10:05:37.422106+0000    29     128     15987     15859   136.703    145.25      0.1679    0.232689
2021-10-28T10:05:38.422195+0000    30     128     16569     16441   136.996     145.5   0.0830325    0.232651
2021-10-28T10:05:39.422313+0000    31     128     16890     16762   135.165     80.25    0.238767    0.234314
2021-10-28T10:05:40.422397+0000    32     128     17323     17195   134.324    108.25    0.278771    0.237205
2021-10-28T10:05:41.422473+0000    33     128     17729     17601   133.329     101.5    0.182706    0.238498
2021-10-28T10:05:42.422555+0000    34     128     18385     18257   134.231       164    0.214135    0.237418
2021-10-28T10:05:43.422669+0000    35     128     18800     18672   133.359    103.75    0.327978    0.238202
2021-10-28T10:05:44.422749+0000    36     128     19183     19055   132.314     95.75    0.294653    0.240208
2021-10-28T10:05:45.422825+0000    37     128     19610     19482   131.623    106.75    0.328096    0.242054
2021-10-28T10:05:46.422953+0000    38     128     20266     20138   132.475       164    0.150478    0.240778
2021-10-28T10:05:47.423043+0000    39     128     20759     20631   132.238    123.25    0.318096    0.241092
2021-10-28T10:05:48.423130+0000 min lat: 0.0512056 max lat: 0.636975 avg lat: 0.242552
2021-10-28T10:05:48.423130+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-28T10:05:48.423130+0000    40     128     21120     20992   131.188     90.25    0.320468    0.242552
2021-10-28T10:05:49.423217+0000    41     128     21522     21394   130.439     100.5    0.307676    0.244185
2021-10-28T10:05:50.423302+0000    42     128     22062     21934   130.548       135    0.182443     0.24429
2021-10-28T10:05:51.423424+0000    43     128     22674     22546   131.069       153    0.284377    0.243356
2021-10-28T10:05:52.423502+0000    44     128     23023     22895   130.073     87.25    0.322409    0.244621
2021-10-28T10:05:53.423574+0000    45     128     23461     23333   129.616     109.5   0.0552772    0.246142
2021-10-28T10:05:54.423653+0000    46     128     23908     23780   129.227    111.75    0.193167     0.24689
2021-10-28T10:05:55.423763+0000    47     128     24584     24456   130.073       169    0.145593    0.245449
2021-10-28T10:05:56.423887+0000    48     128     24972     24844   129.384        97    0.297532    0.246367
2021-10-28T10:05:57.423964+0000    49     128     25399     25271   128.922    106.75    0.334171    0.247571
2021-10-28T10:05:58.424053+0000    50     128     25762     25634   128.158     90.75    0.305687     0.24876
2021-10-28T10:05:59.424168+0000    51     128     26383     26255   128.689    155.25    0.167358    0.248293
2021-10-28T10:06:00.424252+0000    52     128     26903     26775   128.714       130    0.265505    0.248114
2021-10-28T10:06:01.424336+0000    53     128     27282     27154   128.073     94.75    0.273972    0.249335
2021-10-28T10:06:02.424422+0000    54     128     27712     27584   127.692     107.5     0.25677    0.250318
2021-10-28T10:06:03.424543+0000    55     128     28230     28102   127.725     129.5     0.14399    0.250087
2021-10-28T10:06:04.424624+0000    56     128     28770     28642   127.854       135    0.267701    0.249266
2021-10-28T10:06:05.424700+0000    57     128     29191     29063   127.458    105.25    0.336374     0.25041
2021-10-28T10:06:06.424821+0000    58     128     29619     29491   127.105       107    0.312884    0.251214
2021-10-28T10:06:07.424914+0000    59     128     30064     29936   126.836    111.25     0.13882    0.251856
2021-10-28T10:06:08.425002+0000 min lat: 0.0476194 max lat: 0.636975 avg lat: 0.250717
2021-10-28T10:06:08.425002+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-28T10:06:08.425002+0000    60     128     30717     30589   127.442    163.25    0.158246    0.250717
2021-10-28T10:06:09.425118+0000 Total time run:         60.2524
Total writes made:      30717
Write size:             262144
Object size:            262144
Bandwidth (MB/sec):     127.451
Stddev Bandwidth:       40.005
Max bandwidth (MB/sec): 283.5
Min bandwidth (MB/sec): 78.25
Average IOPS:           509
Stddev IOPS:            160.02
Max IOPS:               1134
Min IOPS:               313
Average Latency(s):     0.250772
Stddev Latency(s):      0.0987131
Max latency(s):         0.636975
Min latency(s):         0.0476194

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:06:10,154610994-07:00] INFO: > Stop system activity collection services[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:06:10,160701862-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:391 - rados_bench() > kill -INT 289057

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:06:10,166692841-07:00] INFO: >> for OSD host[0m
# ./benchmarks/bench-rados:396 - rados_bench() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.2 kill -INT 167750
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:06:10,174288294-07:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.2 kill -INT 167750
[1] 03:06:10 [SUCCESS] ljishen@10.10.2.2

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:06:10,361889054-07:00] INFO: >>> Copy system activity file from OSD host[0m
## ./benchmarks/bench-rados:403 - rados_bench() > basename -- /tmp/bench-rados/sys_activity_256KB_write.dat.osd_host.3
# ./benchmarks/bench-rados:403 - rados_bench() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.2:/tmp/bench-rados/sys_activity_256KB_write.dat.osd_host.3 /mnt/sda8/ceph-research/scripts/output/sys_activity_256KB_write.dat.osd_host.3


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:06:10,535264207-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:06:34,636550531-07:00] INFO: >>   data:
    pools:   2 pools, 256 pgs
    objects: 30.72k objects, 7.5 GiB
    usage:   23 GiB used, 77 GiB / 100 GiB avail
    pgs:     256 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:06:34,644680792-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:06:43,984842904-07:00] INFO: >>   data:
    pools:   2 pools, 256 pgs
    objects: 30.72k objects, 7.5 GiB
    usage:   23 GiB used, 77 GiB / 100 GiB avail
    pgs:     256 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:06:43,992648744-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:06:53,166196023-07:00] INFO: >>   data:
    pools:   2 pools, 256 pgs
    objects: 30.72k objects, 7.5 GiB
    usage:   23 GiB used, 77 GiB / 100 GiB avail
    pgs:     256 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:06:53,173810191-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:07:02,450548175-07:00] INFO: >>   data:
    pools:   2 pools, 256 pgs
    objects: 30.72k objects, 7.5 GiB
    usage:   23 GiB used, 77 GiB / 100 GiB avail
    pgs:     256 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:07:02,457950224-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:07:11,691058060-07:00] INFO: >>   data:
    pools:   2 pools, 256 pgs
    objects: 30.72k objects, 7.5 GiB
    usage:   23 GiB used, 77 GiB / 100 GiB avail
    pgs:     256 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:07:11,698780291-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:07:11,704933156-07:00] INFO: > The cluster is idle now.[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:07:11,708818498-07:00] STAGE: Start benchmarking of mode seq ...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:07:11,715979502-07:00] INFO: > Start system activity collection services[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:07:11,721649076-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:341 - rados_bench() > rm --force /mnt/sda8/ceph-research/scripts/output/sys_activity_256KB_seq.dat.rados_bench_host.3
# ./benchmarks/bench-rados:342 - rados_bench() > sar_pid_rados_bench_host=290436
# ./benchmarks/bench-rados:342 - rados_bench() > S_TIME_FORMAT=ISO
# ./benchmarks/bench-rados:342 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/output/sys_activity_256KB_seq.dat.rados_bench_host.3 2

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:07:11,728549750-07:00] INFO: >> for OSD host[0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:07:11,737592170-07:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.2 bash
10.10.2.2: b'167848\n'
[1] 03:07:11 [SUCCESS] ljishen@10.10.2.2
167848

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:07:11,926945673-07:00] INFO: > Run rados bench[0m
# ./benchmarks/bench-rados:379 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/output/rados_bench_256KB_seq.log.3
## ./benchmarks/bench-rados:376 - rados_bench() > count_cpus 32-63,96-127
# ./benchmarks/bench-rados:376 - rados_bench() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 128 --show-time
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:07:11,946821577-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:07:11,949800881-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'd4e4a370-37d5-11ec-b51d-53e6e728d2d3', '--', 'rados', 'bench', '99999999', 'seq', '--pool', 'bench_rados', '--concurrent-ios', '128', '--show-time'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid d4e4a370-37d5-11ec-b51d-53e6e728d2d3 -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 128 --show-time
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
hints = 1
2021-10-28T10:07:15.247838+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-28T10:07:15.247838+0000     0       0         0         0         0         0           -           0
2021-10-28T10:07:16.247988+0000     1     127      7896      7769   1941.83   1942.25   0.0214849   0.0163004
2021-10-28T10:07:17.248113+0000     2     128     15856     15728   1965.67   1989.75    0.016103   0.0161824
2021-10-28T10:07:18.248243+0000     3     128     23856     23728   1977.02      2000   0.0122786   0.0161087
2021-10-28T10:07:19.248403+0000 Total time run:       3.87269
Total reads made:     30717
Read size:            262144
Object size:          262144
Bandwidth (MB/sec):   1982.92
Average IOPS:         7931
Stddev IOPS:          123.249
Max IOPS:             8000
Min IOPS:             7769
Average Latency(s):   0.0160769
Max latency(s):       0.0273366
Min latency(s):       0.00652344

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:07:20,055339115-07:00] INFO: > Stop system activity collection services[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:07:20,061587830-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:391 - rados_bench() > kill -INT 290436

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:07:20,067615218-07:00] INFO: >> for OSD host[0m
# ./benchmarks/bench-rados:396 - rados_bench() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.2 kill -INT 167848
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:07:20,075800132-07:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.2 kill -INT 167848
[1] 03:07:20 [SUCCESS] ljishen@10.10.2.2

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:07:20,261749921-07:00] INFO: >>> Copy system activity file from OSD host[0m
## ./benchmarks/bench-rados:403 - rados_bench() > basename -- /tmp/bench-rados/sys_activity_256KB_seq.dat.osd_host.3
# ./benchmarks/bench-rados:403 - rados_bench() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.2:/tmp/bench-rados/sys_activity_256KB_seq.dat.osd_host.3 /mnt/sda8/ceph-research/scripts/output/sys_activity_256KB_seq.dat.osd_host.3


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:07:20,413612304-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:07:44,617967095-07:00] INFO: >>   data:
    pools:   2 pools, 256 pgs
    objects: 30.72k objects, 7.5 GiB
    usage:   23 GiB used, 77 GiB / 100 GiB avail
    pgs:     256 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:07:44,626137162-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:07:54,003383103-07:00] INFO: >>   data:
    pools:   2 pools, 256 pgs
    objects: 30.72k objects, 7.5 GiB
    usage:   23 GiB used, 77 GiB / 100 GiB avail
    pgs:     256 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:07:54,011071552-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:08:03,291713811-07:00] INFO: >>   data:
    pools:   2 pools, 256 pgs
    objects: 30.72k objects, 7.5 GiB
    usage:   23 GiB used, 77 GiB / 100 GiB avail
    pgs:     256 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:08:03,299155655-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:08:12,516343879-07:00] INFO: >>   data:
    pools:   2 pools, 256 pgs
    objects: 30.72k objects, 7.5 GiB
    usage:   23 GiB used, 77 GiB / 100 GiB avail
    pgs:     256 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:08:12,524614955-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:08:21,764984833-07:00] INFO: >>   data:
    pools:   2 pools, 256 pgs
    objects: 30.72k objects, 7.5 GiB
    usage:   23 GiB used, 77 GiB / 100 GiB avail
    pgs:     256 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:08:21,772821260-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:08:21,778682135-07:00] INFO: > The cluster is idle now.[0m


[1;7;39;49m[2021-10-28T03:08:21,782324799-07:00][RUNNING][ROUND 1/5/21] object_size=1MB[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:08:21,785751216-07:00] STAGE: Launch a Ceph cluster (CEPH_HOST: 10.10.2.1, OSD_HOST: 10.10.2.2) ...[0m
# ./benchmarks/bench-rados:183 - launch_ceph_cluster() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.1 bash -euo pipefail
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:08:21,795401161-07:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.1 bash -euo pipefail
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T03:08:22,242891108-07:00] INFO: > Remove existing clusters\x1b[0m\n'
10.10.2.1: b'## bash:17 -  > basename -- /var/lib/ceph/d4e4a370-37d5-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'# bash:17 -  > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm --host 10.10.2.1 --host 10.10.2.2 rm-cluster --force --fsid d4e4a370-37d5-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T03:08:22,253861260-07:00] DEBUG: HOST_PARAMS: ['--host', '10.10.2.1', '--host', '10.10.2.2']\x1b[0m\n"
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T03:08:22,257695278-07:00] DEBUG: CEPHADM_PARAMS: ['rm-cluster', '--force', '--fsid', 'd4e4a370-37d5-11ec-b51d-53e6e728d2d3']\x1b[0m\n"
10.10.2.1: b"# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:63 -  > /tmp/bench-rados/ceph-research/scripts/parallel-ssh --host 10.10.2.1 --host 10.10.2.2 'sudo python3 -u - rm-cluster --force --fsid d4e4a370-37d5-11ec-b51d-53e6e728d2d3'\n"
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T03:08:22,266098585-07:00] DEBUG: command from stdin\x1b[0m\n'
10.10.2.1: b"# /tmp/bench-rados/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host 10.10.2.1 --host 10.10.2.2 'sudo python3 -u - rm-cluster --force --fsid d4e4a370-37d5-11ec-b51d-53e6e728d2d3'\n"
10.10.2.1: b'[1] 03:08:27 [SUCCESS] 10.10.2.1\n[2] 03:08:35 [SUCCESS] 10.10.2.2\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T03:08:35,467530508-07:00] INFO: > Deploy a new cluster\x1b[0m\n'
10.10.2.1: b'# bash:24 -  > /tmp/bench-rados/ceph-research/scripts/ceph-deploy -m 10.10.2.1 -o node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/10.10.2.2:/dev/nvme0n1\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T03:08:35,479017562-07:00] STAGE: Verify passwordless SSH to hosts, passwordless sudo via ljishen, and the existence of devices on hosts...\x1b[0m\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T03:08:35,484559169-07:00] INFO: > Check the host for the monitor\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:181 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@10.10.2.1 sudo --non-interactive --validate\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T03:08:35,631033721-07:00] INFO: > Check hosts for OSDs\x1b[0m\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T03:08:35,635425988-07:00] INFO: >> Check host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:200 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us sudo --non-interactive --validate\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T03:08:35,787006677-07:00] INFO: >>> Check device '/dev/nvme0n1'\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:215 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us test -b /dev/nvme0n1\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:226 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us lsblk --nodeps --noheadings --output TYPE /dev/nvme0n1\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:227 - verify_parameters() > grep --word-regexp disk\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T03:08:36,075738983-07:00] STAGE: Reset devices on hosts...\x1b[0m\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T03:08:36,080839391-07:00] INFO: > Resetting device '/dev/nvme0n1' on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:288 - reset_host_devices() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us sudo sh\n'
10.10.2.1: b'  Removing ceph--9155387f--b377--4c1a--905e--3cd31030c2da-osd--block--8d0aa46d--4bb6--4165--a043--26105f5700bd (253:0)\n'
10.10.2.1: b'  Archiving volume group "ceph-9155387f-b377-4c1a-905e-3cd31030c2da" metadata (seqno 5).\n'
10.10.2.1: b'  Releasing logical volume "osd-block-8d0aa46d-4bb6-4165-a043-26105f5700bd"\n'
10.10.2.1: b'  Creating volume group backup "/etc/lvm/backup/ceph-9155387f-b377-4c1a-905e-3cd31030c2da" (seqno 6).\n'
10.10.2.1: b'  Logical volume "osd-block-8d0aa46d-4bb6-4165-a043-26105f5700bd" successfully removed\n'
10.10.2.1: b'  Removing physical volume "/dev/nvme0n1" from volume group "ceph-9155387f-b377-4c1a-905e-3cd31030c2da"\n'
10.10.2.1: b'  Volume group "ceph-9155387f-b377-4c1a-905e-3cd31030c2da" successfully removed\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T03:08:36,401843164-07:00] STAGE: Bootstrap a new cluster...\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:317 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.1 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T03:08:36,411800542-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T03:08:36,415547095-07:00] DEBUG: CEPHADM_PARAMS: ['bootstrap', '--config', '/tmp/bench-rados/ceph-research/scripts/ceph.conf', '--mon-ip', '10.10.2.1', '--output-dir', '/tmp/bench-rados/deployment_data_root/etc/ceph', '--ssh-user', 'ljishen', '--skip-dashboard', '--no-minimize-config', '--allow-overwrite', '--allow-fqdn-hostname', '--orphan-initial-daemons', '--skip-monitoring-stack', '--single-host-defaults']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.1 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults\n'
10.10.2.1: b'Verifying podman|docker is present...\nVerifying lvm2 is present...\n'
10.10.2.1: b'Verifying time synchronization is in place...\n'
10.10.2.1: b'Unit ntp.service is enabled and running\n'
10.10.2.1: b'Repeating the final host check...\n'
10.10.2.1: b'podman|docker (/usr/bin/docker) is present\n'
10.10.2.1: b'systemctl is present\n'
10.10.2.1: b'lvcreate is present\n'
10.10.2.1: b'Unit ntp.service is enabled and running\n'
10.10.2.1: b'Host looks OK\n'
10.10.2.1: b'Cluster fsid: 0400b7d8-37d7-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Verifying IP 10.10.2.1 port 3300 ...\n'
10.10.2.1: b'Verifying IP 10.10.2.1 port 6789 ...\n'
10.10.2.1: b'Mon IP `10.10.2.1` is in CIDR network `10.10.2.0/24`\n'
10.10.2.1: b'- internal network (--cluster-network) has not been provided, OSD replication will default to the public_network\n'
10.10.2.1: b'Adjusting default settings to suit single-host cluster...\n'
10.10.2.1: b'Pulling container image quay.io/ceph/ceph:v16...\n'
10.10.2.1: b'Ceph version: ceph version 16.2.6 (ee28fb57e47e9f88813e24bbf4c14496ca299d31) pacific (stable)\n'
10.10.2.1: b'Extracting ceph user uid/gid from container image...\n'
10.10.2.1: b'Creating initial keys...\n'
10.10.2.1: b'Creating initial monmap...\n'
10.10.2.1: b'Creating mon...\n'
10.10.2.1: b'Waiting for mon to start...\n'
10.10.2.1: b'Waiting for mon...\n'
10.10.2.1: b'mon is available\n'
10.10.2.1: b'Setting mon public_network to 10.10.2.0/24\n'
10.10.2.1: b'Wrote config to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf\n'
10.10.2.1: b'Wrote keyring to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring\n'
10.10.2.1: b'Creating mgr...\n'
10.10.2.1: b'Verifying port 9283 ...\n'
10.10.2.1: b'Waiting for mgr to start...\n'
10.10.2.1: b'Waiting for mgr...\n'
10.10.2.1: b'mgr not available, waiting (1/15)...\n'
10.10.2.1: b'mgr not available, waiting (2/15)...\n'
10.10.2.1: b'mgr is available\n'
10.10.2.1: b'Enabling cephadm module...\n'
10.10.2.1: b'Waiting for the mgr to restart...\n'
10.10.2.1: b'Waiting for mgr epoch 5...\n'
10.10.2.1: b'mgr epoch 5 is available\nSetting orchestrator backend to cephadm...\n'
10.10.2.1: b'Generating ssh key...\n'
10.10.2.1: b'Wrote public SSH key to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub\n'
10.10.2.1: b'Adding key to ljishen@localhost authorized_keys...\n'
10.10.2.1: b'Adding host node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us...\n'
10.10.2.1: b'Deploying unmanaged mon service...\n'
10.10.2.1: b'Deploying unmanaged mgr service...\n'
10.10.2.1: b'You can access the Ceph CLI with:\n\n\tsudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --fsid 0400b7d8-37d7-11ec-b51d-53e6e728d2d3 -c /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf -k /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring\n\nPlease consider enabling telemetry to help improve Ceph:\n\n\tceph telemetry on\n\nFor more information see:\n\n\thttps://docs.ceph.com/docs/pacific/mgr/telemetry/\n\n'
10.10.2.1: b'Bootstrap complete.\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T03:09:30,946290203-07:00] INFO: Sleep for 20 seconds\x1b[0m\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T03:09:50,953665303-07:00] INFO: > Set the default number of monitor daemons to 1\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:337 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T03:09:50,964537953-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T03:09:50,968299714-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mon', '1']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1\n'
10.10.2.1: b'Inferring fsid 0400b7d8-37d7-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/0400b7d8-37d7-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b'Scheduled mon update...\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T03:09:59,760027200-07:00] INFO: > Set the default number of manager daemons to 1\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:342 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T03:09:59,769865886-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T03:09:59,773768111-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mgr', '1']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1\n'
10.10.2.1: b'Inferring fsid 0400b7d8-37d7-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/0400b7d8-37d7-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b'Scheduled mgr update...\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T03:10:08,960326494-07:00] STAGE: Add hosts to the cluster...\x1b[0m\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T03:10:08,966144862-07:00] INFO: > Adding host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:361 - add_hosts() > ssh-copy-id -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -f -i /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\n'
10.10.2.1: b'/usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: "/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub"\n'
10.10.2.1: b'\nNumber of key(s) added: 1\n\nNow try logging into the machine, with:   "ssh -o \'GlobalKnownHostsFile=/dev/null\' -o \'LogLevel=ERROR\' -o \'PasswordAuthentication=no\' -o \'StrictHostKeyChecking=no\' -o \'UserKnownHostsFile=/dev/null\' \'ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\'"\nand check to make sure that only the key(s) you wanted were added.\n\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:367 - add_hosts() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us 10.10.2.2\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T03:10:09,176858123-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T03:10:09,180557306-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'host', 'add', 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us', '10.10.2.2']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us 10.10.2.2\n'
10.10.2.1: b'Inferring fsid 0400b7d8-37d7-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/0400b7d8-37d7-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b"Added host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us' with addr '10.10.2.2'\n"
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T03:10:18,457760168-07:00] INFO: Sleep for 20 seconds\x1b[0m\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T03:10:38,462366050-07:00] STAGE: Add OSDs to the cluster...\x1b[0m\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T03:10:38,469419851-07:00] INFO: > Adding devices ['/dev/nvme0n1'] on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us' as OSDs\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:388 - add_host_devices_as_osds() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T03:10:38,479050515-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T03:10:38,482787309-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'daemon', 'add', 'osd', 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1\n'
10.10.2.1: b'Inferring fsid 0400b7d8-37d7-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/0400b7d8-37d7-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b"Created osd(s) 0 on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'\n"
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T03:11:02,077003362-07:00] INFO: > Sleep for 20 seconds\x1b[0m\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T03:11:22,082678312-07:00] STAGE: Show Cluster Status\x1b[0m\n# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:408 - show_status() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T03:11:22,092494205-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T03:11:22,096434041-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', '--status']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status\n'
10.10.2.1: b'Inferring fsid 0400b7d8-37d7-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/0400b7d8-37d7-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b'  cluster:\n    id:     0400b7d8-37d7-11ec-b51d-53e6e728d2d3\n    health: HEALTH_WARN\n            1 pool(s) have no replicas configured\n \n  services:\n    mon: 1 daemons, quorum node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us (age 2m)\n    mgr: node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us.gpfkzx(active, since 2m)\n    osd: 1 osds: 1 up (since 19s), 1 in (since 40s)\n \n  data:\n    pools:   1 pools, 1 pgs\n    objects: 0 objects, 0 B\n    usage:   4.7 MiB used, 100 GiB / 100 GiB avail\n    pgs:     1 active+clean\n \n'
[1] 03:11:30 [SUCCESS] ljishen@10.10.2.1

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:08:22,242891108-07:00] INFO: > Remove existing clusters[0m
## bash:17 -  > basename -- /var/lib/ceph/d4e4a370-37d5-11ec-b51d-53e6e728d2d3
# bash:17 -  > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm --host 10.10.2.1 --host 10.10.2.2 rm-cluster --force --fsid d4e4a370-37d5-11ec-b51d-53e6e728d2d3
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:08:22,253861260-07:00] DEBUG: HOST_PARAMS: ['--host', '10.10.2.1', '--host', '10.10.2.2'][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:08:22,257695278-07:00] DEBUG: CEPHADM_PARAMS: ['rm-cluster', '--force', '--fsid', 'd4e4a370-37d5-11ec-b51d-53e6e728d2d3'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:63 -  > /tmp/bench-rados/ceph-research/scripts/parallel-ssh --host 10.10.2.1 --host 10.10.2.2 'sudo python3 -u - rm-cluster --force --fsid d4e4a370-37d5-11ec-b51d-53e6e728d2d3'
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:08:22,266098585-07:00] DEBUG: command from stdin[0m
# /tmp/bench-rados/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host 10.10.2.1 --host 10.10.2.2 'sudo python3 -u - rm-cluster --force --fsid d4e4a370-37d5-11ec-b51d-53e6e728d2d3'
[1] 03:08:27 [SUCCESS] 10.10.2.1
[2] 03:08:35 [SUCCESS] 10.10.2.2

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:08:35,467530508-07:00] INFO: > Deploy a new cluster[0m
# bash:24 -  > /tmp/bench-rados/ceph-research/scripts/ceph-deploy -m 10.10.2.1 -o node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/10.10.2.2:/dev/nvme0n1


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:08:35,479017562-07:00] STAGE: Verify passwordless SSH to hosts, passwordless sudo via ljishen, and the existence of devices on hosts...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:08:35,484559169-07:00] INFO: > Check the host for the monitor[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:181 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@10.10.2.1 sudo --non-interactive --validate

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:08:35,631033721-07:00] INFO: > Check hosts for OSDs[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:08:35,635425988-07:00] INFO: >> Check host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:200 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us sudo --non-interactive --validate

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:08:35,787006677-07:00] INFO: >>> Check device '/dev/nvme0n1'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:215 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us test -b /dev/nvme0n1
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:226 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us lsblk --nodeps --noheadings --output TYPE /dev/nvme0n1
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:227 - verify_parameters() > grep --word-regexp disk


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:08:36,075738983-07:00] STAGE: Reset devices on hosts...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:08:36,080839391-07:00] INFO: > Resetting device '/dev/nvme0n1' on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:288 - reset_host_devices() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us sudo sh
  Removing ceph--9155387f--b377--4c1a--905e--3cd31030c2da-osd--block--8d0aa46d--4bb6--4165--a043--26105f5700bd (253:0)
  Archiving volume group "ceph-9155387f-b377-4c1a-905e-3cd31030c2da" metadata (seqno 5).
  Releasing logical volume "osd-block-8d0aa46d-4bb6-4165-a043-26105f5700bd"
  Creating volume group backup "/etc/lvm/backup/ceph-9155387f-b377-4c1a-905e-3cd31030c2da" (seqno 6).
  Logical volume "osd-block-8d0aa46d-4bb6-4165-a043-26105f5700bd" successfully removed
  Removing physical volume "/dev/nvme0n1" from volume group "ceph-9155387f-b377-4c1a-905e-3cd31030c2da"
  Volume group "ceph-9155387f-b377-4c1a-905e-3cd31030c2da" successfully removed


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:08:36,401843164-07:00] STAGE: Bootstrap a new cluster...[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:317 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.1 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:08:36,411800542-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:08:36,415547095-07:00] DEBUG: CEPHADM_PARAMS: ['bootstrap', '--config', '/tmp/bench-rados/ceph-research/scripts/ceph.conf', '--mon-ip', '10.10.2.1', '--output-dir', '/tmp/bench-rados/deployment_data_root/etc/ceph', '--ssh-user', 'ljishen', '--skip-dashboard', '--no-minimize-config', '--allow-overwrite', '--allow-fqdn-hostname', '--orphan-initial-daemons', '--skip-monitoring-stack', '--single-host-defaults'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.1 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults
Verifying podman|docker is present...
Verifying lvm2 is present...
Verifying time synchronization is in place...
Unit ntp.service is enabled and running
Repeating the final host check...
podman|docker (/usr/bin/docker) is present
systemctl is present
lvcreate is present
Unit ntp.service is enabled and running
Host looks OK
Cluster fsid: 0400b7d8-37d7-11ec-b51d-53e6e728d2d3
Verifying IP 10.10.2.1 port 3300 ...
Verifying IP 10.10.2.1 port 6789 ...
Mon IP `10.10.2.1` is in CIDR network `10.10.2.0/24`
- internal network (--cluster-network) has not been provided, OSD replication will default to the public_network
Adjusting default settings to suit single-host cluster...
Pulling container image quay.io/ceph/ceph:v16...
Ceph version: ceph version 16.2.6 (ee28fb57e47e9f88813e24bbf4c14496ca299d31) pacific (stable)
Extracting ceph user uid/gid from container image...
Creating initial keys...
Creating initial monmap...
Creating mon...
Waiting for mon to start...
Waiting for mon...
mon is available
Setting mon public_network to 10.10.2.0/24
Wrote config to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf
Wrote keyring to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring
Creating mgr...
Verifying port 9283 ...
Waiting for mgr to start...
Waiting for mgr...
mgr not available, waiting (1/15)...
mgr not available, waiting (2/15)...
mgr is available
Enabling cephadm module...
Waiting for the mgr to restart...
Waiting for mgr epoch 5...
mgr epoch 5 is available
Setting orchestrator backend to cephadm...
Generating ssh key...
Wrote public SSH key to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub
Adding key to ljishen@localhost authorized_keys...
Adding host node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us...
Deploying unmanaged mon service...
Deploying unmanaged mgr service...
You can access the Ceph CLI with:

	sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --fsid 0400b7d8-37d7-11ec-b51d-53e6e728d2d3 -c /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf -k /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring

Please consider enabling telemetry to help improve Ceph:

	ceph telemetry on

For more information see:

	https://docs.ceph.com/docs/pacific/mgr/telemetry/

Bootstrap complete.

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:09:30,946290203-07:00] INFO: Sleep for 20 seconds[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:09:50,953665303-07:00] INFO: > Set the default number of monitor daemons to 1[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:337 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:09:50,964537953-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:09:50,968299714-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mon', '1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1
Inferring fsid 0400b7d8-37d7-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/0400b7d8-37d7-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Scheduled mon update...

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:09:59,760027200-07:00] INFO: > Set the default number of manager daemons to 1[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:342 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:09:59,769865886-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:09:59,773768111-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mgr', '1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1
Inferring fsid 0400b7d8-37d7-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/0400b7d8-37d7-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Scheduled mgr update...


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:10:08,960326494-07:00] STAGE: Add hosts to the cluster...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:10:08,966144862-07:00] INFO: > Adding host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:361 - add_hosts() > ssh-copy-id -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -f -i /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us
/usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: "/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub"

Number of key(s) added: 1

Now try logging into the machine, with:   "ssh -o 'GlobalKnownHostsFile=/dev/null' -o 'LogLevel=ERROR' -o 'PasswordAuthentication=no' -o 'StrictHostKeyChecking=no' -o 'UserKnownHostsFile=/dev/null' 'ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'"
and check to make sure that only the key(s) you wanted were added.

# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:367 - add_hosts() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us 10.10.2.2
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:10:09,176858123-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:10:09,180557306-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'host', 'add', 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us', '10.10.2.2'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us 10.10.2.2
Inferring fsid 0400b7d8-37d7-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/0400b7d8-37d7-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Added host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us' with addr '10.10.2.2'

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:10:18,457760168-07:00] INFO: Sleep for 20 seconds[0m


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:10:38,462366050-07:00] STAGE: Add OSDs to the cluster...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:10:38,469419851-07:00] INFO: > Adding devices ['/dev/nvme0n1'] on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us' as OSDs[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:388 - add_host_devices_as_osds() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:10:38,479050515-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:10:38,482787309-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'daemon', 'add', 'osd', 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1
Inferring fsid 0400b7d8-37d7-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/0400b7d8-37d7-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Created osd(s) 0 on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:11:02,077003362-07:00] INFO: > Sleep for 20 seconds[0m


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:11:22,082678312-07:00] STAGE: Show Cluster Status[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:408 - show_status() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:11:22,092494205-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:11:22,096434041-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', '--status'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status
Inferring fsid 0400b7d8-37d7-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/0400b7d8-37d7-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
  cluster:
    id:     0400b7d8-37d7-11ec-b51d-53e6e728d2d3
    health: HEALTH_WARN
            1 pool(s) have no replicas configured
 
  services:
    mon: 1 daemons, quorum node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us (age 2m)
    mgr: node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us.gpfkzx(active, since 2m)
    osd: 1 osds: 1 up (since 19s), 1 in (since 40s)
 
  data:
    pools:   1 pools, 1 pgs
    objects: 0 objects, 0 B
    usage:   4.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     1 active+clean
 

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:11:30,864426322-07:00] INFO: > Copy configuration files to local[0m
# ./benchmarks/bench-rados:220 - launch_ceph_cluster() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.1 sudo chown --changes --recursive ljishen /tmp/bench-rados/deployment_data_root/etc/ceph
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:11:30,872238744-07:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.1 sudo chown --changes --recursive ljishen /tmp/bench-rados/deployment_data_root/etc/ceph
[1] 03:11:31 [SUCCESS] ljishen@10.10.2.1
# ./benchmarks/bench-rados:223 - launch_ceph_cluster() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.1:/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf
# ./benchmarks/bench-rados:226 - launch_ceph_cluster() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.1:/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:11:31,353093049-07:00] STAGE: Update local ceph.conf as a client...[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:11:31,356846491-07:00] STAGE: Configure Ceph pool...[0m
# ./benchmarks/bench-rados:257 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_max_backfills 32
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:11:31,378989268-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:11:31,381803781-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '0400b7d8-37d7-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'config', 'set', 'osd', 'osd_max_backfills', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 0400b7d8-37d7-11ec-b51d-53e6e728d2d3 -- ceph config set osd osd_max_backfills 32
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:258 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_active 32
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:11:35,677443276-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:11:35,680610304-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '0400b7d8-37d7-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_active', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 0400b7d8-37d7-11ec-b51d-53e6e728d2d3 -- ceph config set osd osd_recovery_max_active 32
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:259 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_single_start 8
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:11:39,912386191-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:11:39,915181308-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '0400b7d8-37d7-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_single_start', '8'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 0400b7d8-37d7-11ec-b51d-53e6e728d2d3 -- ceph config set osd osd_recovery_max_single_start 8
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:260 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_op_priority 63
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:11:44,213596523-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:11:44,216563855-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '0400b7d8-37d7-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_op_priority', '63'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 0400b7d8-37d7-11ec-b51d-53e6e728d2d3 -- ceph config set osd osd_recovery_op_priority 63
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:262 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config show-with-defaults osd.0
# ./benchmarks/bench-rados:263 - configure_ceph_pool() > grep 'osd_max_backfills\|osd_recovery'
# ./benchmarks/bench-rados:263 - configure_ceph_pool() > column -t -s ' '
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
osd_max_backfills                        32         mon
osd_recovery_cost                        20971520   default
osd_recovery_delay_start                 0.000000   default
osd_recovery_max_active                  32         mon
osd_recovery_max_active_hdd              3          default
osd_recovery_max_active_ssd              10         default
osd_recovery_max_chunk                   8388608    default
osd_recovery_max_omap_entries_per_chunk  8096       default
osd_recovery_max_single_start            8          mon
osd_recovery_op_priority                 63         mon
osd_recovery_op_warn_multiple            16         default
osd_recovery_priority                    5          default
osd_recovery_retry_interval              30.000000  default
osd_recovery_sleep                       0.000000   default
osd_recovery_sleep_hdd                   0.100000   default
osd_recovery_sleep_hybrid                0.025000   default
osd_recovery_sleep_ssd                   0.000000   default
# ./benchmarks/bench-rados:265 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool create bench_rados 128 128 replicated --size 1 --pg-num-min 128
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:11:52,548580301-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:11:52,551457353-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '0400b7d8-37d7-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'create', 'bench_rados', '128', '128', 'replicated', '--size', '1', '--pg-num-min', '128'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 0400b7d8-37d7-11ec-b51d-53e6e728d2d3 -- ceph osd pool create bench_rados 128 128 replicated --size 1 --pg-num-min 128
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
pool 'bench_rados' created
# ./benchmarks/bench-rados:267 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados min_size 1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:11:57,458062396-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:11:57,461042983-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '0400b7d8-37d7-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'min_size', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 0400b7d8-37d7-11ec-b51d-53e6e728d2d3 -- ceph osd pool set bench_rados min_size 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 min_size to 1
# ./benchmarks/bench-rados:268 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados pg_autoscale_mode off
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:12:01,889024335-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:12:01,892100332-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '0400b7d8-37d7-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'pg_autoscale_mode', 'off'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 0400b7d8-37d7-11ec-b51d-53e6e728d2d3 -- ceph osd pool set bench_rados pg_autoscale_mode off
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 pg_autoscale_mode to off
# ./benchmarks/bench-rados:269 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool application enable bench_rados benchmark
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:12:06,396560603-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:12:06,399476387-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '0400b7d8-37d7-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'application', 'enable', 'bench_rados', 'benchmark'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 0400b7d8-37d7-11ec-b51d-53e6e728d2d3 -- ceph osd pool application enable bench_rados benchmark
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
enabled application 'benchmark' on pool 'bench_rados'
# ./benchmarks/bench-rados:270 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados noscrub 1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:12:11,338311375-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:12:11,341170903-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '0400b7d8-37d7-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'noscrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 0400b7d8-37d7-11ec-b51d-53e6e728d2d3 -- ceph osd pool set bench_rados noscrub 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 noscrub to 1
# ./benchmarks/bench-rados:271 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados nodeep-scrub 1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:12:15,712702790-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:12:15,715522464-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '0400b7d8-37d7-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'nodeep-scrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 0400b7d8-37d7-11ec-b51d-53e6e728d2d3 -- ceph osd pool set bench_rados nodeep-scrub 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 nodeep-scrub to 1
# ./benchmarks/bench-rados:272 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool ls detail
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:12:20,171054433-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:12:20,174213446-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '0400b7d8-37d7-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'ls', 'detail'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 0400b7d8-37d7-11ec-b51d-53e6e728d2d3 -- ceph osd pool ls detail
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
pool 1 'device_health_metrics' replicated size 1 min_size 1 crush_rule 0 object_hash rjenkins pg_num 64 pgp_num 64 autoscale_mode on last_change 19 lfor 0/0/17 flags hashpspool stripe_width 0 pg_num_min 1 application mgr_devicehealth
pool 2 'bench_rados' replicated size 1 min_size 1 crush_rule 0 object_hash rjenkins pg_num 128 pgp_num 128 autoscale_mode off last_change 20 flags hashpspool,noscrub,nodeep-scrub stripe_width 0 pg_num_min 128 application benchmark

# ./benchmarks/bench-rados:273 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd df tree
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:12:24,495765990-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:12:24,498699187-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '0400b7d8-37d7-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'df', 'tree'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 0400b7d8-37d7-11ec-b51d-53e6e728d2d3 -- ceph osd df tree
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
ID  CLASS  WEIGHT   REWEIGHT  SIZE     RAW USE  DATA     OMAP  META     AVAIL    %USE  VAR   PGS  STATUS  TYPE NAME      
-1         0.09769         -  100 GiB  5.7 MiB  168 KiB   0 B  5.6 MiB  100 GiB  0.01  1.00    -          root default   
-3         0.09769         -  100 GiB  5.7 MiB  168 KiB   0 B  5.6 MiB  100 GiB  0.01  1.00    -              host node-1
 0    ssd  0.09769   1.00000  100 GiB  5.7 MiB  168 KiB   0 B  5.6 MiB  100 GiB  0.01  1.00  192      up          osd.0  
                       TOTAL  100 GiB  5.7 MiB  168 KiB   0 B  5.6 MiB  100 GiB  0.01                                    
MIN/MAX VAR: 1.00/1.00  STDDEV: 0


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:12:28,628553641-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:12:52,885825075-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:13:02,066453115-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:13:11,564724672-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:13:20,931292912-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:13:20,938998393-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:13:30,238706650-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:13:30,246621004-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:13:39,419029343-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:13:39,426951893-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:13:48,785333555-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:13:48,793417619-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:13:58,099858483-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:13:58,107459967-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:13:58,113842735-07:00] INFO: > The cluster is idle now.[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:13:58,117724530-07:00] STAGE: Start benchmarking of mode write ...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:13:58,124870817-07:00] INFO: > Start system activity collection services[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:13:58,130683199-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:341 - rados_bench() > rm --force /mnt/sda8/ceph-research/scripts/output/sys_activity_1MB_write.dat.rados_bench_host.1
# ./benchmarks/bench-rados:342 - rados_bench() > sar_pid_rados_bench_host=296175
# ./benchmarks/bench-rados:342 - rados_bench() > S_TIME_FORMAT=ISO
# ./benchmarks/bench-rados:342 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/output/sys_activity_1MB_write.dat.rados_bench_host.1 2

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:13:58,137592700-07:00] INFO: >> for OSD host[0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:13:58,146960082-07:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.2 bash
10.10.2.2: b'171492\n'
[1] 03:13:58 [SUCCESS] ljishen@10.10.2.2
171492

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:13:58,339447470-07:00] INFO: > Run rados bench[0m
# ./benchmarks/bench-rados:370 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/output/rados_bench_1MB_write.log.1
## ./benchmarks/bench-rados:361 - rados_bench() > count_cpus 32-63,96-127
# ./benchmarks/bench-rados:361 - rados_bench() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 60 write --pool bench_rados -b 1048576 -O 1048576 --concurrent-ios 128 --show-time --write-object --write-omap --write-xattr --no-cleanup
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:13:58,359948964-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:13:58,362761804-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '0400b7d8-37d7-11ec-b51d-53e6e728d2d3', '--', 'rados', 'bench', '60', 'write', '--pool', 'bench_rados', '-b', '1048576', '-O', '1048576', '--concurrent-ios', '128', '--show-time', '--write-object', '--write-omap', '--write-xattr', '--no-cleanup'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 0400b7d8-37d7-11ec-b51d-53e6e728d2d3 -- rados bench 60 write --pool bench_rados -b 1048576 -O 1048576 --concurrent-ios 128 --show-time --write-object --write-omap --write-xattr --no-cleanup
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
hints = 1
2021-10-28T10:14:01.704615+0000 Maintaining 128 concurrent writes of 1048576 bytes to objects of size 1048576 for up to 60 seconds or 0 objects
2021-10-28T10:14:01.704628+0000 Object prefix: benchmark_data_node-2.bluefield2.ucsc-cmps10_7
2021-10-28T10:14:01.736611+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-28T10:14:01.736611+0000     0       0         0         0         0         0           -           0
2021-10-28T10:14:02.736729+0000     1     127       332       205   204.989       205    0.391734     0.41077
2021-10-28T10:14:03.736835+0000     2     127       582       455   227.482       250    0.547194    0.419516
2021-10-28T10:14:04.736916+0000     3     127       728       601   200.317       146    0.821739    0.515709
2021-10-28T10:14:05.737003+0000     4     127      1003       876   218.982       275    0.385003    0.531351
2021-10-28T10:14:06.737101+0000     5     127      1128      1001   200.183       125    0.819842      0.5322
2021-10-28T10:14:07.737220+0000     6     127      1225      1098   182.983        97      1.2389    0.595631
2021-10-28T10:14:08.737300+0000     7     127      1344      1217   173.842       119     1.10485    0.671467
2021-10-28T10:14:09.737384+0000     8     127      1467      1340   167.485       123     0.92332    0.701073
2021-10-28T10:14:10.737462+0000     9     127      1610      1483   164.763       143    0.848327    0.722013
2021-10-28T10:14:11.737573+0000    10     127      1746      1619   161.885       136     0.88431    0.737096
2021-10-28T10:14:12.737660+0000    11     127      2061      1934   175.802       315    0.389945    0.705207
2021-10-28T10:14:13.737751+0000    12     127      2173      2046   170.485       112    0.987592    0.703407
2021-10-28T10:14:14.737830+0000    13     127      2251      2124    163.37        78     1.48351    0.726698
2021-10-28T10:14:15.737941+0000    14     127      2366      2239   159.914       115     1.04588    0.759748
2021-10-28T10:14:16.738021+0000    15     127      2513      2386   159.052       147    0.877813    0.769578
2021-10-28T10:14:17.738108+0000    16     127      2639      2512   156.986       126     1.04335    0.778089
2021-10-28T10:14:18.738191+0000    17     127      2722      2595   152.633        83     1.37366    0.795991
2021-10-28T10:14:19.738300+0000    18     127      2801      2674   148.542        79     1.31228    0.817333
2021-10-28T10:14:20.738378+0000    19     127      2980      2853   150.144       179     0.54471    0.832999
2021-10-28T10:14:21.738465+0000 min lat: 0.333197 max lat: 1.67107 avg lat: 0.818652
2021-10-28T10:14:21.738465+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-28T10:14:21.738465+0000    20     127      3138      3011   150.537       158    0.664301    0.818652
2021-10-28T10:14:22.738553+0000    21     127      3231      3104   147.796        93     1.37517    0.826993
2021-10-28T10:14:23.738668+0000    22     127      3321      3194   145.169        90     1.52769    0.845452
2021-10-28T10:14:24.738751+0000    23     127      3455      3328   144.683       134    0.904475     0.86045
2021-10-28T10:14:25.738836+0000    24     127      3594      3467   144.445       139    0.865156    0.863766
2021-10-28T10:14:26.738915+0000    25     127      3686      3559   142.347        92     1.19882     0.86861
2021-10-28T10:14:27.739029+0000    26     127      3777      3650   140.372        91     1.41511    0.882203
2021-10-28T10:14:28.739108+0000    27     127      3871      3744   138.654        94     1.37906    0.897578
2021-10-28T10:14:29.739196+0000    28     127      4011      3884   138.702       140    0.865678    0.904938
2021-10-28T10:14:30.739273+0000    29     127      4131      4004   138.057       120    0.857905    0.904923
2021-10-28T10:14:31.739382+0000    30     127      4229      4102   136.721        98     1.22106    0.911141
2021-10-28T10:14:32.739457+0000    31     127      4306      4179   134.794        77     1.44723     0.92202
2021-10-28T10:14:33.739543+0000    32     127      4420      4293   134.144       114     1.10005     0.93515
2021-10-28T10:14:34.739623+0000    33     127      4548      4421   133.958       128    0.902564    0.936467
2021-10-28T10:14:35.739735+0000    34     127      4674      4547   133.723       126     1.05875    0.936452
2021-10-28T10:14:36.739811+0000    35     127      4752      4625   132.131        78     1.30563    0.942198
2021-10-28T10:14:37.739893+0000    36     127      4850      4723   131.183        98     1.47726    0.953884
2021-10-28T10:14:38.739971+0000    37     127      4980      4853    131.15       130    0.796093    0.961584
2021-10-28T10:14:39.740083+0000    38     127      5127      5000   131.567       147    0.834429     0.95857
2021-10-28T10:14:40.740158+0000    39     127      5209      5082   130.296        82     1.09206     0.95901
2021-10-28T10:14:41.740248+0000 min lat: 0.333197 max lat: 1.70366 avg lat: 0.967389
2021-10-28T10:14:41.740248+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-28T10:14:41.740248+0000    40     127      5306      5179   129.463        97     1.33444    0.967389
2021-10-28T10:14:42.740337+0000    41     127      5405      5278    128.72        99     1.25527    0.976889
2021-10-28T10:14:43.740448+0000    42     127      5531      5404   128.655       126    0.934715    0.979808
2021-10-28T10:14:44.740530+0000    43     127      5672      5545   128.942       141    0.866885    0.977534
2021-10-28T10:14:45.740616+0000    44     127      5760      5633   128.011        88     1.30064     0.98088
2021-10-28T10:14:46.740694+0000    45     127      5838      5711     126.9        78      1.2583    0.987828
2021-10-28T10:14:47.740808+0000    46     127      5960      5833   126.793       122     1.07018     0.99567
2021-10-28T10:14:48.740888+0000    47     127      6111      5984   127.308       151    0.852346    0.993367
2021-10-28T10:14:49.740957+0000    48     127      6226      6099   127.051       115      1.0699    0.992402
2021-10-28T10:14:50.741032+0000    49     127      6298      6171   125.928        72     1.35518    0.996075
2021-10-28T10:14:51.741141+0000    50     127      6400      6273   125.449       102     1.26276     1.00351
2021-10-28T10:14:52.741215+0000    51     127      6523      6396   125.401       123    0.969357      1.0078
2021-10-28T10:14:53.741303+0000    52     127      6665      6538    125.72       142    0.929131      1.0062
2021-10-28T10:14:54.741384+0000    53     127      6761      6634   125.159        96     1.23249     1.00731
2021-10-28T10:14:55.741496+0000    54     127      6852      6725   124.526        91     1.42674     1.01312
2021-10-28T10:14:56.741572+0000    55     127      6938      6811   123.825        86     1.27484     1.01874
2021-10-28T10:14:57.741660+0000    56     127      7085      6958   124.239       147    0.893979        1.02
2021-10-28T10:14:58.741734+0000    57     127      7198      7071   124.042       113    0.839039     1.01818
2021-10-28T10:14:59.741843+0000    58     127      7299      7172   123.644       101     1.32747     1.02045
2021-10-28T10:15:00.741919+0000    59     127      7385      7258   123.006        86     1.43657     1.02571
2021-10-28T10:15:01.742008+0000 min lat: 0.333197 max lat: 1.70366 avg lat: 1.03047
2021-10-28T10:15:01.742008+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-28T10:15:01.742008+0000    60     127      7511      7384   123.056       126     1.06763     1.03047
2021-10-28T10:15:02.742117+0000 Total time run:         60.2085
Total writes made:      7512
Write size:             1048576
Object size:            1048576
Bandwidth (MB/sec):     124.767
Stddev Bandwidth:       45.725
Max bandwidth (MB/sec): 315
Min bandwidth (MB/sec): 72
Average IOPS:           124
Stddev IOPS:            45.725
Max IOPS:               315
Min IOPS:               72
Average Latency(s):     1.02149
Stddev Latency(s):      0.353646
Max latency(s):         1.70366
Min latency(s):         0.185564

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:15:03,481917315-07:00] INFO: > Stop system activity collection services[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:15:03,487992753-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:391 - rados_bench() > kill -INT 296175

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:15:03,493763628-07:00] INFO: >> for OSD host[0m
# ./benchmarks/bench-rados:396 - rados_bench() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.2 kill -INT 171492
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:15:03,501650140-07:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.2 kill -INT 171492
[1] 03:15:03 [SUCCESS] ljishen@10.10.2.2

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:15:03,685540137-07:00] INFO: >>> Copy system activity file from OSD host[0m
## ./benchmarks/bench-rados:403 - rados_bench() > basename -- /tmp/bench-rados/sys_activity_1MB_write.dat.osd_host.1
# ./benchmarks/bench-rados:403 - rados_bench() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.2:/tmp/bench-rados/sys_activity_1MB_write.dat.osd_host.1 /mnt/sda8/ceph-research/scripts/output/sys_activity_1MB_write.dat.osd_host.1


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:15:03,863041424-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:15:28,127091019-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 7.51k objects, 7.3 GiB
    usage:   22 GiB used, 78 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:15:28,134586433-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:15:37,449362571-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 7.51k objects, 7.3 GiB
    usage:   22 GiB used, 78 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:15:37,457633847-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:15:46,490835548-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 7.51k objects, 7.3 GiB
    usage:   22 GiB used, 78 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:15:46,498739854-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:15:55,881201929-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 7.51k objects, 7.3 GiB
    usage:   22 GiB used, 78 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:15:55,889050720-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:16:05,221038611-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 7.51k objects, 7.3 GiB
    usage:   22 GiB used, 78 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:16:05,228567458-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:16:05,234854135-07:00] INFO: > The cluster is idle now.[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:16:05,238767089-07:00] STAGE: Start benchmarking of mode seq ...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:16:05,245832433-07:00] INFO: > Start system activity collection services[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:16:05,251466039-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:341 - rados_bench() > rm --force /mnt/sda8/ceph-research/scripts/output/sys_activity_1MB_seq.dat.rados_bench_host.1
# ./benchmarks/bench-rados:342 - rados_bench() > sar_pid_rados_bench_host=297535
# ./benchmarks/bench-rados:342 - rados_bench() > S_TIME_FORMAT=ISO
# ./benchmarks/bench-rados:342 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/output/sys_activity_1MB_seq.dat.rados_bench_host.1 2

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:16:05,258611384-07:00] INFO: >> for OSD host[0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:16:05,267806631-07:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.2 bash
10.10.2.2: b'171594\n'
[1] 03:16:05 [SUCCESS] ljishen@10.10.2.2
171594

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:16:05,458779245-07:00] INFO: > Run rados bench[0m
# ./benchmarks/bench-rados:379 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/output/rados_bench_1MB_seq.log.1
## ./benchmarks/bench-rados:376 - rados_bench() > count_cpus 32-63,96-127
# ./benchmarks/bench-rados:376 - rados_bench() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 128 --show-time
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:16:05,478991193-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:16:05,481819403-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '0400b7d8-37d7-11ec-b51d-53e6e728d2d3', '--', 'rados', 'bench', '99999999', 'seq', '--pool', 'bench_rados', '--concurrent-ios', '128', '--show-time'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 0400b7d8-37d7-11ec-b51d-53e6e728d2d3 -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 128 --show-time
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
hints = 1
2021-10-28T10:16:08.832245+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-28T10:16:08.832245+0000     0       0         0         0         0         0           -           0
2021-10-28T10:16:09.832398+0000     1     127      1880      1753   1752.61      1753   0.0689307   0.0700565
2021-10-28T10:16:10.832508+0000     2     127      3633      3506   1752.71      1753   0.0700392   0.0714823
2021-10-28T10:16:11.832589+0000     3     127      5409      5282   1760.42      1776   0.0721505   0.0716954
2021-10-28T10:16:12.832659+0000     4     127      7174      7047   1761.54      1765   0.0699798   0.0718664
2021-10-28T10:16:13.832762+0000 Total time run:       4.249
Total reads made:     7512
Read size:            1048576
Object size:          1048576
Bandwidth (MB/sec):   1767.95
Average IOPS:         1767
Stddev IOPS:          11.0567
Max IOPS:             1776
Min IOPS:             1753
Average Latency(s):   0.0716953
Max latency(s):       0.128349
Min latency(s):       0.0193894

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:16:14,502532058-07:00] INFO: > Stop system activity collection services[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:16:14,508912753-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:391 - rados_bench() > kill -INT 297535

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:16:14,514716890-07:00] INFO: >> for OSD host[0m
# ./benchmarks/bench-rados:396 - rados_bench() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.2 kill -INT 171594
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:16:14,522890452-07:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.2 kill -INT 171594
[1] 03:16:14 [SUCCESS] ljishen@10.10.2.2

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:16:14,705848262-07:00] INFO: >>> Copy system activity file from OSD host[0m
## ./benchmarks/bench-rados:403 - rados_bench() > basename -- /tmp/bench-rados/sys_activity_1MB_seq.dat.osd_host.1
# ./benchmarks/bench-rados:403 - rados_bench() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.2:/tmp/bench-rados/sys_activity_1MB_seq.dat.osd_host.1 /mnt/sda8/ceph-research/scripts/output/sys_activity_1MB_seq.dat.osd_host.1


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:16:14,861503724-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:16:39,181490832-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 7.51k objects, 7.3 GiB
    usage:   22 GiB used, 78 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:16:39,189015091-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:16:48,596110094-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 7.51k objects, 7.3 GiB
    usage:   22 GiB used, 78 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:16:48,603980475-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:16:57,889155249-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 7.51k objects, 7.3 GiB
    usage:   22 GiB used, 78 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:16:57,896794504-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:17:07,403214167-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 7.51k objects, 7.3 GiB
    usage:   22 GiB used, 78 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:17:07,411512174-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:17:16,792970225-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 7.51k objects, 7.3 GiB
    usage:   22 GiB used, 78 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:17:16,800722494-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:17:16,807015162-07:00] INFO: > The cluster is idle now.[0m


[1;7;39;49m[2021-10-28T03:17:16,809531364-07:00][RUNNING][ROUND 2/5/21] object_size=1MB[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:17:16,813219173-07:00] STAGE: Launch a Ceph cluster (CEPH_HOST: 10.10.2.1, OSD_HOST: 10.10.2.2) ...[0m
# ./benchmarks/bench-rados:183 - launch_ceph_cluster() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.1 bash -euo pipefail
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:17:16,822761205-07:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.1 bash -euo pipefail
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T03:17:17,248078743-07:00] INFO: > Remove existing clusters\x1b[0m\n'
10.10.2.1: b'## bash:17 -  > basename -- /var/lib/ceph/0400b7d8-37d7-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'# bash:17 -  > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm --host 10.10.2.1 --host 10.10.2.2 rm-cluster --force --fsid 0400b7d8-37d7-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T03:17:17,258300017-07:00] DEBUG: HOST_PARAMS: ['--host', '10.10.2.1', '--host', '10.10.2.2']\x1b[0m\n"
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T03:17:17,262005724-07:00] DEBUG: CEPHADM_PARAMS: ['rm-cluster', '--force', '--fsid', '0400b7d8-37d7-11ec-b51d-53e6e728d2d3']\x1b[0m\n"
10.10.2.1: b"# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:63 -  > /tmp/bench-rados/ceph-research/scripts/parallel-ssh --host 10.10.2.1 --host 10.10.2.2 'sudo python3 -u - rm-cluster --force --fsid 0400b7d8-37d7-11ec-b51d-53e6e728d2d3'\n"
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T03:17:17,270273646-07:00] DEBUG: command from stdin\x1b[0m\n'
10.10.2.1: b"# /tmp/bench-rados/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host 10.10.2.1 --host 10.10.2.2 'sudo python3 -u - rm-cluster --force --fsid 0400b7d8-37d7-11ec-b51d-53e6e728d2d3'\n"
10.10.2.1: b'[1] 03:17:22 [SUCCESS] 10.10.2.1\n[2] 03:17:28 [SUCCESS] 10.10.2.2\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T03:17:28,392466610-07:00] INFO: > Deploy a new cluster\x1b[0m\n'
10.10.2.1: b'# bash:24 -  > /tmp/bench-rados/ceph-research/scripts/ceph-deploy -m 10.10.2.1 -o node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/10.10.2.2:/dev/nvme0n1\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T03:17:28,404755441-07:00] STAGE: Verify passwordless SSH to hosts, passwordless sudo via ljishen, and the existence of devices on hosts...\x1b[0m\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T03:17:28,409500071-07:00] INFO: > Check the host for the monitor\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:181 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@10.10.2.1 sudo --non-interactive --validate\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T03:17:28,559164949-07:00] INFO: > Check hosts for OSDs\x1b[0m\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T03:17:28,564133399-07:00] INFO: >> Check host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:200 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us sudo --non-interactive --validate\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T03:17:28,715147013-07:00] INFO: >>> Check device '/dev/nvme0n1'\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:215 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us test -b /dev/nvme0n1\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:226 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us lsblk --nodeps --noheadings --output TYPE /dev/nvme0n1\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:227 - verify_parameters() > grep --word-regexp disk\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T03:17:29,004046363-07:00] STAGE: Reset devices on hosts...\x1b[0m\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T03:17:29,009094533-07:00] INFO: > Resetting device '/dev/nvme0n1' on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:288 - reset_host_devices() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us sudo sh\n'
10.10.2.1: b'  Removing ceph--18ffe2ee--ed0b--43b7--b37a--3a3aa6ced6d9-osd--block--125d475d--3e1b--424f--b948--3a817efabf41 (253:0)\n'
10.10.2.1: b'  Archiving volume group "ceph-18ffe2ee-ed0b-43b7-b37a-3a3aa6ced6d9" metadata (seqno 5).\n'
10.10.2.1: b'  Releasing logical volume "osd-block-125d475d-3e1b-424f-b948-3a817efabf41"\n'
10.10.2.1: b'  Creating volume group backup "/etc/lvm/backup/ceph-18ffe2ee-ed0b-43b7-b37a-3a3aa6ced6d9" (seqno 6).\n'
10.10.2.1: b'  Logical volume "osd-block-125d475d-3e1b-424f-b948-3a817efabf41" successfully removed\n  Removing physical volume "/dev/nvme0n1" from volume group "ceph-18ffe2ee-ed0b-43b7-b37a-3a3aa6ced6d9"\n'
10.10.2.1: b'  Volume group "ceph-18ffe2ee-ed0b-43b7-b37a-3a3aa6ced6d9" successfully removed\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T03:17:29,366015003-07:00] STAGE: Bootstrap a new cluster...\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:317 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.1 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T03:17:29,375811750-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T03:17:29,379432196-07:00] DEBUG: CEPHADM_PARAMS: ['bootstrap', '--config', '/tmp/bench-rados/ceph-research/scripts/ceph.conf', '--mon-ip', '10.10.2.1', '--output-dir', '/tmp/bench-rados/deployment_data_root/etc/ceph', '--ssh-user', 'ljishen', '--skip-dashboard', '--no-minimize-config', '--allow-overwrite', '--allow-fqdn-hostname', '--orphan-initial-daemons', '--skip-monitoring-stack', '--single-host-defaults']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.1 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults\n'
10.10.2.1: b'Verifying podman|docker is present...\nVerifying lvm2 is present...\n'
10.10.2.1: b'Verifying time synchronization is in place...\n'
10.10.2.1: b'Unit ntp.service is enabled and running\n'
10.10.2.1: b'Repeating the final host check...\n'
10.10.2.1: b'podman|docker (/usr/bin/docker) is present\n'
10.10.2.1: b'systemctl is present\n'
10.10.2.1: b'lvcreate is present\n'
10.10.2.1: b'Unit ntp.service is enabled and running\n'
10.10.2.1: b'Host looks OK\nCluster fsid: 41acbaae-37d8-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Verifying IP 10.10.2.1 port 3300 ...\n'
10.10.2.1: b'Verifying IP 10.10.2.1 port 6789 ...\n'
10.10.2.1: b'Mon IP `10.10.2.1` is in CIDR network `10.10.2.0/24`\n'
10.10.2.1: b'- internal network (--cluster-network) has not been provided, OSD replication will default to the public_network\n'
10.10.2.1: b'Adjusting default settings to suit single-host cluster...\n'
10.10.2.1: b'Pulling container image quay.io/ceph/ceph:v16...\n'
10.10.2.1: b'Ceph version: ceph version 16.2.6 (ee28fb57e47e9f88813e24bbf4c14496ca299d31) pacific (stable)\nExtracting ceph user uid/gid from container image...\n'
10.10.2.1: b'Creating initial keys...\n'
10.10.2.1: b'Creating initial monmap...\n'
10.10.2.1: b'Creating mon...\n'
10.10.2.1: b'Waiting for mon to start...\n'
10.10.2.1: b'Waiting for mon...\n'
10.10.2.1: b'mon is available\n'
10.10.2.1: b'Setting mon public_network to 10.10.2.0/24\n'
10.10.2.1: b'Wrote config to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf\n'
10.10.2.1: b'Wrote keyring to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring\n'
10.10.2.1: b'Creating mgr...\n'
10.10.2.1: b'Verifying port 9283 ...\n'
10.10.2.1: b'Waiting for mgr to start...\nWaiting for mgr...\n'
10.10.2.1: b'mgr not available, waiting (1/15)...\n'
10.10.2.1: b'mgr not available, waiting (2/15)...\n'
10.10.2.1: b'mgr is available\n'
10.10.2.1: b'Enabling cephadm module...\n'
10.10.2.1: b'Waiting for the mgr to restart...\n'
10.10.2.1: b'Waiting for mgr epoch 5...\n'
10.10.2.1: b'mgr epoch 5 is available\nSetting orchestrator backend to cephadm...\n'
10.10.2.1: b'Generating ssh key...\n'
10.10.2.1: b'Wrote public SSH key to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub\n'
10.10.2.1: b'Adding key to ljishen@localhost authorized_keys...\n'
10.10.2.1: b'Adding host node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us...\n'
10.10.2.1: b'Deploying unmanaged mon service...\n'
10.10.2.1: b'Deploying unmanaged mgr service...\n'
10.10.2.1: b'You can access the Ceph CLI with:\n\n\tsudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --fsid 41acbaae-37d8-11ec-b51d-53e6e728d2d3 -c /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf -k /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring\n\n'
10.10.2.1: b'Please consider enabling telemetry to help improve Ceph:\n\n\tceph telemetry on\n\nFor more information see:\n\n\thttps://docs.ceph.com/docs/pacific/mgr/telemetry/\n\n'
10.10.2.1: b'Bootstrap complete.\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T03:18:29,525093548-07:00] INFO: Sleep for 20 seconds\x1b[0m\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T03:18:49,532480313-07:00] INFO: > Set the default number of monitor daemons to 1\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:337 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T03:18:49,542043290-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T03:18:49,545546556-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mon', '1']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1\n'
10.10.2.1: b'Inferring fsid 41acbaae-37d8-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/41acbaae-37d8-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b'Scheduled mon update...\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T03:18:58,726098108-07:00] INFO: > Set the default number of manager daemons to 1\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:342 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T03:18:58,736884234-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T03:18:58,740833939-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mgr', '1']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1\n'
10.10.2.1: b'Inferring fsid 41acbaae-37d8-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/41acbaae-37d8-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b'Scheduled mgr update...\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T03:19:08,231350681-07:00] STAGE: Add hosts to the cluster...\x1b[0m\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T03:19:08,237154261-07:00] INFO: > Adding host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:361 - add_hosts() > ssh-copy-id -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -f -i /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\n'
10.10.2.1: b'/usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: "/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub"\n'
10.10.2.1: b'\nNumber of key(s) added: 1\n\nNow try logging into the machine, with:   "ssh -o \'GlobalKnownHostsFile=/dev/null\' -o \'LogLevel=ERROR\' -o \'PasswordAuthentication=no\' -o \'StrictHostKeyChecking=no\' -o \'UserKnownHostsFile=/dev/null\' \'ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\'"\nand check to make sure that only the key(s) you wanted were added.\n\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:367 - add_hosts() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us 10.10.2.2\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T03:19:08,449195088-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T03:19:08,453023705-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'host', 'add', 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us', '10.10.2.2']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us 10.10.2.2\n'
10.10.2.1: b'Inferring fsid 41acbaae-37d8-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/41acbaae-37d8-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b"Added host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us' with addr '10.10.2.2'\n"
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T03:19:17,655401728-07:00] INFO: Sleep for 20 seconds\x1b[0m\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T03:19:37,660067797-07:00] STAGE: Add OSDs to the cluster...\x1b[0m\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T03:19:37,667326562-07:00] INFO: > Adding devices ['/dev/nvme0n1'] on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us' as OSDs\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:388 - add_host_devices_as_osds() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T03:19:37,676865614-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T03:19:37,680561532-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'daemon', 'add', 'osd', 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1\n'
10.10.2.1: b'Inferring fsid 41acbaae-37d8-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/41acbaae-37d8-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b"Created osd(s) 0 on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'\n"
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T03:20:02,124324601-07:00] INFO: > Sleep for 20 seconds\x1b[0m\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T03:20:22,129867657-07:00] STAGE: Show Cluster Status\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:408 - show_status() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T03:20:22,140628315-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T03:20:22,144307582-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', '--status']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status\n'
10.10.2.1: b'Inferring fsid 41acbaae-37d8-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/41acbaae-37d8-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b'  cluster:\n    id:     41acbaae-37d8-11ec-b51d-53e6e728d2d3\n    health: HEALTH_WARN\n            1 pool(s) have no replicas configured\n \n  services:\n    mon: 1 daemons, quorum node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us (age 2m)\n    mgr: node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us.kmudvf(active, since 2m)\n    osd: 1 osds: 1 up (since 17s), 1 in (since 40s)\n \n  data:\n    pools:   1 pools, 1 pgs\n    objects: 0 objects, 0 B\n    usage:   4.7 MiB used, 100 GiB / 100 GiB avail\n    pgs:     1 active+clean\n \n'
[1] 03:20:30 [SUCCESS] ljishen@10.10.2.1

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:17:17,248078743-07:00] INFO: > Remove existing clusters[0m
## bash:17 -  > basename -- /var/lib/ceph/0400b7d8-37d7-11ec-b51d-53e6e728d2d3
# bash:17 -  > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm --host 10.10.2.1 --host 10.10.2.2 rm-cluster --force --fsid 0400b7d8-37d7-11ec-b51d-53e6e728d2d3
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:17:17,258300017-07:00] DEBUG: HOST_PARAMS: ['--host', '10.10.2.1', '--host', '10.10.2.2'][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:17:17,262005724-07:00] DEBUG: CEPHADM_PARAMS: ['rm-cluster', '--force', '--fsid', '0400b7d8-37d7-11ec-b51d-53e6e728d2d3'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:63 -  > /tmp/bench-rados/ceph-research/scripts/parallel-ssh --host 10.10.2.1 --host 10.10.2.2 'sudo python3 -u - rm-cluster --force --fsid 0400b7d8-37d7-11ec-b51d-53e6e728d2d3'
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:17:17,270273646-07:00] DEBUG: command from stdin[0m
# /tmp/bench-rados/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host 10.10.2.1 --host 10.10.2.2 'sudo python3 -u - rm-cluster --force --fsid 0400b7d8-37d7-11ec-b51d-53e6e728d2d3'
[1] 03:17:22 [SUCCESS] 10.10.2.1
[2] 03:17:28 [SUCCESS] 10.10.2.2

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:17:28,392466610-07:00] INFO: > Deploy a new cluster[0m
# bash:24 -  > /tmp/bench-rados/ceph-research/scripts/ceph-deploy -m 10.10.2.1 -o node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/10.10.2.2:/dev/nvme0n1


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:17:28,404755441-07:00] STAGE: Verify passwordless SSH to hosts, passwordless sudo via ljishen, and the existence of devices on hosts...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:17:28,409500071-07:00] INFO: > Check the host for the monitor[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:181 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@10.10.2.1 sudo --non-interactive --validate

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:17:28,559164949-07:00] INFO: > Check hosts for OSDs[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:17:28,564133399-07:00] INFO: >> Check host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:200 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us sudo --non-interactive --validate

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:17:28,715147013-07:00] INFO: >>> Check device '/dev/nvme0n1'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:215 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us test -b /dev/nvme0n1
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:226 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us lsblk --nodeps --noheadings --output TYPE /dev/nvme0n1
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:227 - verify_parameters() > grep --word-regexp disk


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:17:29,004046363-07:00] STAGE: Reset devices on hosts...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:17:29,009094533-07:00] INFO: > Resetting device '/dev/nvme0n1' on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:288 - reset_host_devices() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us sudo sh
  Removing ceph--18ffe2ee--ed0b--43b7--b37a--3a3aa6ced6d9-osd--block--125d475d--3e1b--424f--b948--3a817efabf41 (253:0)
  Archiving volume group "ceph-18ffe2ee-ed0b-43b7-b37a-3a3aa6ced6d9" metadata (seqno 5).
  Releasing logical volume "osd-block-125d475d-3e1b-424f-b948-3a817efabf41"
  Creating volume group backup "/etc/lvm/backup/ceph-18ffe2ee-ed0b-43b7-b37a-3a3aa6ced6d9" (seqno 6).
  Logical volume "osd-block-125d475d-3e1b-424f-b948-3a817efabf41" successfully removed
  Removing physical volume "/dev/nvme0n1" from volume group "ceph-18ffe2ee-ed0b-43b7-b37a-3a3aa6ced6d9"
  Volume group "ceph-18ffe2ee-ed0b-43b7-b37a-3a3aa6ced6d9" successfully removed


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:17:29,366015003-07:00] STAGE: Bootstrap a new cluster...[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:317 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.1 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:17:29,375811750-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:17:29,379432196-07:00] DEBUG: CEPHADM_PARAMS: ['bootstrap', '--config', '/tmp/bench-rados/ceph-research/scripts/ceph.conf', '--mon-ip', '10.10.2.1', '--output-dir', '/tmp/bench-rados/deployment_data_root/etc/ceph', '--ssh-user', 'ljishen', '--skip-dashboard', '--no-minimize-config', '--allow-overwrite', '--allow-fqdn-hostname', '--orphan-initial-daemons', '--skip-monitoring-stack', '--single-host-defaults'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.1 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults
Verifying podman|docker is present...
Verifying lvm2 is present...
Verifying time synchronization is in place...
Unit ntp.service is enabled and running
Repeating the final host check...
podman|docker (/usr/bin/docker) is present
systemctl is present
lvcreate is present
Unit ntp.service is enabled and running
Host looks OK
Cluster fsid: 41acbaae-37d8-11ec-b51d-53e6e728d2d3
Verifying IP 10.10.2.1 port 3300 ...
Verifying IP 10.10.2.1 port 6789 ...
Mon IP `10.10.2.1` is in CIDR network `10.10.2.0/24`
- internal network (--cluster-network) has not been provided, OSD replication will default to the public_network
Adjusting default settings to suit single-host cluster...
Pulling container image quay.io/ceph/ceph:v16...
Ceph version: ceph version 16.2.6 (ee28fb57e47e9f88813e24bbf4c14496ca299d31) pacific (stable)
Extracting ceph user uid/gid from container image...
Creating initial keys...
Creating initial monmap...
Creating mon...
Waiting for mon to start...
Waiting for mon...
mon is available
Setting mon public_network to 10.10.2.0/24
Wrote config to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf
Wrote keyring to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring
Creating mgr...
Verifying port 9283 ...
Waiting for mgr to start...
Waiting for mgr...
mgr not available, waiting (1/15)...
mgr not available, waiting (2/15)...
mgr is available
Enabling cephadm module...
Waiting for the mgr to restart...
Waiting for mgr epoch 5...
mgr epoch 5 is available
Setting orchestrator backend to cephadm...
Generating ssh key...
Wrote public SSH key to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub
Adding key to ljishen@localhost authorized_keys...
Adding host node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us...
Deploying unmanaged mon service...
Deploying unmanaged mgr service...
You can access the Ceph CLI with:

	sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --fsid 41acbaae-37d8-11ec-b51d-53e6e728d2d3 -c /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf -k /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring

Please consider enabling telemetry to help improve Ceph:

	ceph telemetry on

For more information see:

	https://docs.ceph.com/docs/pacific/mgr/telemetry/

Bootstrap complete.

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:18:29,525093548-07:00] INFO: Sleep for 20 seconds[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:18:49,532480313-07:00] INFO: > Set the default number of monitor daemons to 1[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:337 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:18:49,542043290-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:18:49,545546556-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mon', '1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1
Inferring fsid 41acbaae-37d8-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/41acbaae-37d8-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Scheduled mon update...

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:18:58,726098108-07:00] INFO: > Set the default number of manager daemons to 1[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:342 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:18:58,736884234-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:18:58,740833939-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mgr', '1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1
Inferring fsid 41acbaae-37d8-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/41acbaae-37d8-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Scheduled mgr update...


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:19:08,231350681-07:00] STAGE: Add hosts to the cluster...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:19:08,237154261-07:00] INFO: > Adding host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:361 - add_hosts() > ssh-copy-id -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -f -i /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us
/usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: "/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub"

Number of key(s) added: 1

Now try logging into the machine, with:   "ssh -o 'GlobalKnownHostsFile=/dev/null' -o 'LogLevel=ERROR' -o 'PasswordAuthentication=no' -o 'StrictHostKeyChecking=no' -o 'UserKnownHostsFile=/dev/null' 'ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'"
and check to make sure that only the key(s) you wanted were added.

# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:367 - add_hosts() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us 10.10.2.2
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:19:08,449195088-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:19:08,453023705-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'host', 'add', 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us', '10.10.2.2'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us 10.10.2.2
Inferring fsid 41acbaae-37d8-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/41acbaae-37d8-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Added host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us' with addr '10.10.2.2'

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:19:17,655401728-07:00] INFO: Sleep for 20 seconds[0m


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:19:37,660067797-07:00] STAGE: Add OSDs to the cluster...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:19:37,667326562-07:00] INFO: > Adding devices ['/dev/nvme0n1'] on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us' as OSDs[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:388 - add_host_devices_as_osds() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:19:37,676865614-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:19:37,680561532-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'daemon', 'add', 'osd', 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1
Inferring fsid 41acbaae-37d8-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/41acbaae-37d8-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Created osd(s) 0 on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:20:02,124324601-07:00] INFO: > Sleep for 20 seconds[0m


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:20:22,129867657-07:00] STAGE: Show Cluster Status[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:408 - show_status() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:20:22,140628315-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:20:22,144307582-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', '--status'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status
Inferring fsid 41acbaae-37d8-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/41acbaae-37d8-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
  cluster:
    id:     41acbaae-37d8-11ec-b51d-53e6e728d2d3
    health: HEALTH_WARN
            1 pool(s) have no replicas configured
 
  services:
    mon: 1 daemons, quorum node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us (age 2m)
    mgr: node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us.kmudvf(active, since 2m)
    osd: 1 osds: 1 up (since 17s), 1 in (since 40s)
 
  data:
    pools:   1 pools, 1 pgs
    objects: 0 objects, 0 B
    usage:   4.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     1 active+clean
 

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:20:30,911880925-07:00] INFO: > Copy configuration files to local[0m
# ./benchmarks/bench-rados:220 - launch_ceph_cluster() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.1 sudo chown --changes --recursive ljishen /tmp/bench-rados/deployment_data_root/etc/ceph
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:20:30,919801060-07:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.1 sudo chown --changes --recursive ljishen /tmp/bench-rados/deployment_data_root/etc/ceph
[1] 03:20:31 [SUCCESS] ljishen@10.10.2.1
# ./benchmarks/bench-rados:223 - launch_ceph_cluster() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.1:/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf
# ./benchmarks/bench-rados:226 - launch_ceph_cluster() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.1:/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:20:31,396707373-07:00] STAGE: Update local ceph.conf as a client...[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:20:31,400490222-07:00] STAGE: Configure Ceph pool...[0m
# ./benchmarks/bench-rados:257 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_max_backfills 32
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:20:31,422567775-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:20:31,425393640-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '41acbaae-37d8-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'config', 'set', 'osd', 'osd_max_backfills', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 41acbaae-37d8-11ec-b51d-53e6e728d2d3 -- ceph config set osd osd_max_backfills 32
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:258 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_active 32
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:20:35,782219768-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:20:35,785274885-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '41acbaae-37d8-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_active', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 41acbaae-37d8-11ec-b51d-53e6e728d2d3 -- ceph config set osd osd_recovery_max_active 32
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:259 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_single_start 8
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:20:40,114188665-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:20:40,117217473-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '41acbaae-37d8-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_single_start', '8'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 41acbaae-37d8-11ec-b51d-53e6e728d2d3 -- ceph config set osd osd_recovery_max_single_start 8
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:260 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_op_priority 63
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:20:44,465421053-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:20:44,468646771-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '41acbaae-37d8-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_op_priority', '63'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 41acbaae-37d8-11ec-b51d-53e6e728d2d3 -- ceph config set osd osd_recovery_op_priority 63
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:262 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config show-with-defaults osd.0
# ./benchmarks/bench-rados:263 - configure_ceph_pool() > grep 'osd_max_backfills\|osd_recovery'
# ./benchmarks/bench-rados:263 - configure_ceph_pool() > column -t -s ' '
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
osd_max_backfills                        32         mon
osd_recovery_cost                        20971520   default
osd_recovery_delay_start                 0.000000   default
osd_recovery_max_active                  32         mon
osd_recovery_max_active_hdd              3          default
osd_recovery_max_active_ssd              10         default
osd_recovery_max_chunk                   8388608    default
osd_recovery_max_omap_entries_per_chunk  8096       default
osd_recovery_max_single_start            8          mon
osd_recovery_op_priority                 63         mon
osd_recovery_op_warn_multiple            16         default
osd_recovery_priority                    5          default
osd_recovery_retry_interval              30.000000  default
osd_recovery_sleep                       0.000000   default
osd_recovery_sleep_hdd                   0.100000   default
osd_recovery_sleep_hybrid                0.025000   default
osd_recovery_sleep_ssd                   0.000000   default
# ./benchmarks/bench-rados:265 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool create bench_rados 128 128 replicated --size 1 --pg-num-min 128
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:20:52,846861340-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:20:52,849803735-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '41acbaae-37d8-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'create', 'bench_rados', '128', '128', 'replicated', '--size', '1', '--pg-num-min', '128'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 41acbaae-37d8-11ec-b51d-53e6e728d2d3 -- ceph osd pool create bench_rados 128 128 replicated --size 1 --pg-num-min 128
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
pool 'bench_rados' created
# ./benchmarks/bench-rados:267 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados min_size 1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:20:57,853302909-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:20:57,856579212-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '41acbaae-37d8-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'min_size', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 41acbaae-37d8-11ec-b51d-53e6e728d2d3 -- ceph osd pool set bench_rados min_size 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 min_size to 1
# ./benchmarks/bench-rados:268 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados pg_autoscale_mode off
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:21:02,905011890-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:21:02,908093978-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '41acbaae-37d8-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'pg_autoscale_mode', 'off'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 41acbaae-37d8-11ec-b51d-53e6e728d2d3 -- ceph osd pool set bench_rados pg_autoscale_mode off
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 pg_autoscale_mode to off
# ./benchmarks/bench-rados:269 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool application enable bench_rados benchmark
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:21:08,103328573-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:21:08,106389661-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '41acbaae-37d8-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'application', 'enable', 'bench_rados', 'benchmark'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 41acbaae-37d8-11ec-b51d-53e6e728d2d3 -- ceph osd pool application enable bench_rados benchmark
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
enabled application 'benchmark' on pool 'bench_rados'
# ./benchmarks/bench-rados:270 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados noscrub 1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:21:13,495056875-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:21:13,498156677-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '41acbaae-37d8-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'noscrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 41acbaae-37d8-11ec-b51d-53e6e728d2d3 -- ceph osd pool set bench_rados noscrub 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 noscrub to 1
# ./benchmarks/bench-rados:271 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados nodeep-scrub 1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:21:18,777826162-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:21:18,781006997-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '41acbaae-37d8-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'nodeep-scrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 41acbaae-37d8-11ec-b51d-53e6e728d2d3 -- ceph osd pool set bench_rados nodeep-scrub 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 nodeep-scrub to 1
# ./benchmarks/bench-rados:272 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool ls detail
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:21:24,027765404-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:21:24,030645080-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '41acbaae-37d8-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'ls', 'detail'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 41acbaae-37d8-11ec-b51d-53e6e728d2d3 -- ceph osd pool ls detail
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
pool 1 'device_health_metrics' replicated size 1 min_size 1 crush_rule 0 object_hash rjenkins pg_num 64 pgp_num 64 autoscale_mode on last_change 19 lfor 0/0/16 flags hashpspool stripe_width 0 pg_num_min 1 application mgr_devicehealth
pool 2 'bench_rados' replicated size 1 min_size 1 crush_rule 0 object_hash rjenkins pg_num 128 pgp_num 128 autoscale_mode off last_change 21 flags hashpspool,noscrub,nodeep-scrub stripe_width 0 pg_num_min 128 application benchmark

# ./benchmarks/bench-rados:273 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd df tree
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:21:28,269490863-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:21:28,272688458-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '41acbaae-37d8-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'df', 'tree'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 41acbaae-37d8-11ec-b51d-53e6e728d2d3 -- ceph osd df tree
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
ID  CLASS  WEIGHT   REWEIGHT  SIZE     RAW USE  DATA     OMAP  META     AVAIL    %USE  VAR   PGS  STATUS  TYPE NAME      
-1         0.09769         -  100 GiB  5.7 MiB  176 KiB   0 B  5.6 MiB  100 GiB  0.01  1.00    -          root default   
-3         0.09769         -  100 GiB  5.7 MiB  176 KiB   0 B  5.6 MiB  100 GiB  0.01  1.00    -              host node-1
 0    ssd  0.09769   1.00000  100 GiB  5.7 MiB  176 KiB   0 B  5.6 MiB  100 GiB  0.01  1.00  192      up          osd.0  
                       TOTAL  100 GiB  5.7 MiB  176 KiB   0 B  5.6 MiB  100 GiB  0.01                                    
MIN/MAX VAR: 1.00/1.00  STDDEV: 0


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:21:32,471373631-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:21:56,818573606-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:22:06,160260513-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:22:15,408124859-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:22:15,415863392-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:22:24,831730445-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:22:24,839157100-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:22:34,297230969-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:22:34,304621997-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:22:43,663083454-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:22:43,670811587-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:22:53,061738723-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:22:53,069981425-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:22:53,076207268-07:00] INFO: > The cluster is idle now.[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:22:53,079976410-07:00] STAGE: Start benchmarking of mode write ...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:22:53,087131784-07:00] INFO: > Start system activity collection services[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:22:53,093033064-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:341 - rados_bench() > rm --force /mnt/sda8/ceph-research/scripts/output/sys_activity_1MB_write.dat.rados_bench_host.2
# ./benchmarks/bench-rados:342 - rados_bench() > sar_pid_rados_bench_host=303118
# ./benchmarks/bench-rados:342 - rados_bench() > S_TIME_FORMAT=ISO
# ./benchmarks/bench-rados:342 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/output/sys_activity_1MB_write.dat.rados_bench_host.2 2

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:22:53,100204599-07:00] INFO: >> for OSD host[0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:22:53,109159143-07:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.2 bash
10.10.2.2: b'175313\n'
[1] 03:22:53 [SUCCESS] ljishen@10.10.2.2
175313

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:22:53,299229596-07:00] INFO: > Run rados bench[0m
# ./benchmarks/bench-rados:370 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/output/rados_bench_1MB_write.log.2
## ./benchmarks/bench-rados:361 - rados_bench() > count_cpus 32-63,96-127
# ./benchmarks/bench-rados:361 - rados_bench() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 60 write --pool bench_rados -b 1048576 -O 1048576 --concurrent-ios 128 --show-time --write-object --write-omap --write-xattr --no-cleanup
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:22:53,319527145-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:22:53,322738817-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '41acbaae-37d8-11ec-b51d-53e6e728d2d3', '--', 'rados', 'bench', '60', 'write', '--pool', 'bench_rados', '-b', '1048576', '-O', '1048576', '--concurrent-ios', '128', '--show-time', '--write-object', '--write-omap', '--write-xattr', '--no-cleanup'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 41acbaae-37d8-11ec-b51d-53e6e728d2d3 -- rados bench 60 write --pool bench_rados -b 1048576 -O 1048576 --concurrent-ios 128 --show-time --write-object --write-omap --write-xattr --no-cleanup
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
hints = 1
2021-10-28T10:22:56.663919+0000 Maintaining 128 concurrent writes of 1048576 bytes to objects of size 1048576 for up to 60 seconds or 0 objects
2021-10-28T10:22:56.663932+0000 Object prefix: benchmark_data_node-2.bluefield2.ucsc-cmps10_7
2021-10-28T10:22:56.695861+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-28T10:22:56.695861+0000     0       0         0         0         0         0           -           0
2021-10-28T10:22:57.695995+0000     1     127       336       209   208.985       209     0.41388    0.404328
2021-10-28T10:22:58.696109+0000     2     127       579       452   225.979       243    0.510351    0.414577
2021-10-28T10:22:59.696221+0000     3     127       727       600    199.98       148    0.924873    0.515385
2021-10-28T10:23:00.696332+0000     4     127      1026       899   224.727       299    0.348494    0.522847
2021-10-28T10:23:01.696420+0000     5     127      1135      1008    201.58       109    0.841928    0.525981
2021-10-28T10:23:02.696529+0000     6     127      1234      1107   184.481        99     1.30284    0.594449
2021-10-28T10:23:03.696642+0000     7     127      1346      1219   174.125       112     1.21292    0.667337
2021-10-28T10:23:04.696754+0000     8     127      1494      1367   170.857       148    0.810923    0.695081
2021-10-28T10:23:05.696842+0000     9     127      1629      1502   166.872       135    0.825012    0.713082
2021-10-28T10:23:06.696952+0000    10     127      1761      1634   163.383       132    0.858705    0.729448
2021-10-28T10:23:07.697053+0000    11     127      2069      1942   176.527       308    0.407521    0.701666
2021-10-28T10:23:08.697175+0000    12     127      2175      2048   170.649       106     1.03856    0.701625
2021-10-28T10:23:09.697268+0000    13     127      2254      2127   163.598        79      1.2975    0.726309
2021-10-28T10:23:10.697379+0000    14     127      2375      2248   160.555       121     1.09525    0.760323
2021-10-28T10:23:11.697490+0000    15     127      2524      2397   159.783       149    0.865977    0.769537
2021-10-28T10:23:12.697605+0000    16     127      2632      2505   156.546       108     1.03739    0.776083
2021-10-28T10:23:13.697696+0000    17     127      2717      2590   152.337        85     1.36838    0.793178
2021-10-28T10:23:14.697810+0000    18     127      2814      2687   149.262        97     1.36718    0.818187
2021-10-28T10:23:15.697920+0000    19     127      3011      2884   151.774       197    0.346717     0.82722
2021-10-28T10:23:16.698028+0000 min lat: 0.331386 max lat: 1.5406 avg lat: 0.809451
2021-10-28T10:23:16.698028+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-28T10:23:16.698028+0000    20     127      3174      3047   152.334       163    0.804302    0.809451
2021-10-28T10:23:17.698123+0000    21     127      3258      3131    149.08        84     1.33163    0.820069
2021-10-28T10:23:18.698236+0000    22     127      3357      3230   146.803        99      1.2531    0.839057
2021-10-28T10:23:19.698351+0000    23     127      3505      3378   146.854       148     0.85185    0.847085
2021-10-28T10:23:20.698462+0000    24     127      3633      3506   146.068       128    0.953774    0.848575
2021-10-28T10:23:21.698557+0000    25     127      3729      3602   144.065        96     1.38743    0.858551
2021-10-28T10:23:22.698683+0000    26     127      3812      3685   141.716        83     1.46747     0.87271
2021-10-28T10:23:23.698793+0000    27     127      3936      3809   141.059       124     1.04411    0.887198
2021-10-28T10:23:24.698902+0000    28     127      4079      3952   141.128       143    0.800989    0.887872
2021-10-28T10:23:25.698992+0000    29     127      4178      4051   139.675        99     1.07822    0.889612
2021-10-28T10:23:26.699105+0000    30     127      4276      4149   138.285        98     1.33082    0.900717
2021-10-28T10:23:27.699207+0000    31     127      4360      4233   136.534        84     1.39304    0.912322
2021-10-28T10:23:28.699319+0000    32     127      4506      4379   136.829       146    0.798248    0.918687
2021-10-28T10:23:29.699410+0000    33     127      4642      4515   136.804       136    0.773599    0.917407
2021-10-28T10:23:30.699522+0000    34     127      4736      4609   135.544        94     1.24567    0.922232
2021-10-28T10:23:31.699632+0000    35     127      4824      4697   134.186        88     1.36171    0.932486
2021-10-28T10:23:32.699741+0000    36     127      4929      4802   133.375       105     1.10104    0.941763
2021-10-28T10:23:33.699832+0000    37     127      5082      4955   133.905       153    0.855086     0.94166
2021-10-28T10:23:34.699942+0000    38     127      5197      5070   133.407       115     1.08405    0.941515
2021-10-28T10:23:35.700052+0000    39     127      5276      5149   132.012        79     1.28969    0.947287
2021-10-28T10:23:36.700166+0000 min lat: 0.331386 max lat: 1.70773 avg lat: 0.956612
2021-10-28T10:23:36.700166+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-28T10:23:36.700166+0000    40     127      5370      5243   131.061        94     1.42453    0.956612
2021-10-28T10:23:37.700271+0000    41     127      5500      5373   131.035       130    0.760923    0.961736
2021-10-28T10:23:38.700382+0000    42     127      5646      5519   131.391       146    0.880366    0.959223
2021-10-28T10:23:39.700497+0000    43     127      5753      5626   130.823       107     1.14834    0.961529
2021-10-28T10:23:40.700610+0000    44     127      5829      5702   129.577        76     1.41193    0.967444
2021-10-28T10:23:41.700699+0000    45     127      5947      5820    129.32       118     1.12928    0.976017
2021-10-28T10:23:42.700817+0000    46     127      6091      5964   129.638       144     0.81555    0.974623
2021-10-28T10:23:43.700935+0000    47     127      6209      6082    129.39       118    0.974826    0.973216
2021-10-28T10:23:44.701047+0000    48     127      6289      6162   128.361        80     1.38169    0.977015
2021-10-28T10:23:45.701138+0000    49     127      6388      6261   127.762        99     1.34114    0.984843
2021-10-28T10:23:46.701250+0000    50     127      6522      6395   127.886       134    0.862553    0.989635
2021-10-28T10:23:47.701366+0000    51     127      6663      6536   128.143       141    0.826912    0.987358
2021-10-28T10:23:48.701478+0000    52     127      6750      6623   127.352        87     1.14216    0.987815
2021-10-28T10:23:49.701569+0000    53     127      6845      6718   126.741        95     1.43823    0.993425
2021-10-28T10:23:50.701682+0000    54     127      6948      6821   126.301       103     1.14268    0.999434
2021-10-28T10:23:51.701794+0000    55     127      7103      6976   126.823       155    0.828786    0.998685
2021-10-28T10:23:52.701909+0000    56     127      7236      7109   126.933       133     0.97969    0.996616
2021-10-28T10:23:53.701999+0000    57     127      7319      7192   126.162        83     1.31458    0.999548
2021-10-28T10:23:54.702111+0000    58     127      7399      7272   125.366        80     1.29039     1.00455
2021-10-28T10:23:55.702223+0000    59     127      7544      7417   125.698       145     0.95653     1.00892
2021-10-28T10:23:56.702337+0000 min lat: 0.331386 max lat: 1.70773 avg lat: 1.00586
2021-10-28T10:23:56.702337+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-28T10:23:56.702337+0000    60     127      7689      7562    126.02       145    0.839512     1.00586
2021-10-28T10:23:57.702468+0000 Total time run:         60.2132
Total writes made:      7690
Write size:             1048576
Object size:            1048576
Bandwidth (MB/sec):     127.713
Stddev Bandwidth:       47.047
Max bandwidth (MB/sec): 308
Min bandwidth (MB/sec): 76
Average IOPS:           127
Stddev IOPS:            47.047
Max IOPS:               308
Min IOPS:               76
Average Latency(s):     0.997114
Stddev Latency(s):      0.347584
Max latency(s):         1.70773
Min latency(s):         0.185777

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:23:58,399480023-07:00] INFO: > Stop system activity collection services[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:23:58,405482564-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:391 - rados_bench() > kill -INT 303118

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:23:58,411650958-07:00] INFO: >> for OSD host[0m
# ./benchmarks/bench-rados:396 - rados_bench() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.2 kill -INT 175313
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:23:58,419486905-07:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.2 kill -INT 175313
[1] 03:23:58 [SUCCESS] ljishen@10.10.2.2

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:23:58,602011398-07:00] INFO: >>> Copy system activity file from OSD host[0m
## ./benchmarks/bench-rados:403 - rados_bench() > basename -- /tmp/bench-rados/sys_activity_1MB_write.dat.osd_host.2
# ./benchmarks/bench-rados:403 - rados_bench() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.2:/tmp/bench-rados/sys_activity_1MB_write.dat.osd_host.2 /mnt/sda8/ceph-research/scripts/output/sys_activity_1MB_write.dat.osd_host.2


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:23:58,780026654-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:24:23,004405611-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 7.69k objects, 7.5 GiB
    usage:   23 GiB used, 77 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:24:23,012246066-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:24:32,419540743-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 7.69k objects, 7.5 GiB
    usage:   23 GiB used, 77 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:24:32,427751004-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:24:41,870910995-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 7.69k objects, 7.5 GiB
    usage:   23 GiB used, 77 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:24:41,878817494-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:24:51,176359749-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 7.69k objects, 7.5 GiB
    usage:   23 GiB used, 77 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:24:51,185098778-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:25:00,635066998-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 7.69k objects, 7.5 GiB
    usage:   23 GiB used, 77 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:25:00,643207078-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:25:00,649445513-07:00] INFO: > The cluster is idle now.[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:25:00,653069683-07:00] STAGE: Start benchmarking of mode seq ...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:25:00,660347647-07:00] INFO: > Start system activity collection services[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:25:00,666215625-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:341 - rados_bench() > rm --force /mnt/sda8/ceph-research/scripts/output/sys_activity_1MB_seq.dat.rados_bench_host.2
# ./benchmarks/bench-rados:342 - rados_bench() > sar_pid_rados_bench_host=304488
# ./benchmarks/bench-rados:342 - rados_bench() > S_TIME_FORMAT=ISO
# ./benchmarks/bench-rados:342 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/output/sys_activity_1MB_seq.dat.rados_bench_host.2 2

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:25:00,673598698-07:00] INFO: >> for OSD host[0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:25:00,682864960-07:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.2 bash
10.10.2.2: b'175409\n'
[1] 03:25:00 [SUCCESS] ljishen@10.10.2.2
175409

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:25:00,875211001-07:00] INFO: > Run rados bench[0m
# ./benchmarks/bench-rados:379 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/output/rados_bench_1MB_seq.log.2
## ./benchmarks/bench-rados:376 - rados_bench() > count_cpus 32-63,96-127
# ./benchmarks/bench-rados:376 - rados_bench() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 128 --show-time
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:25:00,895465609-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:25:00,898387916-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '41acbaae-37d8-11ec-b51d-53e6e728d2d3', '--', 'rados', 'bench', '99999999', 'seq', '--pool', 'bench_rados', '--concurrent-ios', '128', '--show-time'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 41acbaae-37d8-11ec-b51d-53e6e728d2d3 -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 128 --show-time
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
hints = 1
2021-10-28T10:25:04.579544+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-28T10:25:04.579544+0000     0       0         0         0         0         0           -           0
2021-10-28T10:25:05.579696+0000     1     127      1630      1503   1502.66      1503   0.0836948   0.0811895
2021-10-28T10:25:06.579777+0000     2     127      3125      2998   1498.77      1495    0.116044   0.0833443
2021-10-28T10:25:07.579895+0000     3     127      4641      4514   1504.45      1516   0.0839776   0.0836957
2021-10-28T10:25:08.580008+0000     4     127      6144      6017   1504.05      1503   0.0839289   0.0840387
2021-10-28T10:25:09.580084+0000     5     127      7643      7516   1503.02      1499   0.0843504    0.084275
2021-10-28T10:25:10.580245+0000 Total time run:       5.09659
Total reads made:     7690
Read size:            1048576
Object size:          1048576
Bandwidth (MB/sec):   1508.85
Average IOPS:         1508
Stddev IOPS:          7.8867
Max IOPS:             1516
Min IOPS:             1495
Average Latency(s):   0.0840424
Max latency(s):       0.141611
Min latency(s):       0.0217897

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:25:11,257750941-07:00] INFO: > Stop system activity collection services[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:25:11,264131224-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:391 - rados_bench() > kill -INT 304488

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:25:11,270495648-07:00] INFO: >> for OSD host[0m
# ./benchmarks/bench-rados:396 - rados_bench() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.2 kill -INT 175409
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:25:11,278412817-07:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.2 kill -INT 175409
[1] 03:25:11 [SUCCESS] ljishen@10.10.2.2

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:25:11,465700706-07:00] INFO: >>> Copy system activity file from OSD host[0m
## ./benchmarks/bench-rados:403 - rados_bench() > basename -- /tmp/bench-rados/sys_activity_1MB_seq.dat.osd_host.2
# ./benchmarks/bench-rados:403 - rados_bench() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.2:/tmp/bench-rados/sys_activity_1MB_seq.dat.osd_host.2 /mnt/sda8/ceph-research/scripts/output/sys_activity_1MB_seq.dat.osd_host.2


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:25:11,618554348-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:25:36,063310454-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 7.69k objects, 7.5 GiB
    usage:   23 GiB used, 77 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:25:36,070793866-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:25:45,528572023-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 7.69k objects, 7.5 GiB
    usage:   23 GiB used, 77 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:25:45,536751306-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:25:54,940707399-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 7.69k objects, 7.5 GiB
    usage:   23 GiB used, 77 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:25:54,948835145-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:26:04,313266776-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 7.69k objects, 7.5 GiB
    usage:   23 GiB used, 77 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:26:04,321319932-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:26:13,548869299-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 7.69k objects, 7.5 GiB
    usage:   23 GiB used, 77 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:26:13,556883622-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:26:13,563497124-07:00] INFO: > The cluster is idle now.[0m


[1;7;39;49m[2021-10-28T03:26:13,566147038-07:00][RUNNING][ROUND 3/5/21] object_size=1MB[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:26:13,569815361-07:00] STAGE: Launch a Ceph cluster (CEPH_HOST: 10.10.2.1, OSD_HOST: 10.10.2.2) ...[0m
# ./benchmarks/bench-rados:183 - launch_ceph_cluster() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.1 bash -euo pipefail
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:26:13,579045164-07:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.1 bash -euo pipefail
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T03:26:13,996270533-07:00] INFO: > Remove existing clusters\x1b[0m\n'
10.10.2.1: b'## bash:17 -  > basename -- /var/lib/ceph/41acbaae-37d8-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'# bash:17 -  > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm --host 10.10.2.1 --host 10.10.2.2 rm-cluster --force --fsid 41acbaae-37d8-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T03:26:14,006704959-07:00] DEBUG: HOST_PARAMS: ['--host', '10.10.2.1', '--host', '10.10.2.2']\x1b[0m\n"
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T03:26:14,010055147-07:00] DEBUG: CEPHADM_PARAMS: ['rm-cluster', '--force', '--fsid', '41acbaae-37d8-11ec-b51d-53e6e728d2d3']\x1b[0m\n"
10.10.2.1: b"# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:63 -  > /tmp/bench-rados/ceph-research/scripts/parallel-ssh --host 10.10.2.1 --host 10.10.2.2 'sudo python3 -u - rm-cluster --force --fsid 41acbaae-37d8-11ec-b51d-53e6e728d2d3'\n"
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T03:26:14,018735575-07:00] DEBUG: command from stdin\x1b[0m\n# /tmp/bench-rados/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host 10.10.2.1 --host 10.10.2.2 'sudo python3 -u - rm-cluster --force --fsid 41acbaae-37d8-11ec-b51d-53e6e728d2d3'\n"
10.10.2.1: b'[1] 03:26:19 [SUCCESS] 10.10.2.1\n[2] 03:26:25 [SUCCESS] 10.10.2.2\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T03:26:25,809941035-07:00] INFO: > Deploy a new cluster\x1b[0m\n'
10.10.2.1: b'# bash:24 -  > /tmp/bench-rados/ceph-research/scripts/ceph-deploy -m 10.10.2.1 -o node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/10.10.2.2:/dev/nvme0n1\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T03:26:25,823711552-07:00] STAGE: Verify passwordless SSH to hosts, passwordless sudo via ljishen, and the existence of devices on hosts...\x1b[0m\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T03:26:25,828653943-07:00] INFO: > Check the host for the monitor\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:181 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@10.10.2.1 sudo --non-interactive --validate\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T03:26:25,978947827-07:00] INFO: > Check hosts for OSDs\x1b[0m\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T03:26:25,983611624-07:00] INFO: >> Check host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:200 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us sudo --non-interactive --validate\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T03:26:26,135595596-07:00] INFO: >>> Check device '/dev/nvme0n1'\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:215 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us test -b /dev/nvme0n1\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:226 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us lsblk --nodeps --noheadings --output TYPE /dev/nvme0n1\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:227 - verify_parameters() > grep --word-regexp disk\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T03:26:26,423455958-07:00] STAGE: Reset devices on hosts...\x1b[0m\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T03:26:26,428307057-07:00] INFO: > Resetting device '/dev/nvme0n1' on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:288 - reset_host_devices() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us sudo sh\n'
10.10.2.1: b'  Removing ceph--1f0d1766--7e9e--4805--870f--91e25f0e4b50-osd--block--09e3b319--0fb8--43cb--9ae4--44338a977bd8 (253:0)\n'
10.10.2.1: b'  Archiving volume group "ceph-1f0d1766-7e9e-4805-870f-91e25f0e4b50" metadata (seqno 5).\n'
10.10.2.1: b'  Releasing logical volume "osd-block-09e3b319-0fb8-43cb-9ae4-44338a977bd8"\n'
10.10.2.1: b'  Creating volume group backup "/etc/lvm/backup/ceph-1f0d1766-7e9e-4805-870f-91e25f0e4b50" (seqno 6).\n'
10.10.2.1: b'  Logical volume "osd-block-09e3b319-0fb8-43cb-9ae4-44338a977bd8" successfully removed\n'
10.10.2.1: b'  Removing physical volume "/dev/nvme0n1" from volume group "ceph-1f0d1766-7e9e-4805-870f-91e25f0e4b50"\n'
10.10.2.1: b'  Volume group "ceph-1f0d1766-7e9e-4805-870f-91e25f0e4b50" successfully removed\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T03:26:26,813936092-07:00] STAGE: Bootstrap a new cluster...\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:317 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.1 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T03:26:26,824441631-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T03:26:26,827853806-07:00] DEBUG: CEPHADM_PARAMS: ['bootstrap', '--config', '/tmp/bench-rados/ceph-research/scripts/ceph.conf', '--mon-ip', '10.10.2.1', '--output-dir', '/tmp/bench-rados/deployment_data_root/etc/ceph', '--ssh-user', 'ljishen', '--skip-dashboard', '--no-minimize-config', '--allow-overwrite', '--allow-fqdn-hostname', '--orphan-initial-daemons', '--skip-monitoring-stack', '--single-host-defaults']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.1 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults\n'
10.10.2.1: b'Verifying podman|docker is present...\n'
10.10.2.1: b'Verifying lvm2 is present...\n'
10.10.2.1: b'Verifying time synchronization is in place...\n'
10.10.2.1: b'Unit ntp.service is enabled and running\n'
10.10.2.1: b'Repeating the final host check...\n'
10.10.2.1: b'podman|docker (/usr/bin/docker) is present\n'
10.10.2.1: b'systemctl is present\n'
10.10.2.1: b'lvcreate is present\n'
10.10.2.1: b'Unit ntp.service is enabled and running\n'
10.10.2.1: b'Host looks OK\n'
10.10.2.1: b'Cluster fsid: 8204cf96-37d9-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Verifying IP 10.10.2.1 port 3300 ...\n'
10.10.2.1: b'Verifying IP 10.10.2.1 port 6789 ...\n'
10.10.2.1: b'Mon IP `10.10.2.1` is in CIDR network `10.10.2.0/24`\n'
10.10.2.1: b'- internal network (--cluster-network) has not been provided, OSD replication will default to the public_network\n'
10.10.2.1: b'Adjusting default settings to suit single-host cluster...\n'
10.10.2.1: b'Pulling container image quay.io/ceph/ceph:v16...\n'
10.10.2.1: b'Ceph version: ceph version 16.2.6 (ee28fb57e47e9f88813e24bbf4c14496ca299d31) pacific (stable)\nExtracting ceph user uid/gid from container image...\n'
10.10.2.1: b'Creating initial keys...\n'
10.10.2.1: b'Creating initial monmap...\n'
10.10.2.1: b'Creating mon...\n'
10.10.2.1: b'Waiting for mon to start...\n'
10.10.2.1: b'Waiting for mon...\n'
10.10.2.1: b'mon is available\nSetting mon public_network to 10.10.2.0/24\n'
10.10.2.1: b'Wrote config to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf\n'
10.10.2.1: b'Wrote keyring to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring\n'
10.10.2.1: b'Creating mgr...\n'
10.10.2.1: b'Verifying port 9283 ...\n'
10.10.2.1: b'Waiting for mgr to start...\nWaiting for mgr...\n'
10.10.2.1: b'mgr not available, waiting (1/15)...\n'
10.10.2.1: b'mgr not available, waiting (2/15)...\n'
10.10.2.1: b'mgr is available\n'
10.10.2.1: b'Enabling cephadm module...\n'
10.10.2.1: b'Waiting for the mgr to restart...\nWaiting for mgr epoch 5...\n'
10.10.2.1: b'mgr epoch 5 is available\nSetting orchestrator backend to cephadm...\n'
10.10.2.1: b'Generating ssh key...\n'
10.10.2.1: b'Wrote public SSH key to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub\nAdding key to ljishen@localhost authorized_keys...\n'
10.10.2.1: b'Adding host node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us...\n'
10.10.2.1: b'Deploying unmanaged mon service...\n'
10.10.2.1: b'Deploying unmanaged mgr service...\n'
10.10.2.1: b'You can access the Ceph CLI with:\n\n\tsudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --fsid 8204cf96-37d9-11ec-b51d-53e6e728d2d3 -c /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf -k /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring\n\n'
10.10.2.1: b'Please consider enabling telemetry to help improve Ceph:\n\n\tceph telemetry on\n\nFor more information see:\n\n\thttps://docs.ceph.com/docs/pacific/mgr/telemetry/\n\n'
10.10.2.1: b'Bootstrap complete.\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T03:27:27,793348018-07:00] INFO: Sleep for 20 seconds\x1b[0m\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T03:27:47,800968554-07:00] INFO: > Set the default number of monitor daemons to 1\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:337 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T03:27:47,811686373-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T03:27:47,815558061-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mon', '1']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1\n'
10.10.2.1: b'Inferring fsid 8204cf96-37d9-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/8204cf96-37d9-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b'Scheduled mon update...\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T03:27:56,881123469-07:00] INFO: > Set the default number of manager daemons to 1\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:342 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T03:27:56,891869760-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T03:27:56,895681095-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mgr', '1']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1\n'
10.10.2.1: b'Inferring fsid 8204cf96-37d9-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/8204cf96-37d9-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b'Scheduled mgr update...\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T03:28:05,923086917-07:00] STAGE: Add hosts to the cluster...\x1b[0m\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T03:28:05,929523027-07:00] INFO: > Adding host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:361 - add_hosts() > ssh-copy-id -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -f -i /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\n'
10.10.2.1: b'/usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: "/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub"\n'
10.10.2.1: b'\nNumber of key(s) added: 1\n\nNow try logging into the machine, with:   "ssh -o \'GlobalKnownHostsFile=/dev/null\' -o \'LogLevel=ERROR\' -o \'PasswordAuthentication=no\' -o \'StrictHostKeyChecking=no\' -o \'UserKnownHostsFile=/dev/null\' \'ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\'"\nand check to make sure that only the key(s) you wanted were added.\n\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:367 - add_hosts() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us 10.10.2.2\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T03:28:06,140651241-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T03:28:06,144432349-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'host', 'add', 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us', '10.10.2.2']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us 10.10.2.2\n'
10.10.2.1: b'Inferring fsid 8204cf96-37d9-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/8204cf96-37d9-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b"Added host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us' with addr '10.10.2.2'\n"
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T03:28:15,331295681-07:00] INFO: Sleep for 20 seconds\x1b[0m\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T03:28:35,336011052-07:00] STAGE: Add OSDs to the cluster...\x1b[0m\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T03:28:35,342733239-07:00] INFO: > Adding devices ['/dev/nvme0n1'] on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us' as OSDs\x1b[0m\n# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:388 - add_host_devices_as_osds() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1\n"
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T03:28:35,352895914-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T03:28:35,356678344-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'daemon', 'add', 'osd', 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1\n'
10.10.2.1: b'Inferring fsid 8204cf96-37d9-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/8204cf96-37d9-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b"Created osd(s) 0 on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'\n"
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T03:28:59,925263637-07:00] INFO: > Sleep for 20 seconds\x1b[0m\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T03:29:19,930707905-07:00] STAGE: Show Cluster Status\x1b[0m\n# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:408 - show_status() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T03:29:19,940910434-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T03:29:19,944875107-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', '--status']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status\n'
10.10.2.1: b'Inferring fsid 8204cf96-37d9-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/8204cf96-37d9-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b'  cluster:\n    id:     8204cf96-37d9-11ec-b51d-53e6e728d2d3\n    health: HEALTH_WARN\n            1 pool(s) have no replicas configured\n \n  services:\n    mon: 1 daemons, quorum node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us (age 2m)\n    mgr: node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us.zlcgat(active, since 2m)\n    osd: 1 osds: 1 up (since 18s), 1 in (since 40s)\n \n  data:\n    pools:   1 pools, 1 pgs\n    objects: 0 objects, 0 B\n    usage:   4.7 MiB used, 100 GiB / 100 GiB avail\n    pgs:     1 active+clean\n \n'
[1] 03:29:28 [SUCCESS] ljishen@10.10.2.1

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:26:13,996270533-07:00] INFO: > Remove existing clusters[0m
## bash:17 -  > basename -- /var/lib/ceph/41acbaae-37d8-11ec-b51d-53e6e728d2d3
# bash:17 -  > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm --host 10.10.2.1 --host 10.10.2.2 rm-cluster --force --fsid 41acbaae-37d8-11ec-b51d-53e6e728d2d3
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:26:14,006704959-07:00] DEBUG: HOST_PARAMS: ['--host', '10.10.2.1', '--host', '10.10.2.2'][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:26:14,010055147-07:00] DEBUG: CEPHADM_PARAMS: ['rm-cluster', '--force', '--fsid', '41acbaae-37d8-11ec-b51d-53e6e728d2d3'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:63 -  > /tmp/bench-rados/ceph-research/scripts/parallel-ssh --host 10.10.2.1 --host 10.10.2.2 'sudo python3 -u - rm-cluster --force --fsid 41acbaae-37d8-11ec-b51d-53e6e728d2d3'
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:26:14,018735575-07:00] DEBUG: command from stdin[0m
# /tmp/bench-rados/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host 10.10.2.1 --host 10.10.2.2 'sudo python3 -u - rm-cluster --force --fsid 41acbaae-37d8-11ec-b51d-53e6e728d2d3'
[1] 03:26:19 [SUCCESS] 10.10.2.1
[2] 03:26:25 [SUCCESS] 10.10.2.2

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:26:25,809941035-07:00] INFO: > Deploy a new cluster[0m
# bash:24 -  > /tmp/bench-rados/ceph-research/scripts/ceph-deploy -m 10.10.2.1 -o node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/10.10.2.2:/dev/nvme0n1


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:26:25,823711552-07:00] STAGE: Verify passwordless SSH to hosts, passwordless sudo via ljishen, and the existence of devices on hosts...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:26:25,828653943-07:00] INFO: > Check the host for the monitor[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:181 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@10.10.2.1 sudo --non-interactive --validate

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:26:25,978947827-07:00] INFO: > Check hosts for OSDs[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:26:25,983611624-07:00] INFO: >> Check host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:200 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us sudo --non-interactive --validate

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:26:26,135595596-07:00] INFO: >>> Check device '/dev/nvme0n1'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:215 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us test -b /dev/nvme0n1
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:226 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us lsblk --nodeps --noheadings --output TYPE /dev/nvme0n1
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:227 - verify_parameters() > grep --word-regexp disk


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:26:26,423455958-07:00] STAGE: Reset devices on hosts...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:26:26,428307057-07:00] INFO: > Resetting device '/dev/nvme0n1' on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:288 - reset_host_devices() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us sudo sh
  Removing ceph--1f0d1766--7e9e--4805--870f--91e25f0e4b50-osd--block--09e3b319--0fb8--43cb--9ae4--44338a977bd8 (253:0)
  Archiving volume group "ceph-1f0d1766-7e9e-4805-870f-91e25f0e4b50" metadata (seqno 5).
  Releasing logical volume "osd-block-09e3b319-0fb8-43cb-9ae4-44338a977bd8"
  Creating volume group backup "/etc/lvm/backup/ceph-1f0d1766-7e9e-4805-870f-91e25f0e4b50" (seqno 6).
  Logical volume "osd-block-09e3b319-0fb8-43cb-9ae4-44338a977bd8" successfully removed
  Removing physical volume "/dev/nvme0n1" from volume group "ceph-1f0d1766-7e9e-4805-870f-91e25f0e4b50"
  Volume group "ceph-1f0d1766-7e9e-4805-870f-91e25f0e4b50" successfully removed


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:26:26,813936092-07:00] STAGE: Bootstrap a new cluster...[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:317 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.1 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:26:26,824441631-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:26:26,827853806-07:00] DEBUG: CEPHADM_PARAMS: ['bootstrap', '--config', '/tmp/bench-rados/ceph-research/scripts/ceph.conf', '--mon-ip', '10.10.2.1', '--output-dir', '/tmp/bench-rados/deployment_data_root/etc/ceph', '--ssh-user', 'ljishen', '--skip-dashboard', '--no-minimize-config', '--allow-overwrite', '--allow-fqdn-hostname', '--orphan-initial-daemons', '--skip-monitoring-stack', '--single-host-defaults'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.1 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults
Verifying podman|docker is present...
Verifying lvm2 is present...
Verifying time synchronization is in place...
Unit ntp.service is enabled and running
Repeating the final host check...
podman|docker (/usr/bin/docker) is present
systemctl is present
lvcreate is present
Unit ntp.service is enabled and running
Host looks OK
Cluster fsid: 8204cf96-37d9-11ec-b51d-53e6e728d2d3
Verifying IP 10.10.2.1 port 3300 ...
Verifying IP 10.10.2.1 port 6789 ...
Mon IP `10.10.2.1` is in CIDR network `10.10.2.0/24`
- internal network (--cluster-network) has not been provided, OSD replication will default to the public_network
Adjusting default settings to suit single-host cluster...
Pulling container image quay.io/ceph/ceph:v16...
Ceph version: ceph version 16.2.6 (ee28fb57e47e9f88813e24bbf4c14496ca299d31) pacific (stable)
Extracting ceph user uid/gid from container image...
Creating initial keys...
Creating initial monmap...
Creating mon...
Waiting for mon to start...
Waiting for mon...
mon is available
Setting mon public_network to 10.10.2.0/24
Wrote config to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf
Wrote keyring to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring
Creating mgr...
Verifying port 9283 ...
Waiting for mgr to start...
Waiting for mgr...
mgr not available, waiting (1/15)...
mgr not available, waiting (2/15)...
mgr is available
Enabling cephadm module...
Waiting for the mgr to restart...
Waiting for mgr epoch 5...
mgr epoch 5 is available
Setting orchestrator backend to cephadm...
Generating ssh key...
Wrote public SSH key to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub
Adding key to ljishen@localhost authorized_keys...
Adding host node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us...
Deploying unmanaged mon service...
Deploying unmanaged mgr service...
You can access the Ceph CLI with:

	sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --fsid 8204cf96-37d9-11ec-b51d-53e6e728d2d3 -c /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf -k /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring

Please consider enabling telemetry to help improve Ceph:

	ceph telemetry on

For more information see:

	https://docs.ceph.com/docs/pacific/mgr/telemetry/

Bootstrap complete.

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:27:27,793348018-07:00] INFO: Sleep for 20 seconds[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:27:47,800968554-07:00] INFO: > Set the default number of monitor daemons to 1[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:337 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:27:47,811686373-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:27:47,815558061-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mon', '1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1
Inferring fsid 8204cf96-37d9-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/8204cf96-37d9-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Scheduled mon update...

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:27:56,881123469-07:00] INFO: > Set the default number of manager daemons to 1[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:342 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:27:56,891869760-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:27:56,895681095-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mgr', '1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1
Inferring fsid 8204cf96-37d9-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/8204cf96-37d9-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Scheduled mgr update...


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:28:05,923086917-07:00] STAGE: Add hosts to the cluster...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:28:05,929523027-07:00] INFO: > Adding host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:361 - add_hosts() > ssh-copy-id -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -f -i /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us
/usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: "/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub"

Number of key(s) added: 1

Now try logging into the machine, with:   "ssh -o 'GlobalKnownHostsFile=/dev/null' -o 'LogLevel=ERROR' -o 'PasswordAuthentication=no' -o 'StrictHostKeyChecking=no' -o 'UserKnownHostsFile=/dev/null' 'ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'"
and check to make sure that only the key(s) you wanted were added.

# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:367 - add_hosts() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us 10.10.2.2
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:28:06,140651241-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:28:06,144432349-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'host', 'add', 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us', '10.10.2.2'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us 10.10.2.2
Inferring fsid 8204cf96-37d9-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/8204cf96-37d9-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Added host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us' with addr '10.10.2.2'

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:28:15,331295681-07:00] INFO: Sleep for 20 seconds[0m


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:28:35,336011052-07:00] STAGE: Add OSDs to the cluster...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:28:35,342733239-07:00] INFO: > Adding devices ['/dev/nvme0n1'] on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us' as OSDs[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:388 - add_host_devices_as_osds() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:28:35,352895914-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:28:35,356678344-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'daemon', 'add', 'osd', 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1
Inferring fsid 8204cf96-37d9-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/8204cf96-37d9-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Created osd(s) 0 on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:28:59,925263637-07:00] INFO: > Sleep for 20 seconds[0m


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:29:19,930707905-07:00] STAGE: Show Cluster Status[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:408 - show_status() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:29:19,940910434-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:29:19,944875107-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', '--status'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status
Inferring fsid 8204cf96-37d9-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/8204cf96-37d9-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
  cluster:
    id:     8204cf96-37d9-11ec-b51d-53e6e728d2d3
    health: HEALTH_WARN
            1 pool(s) have no replicas configured
 
  services:
    mon: 1 daemons, quorum node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us (age 2m)
    mgr: node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us.zlcgat(active, since 2m)
    osd: 1 osds: 1 up (since 18s), 1 in (since 40s)
 
  data:
    pools:   1 pools, 1 pgs
    objects: 0 objects, 0 B
    usage:   4.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     1 active+clean
 

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:29:28,920688243-07:00] INFO: > Copy configuration files to local[0m
# ./benchmarks/bench-rados:220 - launch_ceph_cluster() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.1 sudo chown --changes --recursive ljishen /tmp/bench-rados/deployment_data_root/etc/ceph
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:29:28,928564144-07:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.1 sudo chown --changes --recursive ljishen /tmp/bench-rados/deployment_data_root/etc/ceph
[1] 03:29:29 [SUCCESS] ljishen@10.10.2.1
# ./benchmarks/bench-rados:223 - launch_ceph_cluster() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.1:/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf
# ./benchmarks/bench-rados:226 - launch_ceph_cluster() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.1:/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:29:29,405179589-07:00] STAGE: Update local ceph.conf as a client...[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:29:29,409106590-07:00] STAGE: Configure Ceph pool...[0m
# ./benchmarks/bench-rados:257 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_max_backfills 32
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:29:29,431424797-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:29:29,434093946-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '8204cf96-37d9-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'config', 'set', 'osd', 'osd_max_backfills', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 8204cf96-37d9-11ec-b51d-53e6e728d2d3 -- ceph config set osd osd_max_backfills 32
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:258 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_active 32
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:29:33,790292094-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:29:33,793229199-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '8204cf96-37d9-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_active', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 8204cf96-37d9-11ec-b51d-53e6e728d2d3 -- ceph config set osd osd_recovery_max_active 32
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:259 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_single_start 8
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:29:38,105400342-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:29:38,108789949-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '8204cf96-37d9-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_single_start', '8'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 8204cf96-37d9-11ec-b51d-53e6e728d2d3 -- ceph config set osd osd_recovery_max_single_start 8
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:260 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_op_priority 63
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:29:42,539745572-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:29:42,542828050-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '8204cf96-37d9-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_op_priority', '63'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 8204cf96-37d9-11ec-b51d-53e6e728d2d3 -- ceph config set osd osd_recovery_op_priority 63
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:262 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config show-with-defaults osd.0
# ./benchmarks/bench-rados:263 - configure_ceph_pool() > grep 'osd_max_backfills\|osd_recovery'
# ./benchmarks/bench-rados:263 - configure_ceph_pool() > column -t -s ' '
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
osd_max_backfills                        32         mon
osd_recovery_cost                        20971520   default
osd_recovery_delay_start                 0.000000   default
osd_recovery_max_active                  32         mon
osd_recovery_max_active_hdd              3          default
osd_recovery_max_active_ssd              10         default
osd_recovery_max_chunk                   8388608    default
osd_recovery_max_omap_entries_per_chunk  8096       default
osd_recovery_max_single_start            8          mon
osd_recovery_op_priority                 3          default
osd_recovery_op_warn_multiple            16         default
osd_recovery_priority                    5          default
osd_recovery_retry_interval              30.000000  default
osd_recovery_sleep                       0.000000   default
osd_recovery_sleep_hdd                   0.100000   default
osd_recovery_sleep_hybrid                0.025000   default
osd_recovery_sleep_ssd                   0.000000   default
# ./benchmarks/bench-rados:265 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool create bench_rados 128 128 replicated --size 1 --pg-num-min 128
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:29:51,165015104-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:29:51,167982626-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '8204cf96-37d9-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'create', 'bench_rados', '128', '128', 'replicated', '--size', '1', '--pg-num-min', '128'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 8204cf96-37d9-11ec-b51d-53e6e728d2d3 -- ceph osd pool create bench_rados 128 128 replicated --size 1 --pg-num-min 128
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
pool 'bench_rados' created
# ./benchmarks/bench-rados:267 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados min_size 1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:29:56,657832823-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:29:56,660784456-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '8204cf96-37d9-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'min_size', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 8204cf96-37d9-11ec-b51d-53e6e728d2d3 -- ceph osd pool set bench_rados min_size 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 min_size to 1
# ./benchmarks/bench-rados:268 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados pg_autoscale_mode off
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:30:01,289207410-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:30:01,292459017-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '8204cf96-37d9-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'pg_autoscale_mode', 'off'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 8204cf96-37d9-11ec-b51d-53e6e728d2d3 -- ceph osd pool set bench_rados pg_autoscale_mode off
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 pg_autoscale_mode to off
# ./benchmarks/bench-rados:269 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool application enable bench_rados benchmark
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:30:06,588628378-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:30:06,591661073-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '8204cf96-37d9-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'application', 'enable', 'bench_rados', 'benchmark'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 8204cf96-37d9-11ec-b51d-53e6e728d2d3 -- ceph osd pool application enable bench_rados benchmark
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
enabled application 'benchmark' on pool 'bench_rados'
# ./benchmarks/bench-rados:270 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados noscrub 1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:30:11,996695622-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:30:11,999715723-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '8204cf96-37d9-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'noscrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 8204cf96-37d9-11ec-b51d-53e6e728d2d3 -- ceph osd pool set bench_rados noscrub 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 noscrub to 1
# ./benchmarks/bench-rados:271 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados nodeep-scrub 1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:30:16,689973169-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:30:16,692859548-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '8204cf96-37d9-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'nodeep-scrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 8204cf96-37d9-11ec-b51d-53e6e728d2d3 -- ceph osd pool set bench_rados nodeep-scrub 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 nodeep-scrub to 1
# ./benchmarks/bench-rados:272 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool ls detail
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:30:21,415400801-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:30:21,418527854-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '8204cf96-37d9-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'ls', 'detail'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 8204cf96-37d9-11ec-b51d-53e6e728d2d3 -- ceph osd pool ls detail
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
pool 1 'device_health_metrics' replicated size 1 min_size 1 crush_rule 0 object_hash rjenkins pg_num 64 pgp_num 64 autoscale_mode on last_change 19 lfor 0/0/16 flags hashpspool stripe_width 0 pg_num_min 1 application mgr_devicehealth
pool 2 'bench_rados' replicated size 1 min_size 1 crush_rule 0 object_hash rjenkins pg_num 128 pgp_num 128 autoscale_mode off last_change 21 flags hashpspool,noscrub,nodeep-scrub stripe_width 0 pg_num_min 128 application benchmark

# ./benchmarks/bench-rados:273 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd df tree
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:30:26,053269154-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:30:26,056690481-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '8204cf96-37d9-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'df', 'tree'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 8204cf96-37d9-11ec-b51d-53e6e728d2d3 -- ceph osd df tree
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
ID  CLASS  WEIGHT   REWEIGHT  SIZE     RAW USE  DATA     OMAP  META     AVAIL    %USE  VAR   PGS  STATUS  TYPE NAME      
-1         0.09769         -  100 GiB  5.7 MiB  176 KiB   0 B  5.6 MiB  100 GiB  0.01  1.00    -          root default   
-3         0.09769         -  100 GiB  5.7 MiB  176 KiB   0 B  5.6 MiB  100 GiB  0.01  1.00    -              host node-1
 0    ssd  0.09769   1.00000  100 GiB  5.7 MiB  176 KiB   0 B  5.6 MiB  100 GiB  0.01  1.00  192      up          osd.0  
                       TOTAL  100 GiB  5.7 MiB  176 KiB   0 B  5.6 MiB  100 GiB  0.01                                    
MIN/MAX VAR: 1.00/1.00  STDDEV: 0


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:30:30,522943098-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:30:54,937769072-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:31:04,535039152-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:31:13,984342040-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:31:13,992540259-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:31:23,274050914-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:31:23,282489246-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:31:32,540279421-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:31:32,548340592-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:31:41,886234236-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:31:41,894694068-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:31:51,351204096-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:31:51,359704153-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:31:51,366493828-07:00] INFO: > The cluster is idle now.[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:31:51,370454822-07:00] STAGE: Start benchmarking of mode write ...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:31:51,377663336-07:00] INFO: > Start system activity collection services[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:31:51,383555329-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:341 - rados_bench() > rm --force /mnt/sda8/ceph-research/scripts/output/sys_activity_1MB_write.dat.rados_bench_host.3
# ./benchmarks/bench-rados:342 - rados_bench() > sar_pid_rados_bench_host=310208
# ./benchmarks/bench-rados:342 - rados_bench() > S_TIME_FORMAT=ISO
# ./benchmarks/bench-rados:342 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/output/sys_activity_1MB_write.dat.rados_bench_host.3 2

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:31:51,391088625-07:00] INFO: >> for OSD host[0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:31:51,400129302-07:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.2 bash
10.10.2.2: b'179202\n'
[1] 03:31:51 [SUCCESS] ljishen@10.10.2.2
179202

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:31:51,590803592-07:00] INFO: > Run rados bench[0m
# ./benchmarks/bench-rados:370 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/output/rados_bench_1MB_write.log.3
## ./benchmarks/bench-rados:361 - rados_bench() > count_cpus 32-63,96-127
# ./benchmarks/bench-rados:361 - rados_bench() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 60 write --pool bench_rados -b 1048576 -O 1048576 --concurrent-ios 128 --show-time --write-object --write-omap --write-xattr --no-cleanup
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:31:51,611317409-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:31:51,614397403-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '8204cf96-37d9-11ec-b51d-53e6e728d2d3', '--', 'rados', 'bench', '60', 'write', '--pool', 'bench_rados', '-b', '1048576', '-O', '1048576', '--concurrent-ios', '128', '--show-time', '--write-object', '--write-omap', '--write-xattr', '--no-cleanup'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 8204cf96-37d9-11ec-b51d-53e6e728d2d3 -- rados bench 60 write --pool bench_rados -b 1048576 -O 1048576 --concurrent-ios 128 --show-time --write-object --write-omap --write-xattr --no-cleanup
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
hints = 1
2021-10-28T10:31:54.947323+0000 Maintaining 128 concurrent writes of 1048576 bytes to objects of size 1048576 for up to 60 seconds or 0 objects
2021-10-28T10:31:54.947336+0000 Object prefix: benchmark_data_node-2.bluefield2.ucsc-cmps10_7
2021-10-28T10:31:54.978978+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-28T10:31:54.978978+0000     0       0         0         0         0         0           -           0
2021-10-28T10:31:55.979094+0000     1     127       322       195   194.989       195     0.41795    0.420718
2021-10-28T10:31:56.979172+0000     2     127       569       442   220.985       247    0.522457    0.429229
2021-10-28T10:31:57.979245+0000     3     127       708       581   193.653       139    0.855087    0.516595
2021-10-28T10:31:58.979320+0000     4     127       980       853   213.235       272    0.363595    0.550471
2021-10-28T10:31:59.979396+0000     5     127      1132      1005   200.986       152    0.833887    0.542635
2021-10-28T10:32:00.979469+0000     6     127      1211      1084   180.654        79     1.33633    0.593523
2021-10-28T10:32:01.979549+0000     7     127      1312      1185   169.273       101     1.26947     0.66765
2021-10-28T10:32:02.979631+0000     8     127      1457      1330   166.238       145    0.894609    0.709519
2021-10-28T10:32:03.979704+0000     9     127      1577      1450   161.099       120     0.90227    0.725451
2021-10-28T10:32:04.979780+0000    10     127      1738      1611   161.088       161    0.878172    0.750626
2021-10-28T10:32:05.979860+0000    11     127      1973      1846   167.806       235    0.367006    0.739409
2021-10-28T10:32:06.979936+0000    12     127      2146      2019   168.237       173    0.785369    0.717579
2021-10-28T10:32:07.980006+0000    13     127      2240      2113   162.526        94     1.32361    0.737899
2021-10-28T10:32:08.980080+0000    14     127      2337      2210   157.845        97     1.30758    0.766285
2021-10-28T10:32:09.980155+0000    15     127      2495      2368   157.855       158    0.841447    0.780536
2021-10-28T10:32:10.980251+0000    16     127      2622      2495   155.926       127    0.984454    0.785547
2021-10-28T10:32:11.980329+0000    17     127      2706      2579   151.694        84     1.28503    0.800389
2021-10-28T10:32:12.980401+0000    18     127      2800      2673   148.489        94     1.22454    0.822017
2021-10-28T10:32:13.980473+0000    19     127      2908      2781   146.357       108     1.06959    0.840957
2021-10-28T10:32:14.980545+0000 min lat: 0.351243 max lat: 1.67217 avg lat: 0.823405
2021-10-28T10:32:14.980545+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-28T10:32:14.980545+0000    20     127      3153      3026   151.289       245    0.623653    0.823405
2021-10-28T10:32:15.980626+0000    21     127      3243      3116    148.37        90     1.19485    0.829792
2021-10-28T10:32:16.980700+0000    22     127      3325      3198   145.353        82     1.38107    0.845159
2021-10-28T10:32:17.980773+0000    23     127      3469      3342   145.293       144    0.889059    0.859052
2021-10-28T10:32:18.980853+0000    24     127      3620      3493   145.531       151    0.754863    0.859274
2021-10-28T10:32:19.980928+0000    25     127      3708      3581   143.229        88     1.18203    0.865721
2021-10-28T10:32:20.981004+0000    26     127      3795      3668   141.066        87     1.33447    0.878805
2021-10-28T10:32:21.981092+0000    27     127      3904      3777   139.878       109     1.07615    0.893282
2021-10-28T10:32:22.981166+0000    28     127      4058      3931   140.382       154    0.825598    0.893734
2021-10-28T10:32:23.981238+0000    29     127      4168      4041   139.334       110    0.986134    0.893511
2021-10-28T10:32:24.981310+0000    30     127      4260      4133   137.756        92     1.41291    0.901944
2021-10-28T10:32:25.981382+0000    31     127      4354      4227   136.345        94     1.40428    0.914531
2021-10-28T10:32:26.981449+0000    32     127      4487      4360    136.24       133    0.731287    0.923221
2021-10-28T10:32:27.981522+0000    33     127      4632      4505   136.505       145    0.831992    0.921373
2021-10-28T10:32:28.981597+0000    34     127      4714      4587   134.902        82     1.09718    0.923419
2021-10-28T10:32:29.981671+0000    35     127      4803      4676    133.59        89     1.32225    0.932177
2021-10-28T10:32:30.981745+0000    36     127      4921      4794   133.157       118     1.21156    0.942612
2021-10-28T10:32:31.981816+0000    37     127      5080      4953   133.855       159    0.711715    0.941721
2021-10-28T10:32:32.981888+0000    38     127      5200      5073    133.49       120     1.01701    0.940628
2021-10-28T10:32:33.981958+0000    39     127      5282      5155    132.17        82     1.41362    0.946411
2021-10-28T10:32:34.982029+0000 min lat: 0.351243 max lat: 1.67217 avg lat: 0.955681
2021-10-28T10:32:34.982029+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-28T10:32:34.982029+0000    40     127      5378      5251   131.265        96     1.41389    0.955681
2021-10-28T10:32:35.982107+0000    41     127      5524      5397   131.624       146    0.846879     0.95974
2021-10-28T10:32:36.982182+0000    42     127      5669      5542   131.942       145    0.850156    0.957341
2021-10-28T10:32:37.982253+0000    43     127      5749      5622   130.734        80      1.2041    0.959095
2021-10-28T10:32:38.982326+0000    44     127      5830      5703   129.604        81     1.24517    0.965147
2021-10-28T10:32:39.982396+0000    45     127      5951      5824   129.413       121     1.17774    0.973682
2021-10-28T10:32:40.982468+0000    46     127      6100      5973   129.838       149    0.897854    0.973583
2021-10-28T10:32:41.982540+0000    47     127      6207      6080   129.352       107    0.940692     0.97227
2021-10-28T10:32:42.982622+0000    48     127      6299      6172   128.574        92     1.38292    0.976785
2021-10-28T10:32:43.982696+0000    49     127      6389      6262   127.786        90     1.36072    0.983437
2021-10-28T10:32:44.982768+0000    50     127      6536      6409    128.17       147    0.853006     0.98682
2021-10-28T10:32:45.982841+0000    51     127      6686      6559   128.598       150     0.74935     0.98386
2021-10-28T10:32:46.982919+0000    52     127      6770      6643    127.74        84     1.14408    0.984854
2021-10-28T10:32:47.982991+0000    53     127      6864      6737   127.104        94     1.40965    0.990413
2021-10-28T10:32:48.983060+0000    54     127      6967      6840   126.657       103     1.13033     0.99614
2021-10-28T10:32:49.983130+0000    55     127      7132      7005   127.354       165    0.815018    0.994984
2021-10-28T10:32:50.983199+0000    56     127      7248      7121   127.151       116     1.00012    0.993602
2021-10-28T10:32:51.983268+0000    57     127      7332      7205   126.394        84     1.42418    0.997257
2021-10-28T10:32:52.983338+0000    58     127      7412      7285   125.594        80     1.27254     1.00216
2021-10-28T10:32:53.983407+0000    59     127      7573      7446   126.194       161    0.743639     1.00517
2021-10-28T10:32:54.983480+0000 min lat: 0.351243 max lat: 1.67217 avg lat: 1.00235
2021-10-28T10:32:54.983480+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-28T10:32:54.983480+0000    60     127      7711      7584   126.391       138    0.800874     1.00235
2021-10-28T10:32:55.983607+0000 Total time run:         60.2898
Total writes made:      7712
Write size:             1048576
Object size:            1048576
Bandwidth (MB/sec):     127.915
Stddev Bandwidth:       44.7461
Max bandwidth (MB/sec): 272
Min bandwidth (MB/sec): 79
Average IOPS:           127
Stddev IOPS:            44.7461
Max IOPS:               272
Min IOPS:               79
Average Latency(s):     0.995982
Stddev Latency(s):      0.330689
Max latency(s):         1.67217
Min latency(s):         0.28986

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:32:56,694508634-07:00] INFO: > Stop system activity collection services[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:32:56,700575396-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:391 - rados_bench() > kill -INT 310208

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:32:56,706519217-07:00] INFO: >> for OSD host[0m
# ./benchmarks/bench-rados:396 - rados_bench() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.2 kill -INT 179202
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:32:56,714249984-07:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.2 kill -INT 179202
[1] 03:32:56 [SUCCESS] ljishen@10.10.2.2

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:32:56,902146341-07:00] INFO: >>> Copy system activity file from OSD host[0m
## ./benchmarks/bench-rados:403 - rados_bench() > basename -- /tmp/bench-rados/sys_activity_1MB_write.dat.osd_host.3
# ./benchmarks/bench-rados:403 - rados_bench() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.2:/tmp/bench-rados/sys_activity_1MB_write.dat.osd_host.3 /mnt/sda8/ceph-research/scripts/output/sys_activity_1MB_write.dat.osd_host.3


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:32:57,080742221-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:33:21,331327277-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 7.71k objects, 7.5 GiB
    usage:   23 GiB used, 77 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:33:21,339273310-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:33:30,678019208-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 7.71k objects, 7.5 GiB
    usage:   23 GiB used, 77 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:33:30,686648639-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:33:39,967504348-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 7.71k objects, 7.5 GiB
    usage:   23 GiB used, 77 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:33:39,975529921-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:33:49,282749970-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 7.71k objects, 7.5 GiB
    usage:   23 GiB used, 77 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:33:49,290904276-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:33:58,373473088-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 7.71k objects, 7.5 GiB
    usage:   23 GiB used, 77 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:33:58,381257357-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:33:58,387547229-07:00] INFO: > The cluster is idle now.[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:33:58,391635213-07:00] STAGE: Start benchmarking of mode seq ...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:33:58,399102765-07:00] INFO: > Start system activity collection services[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:33:58,404917693-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:341 - rados_bench() > rm --force /mnt/sda8/ceph-research/scripts/output/sys_activity_1MB_seq.dat.rados_bench_host.3
# ./benchmarks/bench-rados:342 - rados_bench() > sar_pid_rados_bench_host=311562
# ./benchmarks/bench-rados:342 - rados_bench() > S_TIME_FORMAT=ISO
# ./benchmarks/bench-rados:342 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/output/sys_activity_1MB_seq.dat.rados_bench_host.3 2

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:33:58,412519718-07:00] INFO: >> for OSD host[0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:33:58,421507104-07:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.2 bash
10.10.2.2: b'179300\n'
[1] 03:33:58 [SUCCESS] ljishen@10.10.2.2
179300

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:33:58,611336934-07:00] INFO: > Run rados bench[0m
# ./benchmarks/bench-rados:379 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/output/rados_bench_1MB_seq.log.3
## ./benchmarks/bench-rados:376 - rados_bench() > count_cpus 32-63,96-127
# ./benchmarks/bench-rados:376 - rados_bench() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 128 --show-time
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:33:58,631419157-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:33:58,634291089-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '8204cf96-37d9-11ec-b51d-53e6e728d2d3', '--', 'rados', 'bench', '99999999', 'seq', '--pool', 'bench_rados', '--concurrent-ios', '128', '--show-time'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 8204cf96-37d9-11ec-b51d-53e6e728d2d3 -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 128 --show-time
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
hints = 1
2021-10-28T10:34:01.808195+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-28T10:34:01.808195+0000     0       0         0         0         0         0           -           0
2021-10-28T10:34:02.808311+0000     1     127      1570      1443   1442.74      1443   0.0845162    0.084418
2021-10-28T10:34:03.808444+0000     2     127      3027      2900   1449.77      1457   0.0868278   0.0861439
2021-10-28T10:34:04.808551+0000     3     127      4496      4369   1456.13      1469    0.086603    0.086444
2021-10-28T10:34:05.808653+0000     4     127      5971      5844   1460.81      1475   0.0857537   0.0865122
2021-10-28T10:34:06.808725+0000     5     127      7446      7319   1463.63      1475   0.0848873   0.0865068
2021-10-28T10:34:07.808817+0000 Total time run:       5.24545
Total reads made:     7712
Read size:            1048576
Object size:          1048576
Bandwidth (MB/sec):   1470.23
Average IOPS:         1470
Stddev IOPS:          13.755
Max IOPS:             1475
Min IOPS:             1443
Average Latency(s):   0.0862588
Max latency(s):       0.142101
Min latency(s):       0.0245663

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:34:08,512035406-07:00] INFO: > Stop system activity collection services[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:34:08,518180226-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:391 - rados_bench() > kill -INT 311562

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:34:08,524634368-07:00] INFO: >> for OSD host[0m
# ./benchmarks/bench-rados:396 - rados_bench() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.2 kill -INT 179300
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:34:08,532911757-07:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.2 kill -INT 179300
[1] 03:34:08 [SUCCESS] ljishen@10.10.2.2

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:34:08,722101209-07:00] INFO: >>> Copy system activity file from OSD host[0m
## ./benchmarks/bench-rados:403 - rados_bench() > basename -- /tmp/bench-rados/sys_activity_1MB_seq.dat.osd_host.3
# ./benchmarks/bench-rados:403 - rados_bench() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.2:/tmp/bench-rados/sys_activity_1MB_seq.dat.osd_host.3 /mnt/sda8/ceph-research/scripts/output/sys_activity_1MB_seq.dat.osd_host.3


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:34:08,878201568-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:34:33,330870060-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 7.71k objects, 7.5 GiB
    usage:   23 GiB used, 77 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:34:33,338703391-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:34:42,497455043-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 7.71k objects, 7.5 GiB
    usage:   23 GiB used, 77 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:34:42,505090501-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:34:51,737008319-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 7.71k objects, 7.5 GiB
    usage:   23 GiB used, 77 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:34:51,744886766-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:35:01,034117301-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 7.71k objects, 7.5 GiB
    usage:   23 GiB used, 77 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:35:01,042401943-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:35:10,349558610-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 7.71k objects, 7.5 GiB
    usage:   23 GiB used, 77 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:35:10,357474277-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:35:10,363710809-07:00] INFO: > The cluster is idle now.[0m


[1;7;39;49m[2021-10-28T03:35:10,367649000-07:00][RUNNING][ROUND 1/6/21] object_size=4MB[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:35:10,371523541-07:00] STAGE: Launch a Ceph cluster (CEPH_HOST: 10.10.2.1, OSD_HOST: 10.10.2.2) ...[0m
# ./benchmarks/bench-rados:183 - launch_ceph_cluster() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.1 bash -euo pipefail
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:35:10,381207430-07:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.1 bash -euo pipefail
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T03:35:10,837398398-07:00] INFO: > Remove existing clusters\x1b[0m\n'
10.10.2.1: b'## bash:17 -  > basename -- /var/lib/ceph/8204cf96-37d9-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'# bash:17 -  > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm --host 10.10.2.1 --host 10.10.2.2 rm-cluster --force --fsid 8204cf96-37d9-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T03:35:10,848809480-07:00] DEBUG: HOST_PARAMS: ['--host', '10.10.2.1', '--host', '10.10.2.2']\x1b[0m\n"
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T03:35:10,852776587-07:00] DEBUG: CEPHADM_PARAMS: ['rm-cluster', '--force', '--fsid', '8204cf96-37d9-11ec-b51d-53e6e728d2d3']\x1b[0m\n"
10.10.2.1: b"# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:63 -  > /tmp/bench-rados/ceph-research/scripts/parallel-ssh --host 10.10.2.1 --host 10.10.2.2 'sudo python3 -u - rm-cluster --force --fsid 8204cf96-37d9-11ec-b51d-53e6e728d2d3'\n"
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T03:35:10,861626815-07:00] DEBUG: command from stdin\x1b[0m\n'
10.10.2.1: b"# /tmp/bench-rados/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host 10.10.2.1 --host 10.10.2.2 'sudo python3 -u - rm-cluster --force --fsid 8204cf96-37d9-11ec-b51d-53e6e728d2d3'\n"
10.10.2.1: b'[1] 03:35:16 [SUCCESS] 10.10.2.1\n[2] 03:35:23 [SUCCESS] 10.10.2.2\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T03:35:23,744619314-07:00] INFO: > Deploy a new cluster\x1b[0m\n'
10.10.2.1: b'# bash:24 -  > /tmp/bench-rados/ceph-research/scripts/ceph-deploy -m 10.10.2.1 -o node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/10.10.2.2:/dev/nvme0n1\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T03:35:23,756409168-07:00] STAGE: Verify passwordless SSH to hosts, passwordless sudo via ljishen, and the existence of devices on hosts...\x1b[0m\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T03:35:23,761126255-07:00] INFO: > Check the host for the monitor\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:181 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@10.10.2.1 sudo --non-interactive --validate\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T03:35:23,911211317-07:00] INFO: > Check hosts for OSDs\x1b[0m\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T03:35:23,916205976-07:00] INFO: >> Check host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:200 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us sudo --non-interactive --validate\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T03:35:24,067447882-07:00] INFO: >>> Check device '/dev/nvme0n1'\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:215 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us test -b /dev/nvme0n1\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:226 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us lsblk --nodeps --noheadings --output TYPE /dev/nvme0n1\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:227 - verify_parameters() > grep --word-regexp disk\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T03:35:24,351031022-07:00] STAGE: Reset devices on hosts...\x1b[0m\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T03:35:24,356157580-07:00] INFO: > Resetting device '/dev/nvme0n1' on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:288 - reset_host_devices() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us sudo sh\n'
10.10.2.1: b'  Removing ceph--8b914811--61d3--4c21--9536--42ea2d3c8ca0-osd--block--60cf40a1--6a60--42f6--9ad9--66df1672ce99 (253:0)\n'
10.10.2.1: b'  Archiving volume group "ceph-8b914811-61d3-4c21-9536-42ea2d3c8ca0" metadata (seqno 5).\n'
10.10.2.1: b'  Releasing logical volume "osd-block-60cf40a1-6a60-42f6-9ad9-66df1672ce99"\n'
10.10.2.1: b'  Creating volume group backup "/etc/lvm/backup/ceph-8b914811-61d3-4c21-9536-42ea2d3c8ca0" (seqno 6).\n'
10.10.2.1: b'  Logical volume "osd-block-60cf40a1-6a60-42f6-9ad9-66df1672ce99" successfully removed\n'
10.10.2.1: b'  Removing physical volume "/dev/nvme0n1" from volume group "ceph-8b914811-61d3-4c21-9536-42ea2d3c8ca0"\n'
10.10.2.1: b'  Volume group "ceph-8b914811-61d3-4c21-9536-42ea2d3c8ca0" successfully removed\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T03:35:24,738065299-07:00] STAGE: Bootstrap a new cluster...\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:317 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.1 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T03:35:24,748213276-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T03:35:24,752137092-07:00] DEBUG: CEPHADM_PARAMS: ['bootstrap', '--config', '/tmp/bench-rados/ceph-research/scripts/ceph.conf', '--mon-ip', '10.10.2.1', '--output-dir', '/tmp/bench-rados/deployment_data_root/etc/ceph', '--ssh-user', 'ljishen', '--skip-dashboard', '--no-minimize-config', '--allow-overwrite', '--allow-fqdn-hostname', '--orphan-initial-daemons', '--skip-monitoring-stack', '--single-host-defaults']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.1 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults\n'
10.10.2.1: b'Verifying podman|docker is present...\nVerifying lvm2 is present...\n'
10.10.2.1: b'Verifying time synchronization is in place...\n'
10.10.2.1: b'Unit ntp.service is enabled and running\n'
10.10.2.1: b'Repeating the final host check...\n'
10.10.2.1: b'podman|docker (/usr/bin/docker) is present\n'
10.10.2.1: b'systemctl is present\n'
10.10.2.1: b'lvcreate is present\n'
10.10.2.1: b'Unit ntp.service is enabled and running\nHost looks OK\n'
10.10.2.1: b'Cluster fsid: c2a5b802-37da-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Verifying IP 10.10.2.1 port 3300 ...\n'
10.10.2.1: b'Verifying IP 10.10.2.1 port 6789 ...\n'
10.10.2.1: b'Mon IP `10.10.2.1` is in CIDR network `10.10.2.0/24`\n'
10.10.2.1: b'- internal network (--cluster-network) has not been provided, OSD replication will default to the public_network\n'
10.10.2.1: b'Adjusting default settings to suit single-host cluster...\n'
10.10.2.1: b'Pulling container image quay.io/ceph/ceph:v16...\n'
10.10.2.1: b'Ceph version: ceph version 16.2.6 (ee28fb57e47e9f88813e24bbf4c14496ca299d31) pacific (stable)\nExtracting ceph user uid/gid from container image...\n'
10.10.2.1: b'Creating initial keys...\n'
10.10.2.1: b'Creating initial monmap...\n'
10.10.2.1: b'Creating mon...\n'
10.10.2.1: b'Waiting for mon to start...\n'
10.10.2.1: b'Waiting for mon...\n'
10.10.2.1: b'mon is available\nSetting mon public_network to 10.10.2.0/24\n'
10.10.2.1: b'Wrote config to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf\n'
10.10.2.1: b'Wrote keyring to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring\n'
10.10.2.1: b'Creating mgr...\n'
10.10.2.1: b'Verifying port 9283 ...\n'
10.10.2.1: b'Waiting for mgr to start...\nWaiting for mgr...\n'
10.10.2.1: b'mgr not available, waiting (1/15)...\n'
10.10.2.1: b'mgr not available, waiting (2/15)...\n'
10.10.2.1: b'mgr is available\n'
10.10.2.1: b'Enabling cephadm module...\n'
10.10.2.1: b'Waiting for the mgr to restart...\nWaiting for mgr epoch 5...\n'
10.10.2.1: b'mgr epoch 5 is available\nSetting orchestrator backend to cephadm...\n'
10.10.2.1: b'Generating ssh key...\n'
10.10.2.1: b'Wrote public SSH key to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub\n'
10.10.2.1: b'Adding key to ljishen@localhost authorized_keys...\n'
10.10.2.1: b'Adding host node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us...\n'
10.10.2.1: b'Deploying unmanaged mon service...\n'
10.10.2.1: b'Deploying unmanaged mgr service...\n'
10.10.2.1: b'You can access the Ceph CLI with:\n\n\tsudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --fsid c2a5b802-37da-11ec-b51d-53e6e728d2d3 -c /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf -k /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring\n\n'
10.10.2.1: b'Please consider enabling telemetry to help improve Ceph:\n\n\tceph telemetry on\n\nFor more information see:\n\n\thttps://docs.ceph.com/docs/pacific/mgr/telemetry/\n\n'
10.10.2.1: b'Bootstrap complete.\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T03:36:25,722011435-07:00] INFO: Sleep for 20 seconds\x1b[0m\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T03:36:45,729429000-07:00] INFO: > Set the default number of monitor daemons to 1\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:337 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T03:36:45,740308402-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T03:36:45,744217240-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mon', '1']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1\n'
10.10.2.1: b'Inferring fsid c2a5b802-37da-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/c2a5b802-37da-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b'Scheduled mon update...\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T03:36:54,655808076-07:00] INFO: > Set the default number of manager daemons to 1\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:342 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T03:36:54,666084675-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T03:36:54,669533488-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mgr', '1']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1\n'
10.10.2.1: b'Inferring fsid c2a5b802-37da-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/c2a5b802-37da-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b'Scheduled mgr update...\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T03:37:03,673114181-07:00] STAGE: Add hosts to the cluster...\x1b[0m\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T03:37:03,680515655-07:00] INFO: > Adding host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:361 - add_hosts() > ssh-copy-id -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -f -i /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\n'
10.10.2.1: b'/usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: "/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub"\n'
10.10.2.1: b'\nNumber of key(s) added: 1\n\nNow try logging into the machine, with:   "ssh -o \'GlobalKnownHostsFile=/dev/null\' -o \'LogLevel=ERROR\' -o \'PasswordAuthentication=no\' -o \'StrictHostKeyChecking=no\' -o \'UserKnownHostsFile=/dev/null\' \'ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\'"\nand check to make sure that only the key(s) you wanted were added.\n\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:367 - add_hosts() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us 10.10.2.2\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T03:37:03,894130705-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T03:37:03,897769175-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'host', 'add', 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us', '10.10.2.2']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us 10.10.2.2\n'
10.10.2.1: b'Inferring fsid c2a5b802-37da-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/c2a5b802-37da-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b"Added host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us' with addr '10.10.2.2'\n"
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T03:37:13,272916764-07:00] INFO: Sleep for 20 seconds\x1b[0m\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T03:37:33,277524215-07:00] STAGE: Add OSDs to the cluster...\x1b[0m\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T03:37:33,284351820-07:00] INFO: > Adding devices ['/dev/nvme0n1'] on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us' as OSDs\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:388 - add_host_devices_as_osds() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T03:37:33,293940054-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T03:37:33,297864522-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'daemon', 'add', 'osd', 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1\n'
10.10.2.1: b'Inferring fsid c2a5b802-37da-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/c2a5b802-37da-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b"Created osd(s) 0 on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'\n"
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T03:37:57,412120281-07:00] INFO: > Sleep for 20 seconds\x1b[0m\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T03:38:17,417433252-07:00] STAGE: Show Cluster Status\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:408 - show_status() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T03:38:17,427670277-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T03:38:17,431579285-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', '--status']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status\n'
10.10.2.1: b'Inferring fsid c2a5b802-37da-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/c2a5b802-37da-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b'  cluster:\n    id:     c2a5b802-37da-11ec-b51d-53e6e728d2d3\n    health: HEALTH_WARN\n            1 pool(s) have no replicas configured\n \n  services:\n    mon: 1 daemons, quorum node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us (age 2m)\n    mgr: node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us.cyckbs(active, since 2m)\n    osd: 1 osds: 1 up (since 18s), 1 in (since 40s)\n \n  data:\n    pools:   1 pools, 1 pgs\n    objects: 0 objects, 0 B\n    usage:   4.7 MiB used, 100 GiB / 100 GiB avail\n    pgs:     1 active+clean\n \n'
[1] 03:38:25 [SUCCESS] ljishen@10.10.2.1

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:35:10,837398398-07:00] INFO: > Remove existing clusters[0m
## bash:17 -  > basename -- /var/lib/ceph/8204cf96-37d9-11ec-b51d-53e6e728d2d3
# bash:17 -  > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm --host 10.10.2.1 --host 10.10.2.2 rm-cluster --force --fsid 8204cf96-37d9-11ec-b51d-53e6e728d2d3
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:35:10,848809480-07:00] DEBUG: HOST_PARAMS: ['--host', '10.10.2.1', '--host', '10.10.2.2'][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:35:10,852776587-07:00] DEBUG: CEPHADM_PARAMS: ['rm-cluster', '--force', '--fsid', '8204cf96-37d9-11ec-b51d-53e6e728d2d3'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:63 -  > /tmp/bench-rados/ceph-research/scripts/parallel-ssh --host 10.10.2.1 --host 10.10.2.2 'sudo python3 -u - rm-cluster --force --fsid 8204cf96-37d9-11ec-b51d-53e6e728d2d3'
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:35:10,861626815-07:00] DEBUG: command from stdin[0m
# /tmp/bench-rados/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host 10.10.2.1 --host 10.10.2.2 'sudo python3 -u - rm-cluster --force --fsid 8204cf96-37d9-11ec-b51d-53e6e728d2d3'
[1] 03:35:16 [SUCCESS] 10.10.2.1
[2] 03:35:23 [SUCCESS] 10.10.2.2

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:35:23,744619314-07:00] INFO: > Deploy a new cluster[0m
# bash:24 -  > /tmp/bench-rados/ceph-research/scripts/ceph-deploy -m 10.10.2.1 -o node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/10.10.2.2:/dev/nvme0n1


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:35:23,756409168-07:00] STAGE: Verify passwordless SSH to hosts, passwordless sudo via ljishen, and the existence of devices on hosts...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:35:23,761126255-07:00] INFO: > Check the host for the monitor[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:181 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@10.10.2.1 sudo --non-interactive --validate

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:35:23,911211317-07:00] INFO: > Check hosts for OSDs[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:35:23,916205976-07:00] INFO: >> Check host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:200 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us sudo --non-interactive --validate

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:35:24,067447882-07:00] INFO: >>> Check device '/dev/nvme0n1'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:215 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us test -b /dev/nvme0n1
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:226 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us lsblk --nodeps --noheadings --output TYPE /dev/nvme0n1
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:227 - verify_parameters() > grep --word-regexp disk


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:35:24,351031022-07:00] STAGE: Reset devices on hosts...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:35:24,356157580-07:00] INFO: > Resetting device '/dev/nvme0n1' on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:288 - reset_host_devices() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us sudo sh
  Removing ceph--8b914811--61d3--4c21--9536--42ea2d3c8ca0-osd--block--60cf40a1--6a60--42f6--9ad9--66df1672ce99 (253:0)
  Archiving volume group "ceph-8b914811-61d3-4c21-9536-42ea2d3c8ca0" metadata (seqno 5).
  Releasing logical volume "osd-block-60cf40a1-6a60-42f6-9ad9-66df1672ce99"
  Creating volume group backup "/etc/lvm/backup/ceph-8b914811-61d3-4c21-9536-42ea2d3c8ca0" (seqno 6).
  Logical volume "osd-block-60cf40a1-6a60-42f6-9ad9-66df1672ce99" successfully removed
  Removing physical volume "/dev/nvme0n1" from volume group "ceph-8b914811-61d3-4c21-9536-42ea2d3c8ca0"
  Volume group "ceph-8b914811-61d3-4c21-9536-42ea2d3c8ca0" successfully removed


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:35:24,738065299-07:00] STAGE: Bootstrap a new cluster...[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:317 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.1 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:35:24,748213276-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:35:24,752137092-07:00] DEBUG: CEPHADM_PARAMS: ['bootstrap', '--config', '/tmp/bench-rados/ceph-research/scripts/ceph.conf', '--mon-ip', '10.10.2.1', '--output-dir', '/tmp/bench-rados/deployment_data_root/etc/ceph', '--ssh-user', 'ljishen', '--skip-dashboard', '--no-minimize-config', '--allow-overwrite', '--allow-fqdn-hostname', '--orphan-initial-daemons', '--skip-monitoring-stack', '--single-host-defaults'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.1 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults
Verifying podman|docker is present...
Verifying lvm2 is present...
Verifying time synchronization is in place...
Unit ntp.service is enabled and running
Repeating the final host check...
podman|docker (/usr/bin/docker) is present
systemctl is present
lvcreate is present
Unit ntp.service is enabled and running
Host looks OK
Cluster fsid: c2a5b802-37da-11ec-b51d-53e6e728d2d3
Verifying IP 10.10.2.1 port 3300 ...
Verifying IP 10.10.2.1 port 6789 ...
Mon IP `10.10.2.1` is in CIDR network `10.10.2.0/24`
- internal network (--cluster-network) has not been provided, OSD replication will default to the public_network
Adjusting default settings to suit single-host cluster...
Pulling container image quay.io/ceph/ceph:v16...
Ceph version: ceph version 16.2.6 (ee28fb57e47e9f88813e24bbf4c14496ca299d31) pacific (stable)
Extracting ceph user uid/gid from container image...
Creating initial keys...
Creating initial monmap...
Creating mon...
Waiting for mon to start...
Waiting for mon...
mon is available
Setting mon public_network to 10.10.2.0/24
Wrote config to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf
Wrote keyring to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring
Creating mgr...
Verifying port 9283 ...
Waiting for mgr to start...
Waiting for mgr...
mgr not available, waiting (1/15)...
mgr not available, waiting (2/15)...
mgr is available
Enabling cephadm module...
Waiting for the mgr to restart...
Waiting for mgr epoch 5...
mgr epoch 5 is available
Setting orchestrator backend to cephadm...
Generating ssh key...
Wrote public SSH key to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub
Adding key to ljishen@localhost authorized_keys...
Adding host node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us...
Deploying unmanaged mon service...
Deploying unmanaged mgr service...
You can access the Ceph CLI with:

	sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --fsid c2a5b802-37da-11ec-b51d-53e6e728d2d3 -c /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf -k /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring

Please consider enabling telemetry to help improve Ceph:

	ceph telemetry on

For more information see:

	https://docs.ceph.com/docs/pacific/mgr/telemetry/

Bootstrap complete.

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:36:25,722011435-07:00] INFO: Sleep for 20 seconds[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:36:45,729429000-07:00] INFO: > Set the default number of monitor daemons to 1[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:337 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:36:45,740308402-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:36:45,744217240-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mon', '1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1
Inferring fsid c2a5b802-37da-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/c2a5b802-37da-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Scheduled mon update...

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:36:54,655808076-07:00] INFO: > Set the default number of manager daemons to 1[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:342 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:36:54,666084675-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:36:54,669533488-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mgr', '1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1
Inferring fsid c2a5b802-37da-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/c2a5b802-37da-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Scheduled mgr update...


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:37:03,673114181-07:00] STAGE: Add hosts to the cluster...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:37:03,680515655-07:00] INFO: > Adding host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:361 - add_hosts() > ssh-copy-id -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -f -i /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us
/usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: "/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub"

Number of key(s) added: 1

Now try logging into the machine, with:   "ssh -o 'GlobalKnownHostsFile=/dev/null' -o 'LogLevel=ERROR' -o 'PasswordAuthentication=no' -o 'StrictHostKeyChecking=no' -o 'UserKnownHostsFile=/dev/null' 'ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'"
and check to make sure that only the key(s) you wanted were added.

# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:367 - add_hosts() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us 10.10.2.2
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:37:03,894130705-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:37:03,897769175-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'host', 'add', 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us', '10.10.2.2'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us 10.10.2.2
Inferring fsid c2a5b802-37da-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/c2a5b802-37da-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Added host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us' with addr '10.10.2.2'

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:37:13,272916764-07:00] INFO: Sleep for 20 seconds[0m


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:37:33,277524215-07:00] STAGE: Add OSDs to the cluster...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:37:33,284351820-07:00] INFO: > Adding devices ['/dev/nvme0n1'] on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us' as OSDs[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:388 - add_host_devices_as_osds() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:37:33,293940054-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:37:33,297864522-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'daemon', 'add', 'osd', 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1
Inferring fsid c2a5b802-37da-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/c2a5b802-37da-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Created osd(s) 0 on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:37:57,412120281-07:00] INFO: > Sleep for 20 seconds[0m


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:38:17,417433252-07:00] STAGE: Show Cluster Status[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:408 - show_status() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:38:17,427670277-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:38:17,431579285-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', '--status'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status
Inferring fsid c2a5b802-37da-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/c2a5b802-37da-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
  cluster:
    id:     c2a5b802-37da-11ec-b51d-53e6e728d2d3
    health: HEALTH_WARN
            1 pool(s) have no replicas configured
 
  services:
    mon: 1 daemons, quorum node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us (age 2m)
    mgr: node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us.cyckbs(active, since 2m)
    osd: 1 osds: 1 up (since 18s), 1 in (since 40s)
 
  data:
    pools:   1 pools, 1 pgs
    objects: 0 objects, 0 B
    usage:   4.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     1 active+clean
 

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:38:25,823417404-07:00] INFO: > Copy configuration files to local[0m
# ./benchmarks/bench-rados:220 - launch_ceph_cluster() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.1 sudo chown --changes --recursive ljishen /tmp/bench-rados/deployment_data_root/etc/ceph
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:38:25,831450652-07:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.1 sudo chown --changes --recursive ljishen /tmp/bench-rados/deployment_data_root/etc/ceph
[1] 03:38:26 [SUCCESS] ljishen@10.10.2.1
# ./benchmarks/bench-rados:223 - launch_ceph_cluster() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.1:/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf
# ./benchmarks/bench-rados:226 - launch_ceph_cluster() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.1:/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:38:26,304921693-07:00] STAGE: Update local ceph.conf as a client...[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:38:26,308890001-07:00] STAGE: Configure Ceph pool...[0m
# ./benchmarks/bench-rados:257 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_max_backfills 32
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:38:26,330885470-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:38:26,333785896-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'c2a5b802-37da-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'config', 'set', 'osd', 'osd_max_backfills', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid c2a5b802-37da-11ec-b51d-53e6e728d2d3 -- ceph config set osd osd_max_backfills 32
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:258 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_active 32
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:38:30,618259985-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:38:30,621340761-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'c2a5b802-37da-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_active', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid c2a5b802-37da-11ec-b51d-53e6e728d2d3 -- ceph config set osd osd_recovery_max_active 32
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:259 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_single_start 8
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:38:34,967246996-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:38:34,970380681-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'c2a5b802-37da-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_single_start', '8'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid c2a5b802-37da-11ec-b51d-53e6e728d2d3 -- ceph config set osd osd_recovery_max_single_start 8
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:260 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_op_priority 63
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:38:39,260382127-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:38:39,263451290-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'c2a5b802-37da-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_op_priority', '63'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid c2a5b802-37da-11ec-b51d-53e6e728d2d3 -- ceph config set osd osd_recovery_op_priority 63
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:262 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config show-with-defaults osd.0
# ./benchmarks/bench-rados:263 - configure_ceph_pool() > grep 'osd_max_backfills\|osd_recovery'
# ./benchmarks/bench-rados:263 - configure_ceph_pool() > column -t -s ' '
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
osd_max_backfills                        32         mon
osd_recovery_cost                        20971520   default
osd_recovery_delay_start                 0.000000   default
osd_recovery_max_active                  32         mon
osd_recovery_max_active_hdd              3          default
osd_recovery_max_active_ssd              10         default
osd_recovery_max_chunk                   8388608    default
osd_recovery_max_omap_entries_per_chunk  8096       default
osd_recovery_max_single_start            8          mon
osd_recovery_op_priority                 63         mon
osd_recovery_op_warn_multiple            16         default
osd_recovery_priority                    5          default
osd_recovery_retry_interval              30.000000  default
osd_recovery_sleep                       0.000000   default
osd_recovery_sleep_hdd                   0.100000   default
osd_recovery_sleep_hybrid                0.025000   default
osd_recovery_sleep_ssd                   0.000000   default
# ./benchmarks/bench-rados:265 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool create bench_rados 128 128 replicated --size 1 --pg-num-min 128
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:38:47,526852191-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:38:47,529985185-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'c2a5b802-37da-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'create', 'bench_rados', '128', '128', 'replicated', '--size', '1', '--pg-num-min', '128'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid c2a5b802-37da-11ec-b51d-53e6e728d2d3 -- ceph osd pool create bench_rados 128 128 replicated --size 1 --pg-num-min 128
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
pool 'bench_rados' created
# ./benchmarks/bench-rados:267 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados min_size 1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:38:52,092595507-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:38:52,095458372-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'c2a5b802-37da-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'min_size', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid c2a5b802-37da-11ec-b51d-53e6e728d2d3 -- ceph osd pool set bench_rados min_size 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 min_size to 1
# ./benchmarks/bench-rados:268 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados pg_autoscale_mode off
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:38:57,416791485-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:38:57,419963152-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'c2a5b802-37da-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'pg_autoscale_mode', 'off'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid c2a5b802-37da-11ec-b51d-53e6e728d2d3 -- ceph osd pool set bench_rados pg_autoscale_mode off
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 pg_autoscale_mode to off
# ./benchmarks/bench-rados:269 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool application enable bench_rados benchmark
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:39:02,657093199-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:39:02,660018973-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'c2a5b802-37da-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'application', 'enable', 'bench_rados', 'benchmark'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid c2a5b802-37da-11ec-b51d-53e6e728d2d3 -- ceph osd pool application enable bench_rados benchmark
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
enabled application 'benchmark' on pool 'bench_rados'
# ./benchmarks/bench-rados:270 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados noscrub 1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:39:06,866168493-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:39:06,869336473-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'c2a5b802-37da-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'noscrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid c2a5b802-37da-11ec-b51d-53e6e728d2d3 -- ceph osd pool set bench_rados noscrub 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 noscrub to 1
# ./benchmarks/bench-rados:271 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados nodeep-scrub 1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:39:11,523087534-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:39:11,526119487-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'c2a5b802-37da-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'nodeep-scrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid c2a5b802-37da-11ec-b51d-53e6e728d2d3 -- ceph osd pool set bench_rados nodeep-scrub 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 nodeep-scrub to 1
# ./benchmarks/bench-rados:272 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool ls detail
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:39:16,814014929-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:39:16,816885829-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'c2a5b802-37da-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'ls', 'detail'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid c2a5b802-37da-11ec-b51d-53e6e728d2d3 -- ceph osd pool ls detail
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
pool 1 'device_health_metrics' replicated size 1 min_size 1 crush_rule 0 object_hash rjenkins pg_num 64 pgp_num 64 autoscale_mode on last_change 18 lfor 0/0/16 flags hashpspool stripe_width 0 pg_num_min 1 application mgr_devicehealth
pool 2 'bench_rados' replicated size 1 min_size 1 crush_rule 0 object_hash rjenkins pg_num 128 pgp_num 128 autoscale_mode off last_change 20 flags hashpspool,noscrub,nodeep-scrub stripe_width 0 pg_num_min 128 application benchmark

# ./benchmarks/bench-rados:273 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd df tree
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:39:21,013534184-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:39:21,016542474-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'c2a5b802-37da-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'df', 'tree'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid c2a5b802-37da-11ec-b51d-53e6e728d2d3 -- ceph osd df tree
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
ID  CLASS  WEIGHT   REWEIGHT  SIZE     RAW USE  DATA     OMAP  META     AVAIL    %USE  VAR   PGS  STATUS  TYPE NAME      
-1         0.09769         -  100 GiB  5.7 MiB  168 KiB   0 B  5.6 MiB  100 GiB  0.01  1.00    -          root default   
-3         0.09769         -  100 GiB  5.7 MiB  168 KiB   0 B  5.6 MiB  100 GiB  0.01  1.00    -              host node-1
 0    ssd  0.09769   1.00000  100 GiB  5.7 MiB  168 KiB   0 B  5.6 MiB  100 GiB  0.01  1.00  192      up          osd.0  
                       TOTAL  100 GiB  5.7 MiB  168 KiB   0 B  5.6 MiB  100 GiB  0.01                                    
MIN/MAX VAR: 1.00/1.00  STDDEV: 0


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:39:25,215404621-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:39:49,463527206-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:39:58,843095611-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:40:08,024980952-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:40:08,032925834-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:40:17,283391380-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:40:17,291866121-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:40:26,604694416-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:40:26,612539078-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:40:35,816934437-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:40:35,825092020-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:40:45,108695281-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:40:45,116972719-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:40:45,123456798-07:00] INFO: > The cluster is idle now.[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:40:45,127254074-07:00] STAGE: Start benchmarking of mode write ...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:40:45,134751753-07:00] INFO: > Start system activity collection services[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:40:45,140507630-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:341 - rados_bench() > rm --force /mnt/sda8/ceph-research/scripts/output/sys_activity_4MB_write.dat.rados_bench_host.1
# ./benchmarks/bench-rados:342 - rados_bench() > sar_pid_rados_bench_host=317171
# ./benchmarks/bench-rados:342 - rados_bench() > S_TIME_FORMAT=ISO
# ./benchmarks/bench-rados:342 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/output/sys_activity_4MB_write.dat.rados_bench_host.1 2

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:40:45,147720232-07:00] INFO: >> for OSD host[0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:40:45,156853302-07:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.2 bash
10.10.2.2: b'182924\n'
[1] 03:40:45 [SUCCESS] ljishen@10.10.2.2
182924

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:40:45,351292141-07:00] INFO: > Run rados bench[0m
# ./benchmarks/bench-rados:370 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/output/rados_bench_4MB_write.log.1
## ./benchmarks/bench-rados:361 - rados_bench() > count_cpus 32-63,96-127
# ./benchmarks/bench-rados:361 - rados_bench() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 60 write --pool bench_rados -b 4194304 -O 4194304 --concurrent-ios 128 --show-time --write-object --write-omap --write-xattr --no-cleanup
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:40:45,372523541-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:40:45,375596892-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'c2a5b802-37da-11ec-b51d-53e6e728d2d3', '--', 'rados', 'bench', '60', 'write', '--pool', 'bench_rados', '-b', '4194304', '-O', '4194304', '--concurrent-ios', '128', '--show-time', '--write-object', '--write-omap', '--write-xattr', '--no-cleanup'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid c2a5b802-37da-11ec-b51d-53e6e728d2d3 -- rados bench 60 write --pool bench_rados -b 4194304 -O 4194304 --concurrent-ios 128 --show-time --write-object --write-omap --write-xattr --no-cleanup
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
hints = 1
2021-10-28T10:40:48.564394+0000 Maintaining 128 concurrent writes of 4194304 bytes to objects of size 4194304 for up to 60 seconds or 0 objects
2021-10-28T10:40:48.564408+0000 Object prefix: benchmark_data_node-2.bluefield2.ucsc-cmps10_7
2021-10-28T10:40:48.689563+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-28T10:40:48.689563+0000     0       0         0         0         0         0           -           0
2021-10-28T10:40:49.689654+0000     1      84        84         0         0         0           -           0
2021-10-28T10:40:50.689720+0000     2     127       139        12   23.9986        24     1.70151     1.63897
2021-10-28T10:40:51.689789+0000     3     127       176        49   65.3293       148     2.41191     2.11437
2021-10-28T10:40:52.689856+0000     4     127       245       118   117.993       276     2.54023     2.34035
2021-10-28T10:40:53.689923+0000     5     127       280       153   122.392       140     2.33248     2.38355
2021-10-28T10:40:54.689992+0000     6     127       296       169   112.659        64     3.04262     2.42792
2021-10-28T10:40:55.690061+0000     7     127       322       195   111.421       104     3.60894     2.57061
2021-10-28T10:40:56.690128+0000     8     127       369       242   120.992       188       4.059     2.83463
2021-10-28T10:40:57.690195+0000     9     127       416       289   128.436       188      3.5131     3.01422
2021-10-28T10:40:58.690216+0000    10     127       452       325   129.992       144     2.98291     3.02494
2021-10-28T10:40:59.690293+0000    11     127       521       394   143.264       276     2.59975     2.97071
2021-10-28T10:41:00.690364+0000    12     127       539       412   137.325        72     3.12764     2.96689
2021-10-28T10:41:01.690431+0000    13     127       563       436   134.145        96     3.44052     2.99343
2021-10-28T10:41:02.690497+0000    14     127       596       469   133.991       132       3.692     3.03613
2021-10-28T10:41:03.690564+0000    15     127       633       506   134.925       148     4.22573      3.1084
2021-10-28T10:41:04.690633+0000    16     127       656       529   132.241        92     4.58016     3.16688
2021-10-28T10:41:05.690703+0000    17     127       681       554   130.345       100     4.21006     3.22397
2021-10-28T10:41:06.690771+0000    18     127       751       624   138.658       280     3.25178     3.28504
2021-10-28T10:41:07.690839+0000    19     127       789       662   139.359       152     2.67515     3.26526
2021-10-28T10:41:08.690905+0000 min lat: 1.56083 max lat: 4.72366 avg lat: 3.25312
2021-10-28T10:41:08.690905+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-28T10:41:08.690905+0000    20     127       805       678   135.591        64     2.78557     3.25312
2021-10-28T10:41:09.690978+0000    21     127       839       712    135.61       136     3.45062     3.25282
2021-10-28T10:41:10.691046+0000    22     127       875       748   135.991       144     4.01421     3.27876
2021-10-28T10:41:11.691113+0000    23     127       907       780   135.643       128     4.48723     3.32056
2021-10-28T10:41:12.691179+0000    24     127       926       799   133.158        76     4.42371     3.35224
2021-10-28T10:41:13.691216+0000    25     127       950       823   131.672        96     4.43186     3.38842
2021-10-28T10:41:14.691285+0000    26     127       984       857   131.838       136     4.50496     3.43495
2021-10-28T10:41:15.691353+0000    27     127      1017       890   131.843       132     4.57833     3.47734
2021-10-28T10:41:16.691419+0000    28     127      1038       911   130.134        84     4.80637      3.5054
2021-10-28T10:41:17.691486+0000    29     127      1054       927   127.854        64     4.79603     3.52736
2021-10-28T10:41:18.691552+0000    30     127      1086       959   127.858       128      4.6012     3.56724
2021-10-28T10:41:19.691620+0000    31     127      1123       996   128.508       148     4.58111     3.60894
2021-10-28T10:41:20.691686+0000    32     127      1154      1027   128.367       124     4.57934     3.64155
2021-10-28T10:41:21.691754+0000    33     127      1173      1046    126.78        76     4.52414     3.65826
2021-10-28T10:41:22.691823+0000    34     127      1192      1065   125.286        76     4.49945     3.67476
2021-10-28T10:41:23.691888+0000    35     127      1229      1102   125.935       148     4.54601     3.70591
2021-10-28T10:41:24.691954+0000    36     127      1266      1139   126.547       148     4.48829     3.73279
2021-10-28T10:41:25.692021+0000    37     127      1294      1167   126.154       112     4.37666     3.75107
2021-10-28T10:41:26.692088+0000    38     127      1308      1181   124.308        56     4.58286      3.7609
2021-10-28T10:41:27.692156+0000    39     127      1339      1212     124.3       124     4.43006     3.78101
2021-10-28T10:41:28.692216+0000 min lat: 1.56083 max lat: 4.98952 avg lat: 3.8028
2021-10-28T10:41:28.692216+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-28T10:41:28.692216+0000    40     127      1376      1249   124.892       148     4.52697      3.8028
2021-10-28T10:41:29.692304+0000    41     127      1412      1285   125.358       144     4.55254     3.82346
2021-10-28T10:41:30.692372+0000    42     127      1431      1304   124.182        76     4.45334      3.8334
2021-10-28T10:41:31.692439+0000    43     127      1447      1320   122.783        64     4.56753     3.84188
2021-10-28T10:41:32.692507+0000    44     127      1479      1352   122.901       128      4.6286     3.85985
2021-10-28T10:41:33.692575+0000    45     127      1511      1384   123.014       128      4.6843     3.87889
2021-10-28T10:41:34.692645+0000    46     127      1547      1420    123.47       144     4.73953     3.90036
2021-10-28T10:41:35.692712+0000    47     127      1562      1435    122.12        60     4.61327     3.90912
2021-10-28T10:41:36.692782+0000    48     127      1583      1456   121.325        84     4.80657     3.92105
2021-10-28T10:41:37.692849+0000    49     127      1615      1488   121.461       128     4.84658     3.94027
2021-10-28T10:41:38.692916+0000    50     127      1648      1521   121.672       132     4.75144     3.95864
2021-10-28T10:41:39.692986+0000    51     127      1676      1549   121.482       112     4.57698     3.97407
2021-10-28T10:41:40.693056+0000    52     127      1690      1563   120.223        56     4.88582     3.98158
2021-10-28T10:41:41.693122+0000    53     127      1716      1589   119.917       104      4.7646     3.99552
2021-10-28T10:41:42.693189+0000    54     127      1747      1620   119.992       124     4.78527     4.01091
2021-10-28T10:41:43.693214+0000    55     127      1785      1658   120.574       152     4.75688     4.02862
2021-10-28T10:41:44.693282+0000    56     127      1808      1681   120.064        92     4.71398      4.0389
2021-10-28T10:41:45.693351+0000    57     127      1826      1699    119.22        72     4.43584     4.04547
2021-10-28T10:41:46.693426+0000    58     127      1850      1723    118.82        96     4.78911      4.0551
2021-10-28T10:41:47.693494+0000    59     127      1884      1757   119.111       136     4.75175     4.06945
2021-10-28T10:41:48.693559+0000 min lat: 1.56083 max lat: 5.08721 avg lat: 4.08338
2021-10-28T10:41:48.693559+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-28T10:41:48.693559+0000    60     127      1919      1792   119.459       140      4.7053     4.08338
2021-10-28T10:41:49.693669+0000 Total time run:         60.1417
Total writes made:      1920
Write size:             4194304
Object size:            4194304
Bandwidth (MB/sec):     127.698
Stddev Bandwidth:       52.42
Max bandwidth (MB/sec): 280
Min bandwidth (MB/sec): 0
Average IOPS:           31
Stddev IOPS:            13.105
Max IOPS:               70
Min IOPS:               0
Average Latency(s):     3.95237
Stddev Latency(s):      0.996626
Max latency(s):         5.08721
Min latency(s):         0.209856

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:41:50,470189840-07:00] INFO: > Stop system activity collection services[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:41:50,476900977-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:391 - rados_bench() > kill -INT 317171

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:41:50,483086373-07:00] INFO: >> for OSD host[0m
# ./benchmarks/bench-rados:396 - rados_bench() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.2 kill -INT 182924
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:41:50,491150640-07:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.2 kill -INT 182924
[1] 03:41:50 [SUCCESS] ljishen@10.10.2.2

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:41:50,677347377-07:00] INFO: >>> Copy system activity file from OSD host[0m
## ./benchmarks/bench-rados:403 - rados_bench() > basename -- /tmp/bench-rados/sys_activity_4MB_write.dat.osd_host.1
# ./benchmarks/bench-rados:403 - rados_bench() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.2:/tmp/bench-rados/sys_activity_4MB_write.dat.osd_host.1 /mnt/sda8/ceph-research/scripts/output/sys_activity_4MB_write.dat.osd_host.1


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:41:50,852806410-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:42:15,100385817-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 1.92k objects, 7.5 GiB
    usage:   23 GiB used, 77 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:42:15,108419927-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:42:24,474896584-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 1.92k objects, 7.5 GiB
    usage:   23 GiB used, 77 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:42:24,482893985-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:42:33,876393214-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 1.92k objects, 7.5 GiB
    usage:   23 GiB used, 77 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:42:33,884816506-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:42:43,358268201-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 1.92k objects, 7.5 GiB
    usage:   23 GiB used, 77 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:42:43,366394515-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:42:52,657167801-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 1.92k objects, 7.5 GiB
    usage:   23 GiB used, 77 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:42:52,664942723-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:42:52,671009595-07:00] INFO: > The cluster is idle now.[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:42:52,674763669-07:00] STAGE: Start benchmarking of mode seq ...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:42:52,682346699-07:00] INFO: > Start system activity collection services[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:42:52,687984574-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:341 - rados_bench() > rm --force /mnt/sda8/ceph-research/scripts/output/sys_activity_4MB_seq.dat.rados_bench_host.1
# ./benchmarks/bench-rados:342 - rados_bench() > sar_pid_rados_bench_host=318507
# ./benchmarks/bench-rados:342 - rados_bench() > S_TIME_FORMAT=ISO
# ./benchmarks/bench-rados:342 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/output/sys_activity_4MB_seq.dat.rados_bench_host.1 2

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:42:52,695481812-07:00] INFO: >> for OSD host[0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:42:52,704892476-07:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.2 bash
10.10.2.2: b'183021\n'
[1] 03:42:52 [SUCCESS] ljishen@10.10.2.2
183021

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:42:52,895767750-07:00] INFO: > Run rados bench[0m
# ./benchmarks/bench-rados:379 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/output/rados_bench_4MB_seq.log.1
## ./benchmarks/bench-rados:376 - rados_bench() > count_cpus 32-63,96-127
# ./benchmarks/bench-rados:376 - rados_bench() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 128 --show-time
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:42:52,915667099-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:42:52,918396642-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'c2a5b802-37da-11ec-b51d-53e6e728d2d3', '--', 'rados', 'bench', '99999999', 'seq', '--pool', 'bench_rados', '--concurrent-ios', '128', '--show-time'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid c2a5b802-37da-11ec-b51d-53e6e728d2d3 -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 128 --show-time
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
hints = 1
2021-10-28T10:42:56.158444+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-28T10:42:56.158444+0000     0       0         0         0         0         0           -           0
2021-10-28T10:42:57.158585+0000     1     127       394       267   1067.77      1068    0.355746    0.342023
2021-10-28T10:42:58.158708+0000     2     127       748       621   1241.79      1416     0.35896    0.352353
2021-10-28T10:42:59.158835+0000     3     127      1100       973   1297.13      1408    0.357124    0.356585
2021-10-28T10:43:00.158956+0000     4     127      1457      1330   1329.81      1428    0.356626    0.356969
2021-10-28T10:43:01.159089+0000     5     127      1815      1688   1350.21      1432    0.359423    0.357042
2021-10-28T10:43:02.159226+0000 Total time run:       5.36491
Total reads made:     1920
Read size:            4194304
Object size:          4194304
Bandwidth (MB/sec):   1431.52
Average IOPS:         357
Stddev IOPS:          39.5386
Max IOPS:             358
Min IOPS:             267
Average Latency(s):   0.346747
Max latency(s):       0.376715
Min latency(s):       0.0710281

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:43:02,821931865-07:00] INFO: > Stop system activity collection services[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:43:02,828379355-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:391 - rados_bench() > kill -INT 318507

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:43:02,834821705-07:00] INFO: >> for OSD host[0m
# ./benchmarks/bench-rados:396 - rados_bench() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.2 kill -INT 183021
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:43:02,843113480-07:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.2 kill -INT 183021
[1] 03:43:03 [SUCCESS] ljishen@10.10.2.2

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:43:03,030643731-07:00] INFO: >>> Copy system activity file from OSD host[0m
## ./benchmarks/bench-rados:403 - rados_bench() > basename -- /tmp/bench-rados/sys_activity_4MB_seq.dat.osd_host.1
# ./benchmarks/bench-rados:403 - rados_bench() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.2:/tmp/bench-rados/sys_activity_4MB_seq.dat.osd_host.1 /mnt/sda8/ceph-research/scripts/output/sys_activity_4MB_seq.dat.osd_host.1


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:43:03,190266010-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:43:27,433197776-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 1.92k objects, 7.5 GiB
    usage:   23 GiB used, 77 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:43:27,441757316-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:43:36,841042395-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 1.92k objects, 7.5 GiB
    usage:   23 GiB used, 77 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:43:36,849350121-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:43:46,417558205-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 1.92k objects, 7.5 GiB
    usage:   23 GiB used, 77 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:43:46,425541830-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:43:55,765640244-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 1.92k objects, 7.5 GiB
    usage:   23 GiB used, 77 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:43:55,774559342-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:44:05,131547754-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 1.92k objects, 7.5 GiB
    usage:   23 GiB used, 77 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:44:05,139841793-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:44:05,146218650-07:00] INFO: > The cluster is idle now.[0m


[1;7;39;49m[2021-10-28T03:44:05,149030549-07:00][RUNNING][ROUND 2/6/21] object_size=4MB[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:44:05,152674446-07:00] STAGE: Launch a Ceph cluster (CEPH_HOST: 10.10.2.1, OSD_HOST: 10.10.2.2) ...[0m
# ./benchmarks/bench-rados:183 - launch_ceph_cluster() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.1 bash -euo pipefail
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:44:05,162011300-07:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.1 bash -euo pipefail
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T03:44:05,593249747-07:00] INFO: > Remove existing clusters\x1b[0m\n'
10.10.2.1: b'## bash:17 -  > basename -- /var/lib/ceph/c2a5b802-37da-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'# bash:17 -  > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm --host 10.10.2.1 --host 10.10.2.2 rm-cluster --force --fsid c2a5b802-37da-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T03:44:05,603658955-07:00] DEBUG: HOST_PARAMS: ['--host', '10.10.2.1', '--host', '10.10.2.2']\x1b[0m\n"
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T03:44:05,607223697-07:00] DEBUG: CEPHADM_PARAMS: ['rm-cluster', '--force', '--fsid', 'c2a5b802-37da-11ec-b51d-53e6e728d2d3']\x1b[0m\n"
10.10.2.1: b"# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:63 -  > /tmp/bench-rados/ceph-research/scripts/parallel-ssh --host 10.10.2.1 --host 10.10.2.2 'sudo python3 -u - rm-cluster --force --fsid c2a5b802-37da-11ec-b51d-53e6e728d2d3'\n"
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T03:44:05,615764562-07:00] DEBUG: command from stdin\x1b[0m\n'
10.10.2.1: b"# /tmp/bench-rados/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host 10.10.2.1 --host 10.10.2.2 'sudo python3 -u - rm-cluster --force --fsid c2a5b802-37da-11ec-b51d-53e6e728d2d3'\n"
10.10.2.1: b'[1] 03:44:11 [SUCCESS] 10.10.2.1\n[2] 03:44:19 [SUCCESS] 10.10.2.2\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T03:44:19,219382139-07:00] INFO: > Deploy a new cluster\x1b[0m\n'
10.10.2.1: b'# bash:24 -  > /tmp/bench-rados/ceph-research/scripts/ceph-deploy -m 10.10.2.1 -o node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/10.10.2.2:/dev/nvme0n1\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T03:44:19,230900642-07:00] STAGE: Verify passwordless SSH to hosts, passwordless sudo via ljishen, and the existence of devices on hosts...\x1b[0m\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T03:44:19,235496803-07:00] INFO: > Check the host for the monitor\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:181 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@10.10.2.1 sudo --non-interactive --validate\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T03:44:19,387889815-07:00] INFO: > Check hosts for OSDs\x1b[0m\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T03:44:19,392961018-07:00] INFO: >> Check host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:200 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us sudo --non-interactive --validate\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T03:44:19,542999015-07:00] INFO: >>> Check device '/dev/nvme0n1'\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:215 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us test -b /dev/nvme0n1\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:226 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us lsblk --nodeps --noheadings --output TYPE /dev/nvme0n1\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:227 - verify_parameters() > grep --word-regexp disk\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T03:44:19,828013531-07:00] STAGE: Reset devices on hosts...\x1b[0m\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T03:44:19,833001178-07:00] INFO: > Resetting device '/dev/nvme0n1' on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:288 - reset_host_devices() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us sudo sh\n'
10.10.2.1: b'  Removing ceph--1a4f6c9c--3acb--4c60--8b88--f649a47b06d1-osd--block--65635090--ff40--4c1e--af38--b2b93750c54c (253:0)\n'
10.10.2.1: b'  Archiving volume group "ceph-1a4f6c9c-3acb-4c60-8b88-f649a47b06d1" metadata (seqno 5).\n'
10.10.2.1: b'  Releasing logical volume "osd-block-65635090-ff40-4c1e-af38-b2b93750c54c"\n'
10.10.2.1: b'  Creating volume group backup "/etc/lvm/backup/ceph-1a4f6c9c-3acb-4c60-8b88-f649a47b06d1" (seqno 6).\n'
10.10.2.1: b'  Logical volume "osd-block-65635090-ff40-4c1e-af38-b2b93750c54c" successfully removed\n'
10.10.2.1: b'  Removing physical volume "/dev/nvme0n1" from volume group "ceph-1a4f6c9c-3acb-4c60-8b88-f649a47b06d1"\n'
10.10.2.1: b'  Volume group "ceph-1a4f6c9c-3acb-4c60-8b88-f649a47b06d1" successfully removed\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T03:44:20,261852132-07:00] STAGE: Bootstrap a new cluster...\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:317 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.1 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T03:44:20,271786328-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T03:44:20,275366518-07:00] DEBUG: CEPHADM_PARAMS: ['bootstrap', '--config', '/tmp/bench-rados/ceph-research/scripts/ceph.conf', '--mon-ip', '10.10.2.1', '--output-dir', '/tmp/bench-rados/deployment_data_root/etc/ceph', '--ssh-user', 'ljishen', '--skip-dashboard', '--no-minimize-config', '--allow-overwrite', '--allow-fqdn-hostname', '--orphan-initial-daemons', '--skip-monitoring-stack', '--single-host-defaults']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.1 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults\n'
10.10.2.1: b'Verifying podman|docker is present...\n'
10.10.2.1: b'Verifying lvm2 is present...\nVerifying time synchronization is in place...\n'
10.10.2.1: b'Unit ntp.service is enabled and running\n'
10.10.2.1: b'Repeating the final host check...\n'
10.10.2.1: b'podman|docker (/usr/bin/docker) is present\n'
10.10.2.1: b'systemctl is present\n'
10.10.2.1: b'lvcreate is present\n'
10.10.2.1: b'Unit ntp.service is enabled and running\n'
10.10.2.1: b'Host looks OK\nCluster fsid: 01d7f8cc-37dc-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Verifying IP 10.10.2.1 port 3300 ...\n'
10.10.2.1: b'Verifying IP 10.10.2.1 port 6789 ...\n'
10.10.2.1: b'Mon IP `10.10.2.1` is in CIDR network `10.10.2.0/24`\n- internal network (--cluster-network) has not been provided, OSD replication will default to the public_network\n'
10.10.2.1: b'Adjusting default settings to suit single-host cluster...\n'
10.10.2.1: b'Pulling container image quay.io/ceph/ceph:v16...\n'
10.10.2.1: b'Ceph version: ceph version 16.2.6 (ee28fb57e47e9f88813e24bbf4c14496ca299d31) pacific (stable)\nExtracting ceph user uid/gid from container image...\n'
10.10.2.1: b'Creating initial keys...\n'
10.10.2.1: b'Creating initial monmap...\n'
10.10.2.1: b'Creating mon...\n'
10.10.2.1: b'Waiting for mon to start...\n'
10.10.2.1: b'Waiting for mon...\n'
10.10.2.1: b'mon is available\nSetting mon public_network to 10.10.2.0/24\n'
10.10.2.1: b'Wrote config to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf\n'
10.10.2.1: b'Wrote keyring to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring\n'
10.10.2.1: b'Creating mgr...\n'
10.10.2.1: b'Verifying port 9283 ...\n'
10.10.2.1: b'Waiting for mgr to start...\nWaiting for mgr...\n'
10.10.2.1: b'mgr not available, waiting (1/15)...\n'
10.10.2.1: b'mgr not available, waiting (2/15)...\n'
10.10.2.1: b'mgr is available\n'
10.10.2.1: b'Enabling cephadm module...\n'
10.10.2.1: b'Waiting for the mgr to restart...\n'
10.10.2.1: b'Waiting for mgr epoch 5...\n'
10.10.2.1: b'mgr epoch 5 is available\nSetting orchestrator backend to cephadm...\n'
10.10.2.1: b'Generating ssh key...\n'
10.10.2.1: b'Wrote public SSH key to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub\n'
10.10.2.1: b'Adding key to ljishen@localhost authorized_keys...\n'
10.10.2.1: b'Adding host node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us...\n'
10.10.2.1: b'Deploying unmanaged mon service...\n'
10.10.2.1: b'Deploying unmanaged mgr service...\n'
10.10.2.1: b'You can access the Ceph CLI with:\n\n\tsudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --fsid 01d7f8cc-37dc-11ec-b51d-53e6e728d2d3 -c /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf -k /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring\n\n'
10.10.2.1: b'Please consider enabling telemetry to help improve Ceph:\n\n\tceph telemetry on\n\nFor more information see:\n\n\thttps://docs.ceph.com/docs/pacific/mgr/telemetry/\n\n'
10.10.2.1: b'Bootstrap complete.\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T03:45:19,919578627-07:00] INFO: Sleep for 20 seconds\x1b[0m\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T03:45:39,926470632-07:00] INFO: > Set the default number of monitor daemons to 1\x1b[0m\n# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:337 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T03:45:39,936575147-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T03:45:39,940806521-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mon', '1']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1\n'
10.10.2.1: b'Inferring fsid 01d7f8cc-37dc-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/01d7f8cc-37dc-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b'Scheduled mon update...\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T03:45:48,916682816-07:00] INFO: > Set the default number of manager daemons to 1\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:342 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T03:45:48,926326775-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T03:45:48,929638170-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mgr', '1']\x1b[0m\n# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1\n"
10.10.2.1: b'Inferring fsid 01d7f8cc-37dc-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/01d7f8cc-37dc-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b'Scheduled mgr update...\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T03:45:58,443229401-07:00] STAGE: Add hosts to the cluster...\x1b[0m\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T03:45:58,450023162-07:00] INFO: > Adding host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:361 - add_hosts() > ssh-copy-id -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -f -i /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\n'
10.10.2.1: b'/usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: "/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub"\n'
10.10.2.1: b'\nNumber of key(s) added: 1\n\nNow try logging into the machine, with:   "ssh -o \'GlobalKnownHostsFile=/dev/null\' -o \'LogLevel=ERROR\' -o \'PasswordAuthentication=no\' -o \'StrictHostKeyChecking=no\' -o \'UserKnownHostsFile=/dev/null\' \'ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\'"\nand check to make sure that only the key(s) you wanted were added.\n\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:367 - add_hosts() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us 10.10.2.2\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T03:45:58,661534591-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T03:45:58,664790782-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'host', 'add', 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us', '10.10.2.2']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us 10.10.2.2\n'
10.10.2.1: b'Inferring fsid 01d7f8cc-37dc-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/01d7f8cc-37dc-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b"Added host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us' with addr '10.10.2.2'\n"
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T03:46:08,067722313-07:00] INFO: Sleep for 20 seconds\x1b[0m\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T03:46:28,072416930-07:00] STAGE: Add OSDs to the cluster...\x1b[0m\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T03:46:28,079653884-07:00] INFO: > Adding devices ['/dev/nvme0n1'] on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us' as OSDs\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:388 - add_host_devices_as_osds() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T03:46:28,089637182-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T03:46:28,093220017-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'daemon', 'add', 'osd', 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1\n'
10.10.2.1: b'Inferring fsid 01d7f8cc-37dc-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/01d7f8cc-37dc-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b"Created osd(s) 0 on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'\n"
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T03:46:52,041222307-07:00] INFO: > Sleep for 20 seconds\x1b[0m\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T03:47:12,046850621-07:00] STAGE: Show Cluster Status\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:408 - show_status() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T03:47:12,056720335-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T03:47:12,061032391-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', '--status']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status\n'
10.10.2.1: b'Inferring fsid 01d7f8cc-37dc-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/01d7f8cc-37dc-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b'  cluster:\n    id:     01d7f8cc-37dc-11ec-b51d-53e6e728d2d3\n    health: HEALTH_WARN\n            1 pool(s) have no replicas configured\n \n  services:\n    mon: 1 daemons, quorum node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us (age 2m)\n    mgr: node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us.adxcwm(active, since 2m)\n    osd: 1 osds: 1 up (since 17s), 1 in (since 39s)\n \n  data:\n    pools:   1 pools, 1 pgs\n    objects: 0 objects, 0 B\n    usage:   4.7 MiB used, 100 GiB / 100 GiB avail\n    pgs:     1 active+clean\n \n'
[1] 03:47:20 [SUCCESS] ljishen@10.10.2.1

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:44:05,593249747-07:00] INFO: > Remove existing clusters[0m
## bash:17 -  > basename -- /var/lib/ceph/c2a5b802-37da-11ec-b51d-53e6e728d2d3
# bash:17 -  > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm --host 10.10.2.1 --host 10.10.2.2 rm-cluster --force --fsid c2a5b802-37da-11ec-b51d-53e6e728d2d3
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:44:05,603658955-07:00] DEBUG: HOST_PARAMS: ['--host', '10.10.2.1', '--host', '10.10.2.2'][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:44:05,607223697-07:00] DEBUG: CEPHADM_PARAMS: ['rm-cluster', '--force', '--fsid', 'c2a5b802-37da-11ec-b51d-53e6e728d2d3'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:63 -  > /tmp/bench-rados/ceph-research/scripts/parallel-ssh --host 10.10.2.1 --host 10.10.2.2 'sudo python3 -u - rm-cluster --force --fsid c2a5b802-37da-11ec-b51d-53e6e728d2d3'
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:44:05,615764562-07:00] DEBUG: command from stdin[0m
# /tmp/bench-rados/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host 10.10.2.1 --host 10.10.2.2 'sudo python3 -u - rm-cluster --force --fsid c2a5b802-37da-11ec-b51d-53e6e728d2d3'
[1] 03:44:11 [SUCCESS] 10.10.2.1
[2] 03:44:19 [SUCCESS] 10.10.2.2

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:44:19,219382139-07:00] INFO: > Deploy a new cluster[0m
# bash:24 -  > /tmp/bench-rados/ceph-research/scripts/ceph-deploy -m 10.10.2.1 -o node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/10.10.2.2:/dev/nvme0n1


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:44:19,230900642-07:00] STAGE: Verify passwordless SSH to hosts, passwordless sudo via ljishen, and the existence of devices on hosts...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:44:19,235496803-07:00] INFO: > Check the host for the monitor[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:181 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@10.10.2.1 sudo --non-interactive --validate

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:44:19,387889815-07:00] INFO: > Check hosts for OSDs[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:44:19,392961018-07:00] INFO: >> Check host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:200 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us sudo --non-interactive --validate

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:44:19,542999015-07:00] INFO: >>> Check device '/dev/nvme0n1'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:215 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us test -b /dev/nvme0n1
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:226 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us lsblk --nodeps --noheadings --output TYPE /dev/nvme0n1
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:227 - verify_parameters() > grep --word-regexp disk


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:44:19,828013531-07:00] STAGE: Reset devices on hosts...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:44:19,833001178-07:00] INFO: > Resetting device '/dev/nvme0n1' on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:288 - reset_host_devices() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us sudo sh
  Removing ceph--1a4f6c9c--3acb--4c60--8b88--f649a47b06d1-osd--block--65635090--ff40--4c1e--af38--b2b93750c54c (253:0)
  Archiving volume group "ceph-1a4f6c9c-3acb-4c60-8b88-f649a47b06d1" metadata (seqno 5).
  Releasing logical volume "osd-block-65635090-ff40-4c1e-af38-b2b93750c54c"
  Creating volume group backup "/etc/lvm/backup/ceph-1a4f6c9c-3acb-4c60-8b88-f649a47b06d1" (seqno 6).
  Logical volume "osd-block-65635090-ff40-4c1e-af38-b2b93750c54c" successfully removed
  Removing physical volume "/dev/nvme0n1" from volume group "ceph-1a4f6c9c-3acb-4c60-8b88-f649a47b06d1"
  Volume group "ceph-1a4f6c9c-3acb-4c60-8b88-f649a47b06d1" successfully removed


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:44:20,261852132-07:00] STAGE: Bootstrap a new cluster...[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:317 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.1 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:44:20,271786328-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:44:20,275366518-07:00] DEBUG: CEPHADM_PARAMS: ['bootstrap', '--config', '/tmp/bench-rados/ceph-research/scripts/ceph.conf', '--mon-ip', '10.10.2.1', '--output-dir', '/tmp/bench-rados/deployment_data_root/etc/ceph', '--ssh-user', 'ljishen', '--skip-dashboard', '--no-minimize-config', '--allow-overwrite', '--allow-fqdn-hostname', '--orphan-initial-daemons', '--skip-monitoring-stack', '--single-host-defaults'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.1 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults
Verifying podman|docker is present...
Verifying lvm2 is present...
Verifying time synchronization is in place...
Unit ntp.service is enabled and running
Repeating the final host check...
podman|docker (/usr/bin/docker) is present
systemctl is present
lvcreate is present
Unit ntp.service is enabled and running
Host looks OK
Cluster fsid: 01d7f8cc-37dc-11ec-b51d-53e6e728d2d3
Verifying IP 10.10.2.1 port 3300 ...
Verifying IP 10.10.2.1 port 6789 ...
Mon IP `10.10.2.1` is in CIDR network `10.10.2.0/24`
- internal network (--cluster-network) has not been provided, OSD replication will default to the public_network
Adjusting default settings to suit single-host cluster...
Pulling container image quay.io/ceph/ceph:v16...
Ceph version: ceph version 16.2.6 (ee28fb57e47e9f88813e24bbf4c14496ca299d31) pacific (stable)
Extracting ceph user uid/gid from container image...
Creating initial keys...
Creating initial monmap...
Creating mon...
Waiting for mon to start...
Waiting for mon...
mon is available
Setting mon public_network to 10.10.2.0/24
Wrote config to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf
Wrote keyring to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring
Creating mgr...
Verifying port 9283 ...
Waiting for mgr to start...
Waiting for mgr...
mgr not available, waiting (1/15)...
mgr not available, waiting (2/15)...
mgr is available
Enabling cephadm module...
Waiting for the mgr to restart...
Waiting for mgr epoch 5...
mgr epoch 5 is available
Setting orchestrator backend to cephadm...
Generating ssh key...
Wrote public SSH key to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub
Adding key to ljishen@localhost authorized_keys...
Adding host node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us...
Deploying unmanaged mon service...
Deploying unmanaged mgr service...
You can access the Ceph CLI with:

	sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --fsid 01d7f8cc-37dc-11ec-b51d-53e6e728d2d3 -c /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf -k /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring

Please consider enabling telemetry to help improve Ceph:

	ceph telemetry on

For more information see:

	https://docs.ceph.com/docs/pacific/mgr/telemetry/

Bootstrap complete.

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:45:19,919578627-07:00] INFO: Sleep for 20 seconds[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:45:39,926470632-07:00] INFO: > Set the default number of monitor daemons to 1[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:337 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:45:39,936575147-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:45:39,940806521-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mon', '1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1
Inferring fsid 01d7f8cc-37dc-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/01d7f8cc-37dc-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Scheduled mon update...

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:45:48,916682816-07:00] INFO: > Set the default number of manager daemons to 1[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:342 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:45:48,926326775-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:45:48,929638170-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mgr', '1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1
Inferring fsid 01d7f8cc-37dc-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/01d7f8cc-37dc-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Scheduled mgr update...


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:45:58,443229401-07:00] STAGE: Add hosts to the cluster...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:45:58,450023162-07:00] INFO: > Adding host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:361 - add_hosts() > ssh-copy-id -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -f -i /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us
/usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: "/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub"

Number of key(s) added: 1

Now try logging into the machine, with:   "ssh -o 'GlobalKnownHostsFile=/dev/null' -o 'LogLevel=ERROR' -o 'PasswordAuthentication=no' -o 'StrictHostKeyChecking=no' -o 'UserKnownHostsFile=/dev/null' 'ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'"
and check to make sure that only the key(s) you wanted were added.

# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:367 - add_hosts() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us 10.10.2.2
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:45:58,661534591-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:45:58,664790782-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'host', 'add', 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us', '10.10.2.2'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us 10.10.2.2
Inferring fsid 01d7f8cc-37dc-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/01d7f8cc-37dc-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Added host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us' with addr '10.10.2.2'

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:46:08,067722313-07:00] INFO: Sleep for 20 seconds[0m


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:46:28,072416930-07:00] STAGE: Add OSDs to the cluster...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:46:28,079653884-07:00] INFO: > Adding devices ['/dev/nvme0n1'] on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us' as OSDs[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:388 - add_host_devices_as_osds() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:46:28,089637182-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:46:28,093220017-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'daemon', 'add', 'osd', 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1
Inferring fsid 01d7f8cc-37dc-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/01d7f8cc-37dc-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Created osd(s) 0 on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:46:52,041222307-07:00] INFO: > Sleep for 20 seconds[0m


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:47:12,046850621-07:00] STAGE: Show Cluster Status[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:408 - show_status() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:47:12,056720335-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:47:12,061032391-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', '--status'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status
Inferring fsid 01d7f8cc-37dc-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/01d7f8cc-37dc-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
  cluster:
    id:     01d7f8cc-37dc-11ec-b51d-53e6e728d2d3
    health: HEALTH_WARN
            1 pool(s) have no replicas configured
 
  services:
    mon: 1 daemons, quorum node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us (age 2m)
    mgr: node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us.adxcwm(active, since 2m)
    osd: 1 osds: 1 up (since 17s), 1 in (since 39s)
 
  data:
    pools:   1 pools, 1 pgs
    objects: 0 objects, 0 B
    usage:   4.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     1 active+clean
 

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:47:20,552860702-07:00] INFO: > Copy configuration files to local[0m
# ./benchmarks/bench-rados:220 - launch_ceph_cluster() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.1 sudo chown --changes --recursive ljishen /tmp/bench-rados/deployment_data_root/etc/ceph
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:47:20,560863573-07:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.1 sudo chown --changes --recursive ljishen /tmp/bench-rados/deployment_data_root/etc/ceph
[1] 03:47:20 [SUCCESS] ljishen@10.10.2.1
# ./benchmarks/bench-rados:223 - launch_ceph_cluster() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.1:/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf
# ./benchmarks/bench-rados:226 - launch_ceph_cluster() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.1:/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:47:21,036849181-07:00] STAGE: Update local ceph.conf as a client...[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:47:21,040351200-07:00] STAGE: Configure Ceph pool...[0m
# ./benchmarks/bench-rados:257 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_max_backfills 32
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:47:21,062616258-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:47:21,065408229-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '01d7f8cc-37dc-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'config', 'set', 'osd', 'osd_max_backfills', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 01d7f8cc-37dc-11ec-b51d-53e6e728d2d3 -- ceph config set osd osd_max_backfills 32
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:258 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_active 32
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:47:25,453490621-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:47:25,456859820-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '01d7f8cc-37dc-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_active', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 01d7f8cc-37dc-11ec-b51d-53e6e728d2d3 -- ceph config set osd osd_recovery_max_active 32
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:259 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_single_start 8
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:47:29,834953999-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:47:29,838076554-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '01d7f8cc-37dc-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_single_start', '8'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 01d7f8cc-37dc-11ec-b51d-53e6e728d2d3 -- ceph config set osd osd_recovery_max_single_start 8
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:260 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_op_priority 63
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:47:34,062075229-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:47:34,064879182-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '01d7f8cc-37dc-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_op_priority', '63'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 01d7f8cc-37dc-11ec-b51d-53e6e728d2d3 -- ceph config set osd osd_recovery_op_priority 63
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:262 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config show-with-defaults osd.0
# ./benchmarks/bench-rados:263 - configure_ceph_pool() > grep 'osd_max_backfills\|osd_recovery'
# ./benchmarks/bench-rados:263 - configure_ceph_pool() > column -t -s ' '
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
osd_max_backfills                        32         mon
osd_recovery_cost                        20971520   default
osd_recovery_delay_start                 0.000000   default
osd_recovery_max_active                  32         mon
osd_recovery_max_active_hdd              3          default
osd_recovery_max_active_ssd              10         default
osd_recovery_max_chunk                   8388608    default
osd_recovery_max_omap_entries_per_chunk  8096       default
osd_recovery_max_single_start            8          mon
osd_recovery_op_priority                 63         mon
osd_recovery_op_warn_multiple            16         default
osd_recovery_priority                    5          default
osd_recovery_retry_interval              30.000000  default
osd_recovery_sleep                       0.000000   default
osd_recovery_sleep_hdd                   0.100000   default
osd_recovery_sleep_hybrid                0.025000   default
osd_recovery_sleep_ssd                   0.000000   default
# ./benchmarks/bench-rados:265 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool create bench_rados 128 128 replicated --size 1 --pg-num-min 128
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:47:42,649272733-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:47:42,652438209-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '01d7f8cc-37dc-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'create', 'bench_rados', '128', '128', 'replicated', '--size', '1', '--pg-num-min', '128'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 01d7f8cc-37dc-11ec-b51d-53e6e728d2d3 -- ceph osd pool create bench_rados 128 128 replicated --size 1 --pg-num-min 128
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
pool 'bench_rados' created
# ./benchmarks/bench-rados:267 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados min_size 1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:47:47,140396430-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:47:47,143713391-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '01d7f8cc-37dc-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'min_size', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 01d7f8cc-37dc-11ec-b51d-53e6e728d2d3 -- ceph osd pool set bench_rados min_size 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 min_size to 1
# ./benchmarks/bench-rados:268 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados pg_autoscale_mode off
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:47:51,517097564-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:47:51,520311410-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '01d7f8cc-37dc-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'pg_autoscale_mode', 'off'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 01d7f8cc-37dc-11ec-b51d-53e6e728d2d3 -- ceph osd pool set bench_rados pg_autoscale_mode off
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 pg_autoscale_mode to off
# ./benchmarks/bench-rados:269 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool application enable bench_rados benchmark
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:47:56,741233340-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:47:56,744251117-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '01d7f8cc-37dc-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'application', 'enable', 'bench_rados', 'benchmark'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 01d7f8cc-37dc-11ec-b51d-53e6e728d2d3 -- ceph osd pool application enable bench_rados benchmark
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
enabled application 'benchmark' on pool 'bench_rados'
# ./benchmarks/bench-rados:270 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados noscrub 1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:48:02,117660832-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:48:02,120693276-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '01d7f8cc-37dc-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'noscrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 01d7f8cc-37dc-11ec-b51d-53e6e728d2d3 -- ceph osd pool set bench_rados noscrub 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 noscrub to 1
# ./benchmarks/bench-rados:271 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados nodeep-scrub 1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:48:06,760349343-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:48:06,763420280-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '01d7f8cc-37dc-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'nodeep-scrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 01d7f8cc-37dc-11ec-b51d-53e6e728d2d3 -- ceph osd pool set bench_rados nodeep-scrub 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 nodeep-scrub to 1
# ./benchmarks/bench-rados:272 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool ls detail
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:48:12,017401377-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:48:12,020804841-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '01d7f8cc-37dc-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'ls', 'detail'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 01d7f8cc-37dc-11ec-b51d-53e6e728d2d3 -- ceph osd pool ls detail
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
pool 1 'device_health_metrics' replicated size 1 min_size 1 crush_rule 0 object_hash rjenkins pg_num 64 pgp_num 64 autoscale_mode on last_change 18 lfor 0/0/16 flags hashpspool stripe_width 0 pg_num_min 1 application mgr_devicehealth
pool 2 'bench_rados' replicated size 1 min_size 1 crush_rule 0 object_hash rjenkins pg_num 128 pgp_num 128 autoscale_mode off last_change 20 flags hashpspool,noscrub,nodeep-scrub stripe_width 0 pg_num_min 128 application benchmark

# ./benchmarks/bench-rados:273 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd df tree
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:48:16,254903107-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:48:16,258192706-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '01d7f8cc-37dc-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'df', 'tree'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 01d7f8cc-37dc-11ec-b51d-53e6e728d2d3 -- ceph osd df tree
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
ID  CLASS  WEIGHT   REWEIGHT  SIZE     RAW USE  DATA     OMAP  META     AVAIL    %USE  VAR   PGS  STATUS  TYPE NAME      
-1         0.09769         -  100 GiB  5.7 MiB  168 KiB   0 B  5.6 MiB  100 GiB  0.01  1.00    -          root default   
-3         0.09769         -  100 GiB  5.7 MiB  168 KiB   0 B  5.6 MiB  100 GiB  0.01  1.00    -              host node-1
 0    ssd  0.09769   1.00000  100 GiB  5.7 MiB  168 KiB   0 B  5.6 MiB  100 GiB  0.01  1.00  192      up          osd.0  
                       TOTAL  100 GiB  5.7 MiB  168 KiB   0 B  5.6 MiB  100 GiB  0.01                                    
MIN/MAX VAR: 1.00/1.00  STDDEV: 0


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:48:20,463187980-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:48:44,820401466-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:48:54,153960493-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:49:03,459137015-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:49:03,467423010-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:49:12,892267158-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:49:12,901044839-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:49:22,232314282-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:49:22,240574708-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:49:31,562263631-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:49:31,570253367-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:49:40,952702313-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:49:40,960722126-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:49:40,967539934-07:00] INFO: > The cluster is idle now.[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:49:40,971928493-07:00] STAGE: Start benchmarking of mode write ...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:49:40,979406566-07:00] INFO: > Start system activity collection services[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:49:40,985274504-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:341 - rados_bench() > rm --force /mnt/sda8/ceph-research/scripts/output/sys_activity_4MB_write.dat.rados_bench_host.2
# ./benchmarks/bench-rados:342 - rados_bench() > sar_pid_rados_bench_host=324056
# ./benchmarks/bench-rados:342 - rados_bench() > S_TIME_FORMAT=ISO
# ./benchmarks/bench-rados:342 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/output/sys_activity_4MB_write.dat.rados_bench_host.2 2

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:49:40,992675350-07:00] INFO: >> for OSD host[0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:49:41,001959555-07:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.2 bash
10.10.2.2: b'186618\n'
[1] 03:49:41 [SUCCESS] ljishen@10.10.2.2
186618

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:49:41,195955890-07:00] INFO: > Run rados bench[0m
# ./benchmarks/bench-rados:370 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/output/rados_bench_4MB_write.log.2
## ./benchmarks/bench-rados:361 - rados_bench() > count_cpus 32-63,96-127
# ./benchmarks/bench-rados:361 - rados_bench() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 60 write --pool bench_rados -b 4194304 -O 4194304 --concurrent-ios 128 --show-time --write-object --write-omap --write-xattr --no-cleanup
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:49:41,216655437-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:49:41,219499597-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '01d7f8cc-37dc-11ec-b51d-53e6e728d2d3', '--', 'rados', 'bench', '60', 'write', '--pool', 'bench_rados', '-b', '4194304', '-O', '4194304', '--concurrent-ios', '128', '--show-time', '--write-object', '--write-omap', '--write-xattr', '--no-cleanup'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 01d7f8cc-37dc-11ec-b51d-53e6e728d2d3 -- rados bench 60 write --pool bench_rados -b 4194304 -O 4194304 --concurrent-ios 128 --show-time --write-object --write-omap --write-xattr --no-cleanup
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
hints = 1
2021-10-28T10:49:44.565005+0000 Maintaining 128 concurrent writes of 4194304 bytes to objects of size 4194304 for up to 60 seconds or 0 objects
2021-10-28T10:49:44.565019+0000 Object prefix: benchmark_data_node-2.bluefield2.ucsc-cmps10_7
2021-10-28T10:49:44.689919+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-28T10:49:44.689919+0000     0       0         0         0         0         0           -           0
2021-10-28T10:49:45.690025+0000     1      84        84         0         0         0           -           0
2021-10-28T10:49:46.690099+0000     2     127       144        17   33.9978        34     1.79618     1.67645
2021-10-28T10:49:47.690173+0000     3     127       193        66    87.994       196     2.22017     2.04331
2021-10-28T10:49:48.690241+0000     4     127       268       141    140.99       300     1.99395      2.1179
2021-10-28T10:49:49.690324+0000     5     127       284       157   125.591        64     2.49863     2.14562
2021-10-28T10:49:50.690397+0000     6     127       313       186   123.991       116     3.08403     2.26427
2021-10-28T10:49:51.690472+0000     7     127       350       223   127.419       148     3.65887     2.45489
2021-10-28T10:49:52.690546+0000     8     127       401       274    136.99       204     3.64349     2.69425
2021-10-28T10:49:53.690621+0000     9     127       451       324    143.99       200      2.7057     2.75247
2021-10-28T10:49:54.690706+0000    10     127       524       397   158.788       292     2.12129     2.68054
2021-10-28T10:49:55.690787+0000    11     127       540       413   150.171        64     2.71842     2.67469
2021-10-28T10:49:56.690855+0000    12     127       564       437   145.656        96     3.14609      2.6931
2021-10-28T10:49:57.690924+0000    13     127       603       476   146.451       156     3.69922      2.7563
2021-10-28T10:49:58.690992+0000    14     127       640       513   146.561       148     4.22995     2.84711
2021-10-28T10:49:59.691068+0000    15     127       659       532   141.856        76      4.5115     2.90417
2021-10-28T10:50:00.691146+0000    16     127       701       574   143.489       168     3.68458     2.99181
2021-10-28T10:50:01.691222+0000    17     127       776       649   152.695       300     2.79781     3.01653
2021-10-28T10:50:02.691294+0000    18     127       791       664   147.545        60     2.61206     3.00597
2021-10-28T10:50:03.691366+0000    19     127       815       688   144.831        96     3.10407     3.00578
2021-10-28T10:50:04.691438+0000 min lat: 1.55781 max lat: 4.7881 avg lat: 3.02921
2021-10-28T10:50:04.691438+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-28T10:50:04.691438+0000    20     127       853       726   145.189       152     3.68687     3.02921
2021-10-28T10:50:05.691527+0000    21     127       887       760   144.751       136      4.2052     3.07185
2021-10-28T10:50:06.691599+0000    22     127       911       784   142.535        96     4.67305     3.11346
2021-10-28T10:50:07.691669+0000    23     127       927       800    139.12        64     4.49291     3.14162
2021-10-28T10:50:08.691741+0000    24     127       964       837    139.49       148     4.46953     3.20152
2021-10-28T10:50:09.691817+0000    25     127       998       871    139.35       136     4.45909     3.25201
2021-10-28T10:50:10.691894+0000    26     127      1030       903   138.913       128     4.66909     3.29719
2021-10-28T10:50:11.691975+0000    27     127      1042       915   135.545        48     4.71609     3.31631
2021-10-28T10:50:12.692046+0000    28     127      1071       944   134.847       116     4.50765     3.35593
2021-10-28T10:50:13.692120+0000    29     127      1106       979   135.024       140     4.42067     3.39756
2021-10-28T10:50:14.692195+0000    30     127      1147      1020    135.99       164     4.39258     3.44142
2021-10-28T10:50:15.692267+0000    31     127      1165      1038   133.926        72      4.2571       3.458
2021-10-28T10:50:16.692337+0000    32     127      1186      1059   132.365        84     4.40431     3.47514
2021-10-28T10:50:17.692411+0000    33     127      1222      1095   132.717       144     4.36316     3.50539
2021-10-28T10:50:18.692487+0000    34     127      1259      1132   133.167       148     4.43468     3.53549
2021-10-28T10:50:19.692556+0000    35     127      1286      1159   132.447       108     4.29939     3.55671
2021-10-28T10:50:20.692631+0000    36     127      1302      1175   130.546        64     4.35598      3.5703
2021-10-28T10:50:21.692712+0000    37     127      1331      1204   130.153       116     4.46841     3.59346
2021-10-28T10:50:22.692795+0000    38     127      1367      1240   130.517       144     4.43904     3.62106
2021-10-28T10:50:23.692873+0000    39     127      1403      1276   130.862       144     4.44406     3.64665
2021-10-28T10:50:24.692942+0000 min lat: 1.55781 max lat: 4.99135 avg lat: 3.65934
2021-10-28T10:50:24.692942+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-28T10:50:24.692942+0000    40     127      1422      1295    129.49        76     4.43748     3.65934
2021-10-28T10:50:25.693025+0000    41     127      1443      1316   128.381        84     4.40041     3.67087
2021-10-28T10:50:26.693099+0000    42     127      1479      1352   128.752       144     4.47266     3.69245
2021-10-28T10:50:27.693181+0000    43     127      1515      1388   129.107       144     4.45026     3.71256
2021-10-28T10:50:28.693253+0000    44     127      1545      1418   128.899       120      4.3739     3.72866
2021-10-28T10:50:29.693322+0000    45     127      1559      1432   127.279        56     4.22907     3.73611
2021-10-28T10:50:30.693399+0000    46     127      1590      1463   127.208       124     4.34903     3.75085
2021-10-28T10:50:31.693478+0000    47     127      1627      1500    127.65       148     4.39058     3.76714
2021-10-28T10:50:32.693554+0000    48     127      1659      1532   127.657       128     4.46071     3.78098
2021-10-28T10:50:33.693633+0000    49     127      1681      1554   126.848        88     4.47229     3.79085
2021-10-28T10:50:34.693704+0000    50     127      1702      1575   125.991        84     4.41004     3.79977
2021-10-28T10:50:35.693778+0000    51     127      1735      1608   126.108       132     4.36393     3.81338
2021-10-28T10:50:36.693855+0000    52     127      1773      1646   126.606       152     4.45049     3.82856
2021-10-28T10:50:37.693927+0000    53     127      1802      1675   126.406       116     4.23129       3.839
2021-10-28T10:50:38.693999+0000    54     127      1815      1688   125.028        52     4.56794     3.84537
2021-10-28T10:50:39.694065+0000    55     127      1845      1718   124.936       120     4.50261     3.85898
2021-10-28T10:50:40.694141+0000    56     127      1882      1755   125.348       148     4.51501     3.87419
2021-10-28T10:50:41.694220+0000    57     127      1919      1792   125.745       148     4.50545     3.88884
2021-10-28T10:50:42.694297+0000    58     127      1935      1808    124.68        64      4.4353     3.89429
2021-10-28T10:50:43.694385+0000    59     127      1954      1827   123.855        76     4.44302     3.90015
2021-10-28T10:50:44.694467+0000 min lat: 0.218019 max lat: 4.99135 avg lat: 3.82255
2021-10-28T10:50:44.694467+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-28T10:50:44.694467+0000    60       4      1988      1984   132.257       628    0.218019     3.82255
2021-10-28T10:50:45.694614+0000 Total time run:         60.1093
Total writes made:      1988
Write size:             4194304
Object size:            4194304
Bandwidth (MB/sec):     132.292
Stddev Bandwidth:       87.3876
Max bandwidth (MB/sec): 628
Min bandwidth (MB/sec): 0
Average IOPS:           33
Stddev IOPS:            21.8565
Max IOPS:               157
Min IOPS:               0
Average Latency(s):     3.81536
Stddev Latency(s):      0.961372
Max latency(s):         4.99135
Min latency(s):         0.218019

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:50:46,498579129-07:00] INFO: > Stop system activity collection services[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:50:46,504599815-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:391 - rados_bench() > kill -INT 324056

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:50:46,511061462-07:00] INFO: >> for OSD host[0m
# ./benchmarks/bench-rados:396 - rados_bench() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.2 kill -INT 186618
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:50:46,519057169-07:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.2 kill -INT 186618
[1] 03:50:46 [SUCCESS] ljishen@10.10.2.2

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:50:46,705971959-07:00] INFO: >>> Copy system activity file from OSD host[0m
## ./benchmarks/bench-rados:403 - rados_bench() > basename -- /tmp/bench-rados/sys_activity_4MB_write.dat.osd_host.2
# ./benchmarks/bench-rados:403 - rados_bench() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.2:/tmp/bench-rados/sys_activity_4MB_write.dat.osd_host.2 /mnt/sda8/ceph-research/scripts/output/sys_activity_4MB_write.dat.osd_host.2


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:50:46,884178990-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:51:11,081937656-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 1.99k objects, 7.8 GiB
    usage:   24 GiB used, 76 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:51:11,090178205-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:51:20,376726838-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 1.99k objects, 7.8 GiB
    usage:   24 GiB used, 76 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:51:20,384621375-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:51:29,801120050-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 1.99k objects, 7.8 GiB
    usage:   24 GiB used, 76 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:51:29,809183075-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:51:39,240822622-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 1.99k objects, 7.8 GiB
    usage:   24 GiB used, 76 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:51:39,249236007-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:51:48,705572544-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 1.99k objects, 7.8 GiB
    usage:   24 GiB used, 76 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:51:48,713410554-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:51:48,720143212-07:00] INFO: > The cluster is idle now.[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:51:48,724176462-07:00] STAGE: Start benchmarking of mode seq ...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:51:48,731728824-07:00] INFO: > Start system activity collection services[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:51:48,737558961-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:341 - rados_bench() > rm --force /mnt/sda8/ceph-research/scripts/output/sys_activity_4MB_seq.dat.rados_bench_host.2
# ./benchmarks/bench-rados:342 - rados_bench() > sar_pid_rados_bench_host=325400
# ./benchmarks/bench-rados:342 - rados_bench() > S_TIME_FORMAT=ISO
# ./benchmarks/bench-rados:342 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/output/sys_activity_4MB_seq.dat.rados_bench_host.2 2

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:51:48,745442667-07:00] INFO: >> for OSD host[0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:51:48,754663243-07:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.2 bash
10.10.2.2: b'186718\n'
[1] 03:51:48 [SUCCESS] ljishen@10.10.2.2
186718

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:51:48,947347395-07:00] INFO: > Run rados bench[0m
# ./benchmarks/bench-rados:379 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/output/rados_bench_4MB_seq.log.2
## ./benchmarks/bench-rados:376 - rados_bench() > count_cpus 32-63,96-127
# ./benchmarks/bench-rados:376 - rados_bench() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 128 --show-time
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:51:48,967552360-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:51:48,970421496-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '01d7f8cc-37dc-11ec-b51d-53e6e728d2d3', '--', 'rados', 'bench', '99999999', 'seq', '--pool', 'bench_rados', '--concurrent-ios', '128', '--show-time'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 01d7f8cc-37dc-11ec-b51d-53e6e728d2d3 -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 128 --show-time
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
hints = 1
2021-10-28T10:51:52.254500+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-28T10:51:52.254500+0000     0       0         0         0         0         0           -           0
2021-10-28T10:51:53.254635+0000     1     127       382       255    1019.8      1020    0.376333    0.351738
2021-10-28T10:51:54.254749+0000     2     127       719       592   1183.82      1348     0.37707    0.367111
2021-10-28T10:51:55.254870+0000     3     127      1056       929   1238.49      1348    0.377731    0.371811
2021-10-28T10:51:56.254992+0000     4     127      1395      1268   1267.82      1356    0.377237    0.373266
2021-10-28T10:51:57.255265+0000     5     127      1735      1608   1286.19      1360    0.377165    0.373944
2021-10-28T10:51:58.255420+0000 Total time run:       5.81666
Total reads made:     1988
Read size:            4194304
Object size:          4194304
Bandwidth (MB/sec):   1367.11
Average IOPS:         341
Stddev IOPS:          37.2532
Max IOPS:             340
Min IOPS:             255
Average Latency(s):   0.363738
Max latency(s):       0.385398
Min latency(s):       0.0723214

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:51:58,936651779-07:00] INFO: > Stop system activity collection services[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:51:58,943363396-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:391 - rados_bench() > kill -INT 325400

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:51:58,950045368-07:00] INFO: >> for OSD host[0m
# ./benchmarks/bench-rados:396 - rados_bench() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.2 kill -INT 186718
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:51:58,957949252-07:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.2 kill -INT 186718
[1] 03:51:59 [SUCCESS] ljishen@10.10.2.2

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:51:59,141886582-07:00] INFO: >>> Copy system activity file from OSD host[0m
## ./benchmarks/bench-rados:403 - rados_bench() > basename -- /tmp/bench-rados/sys_activity_4MB_seq.dat.osd_host.2
# ./benchmarks/bench-rados:403 - rados_bench() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.2:/tmp/bench-rados/sys_activity_4MB_seq.dat.osd_host.2 /mnt/sda8/ceph-research/scripts/output/sys_activity_4MB_seq.dat.osd_host.2


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:51:59,298668969-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:52:23,555445412-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 1.99k objects, 7.8 GiB
    usage:   24 GiB used, 76 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:52:23,563575804-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:52:32,979083981-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 1.99k objects, 7.8 GiB
    usage:   24 GiB used, 76 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:52:32,987373061-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:52:42,296282418-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 1.99k objects, 7.8 GiB
    usage:   24 GiB used, 76 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:52:42,304911378-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:52:51,710339179-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 1.99k objects, 7.8 GiB
    usage:   24 GiB used, 76 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:52:51,718532328-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:53:01,200454947-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 1.99k objects, 7.8 GiB
    usage:   24 GiB used, 76 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:53:01,208507722-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:53:01,214853000-07:00] INFO: > The cluster is idle now.[0m


[1;7;39;49m[2021-10-28T03:53:01,217322664-07:00][RUNNING][ROUND 3/6/21] object_size=4MB[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:53:01,220942976-07:00] STAGE: Launch a Ceph cluster (CEPH_HOST: 10.10.2.1, OSD_HOST: 10.10.2.2) ...[0m
# ./benchmarks/bench-rados:183 - launch_ceph_cluster() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.1 bash -euo pipefail
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:53:01,231103603-07:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.1 bash -euo pipefail
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T03:53:01,716545675-07:00] INFO: > Remove existing clusters\x1b[0m\n'
10.10.2.1: b'## bash:17 -  > basename -- /var/lib/ceph/01d7f8cc-37dc-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'# bash:17 -  > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm --host 10.10.2.1 --host 10.10.2.2 rm-cluster --force --fsid 01d7f8cc-37dc-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T03:53:01,727288991-07:00] DEBUG: HOST_PARAMS: ['--host', '10.10.2.1', '--host', '10.10.2.2']\x1b[0m\n"
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T03:53:01,731394379-07:00] DEBUG: CEPHADM_PARAMS: ['rm-cluster', '--force', '--fsid', '01d7f8cc-37dc-11ec-b51d-53e6e728d2d3']\x1b[0m\n"
10.10.2.1: b"# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:63 -  > /tmp/bench-rados/ceph-research/scripts/parallel-ssh --host 10.10.2.1 --host 10.10.2.2 'sudo python3 -u - rm-cluster --force --fsid 01d7f8cc-37dc-11ec-b51d-53e6e728d2d3'\n"
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T03:53:01,740367698-07:00] DEBUG: command from stdin\x1b[0m\n'
10.10.2.1: b"# /tmp/bench-rados/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host 10.10.2.1 --host 10.10.2.2 'sudo python3 -u - rm-cluster --force --fsid 01d7f8cc-37dc-11ec-b51d-53e6e728d2d3'\n"
10.10.2.1: b'[1] 03:53:07 [SUCCESS] 10.10.2.1\n[2] 03:53:14 [SUCCESS] 10.10.2.2\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T03:53:14,183158462-07:00] INFO: > Deploy a new cluster\x1b[0m\n'
10.10.2.1: b'# bash:24 -  > /tmp/bench-rados/ceph-research/scripts/ceph-deploy -m 10.10.2.1 -o node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/10.10.2.2:/dev/nvme0n1\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T03:53:14,194548945-07:00] STAGE: Verify passwordless SSH to hosts, passwordless sudo via ljishen, and the existence of devices on hosts...\x1b[0m\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T03:53:14,199399303-07:00] INFO: > Check the host for the monitor\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:181 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@10.10.2.1 sudo --non-interactive --validate\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T03:53:14,347515796-07:00] INFO: > Check hosts for OSDs\x1b[0m\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T03:53:14,351890069-07:00] INFO: >> Check host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:200 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us sudo --non-interactive --validate\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T03:53:14,503447881-07:00] INFO: >>> Check device '/dev/nvme0n1'\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:215 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us test -b /dev/nvme0n1\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:226 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us lsblk --nodeps --noheadings --output TYPE /dev/nvme0n1\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:227 - verify_parameters() > grep --word-regexp disk\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T03:53:14,787300693-07:00] STAGE: Reset devices on hosts...\x1b[0m\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T03:53:14,792234168-07:00] INFO: > Resetting device '/dev/nvme0n1' on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:288 - reset_host_devices() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us sudo sh\n'
10.10.2.1: b'  Removing ceph--566f76c3--0025--4c35--b0f1--09278b1dad75-osd--block--82a6a2e5--8bb6--4e18--a0c8--77b42d77da21 (253:0)\n'
10.10.2.1: b'  Archiving volume group "ceph-566f76c3-0025-4c35-b0f1-09278b1dad75" metadata (seqno 5).\n'
10.10.2.1: b'  Releasing logical volume "osd-block-82a6a2e5-8bb6-4e18-a0c8-77b42d77da21"\n'
10.10.2.1: b'  Creating volume group backup "/etc/lvm/backup/ceph-566f76c3-0025-4c35-b0f1-09278b1dad75" (seqno 6).\n'
10.10.2.1: b'  Logical volume "osd-block-82a6a2e5-8bb6-4e18-a0c8-77b42d77da21" successfully removed\n'
10.10.2.1: b'  Removing physical volume "/dev/nvme0n1" from volume group "ceph-566f76c3-0025-4c35-b0f1-09278b1dad75"\n'
10.10.2.1: b'  Volume group "ceph-566f76c3-0025-4c35-b0f1-09278b1dad75" successfully removed\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T03:53:15,185929291-07:00] STAGE: Bootstrap a new cluster...\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:317 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.1 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T03:53:15,196095833-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T03:53:15,199706701-07:00] DEBUG: CEPHADM_PARAMS: ['bootstrap', '--config', '/tmp/bench-rados/ceph-research/scripts/ceph.conf', '--mon-ip', '10.10.2.1', '--output-dir', '/tmp/bench-rados/deployment_data_root/etc/ceph', '--ssh-user', 'ljishen', '--skip-dashboard', '--no-minimize-config', '--allow-overwrite', '--allow-fqdn-hostname', '--orphan-initial-daemons', '--skip-monitoring-stack', '--single-host-defaults']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.1 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults\n'
10.10.2.1: b'Verifying podman|docker is present...\nVerifying lvm2 is present...\n'
10.10.2.1: b'Verifying time synchronization is in place...\n'
10.10.2.1: b'Unit ntp.service is enabled and running\n'
10.10.2.1: b'Repeating the final host check...\npodman|docker (/usr/bin/docker) is present\nsystemctl is present\n'
10.10.2.1: b'lvcreate is present\n'
10.10.2.1: b'Unit ntp.service is enabled and running\n'
10.10.2.1: b'Host looks OK\n'
10.10.2.1: b'Cluster fsid: 40af090e-37dd-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Verifying IP 10.10.2.1 port 3300 ...\n'
10.10.2.1: b'Verifying IP 10.10.2.1 port 6789 ...\n'
10.10.2.1: b'Mon IP `10.10.2.1` is in CIDR network `10.10.2.0/24`\n'
10.10.2.1: b'- internal network (--cluster-network) has not been provided, OSD replication will default to the public_network\n'
10.10.2.1: b'Adjusting default settings to suit single-host cluster...\n'
10.10.2.1: b'Pulling container image quay.io/ceph/ceph:v16...\n'
10.10.2.1: b'Ceph version: ceph version 16.2.6 (ee28fb57e47e9f88813e24bbf4c14496ca299d31) pacific (stable)\n'
10.10.2.1: b'Extracting ceph user uid/gid from container image...\n'
10.10.2.1: b'Creating initial keys...\n'
10.10.2.1: b'Creating initial monmap...\n'
10.10.2.1: b'Creating mon...\n'
10.10.2.1: b'Waiting for mon to start...\n'
10.10.2.1: b'Waiting for mon...\n'
10.10.2.1: b'mon is available\n'
10.10.2.1: b'Setting mon public_network to 10.10.2.0/24\n'
10.10.2.1: b'Wrote config to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf\n'
10.10.2.1: b'Wrote keyring to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring\nCreating mgr...\nVerifying port 9283 ...\n'
10.10.2.1: b'Waiting for mgr to start...\nWaiting for mgr...\n'
10.10.2.1: b'mgr not available, waiting (1/15)...\n'
10.10.2.1: b'mgr not available, waiting (2/15)...\n'
10.10.2.1: b'mgr is available\n'
10.10.2.1: b'Enabling cephadm module...\n'
10.10.2.1: b'Waiting for the mgr to restart...\n'
10.10.2.1: b'Waiting for mgr epoch 5...\n'
10.10.2.1: b'mgr epoch 5 is available\n'
10.10.2.1: b'Setting orchestrator backend to cephadm...\n'
10.10.2.1: b'Generating ssh key...\n'
10.10.2.1: b'Wrote public SSH key to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub\n'
10.10.2.1: b'Adding key to ljishen@localhost authorized_keys...\n'
10.10.2.1: b'Adding host node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us...\n'
10.10.2.1: b'Deploying unmanaged mon service...\n'
10.10.2.1: b'Deploying unmanaged mgr service...\n'
10.10.2.1: b'You can access the Ceph CLI with:\n\n\tsudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --fsid 40af090e-37dd-11ec-b51d-53e6e728d2d3 -c /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf -k /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring\n\n'
10.10.2.1: b'Please consider enabling telemetry to help improve Ceph:\n\n\tceph telemetry on\n\nFor more information see:\n\n\thttps://docs.ceph.com/docs/pacific/mgr/telemetry/\n\n'
10.10.2.1: b'Bootstrap complete.\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T03:54:16,254214441-07:00] INFO: Sleep for 20 seconds\x1b[0m\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T03:54:36,260924002-07:00] INFO: > Set the default number of monitor daemons to 1\x1b[0m\n# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:337 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T03:54:36,270977762-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T03:54:36,274387482-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mon', '1']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1\n'
10.10.2.1: b'Inferring fsid 40af090e-37dd-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/40af090e-37dd-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b'Scheduled mon update...\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T03:54:44,830706600-07:00] INFO: > Set the default number of manager daemons to 1\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:342 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T03:54:44,840443825-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T03:54:44,844527231-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mgr', '1']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1\n'
10.10.2.1: b'Inferring fsid 40af090e-37dd-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/40af090e-37dd-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b'Scheduled mgr update...\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T03:54:53,695391810-07:00] STAGE: Add hosts to the cluster...\x1b[0m\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T03:54:53,701315887-07:00] INFO: > Adding host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:361 - add_hosts() > ssh-copy-id -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -f -i /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\n'
10.10.2.1: b'/usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: "/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub"\n'
10.10.2.1: b'\nNumber of key(s) added: 1\n\nNow try logging into the machine, with:   "ssh -o \'GlobalKnownHostsFile=/dev/null\' -o \'LogLevel=ERROR\' -o \'PasswordAuthentication=no\' -o \'StrictHostKeyChecking=no\' -o \'UserKnownHostsFile=/dev/null\' \'ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\'"\nand check to make sure that only the key(s) you wanted were added.\n\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:367 - add_hosts() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us 10.10.2.2\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T03:54:53,912931170-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T03:54:53,916814721-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'host', 'add', 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us', '10.10.2.2']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us 10.10.2.2\n'
10.10.2.1: b'Inferring fsid 40af090e-37dd-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/40af090e-37dd-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b"Added host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us' with addr '10.10.2.2'\n"
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T03:55:03,403909432-07:00] INFO: Sleep for 20 seconds\x1b[0m\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T03:55:23,408741978-07:00] STAGE: Add OSDs to the cluster...\x1b[0m\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T03:55:23,415546831-07:00] INFO: > Adding devices ['/dev/nvme0n1'] on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us' as OSDs\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:388 - add_host_devices_as_osds() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T03:55:23,425340472-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T03:55:23,428753628-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'daemon', 'add', 'osd', 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1\n'
10.10.2.1: b'Inferring fsid 40af090e-37dd-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/40af090e-37dd-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b"Created osd(s) 0 on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'\n"
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T03:55:47,696957027-07:00] INFO: > Sleep for 20 seconds\x1b[0m\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T03:56:07,702660063-07:00] STAGE: Show Cluster Status\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:408 - show_status() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T03:56:07,712495713-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T03:56:07,716227499-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', '--status']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status\n'
10.10.2.1: b'Inferring fsid 40af090e-37dd-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/40af090e-37dd-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b'  cluster:\n    id:     40af090e-37dd-11ec-b51d-53e6e728d2d3\n    health: HEALTH_WARN\n            1 pool(s) have no replicas configured\n \n  services:\n    mon: 1 daemons, quorum node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us (age 2m)\n    mgr: node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us.zvqadh(active, since 2m)\n    osd: 1 osds: 1 up (since 16s), 1 in (since 39s)\n \n  data:\n    pools:   1 pools, 1 pgs\n    objects: 0 objects, 0 B\n    usage:   4.7 MiB used, 100 GiB / 100 GiB avail\n    pgs:     1 active+clean\n \n'
[1] 03:56:16 [SUCCESS] ljishen@10.10.2.1

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:53:01,716545675-07:00] INFO: > Remove existing clusters[0m
## bash:17 -  > basename -- /var/lib/ceph/01d7f8cc-37dc-11ec-b51d-53e6e728d2d3
# bash:17 -  > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm --host 10.10.2.1 --host 10.10.2.2 rm-cluster --force --fsid 01d7f8cc-37dc-11ec-b51d-53e6e728d2d3
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:53:01,727288991-07:00] DEBUG: HOST_PARAMS: ['--host', '10.10.2.1', '--host', '10.10.2.2'][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:53:01,731394379-07:00] DEBUG: CEPHADM_PARAMS: ['rm-cluster', '--force', '--fsid', '01d7f8cc-37dc-11ec-b51d-53e6e728d2d3'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:63 -  > /tmp/bench-rados/ceph-research/scripts/parallel-ssh --host 10.10.2.1 --host 10.10.2.2 'sudo python3 -u - rm-cluster --force --fsid 01d7f8cc-37dc-11ec-b51d-53e6e728d2d3'
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:53:01,740367698-07:00] DEBUG: command from stdin[0m
# /tmp/bench-rados/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host 10.10.2.1 --host 10.10.2.2 'sudo python3 -u - rm-cluster --force --fsid 01d7f8cc-37dc-11ec-b51d-53e6e728d2d3'
[1] 03:53:07 [SUCCESS] 10.10.2.1
[2] 03:53:14 [SUCCESS] 10.10.2.2

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:53:14,183158462-07:00] INFO: > Deploy a new cluster[0m
# bash:24 -  > /tmp/bench-rados/ceph-research/scripts/ceph-deploy -m 10.10.2.1 -o node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/10.10.2.2:/dev/nvme0n1


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:53:14,194548945-07:00] STAGE: Verify passwordless SSH to hosts, passwordless sudo via ljishen, and the existence of devices on hosts...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:53:14,199399303-07:00] INFO: > Check the host for the monitor[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:181 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@10.10.2.1 sudo --non-interactive --validate

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:53:14,347515796-07:00] INFO: > Check hosts for OSDs[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:53:14,351890069-07:00] INFO: >> Check host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:200 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us sudo --non-interactive --validate

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:53:14,503447881-07:00] INFO: >>> Check device '/dev/nvme0n1'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:215 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us test -b /dev/nvme0n1
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:226 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us lsblk --nodeps --noheadings --output TYPE /dev/nvme0n1
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:227 - verify_parameters() > grep --word-regexp disk


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:53:14,787300693-07:00] STAGE: Reset devices on hosts...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:53:14,792234168-07:00] INFO: > Resetting device '/dev/nvme0n1' on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:288 - reset_host_devices() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us sudo sh
  Removing ceph--566f76c3--0025--4c35--b0f1--09278b1dad75-osd--block--82a6a2e5--8bb6--4e18--a0c8--77b42d77da21 (253:0)
  Archiving volume group "ceph-566f76c3-0025-4c35-b0f1-09278b1dad75" metadata (seqno 5).
  Releasing logical volume "osd-block-82a6a2e5-8bb6-4e18-a0c8-77b42d77da21"
  Creating volume group backup "/etc/lvm/backup/ceph-566f76c3-0025-4c35-b0f1-09278b1dad75" (seqno 6).
  Logical volume "osd-block-82a6a2e5-8bb6-4e18-a0c8-77b42d77da21" successfully removed
  Removing physical volume "/dev/nvme0n1" from volume group "ceph-566f76c3-0025-4c35-b0f1-09278b1dad75"
  Volume group "ceph-566f76c3-0025-4c35-b0f1-09278b1dad75" successfully removed


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:53:15,185929291-07:00] STAGE: Bootstrap a new cluster...[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:317 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.1 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:53:15,196095833-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:53:15,199706701-07:00] DEBUG: CEPHADM_PARAMS: ['bootstrap', '--config', '/tmp/bench-rados/ceph-research/scripts/ceph.conf', '--mon-ip', '10.10.2.1', '--output-dir', '/tmp/bench-rados/deployment_data_root/etc/ceph', '--ssh-user', 'ljishen', '--skip-dashboard', '--no-minimize-config', '--allow-overwrite', '--allow-fqdn-hostname', '--orphan-initial-daemons', '--skip-monitoring-stack', '--single-host-defaults'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.1 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults
Verifying podman|docker is present...
Verifying lvm2 is present...
Verifying time synchronization is in place...
Unit ntp.service is enabled and running
Repeating the final host check...
podman|docker (/usr/bin/docker) is present
systemctl is present
lvcreate is present
Unit ntp.service is enabled and running
Host looks OK
Cluster fsid: 40af090e-37dd-11ec-b51d-53e6e728d2d3
Verifying IP 10.10.2.1 port 3300 ...
Verifying IP 10.10.2.1 port 6789 ...
Mon IP `10.10.2.1` is in CIDR network `10.10.2.0/24`
- internal network (--cluster-network) has not been provided, OSD replication will default to the public_network
Adjusting default settings to suit single-host cluster...
Pulling container image quay.io/ceph/ceph:v16...
Ceph version: ceph version 16.2.6 (ee28fb57e47e9f88813e24bbf4c14496ca299d31) pacific (stable)
Extracting ceph user uid/gid from container image...
Creating initial keys...
Creating initial monmap...
Creating mon...
Waiting for mon to start...
Waiting for mon...
mon is available
Setting mon public_network to 10.10.2.0/24
Wrote config to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf
Wrote keyring to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring
Creating mgr...
Verifying port 9283 ...
Waiting for mgr to start...
Waiting for mgr...
mgr not available, waiting (1/15)...
mgr not available, waiting (2/15)...
mgr is available
Enabling cephadm module...
Waiting for the mgr to restart...
Waiting for mgr epoch 5...
mgr epoch 5 is available
Setting orchestrator backend to cephadm...
Generating ssh key...
Wrote public SSH key to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub
Adding key to ljishen@localhost authorized_keys...
Adding host node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us...
Deploying unmanaged mon service...
Deploying unmanaged mgr service...
You can access the Ceph CLI with:

	sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --fsid 40af090e-37dd-11ec-b51d-53e6e728d2d3 -c /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf -k /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring

Please consider enabling telemetry to help improve Ceph:

	ceph telemetry on

For more information see:

	https://docs.ceph.com/docs/pacific/mgr/telemetry/

Bootstrap complete.

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:54:16,254214441-07:00] INFO: Sleep for 20 seconds[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:54:36,260924002-07:00] INFO: > Set the default number of monitor daemons to 1[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:337 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:54:36,270977762-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:54:36,274387482-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mon', '1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1
Inferring fsid 40af090e-37dd-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/40af090e-37dd-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Scheduled mon update...

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:54:44,830706600-07:00] INFO: > Set the default number of manager daemons to 1[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:342 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:54:44,840443825-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:54:44,844527231-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mgr', '1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1
Inferring fsid 40af090e-37dd-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/40af090e-37dd-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Scheduled mgr update...


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:54:53,695391810-07:00] STAGE: Add hosts to the cluster...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:54:53,701315887-07:00] INFO: > Adding host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:361 - add_hosts() > ssh-copy-id -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -f -i /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us
/usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: "/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub"

Number of key(s) added: 1

Now try logging into the machine, with:   "ssh -o 'GlobalKnownHostsFile=/dev/null' -o 'LogLevel=ERROR' -o 'PasswordAuthentication=no' -o 'StrictHostKeyChecking=no' -o 'UserKnownHostsFile=/dev/null' 'ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'"
and check to make sure that only the key(s) you wanted were added.

# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:367 - add_hosts() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us 10.10.2.2
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:54:53,912931170-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:54:53,916814721-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'host', 'add', 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us', '10.10.2.2'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us 10.10.2.2
Inferring fsid 40af090e-37dd-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/40af090e-37dd-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Added host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us' with addr '10.10.2.2'

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:55:03,403909432-07:00] INFO: Sleep for 20 seconds[0m


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:55:23,408741978-07:00] STAGE: Add OSDs to the cluster...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:55:23,415546831-07:00] INFO: > Adding devices ['/dev/nvme0n1'] on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us' as OSDs[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:388 - add_host_devices_as_osds() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:55:23,425340472-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:55:23,428753628-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'daemon', 'add', 'osd', 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1
Inferring fsid 40af090e-37dd-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/40af090e-37dd-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Created osd(s) 0 on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:55:47,696957027-07:00] INFO: > Sleep for 20 seconds[0m


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:56:07,702660063-07:00] STAGE: Show Cluster Status[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:408 - show_status() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:56:07,712495713-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:56:07,716227499-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', '--status'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status
Inferring fsid 40af090e-37dd-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/40af090e-37dd-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
  cluster:
    id:     40af090e-37dd-11ec-b51d-53e6e728d2d3
    health: HEALTH_WARN
            1 pool(s) have no replicas configured
 
  services:
    mon: 1 daemons, quorum node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us (age 2m)
    mgr: node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us.zvqadh(active, since 2m)
    osd: 1 osds: 1 up (since 16s), 1 in (since 39s)
 
  data:
    pools:   1 pools, 1 pgs
    objects: 0 objects, 0 B
    usage:   4.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     1 active+clean
 

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:56:16,231779524-07:00] INFO: > Copy configuration files to local[0m
# ./benchmarks/bench-rados:220 - launch_ceph_cluster() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.1 sudo chown --changes --recursive ljishen /tmp/bench-rados/deployment_data_root/etc/ceph
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:56:16,239850624-07:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.1 sudo chown --changes --recursive ljishen /tmp/bench-rados/deployment_data_root/etc/ceph
[1] 03:56:16 [SUCCESS] ljishen@10.10.2.1
# ./benchmarks/bench-rados:223 - launch_ceph_cluster() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.1:/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf
# ./benchmarks/bench-rados:226 - launch_ceph_cluster() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.1:/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:56:16,716985421-07:00] STAGE: Update local ceph.conf as a client...[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:56:16,720855053-07:00] STAGE: Configure Ceph pool...[0m
# ./benchmarks/bench-rados:257 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_max_backfills 32
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:56:16,742677748-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:56:16,745454901-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '40af090e-37dd-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'config', 'set', 'osd', 'osd_max_backfills', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 40af090e-37dd-11ec-b51d-53e6e728d2d3 -- ceph config set osd osd_max_backfills 32
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:258 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_active 32
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:56:21,234972040-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:56:21,237895038-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '40af090e-37dd-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_active', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 40af090e-37dd-11ec-b51d-53e6e728d2d3 -- ceph config set osd osd_recovery_max_active 32
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:259 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_single_start 8
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:56:25,660345260-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:56:25,663659927-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '40af090e-37dd-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_single_start', '8'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 40af090e-37dd-11ec-b51d-53e6e728d2d3 -- ceph config set osd osd_recovery_max_single_start 8
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:260 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_op_priority 63
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:56:30,118186236-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:56:30,121105497-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '40af090e-37dd-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_op_priority', '63'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 40af090e-37dd-11ec-b51d-53e6e728d2d3 -- ceph config set osd osd_recovery_op_priority 63
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:262 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config show-with-defaults osd.0
# ./benchmarks/bench-rados:263 - configure_ceph_pool() > grep 'osd_max_backfills\|osd_recovery'
# ./benchmarks/bench-rados:263 - configure_ceph_pool() > column -t -s ' '
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
osd_max_backfills                        32         mon
osd_recovery_cost                        20971520   default
osd_recovery_delay_start                 0.000000   default
osd_recovery_max_active                  32         mon
osd_recovery_max_active_hdd              3          default
osd_recovery_max_active_ssd              10         default
osd_recovery_max_chunk                   8388608    default
osd_recovery_max_omap_entries_per_chunk  8096       default
osd_recovery_max_single_start            8          mon
osd_recovery_op_priority                 63         mon
osd_recovery_op_warn_multiple            16         default
osd_recovery_priority                    5          default
osd_recovery_retry_interval              30.000000  default
osd_recovery_sleep                       0.000000   default
osd_recovery_sleep_hdd                   0.100000   default
osd_recovery_sleep_hybrid                0.025000   default
osd_recovery_sleep_ssd                   0.000000   default
# ./benchmarks/bench-rados:265 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool create bench_rados 128 128 replicated --size 1 --pg-num-min 128
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:56:38,878848319-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:56:38,882138138-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '40af090e-37dd-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'create', 'bench_rados', '128', '128', 'replicated', '--size', '1', '--pg-num-min', '128'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 40af090e-37dd-11ec-b51d-53e6e728d2d3 -- ceph osd pool create bench_rados 128 128 replicated --size 1 --pg-num-min 128
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
pool 'bench_rados' created
# ./benchmarks/bench-rados:267 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados min_size 1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:56:44,001578358-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:56:44,004597457-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '40af090e-37dd-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'min_size', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 40af090e-37dd-11ec-b51d-53e6e728d2d3 -- ceph osd pool set bench_rados min_size 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 min_size to 1
# ./benchmarks/bench-rados:268 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados pg_autoscale_mode off
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:56:49,169215263-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:56:49,172592246-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '40af090e-37dd-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'pg_autoscale_mode', 'off'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 40af090e-37dd-11ec-b51d-53e6e728d2d3 -- ceph osd pool set bench_rados pg_autoscale_mode off
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 pg_autoscale_mode to off
# ./benchmarks/bench-rados:269 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool application enable bench_rados benchmark
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:56:54,352494484-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:56:54,355530035-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '40af090e-37dd-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'application', 'enable', 'bench_rados', 'benchmark'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 40af090e-37dd-11ec-b51d-53e6e728d2d3 -- ceph osd pool application enable bench_rados benchmark
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
enabled application 'benchmark' on pool 'bench_rados'
# ./benchmarks/bench-rados:270 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados noscrub 1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:56:59,677171165-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:56:59,680181338-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '40af090e-37dd-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'noscrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 40af090e-37dd-11ec-b51d-53e6e728d2d3 -- ceph osd pool set bench_rados noscrub 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 noscrub to 1
# ./benchmarks/bench-rados:271 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados nodeep-scrub 1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:57:05,103104361-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:57:05,106128339-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '40af090e-37dd-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'nodeep-scrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 40af090e-37dd-11ec-b51d-53e6e728d2d3 -- ceph osd pool set bench_rados nodeep-scrub 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 nodeep-scrub to 1
# ./benchmarks/bench-rados:272 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool ls detail
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:57:10,676606436-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:57:10,679579438-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '40af090e-37dd-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'ls', 'detail'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 40af090e-37dd-11ec-b51d-53e6e728d2d3 -- ceph osd pool ls detail
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
pool 1 'device_health_metrics' replicated size 1 min_size 1 crush_rule 0 object_hash rjenkins pg_num 64 pgp_num 64 autoscale_mode on last_change 19 lfor 0/0/16 flags hashpspool stripe_width 0 pg_num_min 1 application mgr_devicehealth
pool 2 'bench_rados' replicated size 1 min_size 1 crush_rule 0 object_hash rjenkins pg_num 128 pgp_num 128 autoscale_mode off last_change 21 flags hashpspool,noscrub,nodeep-scrub stripe_width 0 pg_num_min 128 application benchmark

# ./benchmarks/bench-rados:273 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd df tree
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:57:14,907223461-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:57:14,910494536-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '40af090e-37dd-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'df', 'tree'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 40af090e-37dd-11ec-b51d-53e6e728d2d3 -- ceph osd df tree
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
ID  CLASS  WEIGHT   REWEIGHT  SIZE     RAW USE  DATA     OMAP  META     AVAIL    %USE  VAR   PGS  STATUS  TYPE NAME      
-1         0.09769         -  100 GiB  5.7 MiB  176 KiB   0 B  5.6 MiB  100 GiB  0.01  1.00    -          root default   
-3         0.09769         -  100 GiB  5.7 MiB  176 KiB   0 B  5.6 MiB  100 GiB  0.01  1.00    -              host node-1
 0    ssd  0.09769   1.00000  100 GiB  5.7 MiB  176 KiB   0 B  5.6 MiB  100 GiB  0.01  1.00  192      up          osd.0  
                       TOTAL  100 GiB  5.7 MiB  176 KiB   0 B  5.6 MiB  100 GiB  0.01                                    
MIN/MAX VAR: 1.00/1.00  STDDEV: 0


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:57:19,169217396-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:57:43,584147775-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:57:52,955657391-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:58:02,329165823-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:58:02,337440015-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:58:11,811222891-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:58:11,819194914-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:58:21,051119824-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:58:21,059317742-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:58:30,399852954-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:58:30,408470183-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:58:39,815344028-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:58:39,823801165-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:58:39,830218639-07:00] INFO: > The cluster is idle now.[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:58:39,834177539-07:00] STAGE: Start benchmarking of mode write ...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:58:39,841464972-07:00] INFO: > Start system activity collection services[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:58:39,847379738-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:341 - rados_bench() > rm --force /mnt/sda8/ceph-research/scripts/output/sys_activity_4MB_write.dat.rados_bench_host.3
# ./benchmarks/bench-rados:342 - rados_bench() > sar_pid_rados_bench_host=330950
# ./benchmarks/bench-rados:342 - rados_bench() > S_TIME_FORMAT=ISO
# ./benchmarks/bench-rados:342 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/output/sys_activity_4MB_write.dat.rados_bench_host.3 2

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:58:39,854742253-07:00] INFO: >> for OSD host[0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:58:39,864220033-07:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.2 bash
10.10.2.2: b'190348\n'
[1] 03:58:40 [SUCCESS] ljishen@10.10.2.2
190348

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:58:40,056280956-07:00] INFO: > Run rados bench[0m
# ./benchmarks/bench-rados:370 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/output/rados_bench_4MB_write.log.3
## ./benchmarks/bench-rados:361 - rados_bench() > count_cpus 32-63,96-127
# ./benchmarks/bench-rados:361 - rados_bench() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 60 write --pool bench_rados -b 4194304 -O 4194304 --concurrent-ios 128 --show-time --write-object --write-omap --write-xattr --no-cleanup
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:58:40,077329522-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:58:40,080305620-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '40af090e-37dd-11ec-b51d-53e6e728d2d3', '--', 'rados', 'bench', '60', 'write', '--pool', 'bench_rados', '-b', '4194304', '-O', '4194304', '--concurrent-ios', '128', '--show-time', '--write-object', '--write-omap', '--write-xattr', '--no-cleanup'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 40af090e-37dd-11ec-b51d-53e6e728d2d3 -- rados bench 60 write --pool bench_rados -b 4194304 -O 4194304 --concurrent-ios 128 --show-time --write-object --write-omap --write-xattr --no-cleanup
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
hints = 1
2021-10-28T10:58:43.428225+0000 Maintaining 128 concurrent writes of 4194304 bytes to objects of size 4194304 for up to 60 seconds or 0 objects
2021-10-28T10:58:43.428239+0000 Object prefix: benchmark_data_node-2.bluefield2.ucsc-cmps10_7
2021-10-28T10:58:43.554175+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-28T10:58:43.554175+0000     0       0         0         0         0         0           -           0
2021-10-28T10:58:44.554324+0000     1      84        84         0         0         0           -           0
2021-10-28T10:58:45.554408+0000     2     127       144        17   33.9969        34     1.77821     1.66426
2021-10-28T10:58:46.554527+0000     3     127       190        63   83.9916       184     2.24413     2.04457
2021-10-28T10:58:47.554640+0000     4     127       265       138   137.986       300     2.18082     2.16151
2021-10-28T10:58:48.554714+0000     5     127       281       154   123.188        64      2.4975     2.19068
2021-10-28T10:58:49.554793+0000     6     127       309       182   121.322       112     3.06832     2.30601
2021-10-28T10:58:50.554910+0000     7     127       346       219   125.131       148     3.65061     2.49385
2021-10-28T10:58:51.555034+0000     8     127       400       273   136.486       216     3.58243     2.74541
2021-10-28T10:58:52.555160+0000     9     127       442       315   139.985       168     2.87214     2.79978
2021-10-28T10:58:53.555236+0000    10     127       521       394   157.584       316     2.23926     2.71449
2021-10-28T10:58:54.555354+0000    11     127       536       409   148.712        60     2.38614     2.70204
2021-10-28T10:58:55.555472+0000    12     127       563       436   145.318       108     3.09085     2.71736
2021-10-28T10:58:56.555585+0000    13     127       601       474   145.831       152     3.66569     2.77468
2021-10-28T10:58:57.555666+0000    14     127       635       508   145.128       136     4.17392     2.85418
2021-10-28T10:58:58.555740+0000    15     127       657       530   141.319        88     4.61481     2.92007
2021-10-28T10:58:59.555859+0000    16     127       695       568   141.986       152     3.89117     3.00684
2021-10-28T10:59:00.555974+0000    17     127       775       648   152.455       320     2.64996     3.03927
2021-10-28T10:59:01.556057+0000    18     127       792       665   147.763        68       2.424     3.02443
2021-10-28T10:59:02.556172+0000    19     127       818       691   145.459       104     3.05512     3.01921
2021-10-28T10:59:03.556249+0000 min lat: 1.54775 max lat: 4.82602 avg lat: 3.03682
2021-10-28T10:59:03.556249+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-28T10:59:03.556249+0000    20     127       855       728   145.585       148     3.58185     3.03682
2021-10-28T10:59:04.556368+0000    21     127       889       762   145.128       136     4.08987     3.07474
2021-10-28T10:59:05.556450+0000    22     127       913       786   142.895        96     4.38434     3.11343
2021-10-28T10:59:06.556562+0000    23     127       925       798   138.769        48     4.34878     3.13505
2021-10-28T10:59:07.556673+0000    24     127       960       833   138.819       140     4.57413     3.19331
2021-10-28T10:59:08.556747+0000    25     127       999       872   139.506       156     4.53521     3.25343
2021-10-28T10:59:09.556828+0000    26     127      1030       903   138.909       124     4.59992     3.29776
2021-10-28T10:59:10.556941+0000    27     127      1043       916    135.69        52     4.66254     3.31896
2021-10-28T10:59:11.557054+0000    28     127      1069       942   134.558       104     4.46508     3.35646
2021-10-28T10:59:12.557169+0000    29     127      1107       980   135.159       152     4.57747     3.40332
2021-10-28T10:59:13.557244+0000    30     127      1144      1017   135.586       148     4.56685     3.44633
2021-10-28T10:59:14.557362+0000    31     127      1163      1036   133.664        76     4.58241     3.46677
2021-10-28T10:59:15.557477+0000    32     127      1176      1049   131.112        52      4.3631     3.48083
2021-10-28T10:59:16.557589+0000    33     127      1214      1087   131.744       152     4.48724     3.51807
2021-10-28T10:59:17.557671+0000    34     127      1251      1124   132.222       148      4.5044     3.55115
2021-10-28T10:59:18.557743+0000    35     127      1284      1157   132.215       132     4.29516     3.57787
2021-10-28T10:59:19.557859+0000    36     127      1294      1167   129.654        40     4.31616     3.58543
2021-10-28T10:59:20.557977+0000    37     127      1324      1197   129.392       120       4.359     3.60839
2021-10-28T10:59:21.558058+0000    38     127      1363      1236   130.092       156     4.41513     3.63553
2021-10-28T10:59:22.558170+0000    39     127      1399      1272   130.448       144     4.50362     3.66005
2021-10-28T10:59:23.558240+0000 min lat: 1.54775 max lat: 4.97566 avg lat: 3.6737
2021-10-28T10:59:23.558240+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-28T10:59:23.558240+0000    40     127      1420      1293   129.287        84     4.40424      3.6737
2021-10-28T10:59:24.558365+0000    41     127      1436      1309   127.694        64     4.45877     3.68239
2021-10-28T10:59:25.558445+0000    42     127      1468      1341   127.701       128      4.4502      3.7018
2021-10-28T10:59:26.558557+0000    43     127      1507      1380   128.359       156     4.42584     3.72499
2021-10-28T10:59:27.558667+0000    44     127      1536      1409   128.078       116     4.36445     3.74135
2021-10-28T10:59:28.558738+0000    45     127      1551      1424   126.565        60     4.68454     3.75085
2021-10-28T10:59:29.558820+0000    46     127      1578      1451   126.161       108      4.6066     3.76732
2021-10-28T10:59:30.558938+0000    47     127      1614      1487   126.541       144     4.51952     3.78719
2021-10-28T10:59:31.559052+0000    48     127      1650      1523   126.904       144     4.60987     3.80726
2021-10-28T10:59:32.559164+0000    49     127      1674      1547   126.273        96     4.61934     3.82045
2021-10-28T10:59:33.559236+0000    50     127      1686      1559   124.708        48     4.43456     3.82613
2021-10-28T10:59:34.559359+0000    51     127      1721      1594   125.007       140     4.50471     3.84204
2021-10-28T10:59:35.559475+0000    52     127      1761      1634    125.68       160     4.46131     3.85795
2021-10-28T10:59:36.559586+0000    53     127      1795      1668   125.874       136     4.15977     3.86958
2021-10-28T10:59:37.559663+0000    54     127      1808      1681   124.506        52     4.16118     3.87351
2021-10-28T10:59:38.559735+0000    55     127      1832      1705   123.988        96     4.42467     3.88112
2021-10-28T10:59:39.559844+0000    56     127      1870      1743   124.488       152     4.47343     3.89438
2021-10-28T10:59:40.559956+0000    57     127      1909      1782    125.04       156     4.38537     3.90727
2021-10-28T10:59:41.560035+0000    58     127      1929      1802   124.263        80     4.26714     3.91356
2021-10-28T10:59:42.560149+0000    59     127      1945      1818   123.242        64     4.23002     3.91755
2021-10-28T10:59:43.560240+0000 min lat: 1.54775 max lat: 5.00334 avg lat: 3.9277
2021-10-28T10:59:43.560240+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-28T10:59:43.560240+0000    60     127      1983      1856   123.721       152     4.37562      3.9277
2021-10-28T10:59:44.560399+0000 Total time run:         60.2159
Total writes made:      1984
Write size:             4194304
Object size:            4194304
Bandwidth (MB/sec):     131.792
Stddev Bandwidth:       61.8399
Max bandwidth (MB/sec): 320
Min bandwidth (MB/sec): 0
Average IOPS:           32
Stddev IOPS:            15.4723
Max IOPS:               80
Min IOPS:               0
Average Latency(s):     3.83051
Stddev Latency(s):      0.969248
Max latency(s):         5.00334
Min latency(s):         0.168772

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:59:45,262212880-07:00] INFO: > Stop system activity collection services[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:59:45,268722517-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:391 - rados_bench() > kill -INT 330950

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:59:45,274639497-07:00] INFO: >> for OSD host[0m
# ./benchmarks/bench-rados:396 - rados_bench() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.2 kill -INT 190348
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:59:45,282946391-07:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.2 kill -INT 190348
[1] 03:59:45 [SUCCESS] ljishen@10.10.2.2

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:59:45,466099347-07:00] INFO: >>> Copy system activity file from OSD host[0m
## ./benchmarks/bench-rados:403 - rados_bench() > basename -- /tmp/bench-rados/sys_activity_4MB_write.dat.osd_host.3
# ./benchmarks/bench-rados:403 - rados_bench() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.2:/tmp/bench-rados/sys_activity_4MB_write.dat.osd_host.3 /mnt/sda8/ceph-research/scripts/output/sys_activity_4MB_write.dat.osd_host.3


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T03:59:45,640054010-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T04:00:09,891363520-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 1.99k objects, 7.8 GiB
    usage:   24 GiB used, 76 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T04:00:09,899905577-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T04:00:19,191067997-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 1.99k objects, 7.8 GiB
    usage:   24 GiB used, 76 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T04:00:19,199390831-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T04:00:28,589476487-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 1.99k objects, 7.8 GiB
    usage:   24 GiB used, 76 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T04:00:28,598088406-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T04:00:37,862377060-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 1.99k objects, 7.8 GiB
    usage:   24 GiB used, 76 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T04:00:37,870539462-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T04:00:47,237477365-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 1.99k objects, 7.8 GiB
    usage:   24 GiB used, 76 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T04:00:47,245806531-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T04:00:47,252450131-07:00] INFO: > The cluster is idle now.[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T04:00:47,256566077-07:00] STAGE: Start benchmarking of mode seq ...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T04:00:47,264221373-07:00] INFO: > Start system activity collection services[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T04:00:47,270260654-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:341 - rados_bench() > rm --force /mnt/sda8/ceph-research/scripts/output/sys_activity_4MB_seq.dat.rados_bench_host.3
# ./benchmarks/bench-rados:342 - rados_bench() > sar_pid_rados_bench_host=332310
# ./benchmarks/bench-rados:342 - rados_bench() > S_TIME_FORMAT=ISO
# ./benchmarks/bench-rados:342 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/output/sys_activity_4MB_seq.dat.rados_bench_host.3 2

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T04:00:47,277530814-07:00] INFO: >> for OSD host[0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T04:00:47,286885303-07:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.2 bash
10.10.2.2: b'190452\n'
[1] 04:00:47 [SUCCESS] ljishen@10.10.2.2
190452

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T04:00:47,479546747-07:00] INFO: > Run rados bench[0m
# ./benchmarks/bench-rados:379 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/output/rados_bench_4MB_seq.log.3
## ./benchmarks/bench-rados:376 - rados_bench() > count_cpus 32-63,96-127
# ./benchmarks/bench-rados:376 - rados_bench() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 128 --show-time
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T04:00:47,499901545-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T04:00:47,502834753-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '40af090e-37dd-11ec-b51d-53e6e728d2d3', '--', 'rados', 'bench', '99999999', 'seq', '--pool', 'bench_rados', '--concurrent-ios', '128', '--show-time'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 40af090e-37dd-11ec-b51d-53e6e728d2d3 -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 128 --show-time
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
hints = 1
2021-10-28T11:00:50.723759+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-28T11:00:50.723759+0000     0       0         0         0         0         0           -           0
2021-10-28T11:00:51.723893+0000     1     127       386       259   1035.78      1036    0.371182    0.346981
2021-10-28T11:00:52.724012+0000     2     127       728       601    1201.8      1368    0.372203    0.361792
2021-10-28T11:00:53.724112+0000     3     127      1072       945   1259.82      1376    0.366128    0.365969
2021-10-28T11:00:54.724192+0000     4     127      1420      1293   1292.84      1392    0.368116    0.366474
2021-10-28T11:00:55.724285+0000     5     127      1768      1641   1312.64      1392    0.365855    0.366606
2021-10-28T11:00:56.724445+0000 Total time run:       5.69169
Total reads made:     1984
Read size:            4194304
Object size:          4194304
Bandwidth (MB/sec):   1394.31
Average IOPS:         348
Stddev IOPS:          38.7711
Max IOPS:             348
Min IOPS:             259
Average Latency(s):   0.356399
Max latency(s):       0.383281
Min latency(s):       0.0716222

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T04:00:57,468171950-07:00] INFO: > Stop system activity collection services[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T04:00:57,474587940-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:391 - rados_bench() > kill -INT 332310

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T04:00:57,480895296-07:00] INFO: >> for OSD host[0m
# ./benchmarks/bench-rados:396 - rados_bench() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.2 kill -INT 190452
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T04:00:57,489271701-07:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.2 kill -INT 190452
[1] 04:00:57 [SUCCESS] ljishen@10.10.2.2

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T04:00:57,674283029-07:00] INFO: >>> Copy system activity file from OSD host[0m
## ./benchmarks/bench-rados:403 - rados_bench() > basename -- /tmp/bench-rados/sys_activity_4MB_seq.dat.osd_host.3
# ./benchmarks/bench-rados:403 - rados_bench() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.2:/tmp/bench-rados/sys_activity_4MB_seq.dat.osd_host.3 /mnt/sda8/ceph-research/scripts/output/sys_activity_4MB_seq.dat.osd_host.3


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T04:00:57,830741611-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T04:01:22,159970748-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 1.99k objects, 7.8 GiB
    usage:   24 GiB used, 76 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T04:01:22,167946339-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T04:01:31,393249631-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 1.99k objects, 7.8 GiB
    usage:   24 GiB used, 76 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T04:01:31,401489378-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T04:01:40,817621822-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 1.99k objects, 7.8 GiB
    usage:   24 GiB used, 76 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T04:01:40,826470476-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T04:01:50,347635639-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 1.99k objects, 7.8 GiB
    usage:   24 GiB used, 76 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T04:01:50,356128173-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T04:01:59,764549443-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 1.99k objects, 7.8 GiB
    usage:   24 GiB used, 76 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T04:01:59,773051746-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T04:01:59,780047118-07:00] INFO: > The cluster is idle now.[0m


[1;7;39;49m[2021-10-28T04:01:59,784312977-07:00][RUNNING][ROUND 1/7/21] object_size=16MB[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T04:01:59,788252581-07:00] STAGE: Launch a Ceph cluster (CEPH_HOST: 10.10.2.1, OSD_HOST: 10.10.2.2) ...[0m
# ./benchmarks/bench-rados:183 - launch_ceph_cluster() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.1 bash -euo pipefail
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T04:01:59,798183285-07:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.1 bash -euo pipefail
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T04:02:00,271031562-07:00] INFO: > Remove existing clusters\x1b[0m\n'
10.10.2.1: b'## bash:17 -  > basename -- /var/lib/ceph/40af090e-37dd-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'# bash:17 -  > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm --host 10.10.2.1 --host 10.10.2.2 rm-cluster --force --fsid 40af090e-37dd-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T04:02:00,281618674-07:00] DEBUG: HOST_PARAMS: ['--host', '10.10.2.1', '--host', '10.10.2.2']\x1b[0m\n"
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T04:02:00,285161394-07:00] DEBUG: CEPHADM_PARAMS: ['rm-cluster', '--force', '--fsid', '40af090e-37dd-11ec-b51d-53e6e728d2d3']\x1b[0m\n"
10.10.2.1: b"# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:63 -  > /tmp/bench-rados/ceph-research/scripts/parallel-ssh --host 10.10.2.1 --host 10.10.2.2 'sudo python3 -u - rm-cluster --force --fsid 40af090e-37dd-11ec-b51d-53e6e728d2d3'\n"
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T04:02:00,293487937-07:00] DEBUG: command from stdin\x1b[0m\n'
10.10.2.1: b"# /tmp/bench-rados/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host 10.10.2.1 --host 10.10.2.2 'sudo python3 -u - rm-cluster --force --fsid 40af090e-37dd-11ec-b51d-53e6e728d2d3'\n"
10.10.2.1: b'[1] 04:02:06 [SUCCESS] 10.10.2.1\n[2] 04:02:12 [SUCCESS] 10.10.2.2\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T04:02:12,814715470-07:00] INFO: > Deploy a new cluster\x1b[0m\n'
10.10.2.1: b'# bash:24 -  > /tmp/bench-rados/ceph-research/scripts/ceph-deploy -m 10.10.2.1 -o node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/10.10.2.2:/dev/nvme0n1\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T04:02:12,826293987-07:00] STAGE: Verify passwordless SSH to hosts, passwordless sudo via ljishen, and the existence of devices on hosts...\x1b[0m\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T04:02:12,831004672-07:00] INFO: > Check the host for the monitor\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:181 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@10.10.2.1 sudo --non-interactive --validate\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T04:02:12,983256572-07:00] INFO: > Check hosts for OSDs\x1b[0m\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T04:02:12,988218761-07:00] INFO: >> Check host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:200 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us sudo --non-interactive --validate\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T04:02:13,142850575-07:00] INFO: >>> Check device '/dev/nvme0n1'\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:215 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us test -b /dev/nvme0n1\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:226 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us lsblk --nodeps --noheadings --output TYPE /dev/nvme0n1\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:227 - verify_parameters() > grep --word-regexp disk\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T04:02:13,431093336-07:00] STAGE: Reset devices on hosts...\x1b[0m\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T04:02:13,436039414-07:00] INFO: > Resetting device '/dev/nvme0n1' on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:288 - reset_host_devices() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us sudo sh\n'
10.10.2.1: b'  Removing ceph--784b12f6--5594--4908--8fd5--679701321cd1-osd--block--e28b235d--c460--46d8--a581--b2f039603730 (253:0)\n'
10.10.2.1: b'  Archiving volume group "ceph-784b12f6-5594-4908-8fd5-679701321cd1" metadata (seqno 5).\n'
10.10.2.1: b'  Releasing logical volume "osd-block-e28b235d-c460-46d8-a581-b2f039603730"\n'
10.10.2.1: b'  Creating volume group backup "/etc/lvm/backup/ceph-784b12f6-5594-4908-8fd5-679701321cd1" (seqno 6).\n'
10.10.2.1: b'  Logical volume "osd-block-e28b235d-c460-46d8-a581-b2f039603730" successfully removed\n'
10.10.2.1: b'  Removing physical volume "/dev/nvme0n1" from volume group "ceph-784b12f6-5594-4908-8fd5-679701321cd1"\n'
10.10.2.1: b'  Volume group "ceph-784b12f6-5594-4908-8fd5-679701321cd1" successfully removed\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T04:02:13,797763263-07:00] STAGE: Bootstrap a new cluster...\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:317 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.1 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T04:02:13,807778420-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T04:02:13,811702297-07:00] DEBUG: CEPHADM_PARAMS: ['bootstrap', '--config', '/tmp/bench-rados/ceph-research/scripts/ceph.conf', '--mon-ip', '10.10.2.1', '--output-dir', '/tmp/bench-rados/deployment_data_root/etc/ceph', '--ssh-user', 'ljishen', '--skip-dashboard', '--no-minimize-config', '--allow-overwrite', '--allow-fqdn-hostname', '--orphan-initial-daemons', '--skip-monitoring-stack', '--single-host-defaults']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.1 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults\n'
10.10.2.1: b'Verifying podman|docker is present...\nVerifying lvm2 is present...\n'
10.10.2.1: b'Verifying time synchronization is in place...\n'
10.10.2.1: b'Unit ntp.service is enabled and running\n'
10.10.2.1: b'Repeating the final host check...\npodman|docker (/usr/bin/docker) is present\n'
10.10.2.1: b'systemctl is present\n'
10.10.2.1: b'lvcreate is present\n'
10.10.2.1: b'Unit ntp.service is enabled and running\n'
10.10.2.1: b'Host looks OK\n'
10.10.2.1: b'Cluster fsid: 81b8a328-37de-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Verifying IP 10.10.2.1 port 3300 ...\n'
10.10.2.1: b'Verifying IP 10.10.2.1 port 6789 ...\n'
10.10.2.1: b'Mon IP `10.10.2.1` is in CIDR network `10.10.2.0/24`\n'
10.10.2.1: b'- internal network (--cluster-network) has not been provided, OSD replication will default to the public_network\n'
10.10.2.1: b'Adjusting default settings to suit single-host cluster...\n'
10.10.2.1: b'Pulling container image quay.io/ceph/ceph:v16...\n'
10.10.2.1: b'Ceph version: ceph version 16.2.6 (ee28fb57e47e9f88813e24bbf4c14496ca299d31) pacific (stable)\n'
10.10.2.1: b'Extracting ceph user uid/gid from container image...\n'
10.10.2.1: b'Creating initial keys...\n'
10.10.2.1: b'Creating initial monmap...\n'
10.10.2.1: b'Creating mon...\n'
10.10.2.1: b'Waiting for mon to start...\n'
10.10.2.1: b'Waiting for mon...\n'
10.10.2.1: b'mon is available\n'
10.10.2.1: b'Setting mon public_network to 10.10.2.0/24\n'
10.10.2.1: b'Wrote config to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf\n'
10.10.2.1: b'Wrote keyring to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring\n'
10.10.2.1: b'Creating mgr...\n'
10.10.2.1: b'Verifying port 9283 ...\n'
10.10.2.1: b'Waiting for mgr to start...\nWaiting for mgr...\n'
10.10.2.1: b'mgr not available, waiting (1/15)...\n'
10.10.2.1: b'mgr not available, waiting (2/15)...\n'
10.10.2.1: b'mgr is available\n'
10.10.2.1: b'Enabling cephadm module...\n'
10.10.2.1: b'Waiting for the mgr to restart...\n'
10.10.2.1: b'Waiting for mgr epoch 5...\n'
10.10.2.1: b'mgr epoch 5 is available\n'
10.10.2.1: b'Setting orchestrator backend to cephadm...\n'
10.10.2.1: b'Generating ssh key...\n'
10.10.2.1: b'Wrote public SSH key to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub\n'
10.10.2.1: b'Adding key to ljishen@localhost authorized_keys...\n'
10.10.2.1: b'Adding host node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us...\n'
10.10.2.1: b'Deploying unmanaged mon service...\n'
10.10.2.1: b'Deploying unmanaged mgr service...\n'
10.10.2.1: b'You can access the Ceph CLI with:\n\n\tsudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --fsid 81b8a328-37de-11ec-b51d-53e6e728d2d3 -c /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf -k /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring\n\n'
10.10.2.1: b'Please consider enabling telemetry to help improve Ceph:\n\n\tceph telemetry on\n\nFor more information see:\n\n\thttps://docs.ceph.com/docs/pacific/mgr/telemetry/\n\n'
10.10.2.1: b'Bootstrap complete.\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T04:03:15,025051280-07:00] INFO: Sleep for 20 seconds\x1b[0m\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T04:03:35,032766379-07:00] INFO: > Set the default number of monitor daemons to 1\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:337 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T04:03:35,042848933-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T04:03:35,046620333-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mon', '1']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1\n'
10.10.2.1: b'Inferring fsid 81b8a328-37de-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/81b8a328-37de-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b'Scheduled mon update...\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T04:03:44,292952902-07:00] INFO: > Set the default number of manager daemons to 1\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:342 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T04:03:44,302555053-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T04:03:44,306585249-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mgr', '1']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1\n'
10.10.2.1: b'Inferring fsid 81b8a328-37de-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/81b8a328-37de-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b'Scheduled mgr update...\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T04:03:53,616104181-07:00] STAGE: Add hosts to the cluster...\x1b[0m\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T04:03:53,621534149-07:00] INFO: > Adding host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:361 - add_hosts() > ssh-copy-id -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -f -i /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\n'
10.10.2.1: b'/usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: "/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub"\n'
10.10.2.1: b'\nNumber of key(s) added: 1\n\nNow try logging into the machine, with:   "ssh -o \'GlobalKnownHostsFile=/dev/null\' -o \'LogLevel=ERROR\' -o \'PasswordAuthentication=no\' -o \'StrictHostKeyChecking=no\' -o \'UserKnownHostsFile=/dev/null\' \'ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\'"\nand check to make sure that only the key(s) you wanted were added.\n\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:367 - add_hosts() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us 10.10.2.2\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T04:03:53,837418723-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T04:03:53,841112928-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'host', 'add', 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us', '10.10.2.2']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us 10.10.2.2\n'
10.10.2.1: b'Inferring fsid 81b8a328-37de-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/81b8a328-37de-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b"Added host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us' with addr '10.10.2.2'\n"
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T04:04:03,107836127-07:00] INFO: Sleep for 20 seconds\x1b[0m\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T04:04:23,112622208-07:00] STAGE: Add OSDs to the cluster...\x1b[0m\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T04:04:23,119667593-07:00] INFO: > Adding devices ['/dev/nvme0n1'] on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us' as OSDs\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:388 - add_host_devices_as_osds() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T04:04:23,129695615-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T04:04:23,133526016-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'daemon', 'add', 'osd', 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1\n'
10.10.2.1: b'Inferring fsid 81b8a328-37de-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/81b8a328-37de-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b"Created osd(s) 0 on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'\n"
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T04:04:47,309814130-07:00] INFO: > Sleep for 20 seconds\x1b[0m\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T04:05:07,315548214-07:00] STAGE: Show Cluster Status\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:408 - show_status() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T04:05:07,325351072-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T04:05:07,329389444-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', '--status']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status\n'
10.10.2.1: b'Inferring fsid 81b8a328-37de-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/81b8a328-37de-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b'  cluster:\n    id:     81b8a328-37de-11ec-b51d-53e6e728d2d3\n    health: HEALTH_WARN\n            1 pool(s) have no replicas configured\n \n  services:\n    mon: 1 daemons, quorum node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us (age 2m)\n    mgr: node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us.kqpuhf(active, since 2m)\n    osd: 1 osds: 1 up (since 18s), 1 in (since 40s)\n \n  data:\n    pools:   1 pools, 1 pgs\n    objects: 0 objects, 0 B\n    usage:   4.7 MiB used, 100 GiB / 100 GiB avail\n    pgs:     1 active+clean\n \n'
[1] 04:05:16 [SUCCESS] ljishen@10.10.2.1

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T04:02:00,271031562-07:00] INFO: > Remove existing clusters[0m
## bash:17 -  > basename -- /var/lib/ceph/40af090e-37dd-11ec-b51d-53e6e728d2d3
# bash:17 -  > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm --host 10.10.2.1 --host 10.10.2.2 rm-cluster --force --fsid 40af090e-37dd-11ec-b51d-53e6e728d2d3
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T04:02:00,281618674-07:00] DEBUG: HOST_PARAMS: ['--host', '10.10.2.1', '--host', '10.10.2.2'][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T04:02:00,285161394-07:00] DEBUG: CEPHADM_PARAMS: ['rm-cluster', '--force', '--fsid', '40af090e-37dd-11ec-b51d-53e6e728d2d3'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:63 -  > /tmp/bench-rados/ceph-research/scripts/parallel-ssh --host 10.10.2.1 --host 10.10.2.2 'sudo python3 -u - rm-cluster --force --fsid 40af090e-37dd-11ec-b51d-53e6e728d2d3'
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T04:02:00,293487937-07:00] DEBUG: command from stdin[0m
# /tmp/bench-rados/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host 10.10.2.1 --host 10.10.2.2 'sudo python3 -u - rm-cluster --force --fsid 40af090e-37dd-11ec-b51d-53e6e728d2d3'
[1] 04:02:06 [SUCCESS] 10.10.2.1
[2] 04:02:12 [SUCCESS] 10.10.2.2

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T04:02:12,814715470-07:00] INFO: > Deploy a new cluster[0m
# bash:24 -  > /tmp/bench-rados/ceph-research/scripts/ceph-deploy -m 10.10.2.1 -o node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/10.10.2.2:/dev/nvme0n1


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T04:02:12,826293987-07:00] STAGE: Verify passwordless SSH to hosts, passwordless sudo via ljishen, and the existence of devices on hosts...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T04:02:12,831004672-07:00] INFO: > Check the host for the monitor[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:181 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@10.10.2.1 sudo --non-interactive --validate

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T04:02:12,983256572-07:00] INFO: > Check hosts for OSDs[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T04:02:12,988218761-07:00] INFO: >> Check host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:200 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us sudo --non-interactive --validate

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T04:02:13,142850575-07:00] INFO: >>> Check device '/dev/nvme0n1'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:215 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us test -b /dev/nvme0n1
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:226 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us lsblk --nodeps --noheadings --output TYPE /dev/nvme0n1
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:227 - verify_parameters() > grep --word-regexp disk


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T04:02:13,431093336-07:00] STAGE: Reset devices on hosts...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T04:02:13,436039414-07:00] INFO: > Resetting device '/dev/nvme0n1' on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:288 - reset_host_devices() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us sudo sh
  Removing ceph--784b12f6--5594--4908--8fd5--679701321cd1-osd--block--e28b235d--c460--46d8--a581--b2f039603730 (253:0)
  Archiving volume group "ceph-784b12f6-5594-4908-8fd5-679701321cd1" metadata (seqno 5).
  Releasing logical volume "osd-block-e28b235d-c460-46d8-a581-b2f039603730"
  Creating volume group backup "/etc/lvm/backup/ceph-784b12f6-5594-4908-8fd5-679701321cd1" (seqno 6).
  Logical volume "osd-block-e28b235d-c460-46d8-a581-b2f039603730" successfully removed
  Removing physical volume "/dev/nvme0n1" from volume group "ceph-784b12f6-5594-4908-8fd5-679701321cd1"
  Volume group "ceph-784b12f6-5594-4908-8fd5-679701321cd1" successfully removed


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T04:02:13,797763263-07:00] STAGE: Bootstrap a new cluster...[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:317 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.1 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T04:02:13,807778420-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T04:02:13,811702297-07:00] DEBUG: CEPHADM_PARAMS: ['bootstrap', '--config', '/tmp/bench-rados/ceph-research/scripts/ceph.conf', '--mon-ip', '10.10.2.1', '--output-dir', '/tmp/bench-rados/deployment_data_root/etc/ceph', '--ssh-user', 'ljishen', '--skip-dashboard', '--no-minimize-config', '--allow-overwrite', '--allow-fqdn-hostname', '--orphan-initial-daemons', '--skip-monitoring-stack', '--single-host-defaults'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.1 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults
Verifying podman|docker is present...
Verifying lvm2 is present...
Verifying time synchronization is in place...
Unit ntp.service is enabled and running
Repeating the final host check...
podman|docker (/usr/bin/docker) is present
systemctl is present
lvcreate is present
Unit ntp.service is enabled and running
Host looks OK
Cluster fsid: 81b8a328-37de-11ec-b51d-53e6e728d2d3
Verifying IP 10.10.2.1 port 3300 ...
Verifying IP 10.10.2.1 port 6789 ...
Mon IP `10.10.2.1` is in CIDR network `10.10.2.0/24`
- internal network (--cluster-network) has not been provided, OSD replication will default to the public_network
Adjusting default settings to suit single-host cluster...
Pulling container image quay.io/ceph/ceph:v16...
Ceph version: ceph version 16.2.6 (ee28fb57e47e9f88813e24bbf4c14496ca299d31) pacific (stable)
Extracting ceph user uid/gid from container image...
Creating initial keys...
Creating initial monmap...
Creating mon...
Waiting for mon to start...
Waiting for mon...
mon is available
Setting mon public_network to 10.10.2.0/24
Wrote config to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf
Wrote keyring to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring
Creating mgr...
Verifying port 9283 ...
Waiting for mgr to start...
Waiting for mgr...
mgr not available, waiting (1/15)...
mgr not available, waiting (2/15)...
mgr is available
Enabling cephadm module...
Waiting for the mgr to restart...
Waiting for mgr epoch 5...
mgr epoch 5 is available
Setting orchestrator backend to cephadm...
Generating ssh key...
Wrote public SSH key to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub
Adding key to ljishen@localhost authorized_keys...
Adding host node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us...
Deploying unmanaged mon service...
Deploying unmanaged mgr service...
You can access the Ceph CLI with:

	sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --fsid 81b8a328-37de-11ec-b51d-53e6e728d2d3 -c /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf -k /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring

Please consider enabling telemetry to help improve Ceph:

	ceph telemetry on

For more information see:

	https://docs.ceph.com/docs/pacific/mgr/telemetry/

Bootstrap complete.

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T04:03:15,025051280-07:00] INFO: Sleep for 20 seconds[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T04:03:35,032766379-07:00] INFO: > Set the default number of monitor daemons to 1[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:337 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T04:03:35,042848933-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T04:03:35,046620333-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mon', '1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1
Inferring fsid 81b8a328-37de-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/81b8a328-37de-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Scheduled mon update...

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T04:03:44,292952902-07:00] INFO: > Set the default number of manager daemons to 1[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:342 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T04:03:44,302555053-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T04:03:44,306585249-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mgr', '1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1
Inferring fsid 81b8a328-37de-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/81b8a328-37de-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Scheduled mgr update...


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T04:03:53,616104181-07:00] STAGE: Add hosts to the cluster...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T04:03:53,621534149-07:00] INFO: > Adding host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:361 - add_hosts() > ssh-copy-id -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -f -i /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us
/usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: "/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub"

Number of key(s) added: 1

Now try logging into the machine, with:   "ssh -o 'GlobalKnownHostsFile=/dev/null' -o 'LogLevel=ERROR' -o 'PasswordAuthentication=no' -o 'StrictHostKeyChecking=no' -o 'UserKnownHostsFile=/dev/null' 'ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'"
and check to make sure that only the key(s) you wanted were added.

# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:367 - add_hosts() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us 10.10.2.2
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T04:03:53,837418723-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T04:03:53,841112928-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'host', 'add', 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us', '10.10.2.2'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us 10.10.2.2
Inferring fsid 81b8a328-37de-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/81b8a328-37de-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Added host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us' with addr '10.10.2.2'

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T04:04:03,107836127-07:00] INFO: Sleep for 20 seconds[0m


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T04:04:23,112622208-07:00] STAGE: Add OSDs to the cluster...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T04:04:23,119667593-07:00] INFO: > Adding devices ['/dev/nvme0n1'] on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us' as OSDs[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:388 - add_host_devices_as_osds() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T04:04:23,129695615-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T04:04:23,133526016-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'daemon', 'add', 'osd', 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1
Inferring fsid 81b8a328-37de-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/81b8a328-37de-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Created osd(s) 0 on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T04:04:47,309814130-07:00] INFO: > Sleep for 20 seconds[0m


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T04:05:07,315548214-07:00] STAGE: Show Cluster Status[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:408 - show_status() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T04:05:07,325351072-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T04:05:07,329389444-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', '--status'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status
Inferring fsid 81b8a328-37de-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/81b8a328-37de-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
  cluster:
    id:     81b8a328-37de-11ec-b51d-53e6e728d2d3
    health: HEALTH_WARN
            1 pool(s) have no replicas configured
 
  services:
    mon: 1 daemons, quorum node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us (age 2m)
    mgr: node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us.kqpuhf(active, since 2m)
    osd: 1 osds: 1 up (since 18s), 1 in (since 40s)
 
  data:
    pools:   1 pools, 1 pgs
    objects: 0 objects, 0 B
    usage:   4.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     1 active+clean
 

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T04:05:16,281181111-07:00] INFO: > Copy configuration files to local[0m
# ./benchmarks/bench-rados:220 - launch_ceph_cluster() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.1 sudo chown --changes --recursive ljishen /tmp/bench-rados/deployment_data_root/etc/ceph
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T04:05:16,289089845-07:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.1 sudo chown --changes --recursive ljishen /tmp/bench-rados/deployment_data_root/etc/ceph
[1] 04:05:16 [SUCCESS] ljishen@10.10.2.1
# ./benchmarks/bench-rados:223 - launch_ceph_cluster() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.1:/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf
# ./benchmarks/bench-rados:226 - launch_ceph_cluster() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.1:/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T04:05:16,765597768-07:00] STAGE: Update local ceph.conf as a client...[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T04:05:16,769514679-07:00] STAGE: Configure Ceph pool...[0m
# ./benchmarks/bench-rados:257 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_max_backfills 32
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T04:05:16,791320792-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T04:05:16,794108846-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '81b8a328-37de-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'config', 'set', 'osd', 'osd_max_backfills', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 81b8a328-37de-11ec-b51d-53e6e728d2d3 -- ceph config set osd osd_max_backfills 32
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:258 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_active 32
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T04:05:21,149691954-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T04:05:21,152642714-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '81b8a328-37de-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_active', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 81b8a328-37de-11ec-b51d-53e6e728d2d3 -- ceph config set osd osd_recovery_max_active 32
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:259 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_single_start 8
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T04:05:25,600129502-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T04:05:25,602944276-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '81b8a328-37de-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_single_start', '8'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 81b8a328-37de-11ec-b51d-53e6e728d2d3 -- ceph config set osd osd_recovery_max_single_start 8
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:260 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_op_priority 63
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T04:05:29,985138161-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T04:05:29,988203909-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '81b8a328-37de-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_op_priority', '63'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 81b8a328-37de-11ec-b51d-53e6e728d2d3 -- ceph config set osd osd_recovery_op_priority 63
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:262 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config show-with-defaults osd.0
# ./benchmarks/bench-rados:263 - configure_ceph_pool() > grep 'osd_max_backfills\|osd_recovery'
# ./benchmarks/bench-rados:263 - configure_ceph_pool() > column -t -s ' '
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
osd_max_backfills                        32         mon
osd_recovery_cost                        20971520   default
osd_recovery_delay_start                 0.000000   default
osd_recovery_max_active                  32         mon
osd_recovery_max_active_hdd              3          default
osd_recovery_max_active_ssd              10         default
osd_recovery_max_chunk                   8388608    default
osd_recovery_max_omap_entries_per_chunk  8096       default
osd_recovery_max_single_start            8          mon
osd_recovery_op_priority                 63         mon
osd_recovery_op_warn_multiple            16         default
osd_recovery_priority                    5          default
osd_recovery_retry_interval              30.000000  default
osd_recovery_sleep                       0.000000   default
osd_recovery_sleep_hdd                   0.100000   default
osd_recovery_sleep_hybrid                0.025000   default
osd_recovery_sleep_ssd                   0.000000   default
# ./benchmarks/bench-rados:265 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool create bench_rados 128 128 replicated --size 1 --pg-num-min 128
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T04:05:38,688901576-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T04:05:38,691756937-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '81b8a328-37de-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'create', 'bench_rados', '128', '128', 'replicated', '--size', '1', '--pg-num-min', '128'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 81b8a328-37de-11ec-b51d-53e6e728d2d3 -- ceph osd pool create bench_rados 128 128 replicated --size 1 --pg-num-min 128
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
pool 'bench_rados' created
# ./benchmarks/bench-rados:267 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados min_size 1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T04:05:44,020930953-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T04:05:44,023757749-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '81b8a328-37de-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'min_size', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 81b8a328-37de-11ec-b51d-53e6e728d2d3 -- ceph osd pool set bench_rados min_size 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 min_size to 1
# ./benchmarks/bench-rados:268 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados pg_autoscale_mode off
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T04:05:48,331201319-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T04:05:48,334241989-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '81b8a328-37de-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'pg_autoscale_mode', 'off'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 81b8a328-37de-11ec-b51d-53e6e728d2d3 -- ceph osd pool set bench_rados pg_autoscale_mode off
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 pg_autoscale_mode to off
# ./benchmarks/bench-rados:269 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool application enable bench_rados benchmark
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T04:05:52,956109891-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T04:05:52,959087792-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '81b8a328-37de-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'application', 'enable', 'bench_rados', 'benchmark'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 81b8a328-37de-11ec-b51d-53e6e728d2d3 -- ceph osd pool application enable bench_rados benchmark
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
enabled application 'benchmark' on pool 'bench_rados'
# ./benchmarks/bench-rados:270 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados noscrub 1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T04:05:58,332915846-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T04:05:58,335847020-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '81b8a328-37de-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'noscrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 81b8a328-37de-11ec-b51d-53e6e728d2d3 -- ceph osd pool set bench_rados noscrub 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 noscrub to 1
# ./benchmarks/bench-rados:271 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados nodeep-scrub 1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T04:06:02,974250682-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T04:06:02,977278608-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '81b8a328-37de-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'nodeep-scrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 81b8a328-37de-11ec-b51d-53e6e728d2d3 -- ceph osd pool set bench_rados nodeep-scrub 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 nodeep-scrub to 1
# ./benchmarks/bench-rados:272 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool ls detail
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T04:06:08,342277623-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T04:06:08,345263541-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '81b8a328-37de-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'ls', 'detail'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 81b8a328-37de-11ec-b51d-53e6e728d2d3 -- ceph osd pool ls detail
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
pool 1 'device_health_metrics' replicated size 1 min_size 1 crush_rule 0 object_hash rjenkins pg_num 64 pgp_num 64 autoscale_mode on last_change 19 lfor 0/0/16 flags hashpspool stripe_width 0 pg_num_min 1 application mgr_devicehealth
pool 2 'bench_rados' replicated size 1 min_size 1 crush_rule 0 object_hash rjenkins pg_num 128 pgp_num 128 autoscale_mode off last_change 21 flags hashpspool,noscrub,nodeep-scrub stripe_width 0 pg_num_min 128 application benchmark

# ./benchmarks/bench-rados:273 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd df tree
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T04:06:12,766490500-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T04:06:12,769687254-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '81b8a328-37de-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'df', 'tree'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 81b8a328-37de-11ec-b51d-53e6e728d2d3 -- ceph osd df tree
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
ID  CLASS  WEIGHT   REWEIGHT  SIZE     RAW USE  DATA     OMAP  META     AVAIL    %USE  VAR   PGS  STATUS  TYPE NAME      
-1         0.09769         -  100 GiB  5.7 MiB  176 KiB   0 B  5.6 MiB  100 GiB  0.01  1.00    -          root default   
-3         0.09769         -  100 GiB  5.7 MiB  176 KiB   0 B  5.6 MiB  100 GiB  0.01  1.00    -              host node-1
 0    ssd  0.09769   1.00000  100 GiB  5.7 MiB  176 KiB   0 B  5.6 MiB  100 GiB  0.01  1.00  192      up          osd.0  
                       TOTAL  100 GiB  5.7 MiB  176 KiB   0 B  5.6 MiB  100 GiB  0.01                                    
MIN/MAX VAR: 1.00/1.00  STDDEV: 0


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T04:06:17,037249838-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T04:06:41,218540450-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T04:06:50,551542227-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T04:07:00,002790173-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T04:07:00,011134056-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T04:07:09,460318594-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T04:07:09,468740574-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T04:07:18,835595102-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T04:07:18,844424561-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T04:07:28,134201678-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T04:07:28,142178791-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T04:07:37,474211937-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T04:07:37,482312492-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T04:07:37,488640076-07:00] INFO: > The cluster is idle now.[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T04:07:37,492479502-07:00] STAGE: Start benchmarking of mode write ...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T04:07:37,499887412-07:00] INFO: > Start system activity collection services[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T04:07:37,505865768-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:341 - rados_bench() > rm --force /mnt/sda8/ceph-research/scripts/output/sys_activity_16MB_write.dat.rados_bench_host.1
# ./benchmarks/bench-rados:342 - rados_bench() > sar_pid_rados_bench_host=337859
# ./benchmarks/bench-rados:342 - rados_bench() > S_TIME_FORMAT=ISO
# ./benchmarks/bench-rados:342 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/output/sys_activity_16MB_write.dat.rados_bench_host.1 2

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T04:07:37,513295980-07:00] INFO: >> for OSD host[0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T04:07:37,522683430-07:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.2 bash
10.10.2.2: b'194078\n'
[1] 04:07:37 [SUCCESS] ljishen@10.10.2.2
194078

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T04:07:37,715680987-07:00] INFO: > Run rados bench[0m
# ./benchmarks/bench-rados:370 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/output/rados_bench_16MB_write.log.1
## ./benchmarks/bench-rados:361 - rados_bench() > count_cpus 32-63,96-127
# ./benchmarks/bench-rados:361 - rados_bench() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 60 write --pool bench_rados -b 16777216 -O 16777216 --concurrent-ios 128 --show-time --write-object --write-omap --write-xattr --no-cleanup
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T04:07:37,736843858-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T04:07:37,739864379-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '81b8a328-37de-11ec-b51d-53e6e728d2d3', '--', 'rados', 'bench', '60', 'write', '--pool', 'bench_rados', '-b', '16777216', '-O', '16777216', '--concurrent-ios', '128', '--show-time', '--write-object', '--write-omap', '--write-xattr', '--no-cleanup'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 81b8a328-37de-11ec-b51d-53e6e728d2d3 -- rados bench 60 write --pool bench_rados -b 16777216 -O 16777216 --concurrent-ios 128 --show-time --write-object --write-omap --write-xattr --no-cleanup
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
hints = 1
2021-10-28T11:07:41.162258+0000 Maintaining 128 concurrent writes of 16777216 bytes to objects of size 16777216 for up to 60 seconds or 0 objects
2021-10-28T11:07:41.162290+0000 Object prefix: benchmark_data_node-2.bluefield2.ucsc-cmps10_7
2021-10-28T11:07:41.828070+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-28T11:07:41.828070+0000     0       0         0         0         0         0           -           0
2021-10-28T11:07:42.828185+0000     1      18        18         0         0         0           -           0
2021-10-28T11:07:43.828299+0000     2      35        35         0         0         0           -           0
2021-10-28T11:07:44.828414+0000     3      45        45         0         0         0           -           0
2021-10-28T11:07:45.828523+0000     4      65        65         0         0         0           -           0
2021-10-28T11:07:46.828637+0000     5      70        70         0         0         0           -           0
2021-10-28T11:07:47.828749+0000     6      77        77         0         0         0           -           0
2021-10-28T11:07:48.828858+0000     7      89        89         0         0         0           -           0
2021-10-28T11:07:49.828996+0000     8     100       100         0         0         0           -           0
2021-10-28T11:07:50.829103+0000     9     108       108         0         0         0           -           0
2021-10-28T11:07:51.829218+0000    10     125       125         0         0         0           -           0
2021-10-28T11:07:52.829336+0000    11     127       131         4   5.81754   5.81818     10.6115     10.4082
2021-10-28T11:07:53.829434+0000    12     127       138        11   14.6651       112     11.4193      10.934
2021-10-28T11:07:54.829550+0000    13     127       149        22    27.074       176     11.8138     11.2825
2021-10-28T11:07:55.829626+0000    14     127       159        32   36.5675       160     12.2906     11.5159
2021-10-28T11:07:56.829744+0000    15     127       165        38    40.529        96     12.7168     11.6732
2021-10-28T11:07:57.829843+0000    16     127       182        55   54.9941       272      12.532     11.9413
2021-10-28T11:07:58.829960+0000    17     127       194        67    63.052       192     12.7818     12.0611
2021-10-28T11:07:59.830073+0000    18     127       197        70   62.2155        48      13.232     12.1076
2021-10-28T11:08:00.830185+0000    19     127       208        81   68.2032       176     12.7919     12.2075
2021-10-28T11:08:01.830285+0000 min lat: 10.2461 max lat: 13.232 avg lat: 12.2803
2021-10-28T11:08:01.830285+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-28T11:08:01.830285+0000    20     127       219        92   73.5921       176     12.8743     12.2803
2021-10-28T11:08:02.830405+0000    21     127       226        99   75.4204       112     13.3042     12.3418
2021-10-28T11:08:03.830524+0000    22     127       229       102   74.1738        48     13.4316     12.3747
2021-10-28T11:08:04.830635+0000    23     127       240       113   78.6001       176     13.7139      12.491
2021-10-28T11:08:05.830732+0000    24     127       252       125   83.3243       192     14.1238     12.6334
2021-10-28T11:08:06.830854+0000    25     127       259       132   84.4708       112     14.2355     12.7186
2021-10-28T11:08:07.830969+0000    26     127       262       135   83.0679        48     14.3972     12.7539
2021-10-28T11:08:08.831086+0000    27     127       273       146   86.5091       176     14.3751     12.8766
2021-10-28T11:08:09.831182+0000    28     127       284       157   89.7045       176      14.387     12.9817
2021-10-28T11:08:10.831297+0000    29     127       291       164   90.4729       112     14.4219     13.0442
2021-10-28T11:08:11.831413+0000    30     127       297       170   90.6567        96     14.6675     13.0988
2021-10-28T11:08:12.831525+0000    31     127       308       181   93.4091       176     15.1376     13.2103
2021-10-28T11:08:13.831620+0000    32     127       317       190   94.9896       144     15.4159     13.3091
2021-10-28T11:08:14.831737+0000    33     127       323       196   95.0199        96     15.6667     13.3797
2021-10-28T11:08:15.831853+0000    34     127       329       202   95.0484        96     15.5543     13.4459
2021-10-28T11:08:16.831968+0000    35     127       340       213   97.3608       176      15.559     13.5554
2021-10-28T11:08:17.832064+0000    36     127       350       223   99.1003       160     15.5453     13.6447
2021-10-28T11:08:18.832175+0000    37     127       355       228   98.5838        80     15.6737      13.687
2021-10-28T11:08:19.832287+0000    38     127       363       236   99.3576       128     15.5185     13.7505
2021-10-28T11:08:20.832401+0000    39     127       374       247   101.322       176     15.4834     13.8293
2021-10-28T11:08:21.832496+0000 min lat: 10.2461 max lat: 15.7236 avg lat: 13.894
2021-10-28T11:08:21.832496+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-28T11:08:21.832496+0000    40     127       384       257   102.789       160     15.4403      13.894
2021-10-28T11:08:22.832614+0000    41     127       388       261   101.843        64     15.5371     13.9181
2021-10-28T11:08:23.832728+0000    42     127       397       270   102.846       144     15.4266      13.968
2021-10-28T11:08:24.832842+0000    43     127       407       280   104.175       160     15.4465     14.0201
2021-10-28T11:08:25.832934+0000    44     127       416       289   105.079       144      15.472     14.0646
2021-10-28T11:08:26.833045+0000    45     127       420       293   104.166        64     15.4898     14.0839
2021-10-28T11:08:27.833158+0000    46     127       429       302   105.032       144     15.6181     14.1296
2021-10-28T11:08:28.833273+0000    47     127       440       313   106.542       176     15.6211     14.1819
2021-10-28T11:08:29.833376+0000    48     127       450       323   107.655       160     15.3872     14.2234
2021-10-28T11:08:30.833496+0000    49     127       453       326   106.437        48     15.2073      14.233
2021-10-28T11:08:31.833610+0000    50     127       463       336   107.508       160     15.5049     14.2714
2021-10-28T11:08:32.833721+0000    51     127       474       347   108.851       176     15.4735       14.31
2021-10-28T11:08:33.833819+0000    52     127       483       356   109.526       144     15.1535      14.336
2021-10-28T11:08:34.833935+0000    53     127       485       358   108.064        32     15.1787     14.3409
2021-10-28T11:08:35.834047+0000    54     127       496       369   109.321       176     15.5139     14.3763
2021-10-28T11:08:36.834159+0000    55     127       507       380   110.533       176     15.5842     14.4105
2021-10-28T11:08:37.834255+0000    56     127       514       387   110.559       112      15.492     14.4302
2021-10-28T11:08:38.834371+0000    57     127       517       390   109.462        48     15.5556     14.4389
2021-10-28T11:08:39.834485+0000    58     127       529       402   110.884       192     15.6224     14.4752
2021-10-28T11:08:40.834599+0000    59     127       539       412   111.717       160     15.6523     14.5035
2021-10-28T11:08:41.834698+0000 min lat: 10.2461 max lat: 15.7236 avg lat: 14.5221
2021-10-28T11:08:41.834698+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-28T11:08:41.834698+0000    60     127       547       420   111.988       128     15.4705     14.5221
2021-10-28T11:08:42.834860+0000 Total time run:         60.8318
Total writes made:      548
Write size:             16777216
Object size:            16777216
Bandwidth (MB/sec):     144.135
Stddev Bandwidth:       70.1091
Max bandwidth (MB/sec): 272
Min bandwidth (MB/sec): 0
Average IOPS:           9
Stddev IOPS:            4.39131
Max IOPS:               17
Min IOPS:               0
Average Latency(s):     12.9696
Stddev Latency(s):      3.73393
Max latency(s):         15.7236
Min latency(s):         0.771009

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T04:08:43,623008881-07:00] INFO: > Stop system activity collection services[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T04:08:43,629721040-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:391 - rados_bench() > kill -INT 337859

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T04:08:43,635916775-07:00] INFO: >> for OSD host[0m
# ./benchmarks/bench-rados:396 - rados_bench() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.2 kill -INT 194078
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T04:08:43,644205906-07:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.2 kill -INT 194078
[1] 04:08:43 [SUCCESS] ljishen@10.10.2.2

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T04:08:43,829332089-07:00] INFO: >>> Copy system activity file from OSD host[0m
## ./benchmarks/bench-rados:403 - rados_bench() > basename -- /tmp/bench-rados/sys_activity_16MB_write.dat.osd_host.1
# ./benchmarks/bench-rados:403 - rados_bench() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.2:/tmp/bench-rados/sys_activity_16MB_write.dat.osd_host.1 /mnt/sda8/ceph-research/scripts/output/sys_activity_16MB_write.dat.osd_host.1


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T04:08:44,004475891-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T04:09:08,312945172-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 549 objects, 8.6 GiB
    usage:   26 GiB used, 74 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T04:09:08,321564404-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T04:09:17,752679748-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 549 objects, 8.6 GiB
    usage:   26 GiB used, 74 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T04:09:17,760975011-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T04:09:27,037243290-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 549 objects, 8.6 GiB
    usage:   26 GiB used, 74 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T04:09:27,045246893-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T04:09:36,395234674-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 549 objects, 8.6 GiB
    usage:   26 GiB used, 74 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T04:09:36,403887239-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T04:09:45,793174327-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 549 objects, 8.6 GiB
    usage:   26 GiB used, 74 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T04:09:45,801760737-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T04:09:45,808691177-07:00] INFO: > The cluster is idle now.[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T04:09:45,812750798-07:00] STAGE: Start benchmarking of mode seq ...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T04:09:45,820472309-07:00] INFO: > Start system activity collection services[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T04:09:45,826779855-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:341 - rados_bench() > rm --force /mnt/sda8/ceph-research/scripts/output/sys_activity_16MB_seq.dat.rados_bench_host.1
# ./benchmarks/bench-rados:342 - rados_bench() > sar_pid_rados_bench_host=339216
# ./benchmarks/bench-rados:342 - rados_bench() > S_TIME_FORMAT=ISO
# ./benchmarks/bench-rados:342 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/output/sys_activity_16MB_seq.dat.rados_bench_host.1 2

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T04:09:45,834191782-07:00] INFO: >> for OSD host[0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T04:09:45,843328450-07:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.2 bash
10.10.2.2: b'194172\n'
[1] 04:09:46 [SUCCESS] ljishen@10.10.2.2
194172

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T04:09:46,035248266-07:00] INFO: > Run rados bench[0m
# ./benchmarks/bench-rados:379 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/output/rados_bench_16MB_seq.log.1
## ./benchmarks/bench-rados:376 - rados_bench() > count_cpus 32-63,96-127
# ./benchmarks/bench-rados:376 - rados_bench() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 128 --show-time
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T04:09:46,055204132-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T04:09:46,058091914-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '81b8a328-37de-11ec-b51d-53e6e728d2d3', '--', 'rados', 'bench', '99999999', 'seq', '--pool', 'bench_rados', '--concurrent-ios', '128', '--show-time'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 81b8a328-37de-11ec-b51d-53e6e728d2d3 -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 128 --show-time
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
hints = 1
2021-10-28T11:09:49.464565+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-28T11:09:49.464565+0000     0       0         0         0         0         0           -           0
2021-10-28T11:09:50.464701+0000     1     109       109         0         0         0           -           0
2021-10-28T11:09:51.464784+0000     2     127       209        82   655.894       656     1.26913     1.21894
2021-10-28T11:09:52.464861+0000     3     127       301       174   927.876      1472     1.36964     1.28233
2021-10-28T11:09:53.464929+0000     4     127       395       268   1071.87      1504     1.35364     1.31269
2021-10-28T11:09:54.464994+0000     5     127       489       362   1158.28      1504     1.34995      1.3234
2021-10-28T11:09:55.465084+0000 Total time run:       5.83967
Total reads made:     548
Read size:            16777216
Object size:          16777216
Bandwidth (MB/sec):   1501.45
Average IOPS:         93
Stddev IOPS:          42.4523
Max IOPS:             94
Min IOPS:             0
Average Latency(s):   1.2063
Max latency(s):       1.39212
Min latency(s):       0.233821

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T04:09:56,224927082-07:00] INFO: > Stop system activity collection services[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T04:09:56,231763094-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:391 - rados_bench() > kill -INT 339216

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T04:09:56,238482446-07:00] INFO: >> for OSD host[0m
# ./benchmarks/bench-rados:396 - rados_bench() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.2 kill -INT 194172
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T04:09:56,246728415-07:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.2 kill -INT 194172
[1] 04:09:56 [SUCCESS] ljishen@10.10.2.2

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T04:09:56,433932996-07:00] INFO: >>> Copy system activity file from OSD host[0m
## ./benchmarks/bench-rados:403 - rados_bench() > basename -- /tmp/bench-rados/sys_activity_16MB_seq.dat.osd_host.1
# ./benchmarks/bench-rados:403 - rados_bench() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.2:/tmp/bench-rados/sys_activity_16MB_seq.dat.osd_host.1 /mnt/sda8/ceph-research/scripts/output/sys_activity_16MB_seq.dat.osd_host.1


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T04:09:56,590740304-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T04:10:20,980610114-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 549 objects, 8.6 GiB
    usage:   26 GiB used, 74 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T04:10:20,989075266-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T04:10:30,264146174-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 549 objects, 8.6 GiB
    usage:   26 GiB used, 74 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T04:10:30,272829307-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T04:10:39,645594495-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 549 objects, 8.6 GiB
    usage:   26 GiB used, 74 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T04:10:39,654088802-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T04:10:49,018679946-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 549 objects, 8.6 GiB
    usage:   26 GiB used, 74 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T04:10:49,027342099-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T04:10:58,426974276-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 549 objects, 8.6 GiB
    usage:   26 GiB used, 74 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T04:10:58,435230775-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T04:10:58,441988920-07:00] INFO: > The cluster is idle now.[0m


[1;7;39;49m[2021-10-28T04:10:58,444877834-07:00][RUNNING][ROUND 2/7/21] object_size=16MB[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T04:10:58,448970226-07:00] STAGE: Launch a Ceph cluster (CEPH_HOST: 10.10.2.1, OSD_HOST: 10.10.2.2) ...[0m
# ./benchmarks/bench-rados:183 - launch_ceph_cluster() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.1 bash -euo pipefail
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T04:10:58,458532265-07:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.1 bash -euo pipefail
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T04:10:58,849607054-07:00] INFO: > Remove existing clusters\x1b[0m\n'
10.10.2.1: b'## bash:17 -  > basename -- /var/lib/ceph/81b8a328-37de-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'# bash:17 -  > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm --host 10.10.2.1 --host 10.10.2.2 rm-cluster --force --fsid 81b8a328-37de-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T04:10:58,859747297-07:00] DEBUG: HOST_PARAMS: ['--host', '10.10.2.1', '--host', '10.10.2.2']\x1b[0m\n"
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T04:10:58,863391458-07:00] DEBUG: CEPHADM_PARAMS: ['rm-cluster', '--force', '--fsid', '81b8a328-37de-11ec-b51d-53e6e728d2d3']\x1b[0m\n"
10.10.2.1: b"# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:63 -  > /tmp/bench-rados/ceph-research/scripts/parallel-ssh --host 10.10.2.1 --host 10.10.2.2 'sudo python3 -u - rm-cluster --force --fsid 81b8a328-37de-11ec-b51d-53e6e728d2d3'\n"
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T04:10:58,872074290-07:00] DEBUG: command from stdin\x1b[0m\n'
10.10.2.1: b"# /tmp/bench-rados/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host 10.10.2.1 --host 10.10.2.2 'sudo python3 -u - rm-cluster --force --fsid 81b8a328-37de-11ec-b51d-53e6e728d2d3'\n"
10.10.2.1: b'[1] 04:11:04 [SUCCESS] 10.10.2.1\n[2] 04:11:11 [SUCCESS] 10.10.2.2\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T04:11:12,010303611-07:00] INFO: > Deploy a new cluster\x1b[0m\n'
10.10.2.1: b'# bash:24 -  > /tmp/bench-rados/ceph-research/scripts/ceph-deploy -m 10.10.2.1 -o node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/10.10.2.2:/dev/nvme0n1\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T04:11:12,022522682-07:00] STAGE: Verify passwordless SSH to hosts, passwordless sudo via ljishen, and the existence of devices on hosts...\x1b[0m\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T04:11:12,027271168-07:00] INFO: > Check the host for the monitor\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:181 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@10.10.2.1 sudo --non-interactive --validate\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T04:11:12,178966712-07:00] INFO: > Check hosts for OSDs\x1b[0m\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T04:11:12,183427137-07:00] INFO: >> Check host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:200 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us sudo --non-interactive --validate\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T04:11:12,334602703-07:00] INFO: >>> Check device '/dev/nvme0n1'\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:215 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us test -b /dev/nvme0n1\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:226 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us lsblk --nodeps --noheadings --output TYPE /dev/nvme0n1\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:227 - verify_parameters() > grep --word-regexp disk\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T04:11:12,619440722-07:00] STAGE: Reset devices on hosts...\x1b[0m\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T04:11:12,624530219-07:00] INFO: > Resetting device '/dev/nvme0n1' on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:288 - reset_host_devices() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us sudo sh\n'
10.10.2.1: b'  Removing ceph--a31e6ffc--c320--4e83--a1ba--bf39bbba4f90-osd--block--0dca275a--0a1b--481d--a6da--a431fc3ec1eb (253:0)\n'
10.10.2.1: b'  Archiving volume group "ceph-a31e6ffc-c320-4e83-a1ba-bf39bbba4f90" metadata (seqno 5).\n'
10.10.2.1: b'  Releasing logical volume "osd-block-0dca275a-0a1b-481d-a6da-a431fc3ec1eb"\n'
10.10.2.1: b'  Creating volume group backup "/etc/lvm/backup/ceph-a31e6ffc-c320-4e83-a1ba-bf39bbba4f90" (seqno 6).\n'
10.10.2.1: b'  Logical volume "osd-block-0dca275a-0a1b-481d-a6da-a431fc3ec1eb" successfully removed\n'
10.10.2.1: b'  Removing physical volume "/dev/nvme0n1" from volume group "ceph-a31e6ffc-c320-4e83-a1ba-bf39bbba4f90"\n'
10.10.2.1: b'  Volume group "ceph-a31e6ffc-c320-4e83-a1ba-bf39bbba4f90" successfully removed\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T04:11:13,029585525-07:00] STAGE: Bootstrap a new cluster...\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:317 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.1 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T04:11:13,039145256-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T04:11:13,043102596-07:00] DEBUG: CEPHADM_PARAMS: ['bootstrap', '--config', '/tmp/bench-rados/ceph-research/scripts/ceph.conf', '--mon-ip', '10.10.2.1', '--output-dir', '/tmp/bench-rados/deployment_data_root/etc/ceph', '--ssh-user', 'ljishen', '--skip-dashboard', '--no-minimize-config', '--allow-overwrite', '--allow-fqdn-hostname', '--orphan-initial-daemons', '--skip-monitoring-stack', '--single-host-defaults']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.1 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults\n'
10.10.2.1: b'Verifying podman|docker is present...\n'
10.10.2.1: b'Verifying lvm2 is present...\nVerifying time synchronization is in place...\n'
10.10.2.1: b'Unit ntp.service is enabled and running\n'
10.10.2.1: b'Repeating the final host check...\npodman|docker (/usr/bin/docker) is present\n'
10.10.2.1: b'systemctl is present\nlvcreate is present\n'
10.10.2.1: b'Unit ntp.service is enabled and running\n'
10.10.2.1: b'Host looks OK\n'
10.10.2.1: b'Cluster fsid: c3210958-37df-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Verifying IP 10.10.2.1 port 3300 ...\n'
10.10.2.1: b'Verifying IP 10.10.2.1 port 6789 ...\n'
10.10.2.1: b'Mon IP `10.10.2.1` is in CIDR network `10.10.2.0/24`\n'
10.10.2.1: b'- internal network (--cluster-network) has not been provided, OSD replication will default to the public_network\n'
10.10.2.1: b'Adjusting default settings to suit single-host cluster...\n'
10.10.2.1: b'Pulling container image quay.io/ceph/ceph:v16...\n'
10.10.2.1: b'Ceph version: ceph version 16.2.6 (ee28fb57e47e9f88813e24bbf4c14496ca299d31) pacific (stable)\n'
10.10.2.1: b'Extracting ceph user uid/gid from container image...\n'
10.10.2.1: b'Creating initial keys...\n'
10.10.2.1: b'Creating initial monmap...\n'
10.10.2.1: b'Creating mon...\n'
10.10.2.1: b'Waiting for mon to start...\n'
10.10.2.1: b'Waiting for mon...\n'
10.10.2.1: b'mon is available\n'
10.10.2.1: b'Setting mon public_network to 10.10.2.0/24\n'
10.10.2.1: b'Wrote config to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf\n'
10.10.2.1: b'Wrote keyring to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring\n'
10.10.2.1: b'Creating mgr...\n'
10.10.2.1: b'Verifying port 9283 ...\n'
10.10.2.1: b'Waiting for mgr to start...\nWaiting for mgr...\n'
10.10.2.1: b'mgr not available, waiting (1/15)...\n'
10.10.2.1: b'mgr not available, waiting (2/15)...\n'
10.10.2.1: b'mgr is available\n'
10.10.2.1: b'Enabling cephadm module...\n'
10.10.2.1: b'Waiting for the mgr to restart...\n'
10.10.2.1: b'Waiting for mgr epoch 5...\n'
10.10.2.1: b'mgr epoch 5 is available\n'
10.10.2.1: b'Setting orchestrator backend to cephadm...\n'
10.10.2.1: b'Generating ssh key...\n'
10.10.2.1: b'Wrote public SSH key to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub\n'
10.10.2.1: b'Adding key to ljishen@localhost authorized_keys...\n'
10.10.2.1: b'Adding host node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us...\n'
10.10.2.1: b'Deploying unmanaged mon service...\n'
10.10.2.1: b'Deploying unmanaged mgr service...\n'
10.10.2.1: b'You can access the Ceph CLI with:\n\n\tsudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --fsid c3210958-37df-11ec-b51d-53e6e728d2d3 -c /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf -k /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring\n\n'
10.10.2.1: b'Please consider enabling telemetry to help improve Ceph:\n\n\tceph telemetry on\n\nFor more information see:\n\n\thttps://docs.ceph.com/docs/pacific/mgr/telemetry/\n\nBootstrap complete.\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T04:12:14,145696378-07:00] INFO: Sleep for 20 seconds\x1b[0m\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T04:12:34,153049076-07:00] INFO: > Set the default number of monitor daemons to 1\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:337 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T04:12:34,163560877-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T04:12:34,167269840-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mon', '1']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1\n'
10.10.2.1: b'Inferring fsid c3210958-37df-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/c3210958-37df-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b'Scheduled mon update...\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T04:12:43,285218764-07:00] INFO: > Set the default number of manager daemons to 1\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:342 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T04:12:43,296017144-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T04:12:43,299521521-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mgr', '1']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1\n'
10.10.2.1: b'Inferring fsid c3210958-37df-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/c3210958-37df-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b'Scheduled mgr update...\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T04:12:52,375063083-07:00] STAGE: Add hosts to the cluster...\x1b[0m\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T04:12:52,381755264-07:00] INFO: > Adding host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:361 - add_hosts() > ssh-copy-id -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -f -i /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\n'
10.10.2.1: b'/usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: "/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub"\n'
10.10.2.1: b'\nNumber of key(s) added: 1\n\nNow try logging into the machine, with:   "ssh -o \'GlobalKnownHostsFile=/dev/null\' -o \'LogLevel=ERROR\' -o \'PasswordAuthentication=no\' -o \'StrictHostKeyChecking=no\' -o \'UserKnownHostsFile=/dev/null\' \'ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\'"\nand check to make sure that only the key(s) you wanted were added.\n\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:367 - add_hosts() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us 10.10.2.2\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T04:12:52,593611384-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T04:12:52,597544347-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'host', 'add', 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us', '10.10.2.2']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us 10.10.2.2\n'
10.10.2.1: b'Inferring fsid c3210958-37df-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/c3210958-37df-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b"Added host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us' with addr '10.10.2.2'\n"
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T04:13:02,090505818-07:00] INFO: Sleep for 20 seconds\x1b[0m\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T04:13:22,095455668-07:00] STAGE: Add OSDs to the cluster...\x1b[0m\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T04:13:22,101973671-07:00] INFO: > Adding devices ['/dev/nvme0n1'] on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us' as OSDs\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:388 - add_host_devices_as_osds() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T04:13:22,111901274-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T04:13:22,115641005-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'daemon', 'add', 'osd', 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1\n'
10.10.2.1: b'Inferring fsid c3210958-37df-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/c3210958-37df-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b"Created osd(s) 0 on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'\n"
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T04:13:46,241376142-07:00] INFO: > Sleep for 20 seconds\x1b[0m\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T04:14:06,247062157-07:00] STAGE: Show Cluster Status\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:408 - show_status() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T04:14:06,257271770-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T04:14:06,260828276-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', '--status']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status\n'
10.10.2.1: b'Inferring fsid c3210958-37df-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/c3210958-37df-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b'  cluster:\n    id:     c3210958-37df-11ec-b51d-53e6e728d2d3\n    health: HEALTH_WARN\n            1 pool(s) have no replicas configured\n \n  services:\n    mon: 1 daemons, quorum node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us (age 2m)\n    mgr: node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us.lqjwgg(active, since 2m)\n    osd: 1 osds: 1 up (since 17s), 1 in (since 40s)\n \n  data:\n    pools:   1 pools, 1 pgs\n    objects: 0 objects, 0 B\n    usage:   4.7 MiB used, 100 GiB / 100 GiB avail\n    pgs:     1 active+clean\n \n'
[1] 04:14:14 [SUCCESS] ljishen@10.10.2.1

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T04:10:58,849607054-07:00] INFO: > Remove existing clusters[0m
## bash:17 -  > basename -- /var/lib/ceph/81b8a328-37de-11ec-b51d-53e6e728d2d3
# bash:17 -  > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm --host 10.10.2.1 --host 10.10.2.2 rm-cluster --force --fsid 81b8a328-37de-11ec-b51d-53e6e728d2d3
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T04:10:58,859747297-07:00] DEBUG: HOST_PARAMS: ['--host', '10.10.2.1', '--host', '10.10.2.2'][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T04:10:58,863391458-07:00] DEBUG: CEPHADM_PARAMS: ['rm-cluster', '--force', '--fsid', '81b8a328-37de-11ec-b51d-53e6e728d2d3'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:63 -  > /tmp/bench-rados/ceph-research/scripts/parallel-ssh --host 10.10.2.1 --host 10.10.2.2 'sudo python3 -u - rm-cluster --force --fsid 81b8a328-37de-11ec-b51d-53e6e728d2d3'
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T04:10:58,872074290-07:00] DEBUG: command from stdin[0m
# /tmp/bench-rados/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host 10.10.2.1 --host 10.10.2.2 'sudo python3 -u - rm-cluster --force --fsid 81b8a328-37de-11ec-b51d-53e6e728d2d3'
[1] 04:11:04 [SUCCESS] 10.10.2.1
[2] 04:11:11 [SUCCESS] 10.10.2.2

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T04:11:12,010303611-07:00] INFO: > Deploy a new cluster[0m
# bash:24 -  > /tmp/bench-rados/ceph-research/scripts/ceph-deploy -m 10.10.2.1 -o node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/10.10.2.2:/dev/nvme0n1


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T04:11:12,022522682-07:00] STAGE: Verify passwordless SSH to hosts, passwordless sudo via ljishen, and the existence of devices on hosts...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T04:11:12,027271168-07:00] INFO: > Check the host for the monitor[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:181 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@10.10.2.1 sudo --non-interactive --validate

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T04:11:12,178966712-07:00] INFO: > Check hosts for OSDs[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T04:11:12,183427137-07:00] INFO: >> Check host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:200 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us sudo --non-interactive --validate

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T04:11:12,334602703-07:00] INFO: >>> Check device '/dev/nvme0n1'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:215 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us test -b /dev/nvme0n1
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:226 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us lsblk --nodeps --noheadings --output TYPE /dev/nvme0n1
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:227 - verify_parameters() > grep --word-regexp disk


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T04:11:12,619440722-07:00] STAGE: Reset devices on hosts...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T04:11:12,624530219-07:00] INFO: > Resetting device '/dev/nvme0n1' on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:288 - reset_host_devices() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us sudo sh
  Removing ceph--a31e6ffc--c320--4e83--a1ba--bf39bbba4f90-osd--block--0dca275a--0a1b--481d--a6da--a431fc3ec1eb (253:0)
  Archiving volume group "ceph-a31e6ffc-c320-4e83-a1ba-bf39bbba4f90" metadata (seqno 5).
  Releasing logical volume "osd-block-0dca275a-0a1b-481d-a6da-a431fc3ec1eb"
  Creating volume group backup "/etc/lvm/backup/ceph-a31e6ffc-c320-4e83-a1ba-bf39bbba4f90" (seqno 6).
  Logical volume "osd-block-0dca275a-0a1b-481d-a6da-a431fc3ec1eb" successfully removed
  Removing physical volume "/dev/nvme0n1" from volume group "ceph-a31e6ffc-c320-4e83-a1ba-bf39bbba4f90"
  Volume group "ceph-a31e6ffc-c320-4e83-a1ba-bf39bbba4f90" successfully removed


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T04:11:13,029585525-07:00] STAGE: Bootstrap a new cluster...[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:317 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.1 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T04:11:13,039145256-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T04:11:13,043102596-07:00] DEBUG: CEPHADM_PARAMS: ['bootstrap', '--config', '/tmp/bench-rados/ceph-research/scripts/ceph.conf', '--mon-ip', '10.10.2.1', '--output-dir', '/tmp/bench-rados/deployment_data_root/etc/ceph', '--ssh-user', 'ljishen', '--skip-dashboard', '--no-minimize-config', '--allow-overwrite', '--allow-fqdn-hostname', '--orphan-initial-daemons', '--skip-monitoring-stack', '--single-host-defaults'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.1 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults
Verifying podman|docker is present...
Verifying lvm2 is present...
Verifying time synchronization is in place...
Unit ntp.service is enabled and running
Repeating the final host check...
podman|docker (/usr/bin/docker) is present
systemctl is present
lvcreate is present
Unit ntp.service is enabled and running
Host looks OK
Cluster fsid: c3210958-37df-11ec-b51d-53e6e728d2d3
Verifying IP 10.10.2.1 port 3300 ...
Verifying IP 10.10.2.1 port 6789 ...
Mon IP `10.10.2.1` is in CIDR network `10.10.2.0/24`
- internal network (--cluster-network) has not been provided, OSD replication will default to the public_network
Adjusting default settings to suit single-host cluster...
Pulling container image quay.io/ceph/ceph:v16...
Ceph version: ceph version 16.2.6 (ee28fb57e47e9f88813e24bbf4c14496ca299d31) pacific (stable)
Extracting ceph user uid/gid from container image...
Creating initial keys...
Creating initial monmap...
Creating mon...
Waiting for mon to start...
Waiting for mon...
mon is available
Setting mon public_network to 10.10.2.0/24
Wrote config to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf
Wrote keyring to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring
Creating mgr...
Verifying port 9283 ...
Waiting for mgr to start...
Waiting for mgr...
mgr not available, waiting (1/15)...
mgr not available, waiting (2/15)...
mgr is available
Enabling cephadm module...
Waiting for the mgr to restart...
Waiting for mgr epoch 5...
mgr epoch 5 is available
Setting orchestrator backend to cephadm...
Generating ssh key...
Wrote public SSH key to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub
Adding key to ljishen@localhost authorized_keys...
Adding host node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us...
Deploying unmanaged mon service...
Deploying unmanaged mgr service...
You can access the Ceph CLI with:

	sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --fsid c3210958-37df-11ec-b51d-53e6e728d2d3 -c /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf -k /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring

Please consider enabling telemetry to help improve Ceph:

	ceph telemetry on

For more information see:

	https://docs.ceph.com/docs/pacific/mgr/telemetry/

Bootstrap complete.

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T04:12:14,145696378-07:00] INFO: Sleep for 20 seconds[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T04:12:34,153049076-07:00] INFO: > Set the default number of monitor daemons to 1[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:337 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T04:12:34,163560877-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T04:12:34,167269840-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mon', '1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1
Inferring fsid c3210958-37df-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/c3210958-37df-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Scheduled mon update...

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T04:12:43,285218764-07:00] INFO: > Set the default number of manager daemons to 1[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:342 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T04:12:43,296017144-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T04:12:43,299521521-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mgr', '1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1
Inferring fsid c3210958-37df-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/c3210958-37df-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Scheduled mgr update...


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T04:12:52,375063083-07:00] STAGE: Add hosts to the cluster...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T04:12:52,381755264-07:00] INFO: > Adding host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:361 - add_hosts() > ssh-copy-id -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -f -i /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us
/usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: "/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub"

Number of key(s) added: 1

Now try logging into the machine, with:   "ssh -o 'GlobalKnownHostsFile=/dev/null' -o 'LogLevel=ERROR' -o 'PasswordAuthentication=no' -o 'StrictHostKeyChecking=no' -o 'UserKnownHostsFile=/dev/null' 'ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'"
and check to make sure that only the key(s) you wanted were added.

# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:367 - add_hosts() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us 10.10.2.2
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T04:12:52,593611384-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T04:12:52,597544347-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'host', 'add', 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us', '10.10.2.2'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us 10.10.2.2
Inferring fsid c3210958-37df-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/c3210958-37df-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Added host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us' with addr '10.10.2.2'

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T04:13:02,090505818-07:00] INFO: Sleep for 20 seconds[0m


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T04:13:22,095455668-07:00] STAGE: Add OSDs to the cluster...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T04:13:22,101973671-07:00] INFO: > Adding devices ['/dev/nvme0n1'] on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us' as OSDs[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:388 - add_host_devices_as_osds() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T04:13:22,111901274-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T04:13:22,115641005-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'daemon', 'add', 'osd', 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1
Inferring fsid c3210958-37df-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/c3210958-37df-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Created osd(s) 0 on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T04:13:46,241376142-07:00] INFO: > Sleep for 20 seconds[0m


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T04:14:06,247062157-07:00] STAGE: Show Cluster Status[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:408 - show_status() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T04:14:06,257271770-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T04:14:06,260828276-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', '--status'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status
Inferring fsid c3210958-37df-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/c3210958-37df-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
  cluster:
    id:     c3210958-37df-11ec-b51d-53e6e728d2d3
    health: HEALTH_WARN
            1 pool(s) have no replicas configured
 
  services:
    mon: 1 daemons, quorum node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us (age 2m)
    mgr: node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us.lqjwgg(active, since 2m)
    osd: 1 osds: 1 up (since 17s), 1 in (since 40s)
 
  data:
    pools:   1 pools, 1 pgs
    objects: 0 objects, 0 B
    usage:   4.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     1 active+clean
 

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T04:14:14,815326851-07:00] INFO: > Copy configuration files to local[0m
# ./benchmarks/bench-rados:220 - launch_ceph_cluster() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.1 sudo chown --changes --recursive ljishen /tmp/bench-rados/deployment_data_root/etc/ceph
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T04:14:14,823549236-07:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.1 sudo chown --changes --recursive ljishen /tmp/bench-rados/deployment_data_root/etc/ceph
[1] 04:14:15 [SUCCESS] ljishen@10.10.2.1
# ./benchmarks/bench-rados:223 - launch_ceph_cluster() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.1:/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf
# ./benchmarks/bench-rados:226 - launch_ceph_cluster() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.1:/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T04:14:15,296903092-07:00] STAGE: Update local ceph.conf as a client...[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T04:14:15,301039297-07:00] STAGE: Configure Ceph pool...[0m
# ./benchmarks/bench-rados:257 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_max_backfills 32
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T04:14:15,323764602-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T04:14:15,326450894-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'c3210958-37df-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'config', 'set', 'osd', 'osd_max_backfills', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid c3210958-37df-11ec-b51d-53e6e728d2d3 -- ceph config set osd osd_max_backfills 32
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:258 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_active 32
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T04:14:19,602214774-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T04:14:19,605312912-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'c3210958-37df-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_active', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid c3210958-37df-11ec-b51d-53e6e728d2d3 -- ceph config set osd osd_recovery_max_active 32
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:259 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_single_start 8
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T04:14:24,011871479-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T04:14:24,014729044-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'c3210958-37df-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_single_start', '8'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid c3210958-37df-11ec-b51d-53e6e728d2d3 -- ceph config set osd osd_recovery_max_single_start 8
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:260 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_op_priority 63
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T04:14:28,352070324-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T04:14:28,355119219-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'c3210958-37df-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_op_priority', '63'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid c3210958-37df-11ec-b51d-53e6e728d2d3 -- ceph config set osd osd_recovery_op_priority 63
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:262 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config show-with-defaults osd.0
# ./benchmarks/bench-rados:263 - configure_ceph_pool() > grep 'osd_max_backfills\|osd_recovery'
# ./benchmarks/bench-rados:263 - configure_ceph_pool() > column -t -s ' '
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
osd_max_backfills                        32         mon
osd_recovery_cost                        20971520   default
osd_recovery_delay_start                 0.000000   default
osd_recovery_max_active                  32         mon
osd_recovery_max_active_hdd              3          default
osd_recovery_max_active_ssd              10         default
osd_recovery_max_chunk                   8388608    default
osd_recovery_max_omap_entries_per_chunk  8096       default
osd_recovery_max_single_start            8          mon
osd_recovery_op_priority                 63         mon
osd_recovery_op_warn_multiple            16         default
osd_recovery_priority                    5          default
osd_recovery_retry_interval              30.000000  default
osd_recovery_sleep                       0.000000   default
osd_recovery_sleep_hdd                   0.100000   default
osd_recovery_sleep_hybrid                0.025000   default
osd_recovery_sleep_ssd                   0.000000   default
# ./benchmarks/bench-rados:265 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool create bench_rados 128 128 replicated --size 1 --pg-num-min 128
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T04:14:37,077383453-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T04:14:37,080878941-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'c3210958-37df-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'create', 'bench_rados', '128', '128', 'replicated', '--size', '1', '--pg-num-min', '128'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid c3210958-37df-11ec-b51d-53e6e728d2d3 -- ceph osd pool create bench_rados 128 128 replicated --size 1 --pg-num-min 128
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
pool 'bench_rados' created
# ./benchmarks/bench-rados:267 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados min_size 1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T04:14:41,676749067-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T04:14:41,679625577-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'c3210958-37df-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'min_size', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid c3210958-37df-11ec-b51d-53e6e728d2d3 -- ceph osd pool set bench_rados min_size 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 min_size to 1
# ./benchmarks/bench-rados:268 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados pg_autoscale_mode off
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T04:14:46,817923047-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T04:14:46,820971221-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'c3210958-37df-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'pg_autoscale_mode', 'off'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid c3210958-37df-11ec-b51d-53e6e728d2d3 -- ceph osd pool set bench_rados pg_autoscale_mode off
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 pg_autoscale_mode to off
# ./benchmarks/bench-rados:269 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool application enable bench_rados benchmark
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T04:14:51,854140079-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T04:14:51,857164378-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'c3210958-37df-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'application', 'enable', 'bench_rados', 'benchmark'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid c3210958-37df-11ec-b51d-53e6e728d2d3 -- ceph osd pool application enable bench_rados benchmark
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
enabled application 'benchmark' on pool 'bench_rados'
# ./benchmarks/bench-rados:270 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados noscrub 1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T04:14:57,094627691-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T04:14:57,097810780-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'c3210958-37df-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'noscrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid c3210958-37df-11ec-b51d-53e6e728d2d3 -- ceph osd pool set bench_rados noscrub 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 noscrub to 1
# ./benchmarks/bench-rados:271 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados nodeep-scrub 1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T04:15:02,008682023-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T04:15:02,011569504-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'c3210958-37df-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'nodeep-scrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid c3210958-37df-11ec-b51d-53e6e728d2d3 -- ceph osd pool set bench_rados nodeep-scrub 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 nodeep-scrub to 1
# ./benchmarks/bench-rados:272 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool ls detail
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T04:15:07,040253420-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T04:15:07,043330499-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'c3210958-37df-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'ls', 'detail'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid c3210958-37df-11ec-b51d-53e6e728d2d3 -- ceph osd pool ls detail
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
pool 1 'device_health_metrics' replicated size 1 min_size 1 crush_rule 0 object_hash rjenkins pg_num 64 pgp_num 64 autoscale_mode on last_change 18 lfor 0/0/16 flags hashpspool stripe_width 0 pg_num_min 1 application mgr_devicehealth
pool 2 'bench_rados' replicated size 1 min_size 1 crush_rule 0 object_hash rjenkins pg_num 128 pgp_num 128 autoscale_mode off last_change 20 flags hashpspool,noscrub,nodeep-scrub stripe_width 0 pg_num_min 128 application benchmark

# ./benchmarks/bench-rados:273 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd df tree
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T04:15:11,448784673-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T04:15:11,451745382-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'c3210958-37df-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'df', 'tree'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid c3210958-37df-11ec-b51d-53e6e728d2d3 -- ceph osd df tree
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
ID  CLASS  WEIGHT   REWEIGHT  SIZE     RAW USE  DATA     OMAP  META     AVAIL    %USE  VAR   PGS  STATUS  TYPE NAME      
-1         0.09769         -  100 GiB  5.7 MiB  168 KiB   0 B  5.6 MiB  100 GiB  0.01  1.00    -          root default   
-3         0.09769         -  100 GiB  5.7 MiB  168 KiB   0 B  5.6 MiB  100 GiB  0.01  1.00    -              host node-1
 0    ssd  0.09769   1.00000  100 GiB  5.7 MiB  168 KiB   0 B  5.6 MiB  100 GiB  0.01  1.00  192      up          osd.0  
                       TOTAL  100 GiB  5.7 MiB  168 KiB   0 B  5.6 MiB  100 GiB  0.01                                    
MIN/MAX VAR: 1.00/1.00  STDDEV: 0


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T04:15:15,855135276-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T04:15:40,191192517-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T04:15:49,530780298-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T04:15:58,870473606-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T04:15:58,878566467-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T04:16:08,201994463-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T04:16:08,210524057-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T04:16:17,650531772-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T04:16:17,658925229-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T04:16:27,016778106-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T04:16:27,024772902-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T04:16:36,398594020-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T04:16:36,407424621-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T04:16:36,413738209-07:00] INFO: > The cluster is idle now.[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T04:16:36,417671621-07:00] STAGE: Start benchmarking of mode write ...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T04:16:36,425345142-07:00] INFO: > Start system activity collection services[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T04:16:36,431786500-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:341 - rados_bench() > rm --force /mnt/sda8/ceph-research/scripts/output/sys_activity_16MB_write.dat.rados_bench_host.2
# ./benchmarks/bench-rados:342 - rados_bench() > sar_pid_rados_bench_host=344792
# ./benchmarks/bench-rados:342 - rados_bench() > S_TIME_FORMAT=ISO
# ./benchmarks/bench-rados:342 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/output/sys_activity_16MB_write.dat.rados_bench_host.2 2

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T04:16:36,439644438-07:00] INFO: >> for OSD host[0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T04:16:36,448791856-07:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.2 bash
10.10.2.2: b'197803\n'
[1] 04:16:36 [SUCCESS] ljishen@10.10.2.2
197803

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T04:16:36,639674096-07:00] INFO: > Run rados bench[0m
# ./benchmarks/bench-rados:370 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/output/rados_bench_16MB_write.log.2
## ./benchmarks/bench-rados:361 - rados_bench() > count_cpus 32-63,96-127
# ./benchmarks/bench-rados:361 - rados_bench() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 60 write --pool bench_rados -b 16777216 -O 16777216 --concurrent-ios 128 --show-time --write-object --write-omap --write-xattr --no-cleanup
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T04:16:36,660999773-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T04:16:36,663865764-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'c3210958-37df-11ec-b51d-53e6e728d2d3', '--', 'rados', 'bench', '60', 'write', '--pool', 'bench_rados', '-b', '16777216', '-O', '16777216', '--concurrent-ios', '128', '--show-time', '--write-object', '--write-omap', '--write-xattr', '--no-cleanup'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid c3210958-37df-11ec-b51d-53e6e728d2d3 -- rados bench 60 write --pool bench_rados -b 16777216 -O 16777216 --concurrent-ios 128 --show-time --write-object --write-omap --write-xattr --no-cleanup
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
hints = 1
2021-10-28T11:16:39.866726+0000 Maintaining 128 concurrent writes of 16777216 bytes to objects of size 16777216 for up to 60 seconds or 0 objects
2021-10-28T11:16:39.866762+0000 Object prefix: benchmark_data_node-2.bluefield2.ucsc-cmps10_7
2021-10-28T11:16:40.531808+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-28T11:16:40.531808+0000     0       0         0         0         0         0           -           0
2021-10-28T11:16:41.531924+0000     1      18        18         0         0         0           -           0
2021-10-28T11:16:42.532038+0000     2      35        35         0         0         0           -           0
2021-10-28T11:16:43.532151+0000     3      44        44         0         0         0           -           0
2021-10-28T11:16:44.532286+0000     4      62        62         0         0         0           -           0
2021-10-28T11:16:45.532364+0000     5      69        69         0         0         0           -           0
2021-10-28T11:16:46.532475+0000     6      76        76         0         0         0           -           0
2021-10-28T11:16:47.532586+0000     7      87        87         0         0         0           -           0
2021-10-28T11:16:48.532700+0000     8      99        99         0         0         0           -           0
2021-10-28T11:16:49.532779+0000     9     106       106         0         0         0           -           0
2021-10-28T11:16:50.532875+0000    10     125       125         0         0         0           -           0
2021-10-28T11:16:51.532987+0000    11     127       131         4   5.81758   5.81818     10.7414     10.5225
2021-10-28T11:16:52.533100+0000    12     127       136         9   11.9988        80     11.4589     10.9671
2021-10-28T11:16:53.533185+0000    13     127       147        20   24.6129       176     11.9339     11.3815
2021-10-28T11:16:54.533303+0000    14     127       157        30   34.2822       160     12.2779     11.6247
2021-10-28T11:16:55.533405+0000    15     127       164        37   39.4626       112     12.7785     11.8087
2021-10-28T11:16:56.533518+0000    16     127       180        53   52.9945       256     12.5593      12.064
2021-10-28T11:16:57.533599+0000    17     127       193        66   62.1113       208     12.7961     12.1713
2021-10-28T11:16:58.533711+0000    18     127       196        69    61.327        48     13.2728     12.2105
2021-10-28T11:16:59.533821+0000    19     127       206        79   66.5194       160     12.8882      12.309
2021-10-28T11:17:00.533920+0000 min lat: 10.3485 max lat: 13.3138 avg lat: 12.3739
2021-10-28T11:17:00.533920+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-28T11:17:00.533920+0000    20     127       218        91   72.7925       192     12.7474     12.3739
2021-10-28T11:17:01.534016+0000    21     127       226        99   75.4208       128     13.0821     12.4197
2021-10-28T11:17:02.534130+0000    22     127       230       103   74.9013        64     13.2041     12.4545
2021-10-28T11:17:03.534245+0000    23     127       241       114   79.2961       176     13.5586     12.5483
2021-10-28T11:17:04.534362+0000    24     127       253       126   83.9912       192     14.0206     12.6669
2021-10-28T11:17:05.534444+0000    25     127       260       133   85.1112       112     13.6417     12.7346
2021-10-28T11:17:06.534556+0000    26     127       265       138   84.9143        80     13.9171     12.7707
2021-10-28T11:17:07.534667+0000    27     127       275       148   87.6946       160       14.05     12.8593
2021-10-28T11:17:08.534780+0000    28     127       285       158   90.2763       160     14.0101     12.9339
2021-10-28T11:17:09.534858+0000    29     127       292       165   91.0251       112      14.048     12.9823
2021-10-28T11:17:10.534959+0000    30     127       298       171   91.1906        96     14.4669     13.0288
2021-10-28T11:17:11.535073+0000    31     127       311       184   94.9579       208     14.8489     13.1444
2021-10-28T11:17:12.535190+0000    32     127       320       193   96.4899       144     15.0835     13.2294
2021-10-28T11:17:13.535272+0000    33     127       327       200   96.9597       112     14.6149     13.2843
2021-10-28T11:17:14.535392+0000    34     127       334       207   97.4016       112     14.9858     13.3395
2021-10-28T11:17:15.535490+0000    35     127       345       218   99.6468       176     15.0769     13.4248
2021-10-28T11:17:16.535610+0000    36     127       354       227   100.878       144     15.1664     13.4913
2021-10-28T11:17:17.535694+0000    37     127       358       231   99.8815        64      15.125     13.5178
2021-10-28T11:17:18.535813+0000    38     127       369       242   101.884       176     15.0468     13.5885
2021-10-28T11:17:19.535926+0000    39     127       381       254   104.194       192     14.9281     13.6551
2021-10-28T11:17:20.536024+0000 min lat: 10.3485 max lat: 15.1664 avg lat: 13.6846
2021-10-28T11:17:20.536024+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-28T11:17:20.536024+0000    40     127       387       260   103.989        96     15.0892     13.6846
2021-10-28T11:17:21.536111+0000    41     127       393       266   103.794        96     15.0747     13.7184
2021-10-28T11:17:22.536229+0000    42     127       404       277   105.513       176     14.9187     13.7658
2021-10-28T11:17:23.536343+0000    43     127       413       286   106.407       144     14.9507     13.8026
2021-10-28T11:17:24.536456+0000    44     127       419       292   106.171        96     15.1935     13.8264
2021-10-28T11:17:25.536539+0000    45     127       426       299     106.3       112     15.0285     13.8571
2021-10-28T11:17:26.536653+0000    46     127       438       311   108.163       192     15.0908     13.9039
2021-10-28T11:17:27.536766+0000    47     127       449       322   109.606       176     14.9594     13.9443
2021-10-28T11:17:28.536879+0000    48     127       452       325   108.322        48     15.3185     13.9552
2021-10-28T11:17:29.536961+0000    49     127       459       332   108.397       112      15.167      13.986
2021-10-28T11:17:30.537062+0000    50     127       471       344   110.069       192     15.2153      14.029
2021-10-28T11:17:31.537179+0000    51     127       482       355   111.361       176     15.0751      14.064
2021-10-28T11:17:32.537296+0000    52     127       485       358   110.142        48     15.2625     14.0736
2021-10-28T11:17:33.537375+0000    53     127       492       365   110.177       112     15.3429     14.0971
2021-10-28T11:17:34.537500+0000    54     127       503       376   111.396       176     15.4875     14.1363
2021-10-28T11:17:35.537607+0000    55     127       514       387    112.57       176      15.426     14.1744
2021-10-28T11:17:36.537692+0000    56     127       517       390   111.417        48     15.5007     14.1839
2021-10-28T11:17:37.537774+0000    57     127       526       399   111.988       144     15.4872     14.2138
2021-10-28T11:17:38.537893+0000    58     127       537       410   113.092       176     15.5395     14.2491
2021-10-28T11:17:39.538009+0000    59     127       546       419   113.615       144     15.3017     14.2736
2021-10-28T11:17:40.538110+0000 min lat: 10.3485 max lat: 15.6225 avg lat: 14.2808
2021-10-28T11:17:40.538110+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-28T11:17:40.538110+0000    60     127       549       422   112.522        48     15.3422     14.2808
2021-10-28T11:17:41.538239+0000 Total time run:         60.3184
Total writes made:      550
Write size:             16777216
Object size:            16777216
Bandwidth (MB/sec):     145.892
Stddev Bandwidth:       70.3
Max bandwidth (MB/sec): 256
Min bandwidth (MB/sec): 0
Average IOPS:           9
Stddev IOPS:            4.40326
Max IOPS:               16
Min IOPS:               0
Average Latency(s):     12.8863
Stddev Latency(s):      3.47803
Max latency(s):         15.6225
Min latency(s):         0.553355

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T04:17:42,318976221-07:00] INFO: > Stop system activity collection services[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T04:17:42,325217693-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:391 - rados_bench() > kill -INT 344792

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T04:17:42,331618665-07:00] INFO: >> for OSD host[0m
# ./benchmarks/bench-rados:396 - rados_bench() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.2 kill -INT 197803
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T04:17:42,339679195-07:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.2 kill -INT 197803
[1] 04:17:42 [SUCCESS] ljishen@10.10.2.2

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T04:17:42,526024996-07:00] INFO: >>> Copy system activity file from OSD host[0m
## ./benchmarks/bench-rados:403 - rados_bench() > basename -- /tmp/bench-rados/sys_activity_16MB_write.dat.osd_host.2
# ./benchmarks/bench-rados:403 - rados_bench() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.2:/tmp/bench-rados/sys_activity_16MB_write.dat.osd_host.2 /mnt/sda8/ceph-research/scripts/output/sys_activity_16MB_write.dat.osd_host.2


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T04:17:42,700911893-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T04:18:06,957327282-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 551 objects, 8.6 GiB
    usage:   26 GiB used, 74 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T04:18:06,965472181-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T04:18:16,190926708-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 551 objects, 8.6 GiB
    usage:   26 GiB used, 74 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T04:18:16,199535401-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T04:18:25,673180051-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 551 objects, 8.6 GiB
    usage:   26 GiB used, 74 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T04:18:25,681497846-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T04:18:35,071425646-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 551 objects, 8.6 GiB
    usage:   26 GiB used, 74 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T04:18:35,080219939-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T04:18:44,361354042-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 551 objects, 8.6 GiB
    usage:   26 GiB used, 74 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T04:18:44,369478572-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T04:18:44,376359519-07:00] INFO: > The cluster is idle now.[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T04:18:44,380904865-07:00] STAGE: Start benchmarking of mode seq ...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T04:18:44,388614222-07:00] INFO: > Start system activity collection services[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T04:18:44,394949601-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:341 - rados_bench() > rm --force /mnt/sda8/ceph-research/scripts/output/sys_activity_16MB_seq.dat.rados_bench_host.2
# ./benchmarks/bench-rados:342 - rados_bench() > sar_pid_rados_bench_host=346155
# ./benchmarks/bench-rados:342 - rados_bench() > S_TIME_FORMAT=ISO
# ./benchmarks/bench-rados:342 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/output/sys_activity_16MB_seq.dat.rados_bench_host.2 2

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T04:18:44,402691681-07:00] INFO: >> for OSD host[0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T04:18:44,412198467-07:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.2 bash
10.10.2.2: b'197903\n'
[1] 04:18:44 [SUCCESS] ljishen@10.10.2.2
197903

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T04:18:44,603114409-07:00] INFO: > Run rados bench[0m
# ./benchmarks/bench-rados:379 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/output/rados_bench_16MB_seq.log.2
## ./benchmarks/bench-rados:376 - rados_bench() > count_cpus 32-63,96-127
# ./benchmarks/bench-rados:376 - rados_bench() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 128 --show-time
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T04:18:44,623864722-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T04:18:44,626769977-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'c3210958-37df-11ec-b51d-53e6e728d2d3', '--', 'rados', 'bench', '99999999', 'seq', '--pool', 'bench_rados', '--concurrent-ios', '128', '--show-time'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid c3210958-37df-11ec-b51d-53e6e728d2d3 -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 128 --show-time
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
hints = 1
2021-10-28T11:18:48.148399+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-28T11:18:48.148399+0000     0       0         0         0         0         0           -           0
2021-10-28T11:18:49.148529+0000     1     112       112         0         0         0           -           0
2021-10-28T11:18:50.148610+0000     2     127       206        79   631.899       632     1.30638     1.23228
2021-10-28T11:18:51.148698+0000     3     127       296       169    901.21      1440     1.42283     1.31185
2021-10-28T11:18:52.148771+0000     4     127       389       262   1047.87      1488      1.3853     1.34516
2021-10-28T11:18:53.148845+0000     5     127       484       357   1142.27      1520     1.35153     1.34983
2021-10-28T11:18:54.148941+0000 Total time run:       5.90698
Total reads made:     550
Read size:            16777216
Object size:          16777216
Bandwidth (MB/sec):   1489.76
Average IOPS:         93
Stddev IOPS:          42.4182
Max IOPS:             95
Min IOPS:             0
Average Latency(s):   1.22173
Max latency(s):       1.42684
Min latency(s):       0.22241

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T04:18:54,883226564-07:00] INFO: > Stop system activity collection services[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T04:18:54,889610524-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:391 - rados_bench() > kill -INT 346155

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T04:18:54,896379901-07:00] INFO: >> for OSD host[0m
# ./benchmarks/bench-rados:396 - rados_bench() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.2 kill -INT 197903
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T04:18:54,904622133-07:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.2 kill -INT 197903
[1] 04:18:55 [SUCCESS] ljishen@10.10.2.2

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T04:18:55,091203578-07:00] INFO: >>> Copy system activity file from OSD host[0m
## ./benchmarks/bench-rados:403 - rados_bench() > basename -- /tmp/bench-rados/sys_activity_16MB_seq.dat.osd_host.2
# ./benchmarks/bench-rados:403 - rados_bench() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.2:/tmp/bench-rados/sys_activity_16MB_seq.dat.osd_host.2 /mnt/sda8/ceph-research/scripts/output/sys_activity_16MB_seq.dat.osd_host.2


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T04:18:55,250469278-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T04:19:19,395479915-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 551 objects, 8.6 GiB
    usage:   26 GiB used, 74 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T04:19:19,403675329-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T04:19:28,761886497-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 551 objects, 8.6 GiB
    usage:   26 GiB used, 74 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T04:19:28,769626753-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T04:19:38,117340117-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 551 objects, 8.6 GiB
    usage:   26 GiB used, 74 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T04:19:38,125658763-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T04:19:47,565628540-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 551 objects, 8.6 GiB
    usage:   26 GiB used, 74 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T04:19:47,574484710-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T04:19:56,839484994-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 551 objects, 8.6 GiB
    usage:   26 GiB used, 74 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T04:19:56,848068338-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T04:19:56,854886517-07:00] INFO: > The cluster is idle now.[0m


[1;7;39;49m[2021-10-28T04:19:56,857468422-07:00][RUNNING][ROUND 3/7/21] object_size=16MB[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T04:19:56,861337243-07:00] STAGE: Launch a Ceph cluster (CEPH_HOST: 10.10.2.1, OSD_HOST: 10.10.2.2) ...[0m
# ./benchmarks/bench-rados:183 - launch_ceph_cluster() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.1 bash -euo pipefail
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T04:19:56,871211200-07:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.1 bash -euo pipefail
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T04:19:57,308665140-07:00] INFO: > Remove existing clusters\x1b[0m\n'
10.10.2.1: b'## bash:17 -  > basename -- /var/lib/ceph/c3210958-37df-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'# bash:17 -  > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm --host 10.10.2.1 --host 10.10.2.2 rm-cluster --force --fsid c3210958-37df-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T04:19:57,319154840-07:00] DEBUG: HOST_PARAMS: ['--host', '10.10.2.1', '--host', '10.10.2.2']\x1b[0m\n"
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T04:19:57,322916071-07:00] DEBUG: CEPHADM_PARAMS: ['rm-cluster', '--force', '--fsid', 'c3210958-37df-11ec-b51d-53e6e728d2d3']\x1b[0m\n"
10.10.2.1: b"# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:63 -  > /tmp/bench-rados/ceph-research/scripts/parallel-ssh --host 10.10.2.1 --host 10.10.2.2 'sudo python3 -u - rm-cluster --force --fsid c3210958-37df-11ec-b51d-53e6e728d2d3'\n"
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T04:19:57,331576302-07:00] DEBUG: command from stdin\x1b[0m\n'
10.10.2.1: b"# /tmp/bench-rados/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host 10.10.2.1 --host 10.10.2.2 'sudo python3 -u - rm-cluster --force --fsid c3210958-37df-11ec-b51d-53e6e728d2d3'\n"
10.10.2.1: b'[1] 04:20:03 [SUCCESS] 10.10.2.1\n[2] 04:20:10 [SUCCESS] 10.10.2.2\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T04:20:10,176118673-07:00] INFO: > Deploy a new cluster\x1b[0m\n'
10.10.2.1: b'# bash:24 -  > /tmp/bench-rados/ceph-research/scripts/ceph-deploy -m 10.10.2.1 -o node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/10.10.2.2:/dev/nvme0n1\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T04:20:10,187514898-07:00] STAGE: Verify passwordless SSH to hosts, passwordless sudo via ljishen, and the existence of devices on hosts...\x1b[0m\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T04:20:10,191922373-07:00] INFO: > Check the host for the monitor\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:181 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@10.10.2.1 sudo --non-interactive --validate\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T04:20:10,343033824-07:00] INFO: > Check hosts for OSDs\x1b[0m\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T04:20:10,347482828-07:00] INFO: >> Check host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:200 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us sudo --non-interactive --validate\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T04:20:10,498946832-07:00] INFO: >>> Check device '/dev/nvme0n1'\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:215 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us test -b /dev/nvme0n1\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:226 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us lsblk --nodeps --noheadings --output TYPE /dev/nvme0n1\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:227 - verify_parameters() > grep --word-regexp disk\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T04:20:10,783192636-07:00] STAGE: Reset devices on hosts...\x1b[0m\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T04:20:10,788168911-07:00] INFO: > Resetting device '/dev/nvme0n1' on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:288 - reset_host_devices() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us sudo sh\n'
10.10.2.1: b'  Removing ceph--1132f0d4--fd24--45a0--8c83--81f53501590b-osd--block--8fb1d3fb--2ded--4e77--82f9--bca1740d1065 (253:0)\n'
10.10.2.1: b'  Archiving volume group "ceph-1132f0d4-fd24-45a0-8c83-81f53501590b" metadata (seqno 5).\n'
10.10.2.1: b'  Releasing logical volume "osd-block-8fb1d3fb-2ded-4e77-82f9-bca1740d1065"\n'
10.10.2.1: b'  Creating volume group backup "/etc/lvm/backup/ceph-1132f0d4-fd24-45a0-8c83-81f53501590b" (seqno 6).\n'
10.10.2.1: b'  Logical volume "osd-block-8fb1d3fb-2ded-4e77-82f9-bca1740d1065" successfully removed\n'
10.10.2.1: b'  Removing physical volume "/dev/nvme0n1" from volume group "ceph-1132f0d4-fd24-45a0-8c83-81f53501590b"\n'
10.10.2.1: b'  Volume group "ceph-1132f0d4-fd24-45a0-8c83-81f53501590b" successfully removed\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T04:20:11,133991034-07:00] STAGE: Bootstrap a new cluster...\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:317 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.1 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T04:20:11,144071204-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T04:20:11,148337394-07:00] DEBUG: CEPHADM_PARAMS: ['bootstrap', '--config', '/tmp/bench-rados/ceph-research/scripts/ceph.conf', '--mon-ip', '10.10.2.1', '--output-dir', '/tmp/bench-rados/deployment_data_root/etc/ceph', '--ssh-user', 'ljishen', '--skip-dashboard', '--no-minimize-config', '--allow-overwrite', '--allow-fqdn-hostname', '--orphan-initial-daemons', '--skip-monitoring-stack', '--single-host-defaults']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.1 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults\n'
10.10.2.1: b'Verifying podman|docker is present...\n'
10.10.2.1: b'Verifying lvm2 is present...\n'
10.10.2.1: b'Verifying time synchronization is in place...\n'
10.10.2.1: b'Unit ntp.service is enabled and running\n'
10.10.2.1: b'Repeating the final host check...\n'
10.10.2.1: b'podman|docker (/usr/bin/docker) is present\n'
10.10.2.1: b'systemctl is present\n'
10.10.2.1: b'lvcreate is present\n'
10.10.2.1: b'Unit ntp.service is enabled and running\n'
10.10.2.1: b'Host looks OK\n'
10.10.2.1: b'Cluster fsid: 03dd21ce-37e1-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Verifying IP 10.10.2.1 port 3300 ...\n'
10.10.2.1: b'Verifying IP 10.10.2.1 port 6789 ...\n'
10.10.2.1: b'Mon IP `10.10.2.1` is in CIDR network `10.10.2.0/24`\n'
10.10.2.1: b'- internal network (--cluster-network) has not been provided, OSD replication will default to the public_network\n'
10.10.2.1: b'Adjusting default settings to suit single-host cluster...\n'
10.10.2.1: b'Pulling container image quay.io/ceph/ceph:v16...\n'
10.10.2.1: b'Ceph version: ceph version 16.2.6 (ee28fb57e47e9f88813e24bbf4c14496ca299d31) pacific (stable)\n'
10.10.2.1: b'Extracting ceph user uid/gid from container image...\n'
10.10.2.1: b'Creating initial keys...\n'
10.10.2.1: b'Creating initial monmap...\n'
10.10.2.1: b'Creating mon...\n'
10.10.2.1: b'Waiting for mon to start...\n'
10.10.2.1: b'Waiting for mon...\n'
10.10.2.1: b'mon is available\nSetting mon public_network to 10.10.2.0/24\n'
10.10.2.1: b'Wrote config to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf\n'
10.10.2.1: b'Wrote keyring to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring\n'
10.10.2.1: b'Creating mgr...\n'
10.10.2.1: b'Verifying port 9283 ...\n'
10.10.2.1: b'Waiting for mgr to start...\nWaiting for mgr...\n'
10.10.2.1: b'mgr not available, waiting (1/15)...\n'
10.10.2.1: b'mgr not available, waiting (2/15)...\n'
10.10.2.1: b'mgr is available\n'
10.10.2.1: b'Enabling cephadm module...\n'
10.10.2.1: b'Waiting for the mgr to restart...\n'
10.10.2.1: b'Waiting for mgr epoch 5...\n'
10.10.2.1: b'mgr epoch 5 is available\nSetting orchestrator backend to cephadm...\n'
10.10.2.1: b'Generating ssh key...\n'
10.10.2.1: b'Wrote public SSH key to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub\n'
10.10.2.1: b'Adding key to ljishen@localhost authorized_keys...\n'
10.10.2.1: b'Adding host node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us...\n'
10.10.2.1: b'Deploying unmanaged mon service...\n'
10.10.2.1: b'Deploying unmanaged mgr service...\n'
10.10.2.1: b'You can access the Ceph CLI with:\n\n\tsudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --fsid 03dd21ce-37e1-11ec-b51d-53e6e728d2d3 -c /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf -k /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring\n\n'
10.10.2.1: b'Please consider enabling telemetry to help improve Ceph:\n\n\tceph telemetry on\n\nFor more information see:\n\n\thttps://docs.ceph.com/docs/pacific/mgr/telemetry/\n\n'
10.10.2.1: b'Bootstrap complete.\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T04:21:11,823306901-07:00] INFO: Sleep for 20 seconds\x1b[0m\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T04:21:31,830419409-07:00] INFO: > Set the default number of monitor daemons to 1\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:337 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T04:21:31,840914379-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T04:21:31,844672243-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mon', '1']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1\n'
10.10.2.1: b'Inferring fsid 03dd21ce-37e1-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/03dd21ce-37e1-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b'Scheduled mon update...\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T04:21:40,646872229-07:00] INFO: > Set the default number of manager daemons to 1\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:342 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T04:21:40,657273693-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T04:21:40,660890252-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mgr', '1']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1\n'
10.10.2.1: b'Inferring fsid 03dd21ce-37e1-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/03dd21ce-37e1-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b'Scheduled mgr update...\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T04:21:49,916785264-07:00] STAGE: Add hosts to the cluster...\x1b[0m\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T04:21:49,923356417-07:00] INFO: > Adding host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:361 - add_hosts() > ssh-copy-id -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -f -i /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\n'
10.10.2.1: b'/usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: "/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub"\n'
10.10.2.1: b'\nNumber of key(s) added: 1\n\nNow try logging into the machine, with:   "ssh -o \'GlobalKnownHostsFile=/dev/null\' -o \'LogLevel=ERROR\' -o \'PasswordAuthentication=no\' -o \'StrictHostKeyChecking=no\' -o \'UserKnownHostsFile=/dev/null\' \'ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\'"\nand check to make sure that only the key(s) you wanted were added.\n\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:367 - add_hosts() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us 10.10.2.2\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T04:21:50,136781271-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T04:21:50,140550036-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'host', 'add', 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us', '10.10.2.2']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us 10.10.2.2\n'
10.10.2.1: b'Inferring fsid 03dd21ce-37e1-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/03dd21ce-37e1-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b"Added host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us' with addr '10.10.2.2'\n"
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T04:21:59,669812438-07:00] INFO: Sleep for 20 seconds\x1b[0m\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T04:22:19,674706128-07:00] STAGE: Add OSDs to the cluster...\x1b[0m\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T04:22:19,681246614-07:00] INFO: > Adding devices ['/dev/nvme0n1'] on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us' as OSDs\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:388 - add_host_devices_as_osds() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T04:22:19,691557738-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T04:22:19,694845569-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'daemon', 'add', 'osd', 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1\n'
10.10.2.1: b'Inferring fsid 03dd21ce-37e1-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/03dd21ce-37e1-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b"Created osd(s) 0 on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'\n"
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T04:22:43,296865258-07:00] INFO: > Sleep for 20 seconds\x1b[0m\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T04:23:03,302379580-07:00] STAGE: Show Cluster Status\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:408 - show_status() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T04:23:03,312444451-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T04:23:03,316277216-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', '--status']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status\n'
10.10.2.1: b'Inferring fsid 03dd21ce-37e1-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/03dd21ce-37e1-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b'  cluster:\n    id:     03dd21ce-37e1-11ec-b51d-53e6e728d2d3\n    health: HEALTH_WARN\n            1 pool(s) have no replicas configured\n \n  services:\n    mon: 1 daemons, quorum node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us (age 2m)\n    mgr: node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us.shhkxh(active, since 2m)\n    osd: 1 osds: 1 up (since 17s), 1 in (since 39s)\n \n  data:\n    pools:   1 pools, 1 pgs\n    objects: 0 objects, 0 B\n    usage:   4.7 MiB used, 100 GiB / 100 GiB avail\n    pgs:     1 active+clean\n \n'
[1] 04:23:11 [SUCCESS] ljishen@10.10.2.1

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T04:19:57,308665140-07:00] INFO: > Remove existing clusters[0m
## bash:17 -  > basename -- /var/lib/ceph/c3210958-37df-11ec-b51d-53e6e728d2d3
# bash:17 -  > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm --host 10.10.2.1 --host 10.10.2.2 rm-cluster --force --fsid c3210958-37df-11ec-b51d-53e6e728d2d3
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T04:19:57,319154840-07:00] DEBUG: HOST_PARAMS: ['--host', '10.10.2.1', '--host', '10.10.2.2'][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T04:19:57,322916071-07:00] DEBUG: CEPHADM_PARAMS: ['rm-cluster', '--force', '--fsid', 'c3210958-37df-11ec-b51d-53e6e728d2d3'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:63 -  > /tmp/bench-rados/ceph-research/scripts/parallel-ssh --host 10.10.2.1 --host 10.10.2.2 'sudo python3 -u - rm-cluster --force --fsid c3210958-37df-11ec-b51d-53e6e728d2d3'
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T04:19:57,331576302-07:00] DEBUG: command from stdin[0m
# /tmp/bench-rados/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host 10.10.2.1 --host 10.10.2.2 'sudo python3 -u - rm-cluster --force --fsid c3210958-37df-11ec-b51d-53e6e728d2d3'
[1] 04:20:03 [SUCCESS] 10.10.2.1
[2] 04:20:10 [SUCCESS] 10.10.2.2

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T04:20:10,176118673-07:00] INFO: > Deploy a new cluster[0m
# bash:24 -  > /tmp/bench-rados/ceph-research/scripts/ceph-deploy -m 10.10.2.1 -o node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/10.10.2.2:/dev/nvme0n1


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T04:20:10,187514898-07:00] STAGE: Verify passwordless SSH to hosts, passwordless sudo via ljishen, and the existence of devices on hosts...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T04:20:10,191922373-07:00] INFO: > Check the host for the monitor[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:181 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@10.10.2.1 sudo --non-interactive --validate

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T04:20:10,343033824-07:00] INFO: > Check hosts for OSDs[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T04:20:10,347482828-07:00] INFO: >> Check host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:200 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us sudo --non-interactive --validate

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T04:20:10,498946832-07:00] INFO: >>> Check device '/dev/nvme0n1'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:215 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us test -b /dev/nvme0n1
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:226 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us lsblk --nodeps --noheadings --output TYPE /dev/nvme0n1
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:227 - verify_parameters() > grep --word-regexp disk


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T04:20:10,783192636-07:00] STAGE: Reset devices on hosts...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T04:20:10,788168911-07:00] INFO: > Resetting device '/dev/nvme0n1' on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:288 - reset_host_devices() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us sudo sh
  Removing ceph--1132f0d4--fd24--45a0--8c83--81f53501590b-osd--block--8fb1d3fb--2ded--4e77--82f9--bca1740d1065 (253:0)
  Archiving volume group "ceph-1132f0d4-fd24-45a0-8c83-81f53501590b" metadata (seqno 5).
  Releasing logical volume "osd-block-8fb1d3fb-2ded-4e77-82f9-bca1740d1065"
  Creating volume group backup "/etc/lvm/backup/ceph-1132f0d4-fd24-45a0-8c83-81f53501590b" (seqno 6).
  Logical volume "osd-block-8fb1d3fb-2ded-4e77-82f9-bca1740d1065" successfully removed
  Removing physical volume "/dev/nvme0n1" from volume group "ceph-1132f0d4-fd24-45a0-8c83-81f53501590b"
  Volume group "ceph-1132f0d4-fd24-45a0-8c83-81f53501590b" successfully removed


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T04:20:11,133991034-07:00] STAGE: Bootstrap a new cluster...[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:317 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.1 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T04:20:11,144071204-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T04:20:11,148337394-07:00] DEBUG: CEPHADM_PARAMS: ['bootstrap', '--config', '/tmp/bench-rados/ceph-research/scripts/ceph.conf', '--mon-ip', '10.10.2.1', '--output-dir', '/tmp/bench-rados/deployment_data_root/etc/ceph', '--ssh-user', 'ljishen', '--skip-dashboard', '--no-minimize-config', '--allow-overwrite', '--allow-fqdn-hostname', '--orphan-initial-daemons', '--skip-monitoring-stack', '--single-host-defaults'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.1 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults
Verifying podman|docker is present...
Verifying lvm2 is present...
Verifying time synchronization is in place...
Unit ntp.service is enabled and running
Repeating the final host check...
podman|docker (/usr/bin/docker) is present
systemctl is present
lvcreate is present
Unit ntp.service is enabled and running
Host looks OK
Cluster fsid: 03dd21ce-37e1-11ec-b51d-53e6e728d2d3
Verifying IP 10.10.2.1 port 3300 ...
Verifying IP 10.10.2.1 port 6789 ...
Mon IP `10.10.2.1` is in CIDR network `10.10.2.0/24`
- internal network (--cluster-network) has not been provided, OSD replication will default to the public_network
Adjusting default settings to suit single-host cluster...
Pulling container image quay.io/ceph/ceph:v16...
Ceph version: ceph version 16.2.6 (ee28fb57e47e9f88813e24bbf4c14496ca299d31) pacific (stable)
Extracting ceph user uid/gid from container image...
Creating initial keys...
Creating initial monmap...
Creating mon...
Waiting for mon to start...
Waiting for mon...
mon is available
Setting mon public_network to 10.10.2.0/24
Wrote config to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf
Wrote keyring to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring
Creating mgr...
Verifying port 9283 ...
Waiting for mgr to start...
Waiting for mgr...
mgr not available, waiting (1/15)...
mgr not available, waiting (2/15)...
mgr is available
Enabling cephadm module...
Waiting for the mgr to restart...
Waiting for mgr epoch 5...
mgr epoch 5 is available
Setting orchestrator backend to cephadm...
Generating ssh key...
Wrote public SSH key to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub
Adding key to ljishen@localhost authorized_keys...
Adding host node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us...
Deploying unmanaged mon service...
Deploying unmanaged mgr service...
You can access the Ceph CLI with:

	sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --fsid 03dd21ce-37e1-11ec-b51d-53e6e728d2d3 -c /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf -k /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring

Please consider enabling telemetry to help improve Ceph:

	ceph telemetry on

For more information see:

	https://docs.ceph.com/docs/pacific/mgr/telemetry/

Bootstrap complete.

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T04:21:11,823306901-07:00] INFO: Sleep for 20 seconds[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T04:21:31,830419409-07:00] INFO: > Set the default number of monitor daemons to 1[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:337 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T04:21:31,840914379-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T04:21:31,844672243-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mon', '1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1
Inferring fsid 03dd21ce-37e1-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/03dd21ce-37e1-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Scheduled mon update...

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T04:21:40,646872229-07:00] INFO: > Set the default number of manager daemons to 1[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:342 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T04:21:40,657273693-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T04:21:40,660890252-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mgr', '1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1
Inferring fsid 03dd21ce-37e1-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/03dd21ce-37e1-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Scheduled mgr update...


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T04:21:49,916785264-07:00] STAGE: Add hosts to the cluster...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T04:21:49,923356417-07:00] INFO: > Adding host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:361 - add_hosts() > ssh-copy-id -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -f -i /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us
/usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: "/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub"

Number of key(s) added: 1

Now try logging into the machine, with:   "ssh -o 'GlobalKnownHostsFile=/dev/null' -o 'LogLevel=ERROR' -o 'PasswordAuthentication=no' -o 'StrictHostKeyChecking=no' -o 'UserKnownHostsFile=/dev/null' 'ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'"
and check to make sure that only the key(s) you wanted were added.

# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:367 - add_hosts() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us 10.10.2.2
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T04:21:50,136781271-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T04:21:50,140550036-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'host', 'add', 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us', '10.10.2.2'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us 10.10.2.2
Inferring fsid 03dd21ce-37e1-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/03dd21ce-37e1-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Added host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us' with addr '10.10.2.2'

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T04:21:59,669812438-07:00] INFO: Sleep for 20 seconds[0m


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T04:22:19,674706128-07:00] STAGE: Add OSDs to the cluster...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T04:22:19,681246614-07:00] INFO: > Adding devices ['/dev/nvme0n1'] on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us' as OSDs[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:388 - add_host_devices_as_osds() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T04:22:19,691557738-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T04:22:19,694845569-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'daemon', 'add', 'osd', 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1
Inferring fsid 03dd21ce-37e1-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/03dd21ce-37e1-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Created osd(s) 0 on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T04:22:43,296865258-07:00] INFO: > Sleep for 20 seconds[0m


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T04:23:03,302379580-07:00] STAGE: Show Cluster Status[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:408 - show_status() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T04:23:03,312444451-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T04:23:03,316277216-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', '--status'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status
Inferring fsid 03dd21ce-37e1-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/03dd21ce-37e1-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
  cluster:
    id:     03dd21ce-37e1-11ec-b51d-53e6e728d2d3
    health: HEALTH_WARN
            1 pool(s) have no replicas configured
 
  services:
    mon: 1 daemons, quorum node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us (age 2m)
    mgr: node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us.shhkxh(active, since 2m)
    osd: 1 osds: 1 up (since 17s), 1 in (since 39s)
 
  data:
    pools:   1 pools, 1 pgs
    objects: 0 objects, 0 B
    usage:   4.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     1 active+clean
 

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T04:23:11,986814577-07:00] INFO: > Copy configuration files to local[0m
# ./benchmarks/bench-rados:220 - launch_ceph_cluster() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.1 sudo chown --changes --recursive ljishen /tmp/bench-rados/deployment_data_root/etc/ceph
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T04:23:11,995023436-07:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.1 sudo chown --changes --recursive ljishen /tmp/bench-rados/deployment_data_root/etc/ceph
[1] 04:23:12 [SUCCESS] ljishen@10.10.2.1
# ./benchmarks/bench-rados:223 - launch_ceph_cluster() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.1:/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf
# ./benchmarks/bench-rados:226 - launch_ceph_cluster() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.1:/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T04:23:12,469460372-07:00] STAGE: Update local ceph.conf as a client...[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T04:23:12,473264691-07:00] STAGE: Configure Ceph pool...[0m
# ./benchmarks/bench-rados:257 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_max_backfills 32
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T04:23:12,495243018-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T04:23:12,498152361-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '03dd21ce-37e1-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'config', 'set', 'osd', 'osd_max_backfills', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 03dd21ce-37e1-11ec-b51d-53e6e728d2d3 -- ceph config set osd osd_max_backfills 32
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:258 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_active 32
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T04:23:16,943300028-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T04:23:16,946265375-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '03dd21ce-37e1-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_active', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 03dd21ce-37e1-11ec-b51d-53e6e728d2d3 -- ceph config set osd osd_recovery_max_active 32
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:259 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_single_start 8
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T04:23:21,300109504-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T04:23:21,303176384-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '03dd21ce-37e1-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_single_start', '8'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 03dd21ce-37e1-11ec-b51d-53e6e728d2d3 -- ceph config set osd osd_recovery_max_single_start 8
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:260 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_op_priority 63
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T04:23:25,667282231-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T04:23:25,670347727-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '03dd21ce-37e1-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_op_priority', '63'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 03dd21ce-37e1-11ec-b51d-53e6e728d2d3 -- ceph config set osd osd_recovery_op_priority 63
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:262 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config show-with-defaults osd.0
# ./benchmarks/bench-rados:263 - configure_ceph_pool() > grep 'osd_max_backfills\|osd_recovery'
# ./benchmarks/bench-rados:263 - configure_ceph_pool() > column -t -s ' '
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
osd_max_backfills                        32         mon
osd_recovery_cost                        20971520   default
osd_recovery_delay_start                 0.000000   default
osd_recovery_max_active                  32         mon
osd_recovery_max_active_hdd              3          default
osd_recovery_max_active_ssd              10         default
osd_recovery_max_chunk                   8388608    default
osd_recovery_max_omap_entries_per_chunk  8096       default
osd_recovery_max_single_start            8          mon
osd_recovery_op_priority                 3          default
osd_recovery_op_warn_multiple            16         default
osd_recovery_priority                    5          default
osd_recovery_retry_interval              30.000000  default
osd_recovery_sleep                       0.000000   default
osd_recovery_sleep_hdd                   0.100000   default
osd_recovery_sleep_hybrid                0.025000   default
osd_recovery_sleep_ssd                   0.000000   default
# ./benchmarks/bench-rados:265 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool create bench_rados 128 128 replicated --size 1 --pg-num-min 128
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T04:23:34,283286043-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T04:23:34,286599637-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '03dd21ce-37e1-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'create', 'bench_rados', '128', '128', 'replicated', '--size', '1', '--pg-num-min', '128'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 03dd21ce-37e1-11ec-b51d-53e6e728d2d3 -- ceph osd pool create bench_rados 128 128 replicated --size 1 --pg-num-min 128
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
pool 'bench_rados' created
# ./benchmarks/bench-rados:267 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados min_size 1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T04:23:38,781681452-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T04:23:38,785128228-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '03dd21ce-37e1-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'min_size', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 03dd21ce-37e1-11ec-b51d-53e6e728d2d3 -- ceph osd pool set bench_rados min_size 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 min_size to 1
# ./benchmarks/bench-rados:268 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados pg_autoscale_mode off
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T04:23:44,332147742-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T04:23:44,335240671-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '03dd21ce-37e1-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'pg_autoscale_mode', 'off'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 03dd21ce-37e1-11ec-b51d-53e6e728d2d3 -- ceph osd pool set bench_rados pg_autoscale_mode off
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 pg_autoscale_mode to off
# ./benchmarks/bench-rados:269 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool application enable bench_rados benchmark
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T04:23:48,907973555-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T04:23:48,911125856-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '03dd21ce-37e1-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'application', 'enable', 'bench_rados', 'benchmark'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 03dd21ce-37e1-11ec-b51d-53e6e728d2d3 -- ceph osd pool application enable bench_rados benchmark
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
enabled application 'benchmark' on pool 'bench_rados'
# ./benchmarks/bench-rados:270 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados noscrub 1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T04:23:54,241141727-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T04:23:54,244119458-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '03dd21ce-37e1-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'noscrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 03dd21ce-37e1-11ec-b51d-53e6e728d2d3 -- ceph osd pool set bench_rados noscrub 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 noscrub to 1
# ./benchmarks/bench-rados:271 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados nodeep-scrub 1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T04:23:58,988852539-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T04:23:58,991838486-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '03dd21ce-37e1-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'nodeep-scrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 03dd21ce-37e1-11ec-b51d-53e6e728d2d3 -- ceph osd pool set bench_rados nodeep-scrub 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 nodeep-scrub to 1
# ./benchmarks/bench-rados:272 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool ls detail
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T04:24:04,576901331-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T04:24:04,580051046-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '03dd21ce-37e1-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'ls', 'detail'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 03dd21ce-37e1-11ec-b51d-53e6e728d2d3 -- ceph osd pool ls detail
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
pool 1 'device_health_metrics' replicated size 1 min_size 1 crush_rule 0 object_hash rjenkins pg_num 64 pgp_num 64 autoscale_mode on last_change 18 lfor 0/0/16 flags hashpspool stripe_width 0 pg_num_min 1 application mgr_devicehealth
pool 2 'bench_rados' replicated size 1 min_size 1 crush_rule 0 object_hash rjenkins pg_num 128 pgp_num 128 autoscale_mode off last_change 20 flags hashpspool,noscrub,nodeep-scrub stripe_width 0 pg_num_min 128 application benchmark

# ./benchmarks/bench-rados:273 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd df tree
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T04:24:08,938700589-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T04:24:08,941584985-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '03dd21ce-37e1-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'df', 'tree'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 03dd21ce-37e1-11ec-b51d-53e6e728d2d3 -- ceph osd df tree
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
ID  CLASS  WEIGHT   REWEIGHT  SIZE     RAW USE  DATA     OMAP  META     AVAIL    %USE  VAR   PGS  STATUS  TYPE NAME      
-1         0.09769         -  100 GiB  5.7 MiB  168 KiB   0 B  5.6 MiB  100 GiB  0.01  1.00    -          root default   
-3         0.09769         -  100 GiB  5.7 MiB  168 KiB   0 B  5.6 MiB  100 GiB  0.01  1.00    -              host node-1
 0    ssd  0.09769   1.00000  100 GiB  5.7 MiB  168 KiB   0 B  5.6 MiB  100 GiB  0.01  1.00  192      up          osd.0  
                       TOTAL  100 GiB  5.7 MiB  168 KiB   0 B  5.6 MiB  100 GiB  0.01                                    
MIN/MAX VAR: 1.00/1.00  STDDEV: 0


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T04:24:13,109273390-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T04:24:37,405419274-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T04:24:46,802880473-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T04:24:56,062767329-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T04:24:56,071245415-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T04:25:05,601066620-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T04:25:05,609804356-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T04:25:14,885798243-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T04:25:14,893998095-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T04:25:24,349685206-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T04:25:24,357970519-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T04:25:33,719509693-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T04:25:33,727995154-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T04:25:33,734580864-07:00] INFO: > The cluster is idle now.[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T04:25:33,738577326-07:00] STAGE: Start benchmarking of mode write ...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T04:25:33,746139276-07:00] INFO: > Start system activity collection services[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T04:25:33,752276862-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:341 - rados_bench() > rm --force /mnt/sda8/ceph-research/scripts/output/sys_activity_16MB_write.dat.rados_bench_host.3
# ./benchmarks/bench-rados:342 - rados_bench() > sar_pid_rados_bench_host=351744
# ./benchmarks/bench-rados:342 - rados_bench() > S_TIME_FORMAT=ISO
# ./benchmarks/bench-rados:342 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/output/sys_activity_16MB_write.dat.rados_bench_host.3 2

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T04:25:33,759833472-07:00] INFO: >> for OSD host[0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T04:25:33,769343683-07:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.2 bash
10.10.2.2: b'201509\n'
[1] 04:25:33 [SUCCESS] ljishen@10.10.2.2
201509

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T04:25:33,963427716-07:00] INFO: > Run rados bench[0m
# ./benchmarks/bench-rados:370 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/output/rados_bench_16MB_write.log.3
## ./benchmarks/bench-rados:361 - rados_bench() > count_cpus 32-63,96-127
# ./benchmarks/bench-rados:361 - rados_bench() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 60 write --pool bench_rados -b 16777216 -O 16777216 --concurrent-ios 128 --show-time --write-object --write-omap --write-xattr --no-cleanup
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T04:25:33,984310028-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T04:25:33,987207217-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '03dd21ce-37e1-11ec-b51d-53e6e728d2d3', '--', 'rados', 'bench', '60', 'write', '--pool', 'bench_rados', '-b', '16777216', '-O', '16777216', '--concurrent-ios', '128', '--show-time', '--write-object', '--write-omap', '--write-xattr', '--no-cleanup'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 03dd21ce-37e1-11ec-b51d-53e6e728d2d3 -- rados bench 60 write --pool bench_rados -b 16777216 -O 16777216 --concurrent-ios 128 --show-time --write-object --write-omap --write-xattr --no-cleanup
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
hints = 1
2021-10-28T11:25:37.244868+0000 Maintaining 128 concurrent writes of 16777216 bytes to objects of size 16777216 for up to 60 seconds or 0 objects
2021-10-28T11:25:37.244906+0000 Object prefix: benchmark_data_node-2.bluefield2.ucsc-cmps10_7
2021-10-28T11:25:37.920994+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-28T11:25:37.920994+0000     0       0         0         0         0         0           -           0
2021-10-28T11:25:38.921108+0000     1      19        19         0         0         0           -           0
2021-10-28T11:25:39.921182+0000     2      35        35         0         0         0           -           0
2021-10-28T11:25:40.921254+0000     3      45        45         0         0         0           -           0
2021-10-28T11:25:41.921329+0000     4      65        65         0         0         0           -           0
2021-10-28T11:25:42.921403+0000     5      70        70         0         0         0           -           0
2021-10-28T11:25:43.921480+0000     6      78        78         0         0         0           -           0
2021-10-28T11:25:44.921549+0000     7      89        89         0         0         0           -           0
2021-10-28T11:25:45.921617+0000     8      99        99         0         0         0           -           0
2021-10-28T11:25:46.921687+0000     9     107       107         0         0         0           -           0
2021-10-28T11:25:47.921758+0000    10     125       125         0         0         0           -           0
2021-10-28T11:25:48.921836+0000    11     127       131         4   5.81777   5.81818     10.6649     10.4494
2021-10-28T11:25:49.921903+0000    12     127       138        11   14.6656       112     11.4327     10.9649
2021-10-28T11:25:50.921977+0000    13     127       149        22    27.075       176     11.8523     11.3114
2021-10-28T11:25:51.922052+0000    14     127       159        32   36.5688       160     12.3437     11.5491
2021-10-28T11:25:52.922128+0000    15     127       165        38   40.5304        96      12.756     11.7095
2021-10-28T11:25:53.922203+0000    16     127       182        55   54.9961       272     12.5567     11.9756
2021-10-28T11:25:54.922274+0000    17     127       194        67   63.0543       192     12.7835     12.0965
2021-10-28T11:25:55.922345+0000    18     127       196        69    61.329        32     13.2561     12.1271
2021-10-28T11:25:56.922414+0000    19     127       207        80   67.3636       176     12.9241      12.244
2021-10-28T11:25:57.922485+0000 min lat: 10.2926 max lat: 13.4321 avg lat: 12.3226
2021-10-28T11:25:57.922485+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-28T11:25:57.922485+0000    20     127       218        91   72.7948       176     12.8715     12.3226
2021-10-28T11:25:58.922560+0000    21     127       226        99   75.4232       128     13.3147     12.3873
2021-10-28T11:25:59.922630+0000    22     127       229       102   74.1765        48     13.5057     12.4214
2021-10-28T11:26:00.922705+0000    23     127       240       113   78.6031       176     13.6959     12.5306
2021-10-28T11:26:01.922781+0000    24     127       252       125   83.3274       192     14.1274     12.6659
2021-10-28T11:26:02.922856+0000    25     127       258       131    83.834        96     14.2027      12.736
2021-10-28T11:26:03.922933+0000    26     127       263       136   83.6863        80     14.3319     12.7915
2021-10-28T11:26:04.923006+0000    27     127       274       147   87.1048       176     14.3025     12.9054
2021-10-28T11:26:05.923077+0000    28     127       285       158   90.2792       176      14.316     13.0027
2021-10-28T11:26:06.923144+0000    29     127       290       163   89.9246        80     14.3972     13.0458
2021-10-28T11:26:07.923214+0000    30     127       296       169   90.1269        96     14.6653     13.1015
2021-10-28T11:26:08.923281+0000    31     127       307       180   92.8966       176     15.1574      13.213
2021-10-28T11:26:09.923346+0000    32     127       317       190   94.9932       160     15.4351     13.3236
2021-10-28T11:26:10.923421+0000    33     127       323       196   95.0235        96     15.6305     13.3942
2021-10-28T11:26:11.923497+0000    34     127       329       202    95.052        96     15.5289      13.458
2021-10-28T11:26:12.923570+0000    35     127       341       214   97.8216       192     15.4604     13.5709
2021-10-28T11:26:13.923644+0000    36     127       350       223    99.104       144      15.491     13.6472
2021-10-28T11:26:14.923716+0000    37     127       356       229   99.0199        96     15.3267     13.6942
2021-10-28T11:26:15.923791+0000    38     127       364       237   99.7823       128     15.4114     13.7523
2021-10-28T11:26:16.923865+0000    39     127       375       248   101.736       176     15.4209      13.827
2021-10-28T11:26:17.923936+0000 min lat: 10.2926 max lat: 15.6485 avg lat: 13.8889
2021-10-28T11:26:17.923936+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-28T11:26:17.923936+0000    40     127       385       258   103.193       160     15.4017     13.8889
2021-10-28T11:26:18.924012+0000    41     127       388       261   101.846        48     15.4673     13.9064
2021-10-28T11:26:19.924086+0000    42     127       397       270    102.85       144     15.4511     13.9577
2021-10-28T11:26:20.924160+0000    43     127       408       281   104.551       176     15.4353     14.0158
2021-10-28T11:26:21.924241+0000    44     127       418       291   105.811       160     15.3369     14.0626
2021-10-28T11:26:22.924313+0000    45     127       421       294   104.526        48     15.2281     14.0746
2021-10-28T11:26:23.924386+0000    46     127       431       304   105.731       160     15.3631     14.1166
2021-10-28T11:26:24.924464+0000    47     127       443       316   107.567       192     15.3782      14.164
2021-10-28T11:26:25.924534+0000    48     127       450       323   107.659       112     15.2064     14.1874
2021-10-28T11:26:26.924607+0000    49     127       453       326   106.441        48     15.1933     14.1966
2021-10-28T11:26:27.924684+0000    50     127       464       337   107.832       176     15.4406     14.2361
2021-10-28T11:26:28.924759+0000    51     127       475       348   109.169       176     15.4436     14.2743
2021-10-28T11:26:29.924828+0000    52     127       483       356   109.531       128     15.2499     14.2975
2021-10-28T11:26:30.924900+0000    53     127       486       359    108.37        48     15.4996     14.3068
2021-10-28T11:26:31.924973+0000    54     127       497       370   109.622       176     15.4734     14.3421
2021-10-28T11:26:32.925046+0000    55     127       508       381   110.828       176     15.5079     14.3752
2021-10-28T11:26:33.925118+0000    56     127       515       388   110.849       112     15.3602     14.3936
2021-10-28T11:26:34.925189+0000    57     127       519       392   110.027        64      15.528     14.4045
2021-10-28T11:26:35.925263+0000    58     127       530       403   111.164       176     15.5697     14.4361
2021-10-28T11:26:36.925335+0000    59     127       541       414   112.263       176     15.5866     14.4664
2021-10-28T11:26:37.925409+0000 min lat: 10.2926 max lat: 15.6485 avg lat: 14.4803
2021-10-28T11:26:37.925409+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-28T11:26:37.925409+0000    60     127       547       420   111.992        96     15.4735     14.4803
2021-10-28T11:26:38.925523+0000 Total time run:         60.6739
Total writes made:      548
Write size:             16777216
Object size:            16777216
Bandwidth (MB/sec):     144.51
Stddev Bandwidth:       70.9091
Max bandwidth (MB/sec): 272
Min bandwidth (MB/sec): 0
Average IOPS:           9
Stddev IOPS:            4.4412
Max IOPS:               17
Min IOPS:               0
Average Latency(s):     12.9242
Stddev Latency(s):      3.73012
Max latency(s):         15.6485
Min latency(s):         0.724707

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T04:26:39,713499210-07:00] INFO: > Stop system activity collection services[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T04:26:39,720442344-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:391 - rados_bench() > kill -INT 351744

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T04:26:39,727270181-07:00] INFO: >> for OSD host[0m
# ./benchmarks/bench-rados:396 - rados_bench() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.2 kill -INT 201509
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T04:26:39,735722910-07:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.2 kill -INT 201509
[1] 04:26:39 [SUCCESS] ljishen@10.10.2.2

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T04:26:39,921555573-07:00] INFO: >>> Copy system activity file from OSD host[0m
## ./benchmarks/bench-rados:403 - rados_bench() > basename -- /tmp/bench-rados/sys_activity_16MB_write.dat.osd_host.3
# ./benchmarks/bench-rados:403 - rados_bench() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.2:/tmp/bench-rados/sys_activity_16MB_write.dat.osd_host.3 /mnt/sda8/ceph-research/scripts/output/sys_activity_16MB_write.dat.osd_host.3


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T04:26:40,096522130-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T04:27:04,285387681-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 549 objects, 8.6 GiB
    usage:   26 GiB used, 74 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T04:27:04,294390647-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T04:27:13,693701091-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 549 objects, 8.6 GiB
    usage:   26 GiB used, 74 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T04:27:13,702086182-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T04:27:23,136420707-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 549 objects, 8.6 GiB
    usage:   26 GiB used, 74 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T04:27:23,145016735-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T04:27:32,418727260-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 549 objects, 8.6 GiB
    usage:   26 GiB used, 74 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T04:27:32,427512365-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T04:27:41,778511828-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 549 objects, 8.6 GiB
    usage:   26 GiB used, 74 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T04:27:41,787305389-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T04:27:41,794262629-07:00] INFO: > The cluster is idle now.[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T04:27:41,798601465-07:00] STAGE: Start benchmarking of mode seq ...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T04:27:41,806242475-07:00] INFO: > Start system activity collection services[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T04:27:41,813012963-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:341 - rados_bench() > rm --force /mnt/sda8/ceph-research/scripts/output/sys_activity_16MB_seq.dat.rados_bench_host.3
# ./benchmarks/bench-rados:342 - rados_bench() > sar_pid_rados_bench_host=353109
# ./benchmarks/bench-rados:342 - rados_bench() > S_TIME_FORMAT=ISO
# ./benchmarks/bench-rados:342 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/output/sys_activity_16MB_seq.dat.rados_bench_host.3 2

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T04:27:41,820832248-07:00] INFO: >> for OSD host[0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T04:27:41,830392023-07:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.2 bash
10.10.2.2: b'201605\n'
[1] 04:27:42 [SUCCESS] ljishen@10.10.2.2
201605

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T04:27:42,023759526-07:00] INFO: > Run rados bench[0m
# ./benchmarks/bench-rados:379 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/output/rados_bench_16MB_seq.log.3
## ./benchmarks/bench-rados:376 - rados_bench() > count_cpus 32-63,96-127
# ./benchmarks/bench-rados:376 - rados_bench() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 128 --show-time
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T04:27:42,044139361-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T04:27:42,046929058-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '03dd21ce-37e1-11ec-b51d-53e6e728d2d3', '--', 'rados', 'bench', '99999999', 'seq', '--pool', 'bench_rados', '--concurrent-ios', '128', '--show-time'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 03dd21ce-37e1-11ec-b51d-53e6e728d2d3 -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 128 --show-time
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
hints = 1
2021-10-28T11:27:45.472478+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-28T11:27:45.472478+0000     0       0         0         0         0         0           -           0
2021-10-28T11:27:46.472574+0000     1     120       120         0         0         0           -           0
2021-10-28T11:27:47.472645+0000     2     127       231       104   831.885       832     1.15046     1.10514
2021-10-28T11:27:48.472727+0000     3     127       337       210   1119.87      1696      1.2061      1.1457
2021-10-28T11:27:49.472805+0000     4     127       444       317   1267.86      1712      1.1998     1.16687
2021-10-28T11:27:50.654687+0000     5       7       548       541    1670.3      3584    0.269271     1.07734
2021-10-28T11:27:51.654782+0000 Total time run:       5.19114
Total reads made:     548
Read size:            16777216
Object size:          16777216
Bandwidth (MB/sec):   1689.03
Average IOPS:         105
Stddev IOPS:          83.2778
Max IOPS:             224
Min IOPS:             0
Average Latency(s):   1.06672
Max latency(s):       1.21982
Min latency(s):       0.222527

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T04:27:52,382385543-07:00] INFO: > Stop system activity collection services[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T04:27:52,389568730-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:391 - rados_bench() > kill -INT 353109

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T04:27:52,396396907-07:00] INFO: >> for OSD host[0m
# ./benchmarks/bench-rados:396 - rados_bench() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.2 kill -INT 201605
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T04:27:52,404799782-07:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.2 kill -INT 201605
[1] 04:27:52 [SUCCESS] ljishen@10.10.2.2

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T04:27:52,590297544-07:00] INFO: >>> Copy system activity file from OSD host[0m
## ./benchmarks/bench-rados:403 - rados_bench() > basename -- /tmp/bench-rados/sys_activity_16MB_seq.dat.osd_host.3
# ./benchmarks/bench-rados:403 - rados_bench() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.2:/tmp/bench-rados/sys_activity_16MB_seq.dat.osd_host.3 /mnt/sda8/ceph-research/scripts/output/sys_activity_16MB_seq.dat.osd_host.3


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T04:27:52,746662166-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T04:28:17,072154976-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 549 objects, 8.6 GiB
    usage:   26 GiB used, 74 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T04:28:17,080418639-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T04:28:26,521022371-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 549 objects, 8.6 GiB
    usage:   26 GiB used, 74 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T04:28:26,529553899-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T04:28:35,988764284-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 549 objects, 8.6 GiB
    usage:   26 GiB used, 74 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T04:28:35,997884822-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T04:28:45,227472504-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 549 objects, 8.6 GiB
    usage:   26 GiB used, 74 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T04:28:45,236239275-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T04:28:54,644171422-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 549 objects, 8.6 GiB
    usage:   26 GiB used, 74 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T04:28:54,653191842-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T04:28:54,660117433-07:00] INFO: > The cluster is idle now.[0m
