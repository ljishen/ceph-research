[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-07T23:44:54,429013209-05:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.3 mkdir --parents /tmp/bench-rados
[1] 23:44:54 [SUCCESS] ljishen@10.10.2.3
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-07T23:44:54,615908580-05:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.2 mkdir --parents /tmp/bench-rados
[1] 23:44:54 [SUCCESS] ljishen@10.10.2.2
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-07T23:44:54,811541839-05:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.2 bash
[1] 23:44:55 [SUCCESS] ljishen@10.10.2.2


[1;7;39;49m[2021-11-07T23:44:55,013394598-05:00][RUNNING][ROUND 1/1/21] object_size=4KB[0m


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-07T23:44:55,015729461-05:00] STAGE: Launch a Ceph cluster (CEPH_HOST: 10.10.2.3, OSD_HOST: 10.10.2.2) ...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-07T23:44:55,020270975-05:00] INFO: > Get OSD hostname[0m
## ./benchmarks/bench-rados:178 - launch_ceph_cluster() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.2 uname --nodename
## ./benchmarks/bench-rados:178 - launch_ceph_cluster() > tail -n1
# ./benchmarks/bench-rados:178 - launch_ceph_cluster() > OSD_HOSTNAME=node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us
# ./benchmarks/bench-rados:183 - launch_ceph_cluster() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.3 bash -euo pipefail
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-07T23:44:55,213448313-05:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.3 bash -euo pipefail
10.10.2.3: b'\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-07T23:45:03,060065913-05:00] INFO: > Remove existing clusters\x1b[0m\n'
10.10.2.3: b'## bash:18 -  > basename -- /var/lib/ceph/5516b778-403b-11ec-af2d-31bd05e237f6\n'
10.10.2.3: b'# bash:18 -  > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm --host 10.10.2.3 --host 10.10.2.2 rm-cluster --force --fsid 5516b778-403b-11ec-af2d-31bd05e237f6\n'
10.10.2.3: b"\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-07T23:45:03,070453315-05:00] DEBUG: HOST_PARAMS: ['--host', '10.10.2.3', '--host', '10.10.2.2']\x1b[0m\n"
10.10.2.3: b"\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-07T23:45:03,073805572-05:00] DEBUG: CEPHADM_PARAMS: ['rm-cluster', '--force', '--fsid', '5516b778-403b-11ec-af2d-31bd05e237f6']\x1b[0m\n"
10.10.2.3: b'\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-07T23:45:03,076105793-05:00] DEBUG: download cephadm\x1b[0m\n'
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:51 -  > mkdir -p /tmp/bench-rados/deployment_data_root/usr/sbin\n'
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:52 -  > cd /tmp/bench-rados/deployment_data_root/usr/sbin\n'
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:53 -  > curl --silent --remote-name --location https://github.com/ceph/ceph/raw/pacific/src/cephadm/cephadm\n'
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:55 -  > chmod +x cephadm\n'
10.10.2.3: b"# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:61 -  > /tmp/bench-rados/ceph-research/scripts/parallel-ssh --host 10.10.2.3 --host 10.10.2.2 'sudo python3 -u - rm-cluster --force --fsid 5516b778-403b-11ec-af2d-31bd05e237f6'\n"
10.10.2.3: b'\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-07T23:45:03,592607212-05:00] DEBUG: command from stdin\x1b[0m\n'
10.10.2.3: b"# /tmp/bench-rados/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host 10.10.2.3 --host 10.10.2.2 'sudo python3 -u - rm-cluster --force --fsid 5516b778-403b-11ec-af2d-31bd05e237f6'\n"
10.10.2.3: b'[1] 23:45:05 [SUCCESS] 10.10.2.2\n[2] 23:45:18 [SUCCESS] 10.10.2.3\n'
10.10.2.3: b'\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-07T23:45:18,898119679-05:00] INFO: > Deploy a new cluster\x1b[0m\n'
10.10.2.3: b'# bash:25 -  > /tmp/bench-rados/ceph-research/scripts/ceph-deploy -m 10.10.2.3 -o node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/10.10.2.2:/dev/nvme0n1\n'
10.10.2.3: b'\n\n\x1b[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-07T23:45:18,909756579-05:00] STAGE: Verify passwordless SSH to hosts, passwordless sudo via ljishen, and the existence of devices on hosts...\x1b[0m\n'
10.10.2.3: b'\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-07T23:45:18,914317036-05:00] INFO: > Check the host for the monitor\x1b[0m\n# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:181 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@10.10.2.3 sudo --non-interactive --validate\n'
10.10.2.3: b'\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-07T23:45:19,061986481-05:00] INFO: > Check hosts for OSDs\x1b[0m\n'
10.10.2.3: b"\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-07T23:45:19,066648619-05:00] INFO: >> Check host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'\x1b[0m\n"
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:200 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us sudo --non-interactive --validate\n'
10.10.2.3: b"\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-07T23:45:19,213898412-05:00] INFO: >>> Check device '/dev/nvme0n1'\x1b[0m\n"
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:215 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us test -b /dev/nvme0n1\n'
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:226 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us lsblk --nodeps --noheadings --output TYPE /dev/nvme0n1\n'
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:227 - verify_parameters() > grep --word-regexp disk\n'
10.10.2.3: b'\n\n\x1b[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-07T23:45:19,490575504-05:00] STAGE: Reset devices on hosts...\x1b[0m\n'
10.10.2.3: b"\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-07T23:45:19,495553739-05:00] INFO: > Resetting device '/dev/nvme0n1' on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'\x1b[0m\n"
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:288 - reset_host_devices() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us sudo sh\n'
10.10.2.3: b'\n\n\x1b[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-07T23:45:19,786698653-05:00] STAGE: Bootstrap a new cluster...\x1b[0m\n'
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:317 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.3 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults\n'
10.10.2.3: b'\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-07T23:45:19,796213410-05:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.3: b"\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-07T23:45:19,799483150-05:00] DEBUG: CEPHADM_PARAMS: ['bootstrap', '--config', '/tmp/bench-rados/ceph-research/scripts/ceph.conf', '--mon-ip', '10.10.2.3', '--output-dir', '/tmp/bench-rados/deployment_data_root/etc/ceph', '--ssh-user', 'ljishen', '--skip-dashboard', '--no-minimize-config', '--allow-overwrite', '--allow-fqdn-hostname', '--orphan-initial-daemons', '--skip-monitoring-stack', '--single-host-defaults']\x1b[0m\n"
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.3 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults\n'
10.10.2.3: b'Creating directory /tmp/bench-rados/deployment_data_root/etc/ceph for ceph.conf\n'
10.10.2.3: b'Verifying podman|docker is present...\n'
10.10.2.3: b'Verifying lvm2 is present...\n'
10.10.2.3: b'Verifying time synchronization is in place...\n'
10.10.2.3: b'Unit ntp.service is enabled and running\n'
10.10.2.3: b'Repeating the final host check...\n'
10.10.2.3: b'docker (/usr/bin/docker) is present\n'
10.10.2.3: b'systemctl is present\n'
10.10.2.3: b'lvcreate is present\n'
10.10.2.3: b'Unit ntp.service is enabled and running\n'
10.10.2.3: b'Host looks OK\n'
10.10.2.3: b'Cluster fsid: ad4e77b0-404e-11ec-aba1-c5f3c07a3dc7\n'
10.10.2.3: b'Verifying IP 10.10.2.3 port 3300 ...\n'
10.10.2.3: b'Verifying IP 10.10.2.3 port 6789 ...\n'
10.10.2.3: b'Mon IP `10.10.2.3` is in CIDR network `10.10.2.0/24`\n'
10.10.2.3: b'- internal network (--cluster-network) has not been provided, OSD replication will default to the public_network\n'
10.10.2.3: b'Adjusting default settings to suit single-host cluster...\n'
10.10.2.3: b'Pulling container image quay.io/ceph/ceph:v16...\n'
10.10.2.3: b'Ceph version: ceph version 16.2.6 (ee28fb57e47e9f88813e24bbf4c14496ca299d31) pacific (stable)\n'
10.10.2.3: b'Extracting ceph user uid/gid from container image...\n'
10.10.2.3: b'Creating initial keys...\n'
10.10.2.3: b'Creating initial monmap...\n'
10.10.2.3: b'Creating mon...\n'
10.10.2.3: b'Waiting for mon to start...\n'
10.10.2.3: b'Waiting for mon...\n'
10.10.2.3: b'mon is available\nSetting mon public_network to 10.10.2.0/24\n'
10.10.2.3: b'Wrote config to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf\n'
10.10.2.3: b'Wrote keyring to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring\nCreating mgr...\n'
10.10.2.3: b'Verifying port 9283 ...\n'
10.10.2.3: b'Waiting for mgr to start...\nWaiting for mgr...\n'
10.10.2.3: b'mgr not available, waiting (1/15)...\n'
10.10.2.3: b'mgr not available, waiting (2/15)...\n'
10.10.2.3: b'mgr is available\n'
10.10.2.3: b'Enabling cephadm module...\n'
10.10.2.3: b'Waiting for the mgr to restart...\n'
10.10.2.3: b'Waiting for mgr epoch 5...\n'
10.10.2.3: b'mgr epoch 5 is available\nSetting orchestrator backend to cephadm...\n'
10.10.2.3: b'Generating ssh key...\n'
10.10.2.3: b'Wrote public SSH key to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub\n'
10.10.2.3: b'Adding key to ljishen@localhost authorized_keys...\n'
10.10.2.3: b'Adding host node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us...\n'
10.10.2.3: b'Deploying unmanaged mon service...\n'
10.10.2.3: b'Deploying unmanaged mgr service...\n'
10.10.2.3: b'You can access the Ceph CLI with:\n\n\tsudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --fsid ad4e77b0-404e-11ec-aba1-c5f3c07a3dc7 -c /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf -k /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring\n\n'
10.10.2.3: b'Please consider enabling telemetry to help improve Ceph:\n\n\tceph telemetry on\n\nFor more information see:\n\n\thttps://docs.ceph.com/docs/pacific/mgr/telemetry/\n\n'
10.10.2.3: b'Bootstrap complete.\n'
10.10.2.3: b'\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-07T23:46:25,684542962-05:00] INFO: Sleep for 20 seconds\x1b[0m\n'
10.10.2.3: b'\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-07T23:46:45,692086880-05:00] INFO: > Set the default number of monitor daemons to 1\x1b[0m\n'
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:337 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1\n'
10.10.2.3: b'\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-07T23:46:45,701363496-05:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.3: b"\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-07T23:46:45,705000510-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mon', '1']\x1b[0m\n"
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1\n'
10.10.2.3: b'Inferring fsid ad4e77b0-404e-11ec-aba1-c5f3c07a3dc7\n'
10.10.2.3: b'Inferring config /var/lib/ceph/ad4e77b0-404e-11ec-aba1-c5f3c07a3dc7/mon.node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.3: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.3: b'Scheduled mon update...\n'
10.10.2.3: b'\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-07T23:46:55,856903896-05:00] INFO: > Set the default number of manager daemons to 1\x1b[0m\n'
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:342 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1\n'
10.10.2.3: b'\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-07T23:46:55,866019067-05:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.3: b"\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-07T23:46:55,869425706-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mgr', '1']\x1b[0m\n"
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1\n'
10.10.2.3: b'Inferring fsid ad4e77b0-404e-11ec-aba1-c5f3c07a3dc7\n'
10.10.2.3: b'Inferring config /var/lib/ceph/ad4e77b0-404e-11ec-aba1-c5f3c07a3dc7/mon.node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.3: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.3: b'Scheduled mgr update...\n'
10.10.2.3: b'\n\n\x1b[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-07T23:47:05,936497869-05:00] STAGE: Add hosts to the cluster...\x1b[0m\n'
10.10.2.3: b"\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-07T23:47:05,942550932-05:00] INFO: > Adding host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'\x1b[0m\n"
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:361 - add_hosts() > ssh-copy-id -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -f -i /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\n'
10.10.2.3: b'/usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: "/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub"\n'
10.10.2.3: b'\nNumber of key(s) added: 1\n\nNow try logging into the machine, with:   "ssh -o \'GlobalKnownHostsFile=/dev/null\' -o \'LogLevel=ERROR\' -o \'PasswordAuthentication=no\' -o \'StrictHostKeyChecking=no\' -o \'UserKnownHostsFile=/dev/null\' \'ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\'"\nand check to make sure that only the key(s) you wanted were added.\n\n'
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:367 - add_hosts() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us 10.10.2.2\n'
10.10.2.3: b'\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-07T23:47:06,259281094-05:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.3: b"\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-07T23:47:06,262400582-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'host', 'add', 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us', '10.10.2.2']\x1b[0m\n"
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us 10.10.2.2\n'
10.10.2.3: b'Inferring fsid ad4e77b0-404e-11ec-aba1-c5f3c07a3dc7\n'
10.10.2.3: b'Inferring config /var/lib/ceph/ad4e77b0-404e-11ec-aba1-c5f3c07a3dc7/mon.node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.3: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.3: b"Added host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us' with addr '10.10.2.2'\n"
10.10.2.3: b'\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-07T23:47:15,975284078-05:00] INFO: Sleep for 20 seconds\x1b[0m\n'
10.10.2.3: b'\n\n\x1b[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-07T23:47:35,979932741-05:00] STAGE: Add OSDs to the cluster...\x1b[0m\n'
10.10.2.3: b"\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-07T23:47:35,986975171-05:00] INFO: > Adding devices ['/dev/nvme0n1'] on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us' as OSDs\x1b[0m\n"
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:388 - add_host_devices_as_osds() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1\n'
10.10.2.3: b'\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-07T23:47:35,996568925-05:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.3: b"\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-07T23:47:36,000201470-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'daemon', 'add', 'osd', 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1']\x1b[0m\n"
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1\n'
10.10.2.3: b'Inferring fsid ad4e77b0-404e-11ec-aba1-c5f3c07a3dc7\n'
10.10.2.3: b'Inferring config /var/lib/ceph/ad4e77b0-404e-11ec-aba1-c5f3c07a3dc7/mon.node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.3: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.3: b"Created osd(s) 0 on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'\n"
10.10.2.3: b'\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-07T23:48:02,707266385-05:00] INFO: > Sleep for 20 seconds\x1b[0m\n'
10.10.2.3: b'\n\n\x1b[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-07T23:48:22,713198787-05:00] STAGE: Show Cluster Status\x1b[0m\n'
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:408 - show_status() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status\n'
10.10.2.3: b'\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-07T23:48:22,723409823-05:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.3: b"\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-07T23:48:22,726731089-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', '--status']\x1b[0m\n"
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status\n'
10.10.2.3: b'Inferring fsid ad4e77b0-404e-11ec-aba1-c5f3c07a3dc7\n'
10.10.2.3: b'Inferring config /var/lib/ceph/ad4e77b0-404e-11ec-aba1-c5f3c07a3dc7/mon.node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.3: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.3: b'  cluster:\n    id:     ad4e77b0-404e-11ec-aba1-c5f3c07a3dc7\n    health: HEALTH_WARN\n            1 pool(s) have no replicas configured\n \n  services:\n    mon: 1 daemons, quorum node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us (age 2m)\n    mgr: node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us.jdkvia(active, since 2m)\n    osd: 1 osds: 1 up (since 18s), 1 in (since 42s)\n \n  data:\n    pools:   1 pools, 1 pgs\n    objects: 0 objects, 0 B\n    usage:   4.7 MiB used, 100 GiB / 100 GiB avail\n    pgs:     1 active+clean\n \n'
[1] 23:48:32 [SUCCESS] ljishen@10.10.2.3

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-07T23:45:03,060065913-05:00] INFO: > Remove existing clusters[0m
## bash:18 -  > basename -- /var/lib/ceph/5516b778-403b-11ec-af2d-31bd05e237f6
# bash:18 -  > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm --host 10.10.2.3 --host 10.10.2.2 rm-cluster --force --fsid 5516b778-403b-11ec-af2d-31bd05e237f6
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-07T23:45:03,070453315-05:00] DEBUG: HOST_PARAMS: ['--host', '10.10.2.3', '--host', '10.10.2.2'][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-07T23:45:03,073805572-05:00] DEBUG: CEPHADM_PARAMS: ['rm-cluster', '--force', '--fsid', '5516b778-403b-11ec-af2d-31bd05e237f6'][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-07T23:45:03,076105793-05:00] DEBUG: download cephadm[0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:51 -  > mkdir -p /tmp/bench-rados/deployment_data_root/usr/sbin
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:52 -  > cd /tmp/bench-rados/deployment_data_root/usr/sbin
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:53 -  > curl --silent --remote-name --location https://github.com/ceph/ceph/raw/pacific/src/cephadm/cephadm
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:55 -  > chmod +x cephadm
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:61 -  > /tmp/bench-rados/ceph-research/scripts/parallel-ssh --host 10.10.2.3 --host 10.10.2.2 'sudo python3 -u - rm-cluster --force --fsid 5516b778-403b-11ec-af2d-31bd05e237f6'
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-07T23:45:03,592607212-05:00] DEBUG: command from stdin[0m
# /tmp/bench-rados/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host 10.10.2.3 --host 10.10.2.2 'sudo python3 -u - rm-cluster --force --fsid 5516b778-403b-11ec-af2d-31bd05e237f6'
[1] 23:45:05 [SUCCESS] 10.10.2.2
[2] 23:45:18 [SUCCESS] 10.10.2.3

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-07T23:45:18,898119679-05:00] INFO: > Deploy a new cluster[0m
# bash:25 -  > /tmp/bench-rados/ceph-research/scripts/ceph-deploy -m 10.10.2.3 -o node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/10.10.2.2:/dev/nvme0n1


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-07T23:45:18,909756579-05:00] STAGE: Verify passwordless SSH to hosts, passwordless sudo via ljishen, and the existence of devices on hosts...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-07T23:45:18,914317036-05:00] INFO: > Check the host for the monitor[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:181 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@10.10.2.3 sudo --non-interactive --validate

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-07T23:45:19,061986481-05:00] INFO: > Check hosts for OSDs[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-07T23:45:19,066648619-05:00] INFO: >> Check host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:200 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us sudo --non-interactive --validate

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-07T23:45:19,213898412-05:00] INFO: >>> Check device '/dev/nvme0n1'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:215 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us test -b /dev/nvme0n1
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:226 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us lsblk --nodeps --noheadings --output TYPE /dev/nvme0n1
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:227 - verify_parameters() > grep --word-regexp disk


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-07T23:45:19,490575504-05:00] STAGE: Reset devices on hosts...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-07T23:45:19,495553739-05:00] INFO: > Resetting device '/dev/nvme0n1' on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:288 - reset_host_devices() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us sudo sh


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-07T23:45:19,786698653-05:00] STAGE: Bootstrap a new cluster...[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:317 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.3 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-07T23:45:19,796213410-05:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-07T23:45:19,799483150-05:00] DEBUG: CEPHADM_PARAMS: ['bootstrap', '--config', '/tmp/bench-rados/ceph-research/scripts/ceph.conf', '--mon-ip', '10.10.2.3', '--output-dir', '/tmp/bench-rados/deployment_data_root/etc/ceph', '--ssh-user', 'ljishen', '--skip-dashboard', '--no-minimize-config', '--allow-overwrite', '--allow-fqdn-hostname', '--orphan-initial-daemons', '--skip-monitoring-stack', '--single-host-defaults'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.3 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults
Creating directory /tmp/bench-rados/deployment_data_root/etc/ceph for ceph.conf
Verifying podman|docker is present...
Verifying lvm2 is present...
Verifying time synchronization is in place...
Unit ntp.service is enabled and running
Repeating the final host check...
docker (/usr/bin/docker) is present
systemctl is present
lvcreate is present
Unit ntp.service is enabled and running
Host looks OK
Cluster fsid: ad4e77b0-404e-11ec-aba1-c5f3c07a3dc7
Verifying IP 10.10.2.3 port 3300 ...
Verifying IP 10.10.2.3 port 6789 ...
Mon IP `10.10.2.3` is in CIDR network `10.10.2.0/24`
- internal network (--cluster-network) has not been provided, OSD replication will default to the public_network
Adjusting default settings to suit single-host cluster...
Pulling container image quay.io/ceph/ceph:v16...
Ceph version: ceph version 16.2.6 (ee28fb57e47e9f88813e24bbf4c14496ca299d31) pacific (stable)
Extracting ceph user uid/gid from container image...
Creating initial keys...
Creating initial monmap...
Creating mon...
Waiting for mon to start...
Waiting for mon...
mon is available
Setting mon public_network to 10.10.2.0/24
Wrote config to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf
Wrote keyring to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring
Creating mgr...
Verifying port 9283 ...
Waiting for mgr to start...
Waiting for mgr...
mgr not available, waiting (1/15)...
mgr not available, waiting (2/15)...
mgr is available
Enabling cephadm module...
Waiting for the mgr to restart...
Waiting for mgr epoch 5...
mgr epoch 5 is available
Setting orchestrator backend to cephadm...
Generating ssh key...
Wrote public SSH key to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub
Adding key to ljishen@localhost authorized_keys...
Adding host node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us...
Deploying unmanaged mon service...
Deploying unmanaged mgr service...
You can access the Ceph CLI with:

	sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --fsid ad4e77b0-404e-11ec-aba1-c5f3c07a3dc7 -c /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf -k /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring

Please consider enabling telemetry to help improve Ceph:

	ceph telemetry on

For more information see:

	https://docs.ceph.com/docs/pacific/mgr/telemetry/

Bootstrap complete.

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-07T23:46:25,684542962-05:00] INFO: Sleep for 20 seconds[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-07T23:46:45,692086880-05:00] INFO: > Set the default number of monitor daemons to 1[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:337 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-07T23:46:45,701363496-05:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-07T23:46:45,705000510-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mon', '1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1
Inferring fsid ad4e77b0-404e-11ec-aba1-c5f3c07a3dc7
Inferring config /var/lib/ceph/ad4e77b0-404e-11ec-aba1-c5f3c07a3dc7/mon.node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Scheduled mon update...

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-07T23:46:55,856903896-05:00] INFO: > Set the default number of manager daemons to 1[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:342 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-07T23:46:55,866019067-05:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-07T23:46:55,869425706-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mgr', '1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1
Inferring fsid ad4e77b0-404e-11ec-aba1-c5f3c07a3dc7
Inferring config /var/lib/ceph/ad4e77b0-404e-11ec-aba1-c5f3c07a3dc7/mon.node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Scheduled mgr update...


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-07T23:47:05,936497869-05:00] STAGE: Add hosts to the cluster...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-07T23:47:05,942550932-05:00] INFO: > Adding host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:361 - add_hosts() > ssh-copy-id -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -f -i /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us
/usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: "/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub"

Number of key(s) added: 1

Now try logging into the machine, with:   "ssh -o 'GlobalKnownHostsFile=/dev/null' -o 'LogLevel=ERROR' -o 'PasswordAuthentication=no' -o 'StrictHostKeyChecking=no' -o 'UserKnownHostsFile=/dev/null' 'ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'"
and check to make sure that only the key(s) you wanted were added.

# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:367 - add_hosts() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us 10.10.2.2
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-07T23:47:06,259281094-05:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-07T23:47:06,262400582-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'host', 'add', 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us', '10.10.2.2'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us 10.10.2.2
Inferring fsid ad4e77b0-404e-11ec-aba1-c5f3c07a3dc7
Inferring config /var/lib/ceph/ad4e77b0-404e-11ec-aba1-c5f3c07a3dc7/mon.node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Added host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us' with addr '10.10.2.2'

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-07T23:47:15,975284078-05:00] INFO: Sleep for 20 seconds[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-07T23:47:35,979932741-05:00] STAGE: Add OSDs to the cluster...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-07T23:47:35,986975171-05:00] INFO: > Adding devices ['/dev/nvme0n1'] on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us' as OSDs[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:388 - add_host_devices_as_osds() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-07T23:47:35,996568925-05:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-07T23:47:36,000201470-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'daemon', 'add', 'osd', 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1
Inferring fsid ad4e77b0-404e-11ec-aba1-c5f3c07a3dc7
Inferring config /var/lib/ceph/ad4e77b0-404e-11ec-aba1-c5f3c07a3dc7/mon.node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Created osd(s) 0 on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-07T23:48:02,707266385-05:00] INFO: > Sleep for 20 seconds[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-07T23:48:22,713198787-05:00] STAGE: Show Cluster Status[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:408 - show_status() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-07T23:48:22,723409823-05:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-07T23:48:22,726731089-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', '--status'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status
Inferring fsid ad4e77b0-404e-11ec-aba1-c5f3c07a3dc7
Inferring config /var/lib/ceph/ad4e77b0-404e-11ec-aba1-c5f3c07a3dc7/mon.node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
  cluster:
    id:     ad4e77b0-404e-11ec-aba1-c5f3c07a3dc7
    health: HEALTH_WARN
            1 pool(s) have no replicas configured
 
  services:
    mon: 1 daemons, quorum node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us (age 2m)
    mgr: node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us.jdkvia(active, since 2m)
    osd: 1 osds: 1 up (since 18s), 1 in (since 42s)
 
  data:
    pools:   1 pools, 1 pgs
    objects: 0 objects, 0 B
    usage:   4.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     1 active+clean
 

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-07T23:48:32,423102884-05:00] INFO: > Copy configuration files to local[0m
# ./benchmarks/bench-rados:220 - launch_ceph_cluster() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.3 sudo chown --changes --recursive ljishen /tmp/bench-rados/deployment_data_root/etc/ceph
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-07T23:48:32,430094659-05:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.3 sudo chown --changes --recursive ljishen /tmp/bench-rados/deployment_data_root/etc/ceph
10.10.2.3: b"changed ownership of '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring' from root to ljishen\nchanged ownership of '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf' from root to ljishen\nchanged ownership of '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub' from root to ljishen\nchanged ownership of '/tmp/bench-rados/deployment_data_root/etc/ceph' from root to ljishen\n"
[1] 23:48:32 [SUCCESS] ljishen@10.10.2.3
changed ownership of '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring' from root to ljishen
changed ownership of '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf' from root to ljishen
changed ownership of '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub' from root to ljishen
changed ownership of '/tmp/bench-rados/deployment_data_root/etc/ceph' from root to ljishen
# ./benchmarks/bench-rados:223 - launch_ceph_cluster() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.3:/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf
# ./benchmarks/bench-rados:226 - launch_ceph_cluster() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.3:/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-07T23:48:32,935781604-05:00] STAGE: Update local ceph.conf as a client...[0m


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-07T23:48:32,938134351-05:00] STAGE: Configure Ceph pool...[0m
# ./benchmarks/bench-rados:257 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_max_backfills 32
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-07T23:48:32,960335345-05:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-07T23:48:32,963010129-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'ad4e77b0-404e-11ec-aba1-c5f3c07a3dc7', '--', 'ceph', 'config', 'set', 'osd', 'osd_max_backfills', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid ad4e77b0-404e-11ec-aba1-c5f3c07a3dc7 -- ceph config set osd osd_max_backfills 32
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:258 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_active 32
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-07T23:48:37,597998018-05:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-07T23:48:37,600958340-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'ad4e77b0-404e-11ec-aba1-c5f3c07a3dc7', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_active', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid ad4e77b0-404e-11ec-aba1-c5f3c07a3dc7 -- ceph config set osd osd_recovery_max_active 32
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:259 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_single_start 8
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-07T23:48:41,947465011-05:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-07T23:48:41,950190350-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'ad4e77b0-404e-11ec-aba1-c5f3c07a3dc7', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_single_start', '8'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid ad4e77b0-404e-11ec-aba1-c5f3c07a3dc7 -- ceph config set osd osd_recovery_max_single_start 8
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:260 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_op_priority 63
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-07T23:48:46,125964381-05:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-07T23:48:46,128987942-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'ad4e77b0-404e-11ec-aba1-c5f3c07a3dc7', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_op_priority', '63'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid ad4e77b0-404e-11ec-aba1-c5f3c07a3dc7 -- ceph config set osd osd_recovery_op_priority 63
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:262 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config show-with-defaults osd.0
# ./benchmarks/bench-rados:263 - configure_ceph_pool() > grep 'osd_max_backfills\|osd_recovery'
# ./benchmarks/bench-rados:263 - configure_ceph_pool() > column -t -s ' '
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
osd_max_backfills                        32         mon
osd_recovery_cost                        20971520   default
osd_recovery_delay_start                 0.000000   default
osd_recovery_max_active                  32         mon
osd_recovery_max_active_hdd              3          default
osd_recovery_max_active_ssd              10         default
osd_recovery_max_chunk                   8388608    default
osd_recovery_max_omap_entries_per_chunk  8096       default
osd_recovery_max_single_start            8          mon
osd_recovery_op_priority                 3          default
osd_recovery_op_warn_multiple            16         default
osd_recovery_priority                    5          default
osd_recovery_retry_interval              30.000000  default
osd_recovery_sleep                       0.000000   default
osd_recovery_sleep_hdd                   0.100000   default
osd_recovery_sleep_hybrid                0.025000   default
osd_recovery_sleep_ssd                   0.000000   default
# ./benchmarks/bench-rados:265 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool create bench_rados 128 128 replicated --size 1 --pg-num-min 128
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-07T23:48:54,422612429-05:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-07T23:48:54,425308653-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'ad4e77b0-404e-11ec-aba1-c5f3c07a3dc7', '--', 'ceph', 'osd', 'pool', 'create', 'bench_rados', '128', '128', 'replicated', '--size', '1', '--pg-num-min', '128'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid ad4e77b0-404e-11ec-aba1-c5f3c07a3dc7 -- ceph osd pool create bench_rados 128 128 replicated --size 1 --pg-num-min 128
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
pool 'bench_rados' created
# ./benchmarks/bench-rados:267 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados min_size 1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-07T23:48:58,933321690-05:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-07T23:48:58,936060675-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'ad4e77b0-404e-11ec-aba1-c5f3c07a3dc7', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'min_size', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid ad4e77b0-404e-11ec-aba1-c5f3c07a3dc7 -- ceph osd pool set bench_rados min_size 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 min_size to 1
# ./benchmarks/bench-rados:268 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados pg_autoscale_mode off
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-07T23:49:03,377587410-05:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-07T23:49:03,380554956-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'ad4e77b0-404e-11ec-aba1-c5f3c07a3dc7', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'pg_autoscale_mode', 'off'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid ad4e77b0-404e-11ec-aba1-c5f3c07a3dc7 -- ceph osd pool set bench_rados pg_autoscale_mode off
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 pg_autoscale_mode to off
# ./benchmarks/bench-rados:269 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool application enable bench_rados benchmark
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-07T23:49:07,678797569-05:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-07T23:49:07,681537115-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'ad4e77b0-404e-11ec-aba1-c5f3c07a3dc7', '--', 'ceph', 'osd', 'pool', 'application', 'enable', 'bench_rados', 'benchmark'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid ad4e77b0-404e-11ec-aba1-c5f3c07a3dc7 -- ceph osd pool application enable bench_rados benchmark
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
enabled application 'benchmark' on pool 'bench_rados'
# ./benchmarks/bench-rados:270 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados noscrub 1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-07T23:49:12,092003472-05:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-07T23:49:12,094925892-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'ad4e77b0-404e-11ec-aba1-c5f3c07a3dc7', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'noscrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid ad4e77b0-404e-11ec-aba1-c5f3c07a3dc7 -- ceph osd pool set bench_rados noscrub 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 noscrub to 1
# ./benchmarks/bench-rados:271 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados nodeep-scrub 1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-07T23:49:16,493736590-05:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-07T23:49:16,496458753-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'ad4e77b0-404e-11ec-aba1-c5f3c07a3dc7', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'nodeep-scrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid ad4e77b0-404e-11ec-aba1-c5f3c07a3dc7 -- ceph osd pool set bench_rados nodeep-scrub 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 nodeep-scrub to 1
# ./benchmarks/bench-rados:272 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool ls detail
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-07T23:49:21,169352602-05:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-07T23:49:21,172288829-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'ad4e77b0-404e-11ec-aba1-c5f3c07a3dc7', '--', 'ceph', 'osd', 'pool', 'ls', 'detail'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid ad4e77b0-404e-11ec-aba1-c5f3c07a3dc7 -- ceph osd pool ls detail
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
pool 1 'device_health_metrics' replicated size 1 min_size 1 crush_rule 0 object_hash rjenkins pg_num 64 pgp_num 64 autoscale_mode on last_change 17 lfor 0/0/14 flags hashpspool stripe_width 0 pg_num_min 1 application mgr_devicehealth
pool 2 'bench_rados' replicated size 1 min_size 1 crush_rule 0 object_hash rjenkins pg_num 128 pgp_num 128 autoscale_mode off last_change 20 flags hashpspool,noscrub,nodeep-scrub stripe_width 0 pg_num_min 128 application benchmark

# ./benchmarks/bench-rados:273 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd df tree
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-07T23:49:25,329201249-05:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-07T23:49:25,331824636-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'ad4e77b0-404e-11ec-aba1-c5f3c07a3dc7', '--', 'ceph', 'osd', 'df', 'tree'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid ad4e77b0-404e-11ec-aba1-c5f3c07a3dc7 -- ceph osd df tree
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
ID  CLASS  WEIGHT   REWEIGHT  SIZE     RAW USE  DATA     OMAP  META     AVAIL    %USE  VAR   PGS  STATUS  TYPE NAME      
-1         0.09769         -  100 GiB  5.7 MiB  168 KiB   0 B  5.6 MiB  100 GiB  0.01  1.00    -          root default   
-3         0.09769         -  100 GiB  5.7 MiB  168 KiB   0 B  5.6 MiB  100 GiB  0.01  1.00    -              host node-1
 0    ssd  0.09769   1.00000  100 GiB  5.7 MiB  168 KiB   0 B  5.6 MiB  100 GiB  0.01  1.00  192      up          osd.0  
                       TOTAL  100 GiB  5.7 MiB  168 KiB   0 B  5.6 MiB  100 GiB  0.01                                    
MIN/MAX VAR: 1.00/1.00  STDDEV: 0


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-07T23:49:29,416874595-05:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-07T23:49:53,343535760-05:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-07T23:50:02,792035869-05:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-07T23:50:11,837319845-05:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-07T23:50:11,842865162-05:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-07T23:50:20,972852666-05:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-07T23:50:20,978402662-05:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-07T23:50:30,031391845-05:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-07T23:50:30,037285819-05:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-07T23:50:38,906927535-05:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-07T23:50:38,912729757-05:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-07T23:50:48,000271161-05:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-07T23:50:48,006031754-05:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-07T23:50:48,010226835-05:00] INFO: > The cluster is idle now.[0m


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-07T23:50:48,013105794-05:00] STAGE: Start benchmarking of mode write ...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-07T23:50:48,017773045-05:00] INFO: > Start system activity collection services[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-07T23:50:48,021573722-05:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:341 - rados_bench() > rm --force /mnt/sda8/ceph-research/scripts/output/sys_activity_4KB_write.dat.rados_bench_host.1
# ./benchmarks/bench-rados:342 - rados_bench() > sar_pid_rados_bench_host=107224
# ./benchmarks/bench-rados:342 - rados_bench() > S_TIME_FORMAT=ISO
# ./benchmarks/bench-rados:342 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/output/sys_activity_4KB_write.dat.rados_bench_host.1 2

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-07T23:50:48,026878575-05:00] INFO: >> for OSD host[0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-07T23:50:48,035097894-05:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.2 bash
10.10.2.2: b'8215\n'
[1] 23:50:48 [SUCCESS] ljishen@10.10.2.2
8215

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-07T23:50:48,219680994-05:00] INFO: > Run rados bench[0m
# ./benchmarks/bench-rados:370 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/output/rados_bench_4KB_write.log.1
## ./benchmarks/bench-rados:361 - rados_bench() > count_cpus 32-63,96-127
# ./benchmarks/bench-rados:361 - rados_bench() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 60 write --pool bench_rados -b 4096 -O 4096 --concurrent-ios 128 --show-time --write-object --write-omap --write-xattr --no-cleanup
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-07T23:50:48,238266371-05:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-07T23:50:48,240986119-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'ad4e77b0-404e-11ec-aba1-c5f3c07a3dc7', '--', 'rados', 'bench', '60', 'write', '--pool', 'bench_rados', '-b', '4096', '-O', '4096', '--concurrent-ios', '128', '--show-time', '--write-object', '--write-omap', '--write-xattr', '--no-cleanup'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid ad4e77b0-404e-11ec-aba1-c5f3c07a3dc7 -- rados bench 60 write --pool bench_rados -b 4096 -O 4096 --concurrent-ios 128 --show-time --write-object --write-omap --write-xattr --no-cleanup
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
hints = 1
2021-11-08T04:50:51.354679+0000 Maintaining 128 concurrent writes of 4096 bytes to objects of size 4096 for up to 60 seconds or 0 objects
2021-11-08T04:50:51.354689+0000 Object prefix: benchmark_data_node-0.bluefield2.ucsc-cmps10_7
2021-11-08T04:50:51.355313+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-11-08T04:50:51.355313+0000     0       0         0         0         0         0           -           0
2021-11-08T04:50:52.355409+0000     1     127     24117     23990   93.7064   93.7109  0.00388979  0.00532029
2021-11-08T04:50:53.355490+0000     2     128     47429     47301   92.3788   91.0586  0.00442743  0.00540169
2021-11-08T04:50:54.355566+0000     3     127     69526     69399   90.3571   86.3203  0.00680092    0.005526
2021-11-08T04:50:55.355634+0000     4     127     88683     88556   86.4745    74.832  0.00861542  0.00577319
2021-11-08T04:50:56.355710+0000     5     128    107819    107691   84.1277   74.7461  0.00493789  0.00593705
2021-11-08T04:50:57.355782+0000     6     128    123127    122999   80.0718   59.7969  0.00858541  0.00623734
2021-11-08T04:50:58.355857+0000     7     128    136498    136370   76.0939   52.2305  0.00984248  0.00656496
2021-11-08T04:50:59.355930+0000     8     128    152520    152392   74.4049   62.5859  0.00534062  0.00671502
2021-11-08T04:51:00.356005+0000     9     127    175330    175203   76.0375   89.1055  0.00381563  0.00657258
2021-11-08T04:51:01.356080+0000    10     127    199001    198874   77.6796   92.4648  0.00499304  0.00643328
2021-11-08T04:51:02.356157+0000    11     128    217637    217509   77.2348    72.793   0.0525686  0.00645694
2021-11-08T04:51:03.356232+0000    12     128    219495    219367   71.4033   7.25781   0.0773734  0.00697539
2021-11-08T04:51:04.356298+0000    13     128    222501    222373    66.814   11.7422   0.0193981  0.00747702
2021-11-08T04:51:05.356374+0000    14     127    235300    235173   65.6127        50   0.0107677  0.00761626
2021-11-08T04:51:06.356461+0000    15     127    248408    248281   64.6518   51.2031   0.0109777  0.00772978
2021-11-08T04:51:07.356536+0000    16     127    267931    267804    65.377   76.2617   0.0054481  0.00764516
2021-11-08T04:51:08.356658+0000    17     128    289223    289095    66.423    83.168  0.00647808  0.00752459
2021-11-08T04:51:09.356739+0000    18     128    310633    310505   67.3787   83.6328  0.00636938  0.00741771
2021-11-08T04:51:10.356831+0000    19     128    327153    327025   67.2286   64.5312  0.00976218  0.00743329
2021-11-08T04:51:11.356910+0000 min lat: 0.00280323 max lat: 0.0860565 avg lat: 0.0075094
2021-11-08T04:51:11.356910+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-11-08T04:51:11.356910+0000    20     128    340864    340736   66.5448   53.5586   0.0109168   0.0075094
2021-11-08T04:51:12.356992+0000    21     128    356696    356568   66.3207   61.8438  0.00753071  0.00753626
2021-11-08T04:51:13.357060+0000    22     128    378690    378562   67.2111   85.9141  0.00473522  0.00743694
2021-11-08T04:51:14.357145+0000    23     128    400003    399875   67.9083   83.2539  0.00446485  0.00736034
2021-11-08T04:51:15.357240+0000    24     128    421434    421306   68.5666   83.7148  0.00495422  0.00728952
2021-11-08T04:51:16.357362+0000    25     128    427627    427499   66.7914   24.1914   0.0439278  0.00747987
2021-11-08T04:51:17.357429+0000    26     128    429714    429586    64.536   8.15234   0.0780973  0.00773992
2021-11-08T04:51:18.357499+0000    27     128    439275    439147    63.529   37.3477  0.00900563  0.00785797
2021-11-08T04:51:19.357574+0000    28     128    452404    452276   63.0916   51.2852     0.01061  0.00792203
2021-11-08T04:51:20.357653+0000    29     128    462514    462386   62.2777   39.4922    0.010308  0.00798374
2021-11-08T04:51:21.357731+0000    30     128    472357    472229   61.4833   38.4492   0.0117066  0.00812898
2021-11-08T04:51:22.357819+0000    31     127    483454    483327   60.8983   43.3516   0.0101971  0.00820716
2021-11-08T04:51:23.357899+0000    32     127    496700    496573    60.612   51.7422   0.0046266  0.00824722
2021-11-08T04:51:24.357972+0000    33     127    517109    516982   61.1909   79.7227  0.00694352   0.0081687
2021-11-08T04:51:25.358044+0000    34     128    534221    534093   61.3569   66.8398  0.00912682  0.00814624
2021-11-08T04:51:26.358145+0000    35     128    547599    547471   61.0968   52.2578  0.00906895  0.00818134
2021-11-08T04:51:27.358225+0000    36     127    566847    566720   61.4882   75.1914   0.0052401  0.00812933
2021-11-08T04:51:28.358319+0000    37     127    587817    587690     62.04   81.9141  0.00748423  0.00805683
2021-11-08T04:51:29.358451+0000    38     128    607806    607678   62.4618   78.0781  0.00833899  0.00800231
2021-11-08T04:51:30.358537+0000    39     128    627669    627541   62.8495   77.5898  0.00383986  0.00795363
2021-11-08T04:51:31.358656+0000 min lat: 0.00180566 max lat: 0.298784 avg lat: 0.00806488
2021-11-08T04:51:31.358656+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-11-08T04:51:31.358656+0000    40     128    634637    634509   61.9587   27.2188    0.057141  0.00806488
2021-11-08T04:51:32.358752+0000    41     128    636737    636609   60.6475   8.20312   0.0530884  0.00823421
2021-11-08T04:51:33.358845+0000    42     128    649170    649042   60.3598   48.5664  0.00953336  0.00828118
2021-11-08T04:51:34.358926+0000    43     128    664348    664220   60.3348   59.2891  0.00860454  0.00828516
2021-11-08T04:51:35.359007+0000    44     127    678087    677960   60.1832   53.6719  0.00900835  0.00830559
2021-11-08T04:51:36.359096+0000    45     128    691350    691222   59.9969   51.8047  0.00989487  0.00833128
2021-11-08T04:51:37.359183+0000    46     128    704425    704297   59.8029   51.0742   0.0108068  0.00835868
2021-11-08T04:51:38.359261+0000    47     128    717302    717174   59.6006   50.3008  0.00963086  0.00838678
2021-11-08T04:51:39.359340+0000    48     128    729766    729638   59.3732   48.6875   0.0105411  0.00841907
2021-11-08T04:51:40.359421+0000    49     128    734176    734048    58.513   17.2266   0.0485581  0.00853833
2021-11-08T04:51:41.359503+0000    50     127    743606    743479   58.0795   36.8398   0.0113714  0.00860676
2021-11-08T04:51:42.359586+0000    51     128    763754    763626   58.4837   78.6992   0.0040246  0.00854744
2021-11-08T04:51:43.359667+0000    52     128    784793    784665   58.9393   82.1836  0.00434747  0.00848142
2021-11-08T04:51:44.359749+0000    53     127    806844    806717   59.4524   86.1406  0.00447171  0.00840831
2021-11-08T04:51:45.359839+0000    54     127    828734    828607   59.9348   85.5078  0.00858078  0.00834027
2021-11-08T04:51:46.359924+0000    55     128    841195    841067   59.7299   48.6719   0.0620317  0.00836557
2021-11-08T04:51:47.360003+0000    56     128    843243    843115   58.8062         8   0.0535968  0.00849704
2021-11-08T04:51:48.360078+0000    57     128    852733    852605   58.4248   37.0703  0.00821361  0.00855611
2021-11-08T04:51:49.360154+0000    58     128    867080    866952   58.3836    56.043   0.0086704  0.00856207
2021-11-08T04:51:50.360226+0000    59     128    880616    880488   58.2902    52.875  0.00947525  0.00857576
2021-11-08T04:51:51.360298+0000 min lat: 0.00180566 max lat: 0.298784 avg lat: 0.00859275
2021-11-08T04:51:51.360298+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-11-08T04:51:51.360298+0000    60      28    893678    893650   58.1756   51.4141  0.00891901  0.00859275
2021-11-08T04:51:52.360410+0000 Total time run:         60.0052
Total writes made:      893678
Write size:             4096
Object size:            4096
Bandwidth (MB/sec):     58.1772
Stddev Bandwidth:       23.8721
Max bandwidth (MB/sec): 93.7109
Min bandwidth (MB/sec): 7.25781
Average IOPS:           14893
Stddev IOPS:            6111.25
Max IOPS:               23990
Min IOPS:               1858
Average Latency(s):     0.00859273
Stddev Latency(s):      0.00906809
Max latency(s):         0.298784
Min latency(s):         0.00180566

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-07T23:51:53,078437675-05:00] INFO: > Stop system activity collection services[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-07T23:51:53,082550010-05:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:391 - rados_bench() > kill -INT 107224

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-07T23:51:53,086436719-05:00] INFO: >> for OSD host[0m
# ./benchmarks/bench-rados:396 - rados_bench() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.2 kill -INT 8215
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-07T23:51:53,093277989-05:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.2 kill -INT 8215
[1] 23:51:53 [SUCCESS] ljishen@10.10.2.2

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-07T23:51:53,275061861-05:00] INFO: >>> Copy system activity file from OSD host[0m
## ./benchmarks/bench-rados:403 - rados_bench() > basename -- /tmp/bench-rados/sys_activity_4KB_write.dat.osd_host.1
# ./benchmarks/bench-rados:403 - rados_bench() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.2:/tmp/bench-rados/sys_activity_4KB_write.dat.osd_host.1 /mnt/sda8/ceph-research/scripts/output/sys_activity_4KB_write.dat.osd_host.1


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-07T23:51:53,468373281-05:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-07T23:52:17,626905629-05:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 893.68k objects, 3.4 GiB
    usage:   11 GiB used, 89 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-07T23:52:17,632802519-05:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-07T23:52:26,662664817-05:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 893.68k objects, 3.4 GiB
    usage:   11 GiB used, 89 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-07T23:52:26,668794075-05:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-07T23:52:35,622031284-05:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 893.68k objects, 3.4 GiB
    usage:   11 GiB used, 89 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-07T23:52:35,627862400-05:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-07T23:52:44,672573215-05:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 893.68k objects, 3.4 GiB
    usage:   11 GiB used, 89 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-07T23:52:44,678492707-05:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-07T23:52:53,507059320-05:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 893.68k objects, 3.4 GiB
    usage:   11 GiB used, 89 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-07T23:52:53,512968583-05:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-07T23:52:53,517346067-05:00] INFO: > The cluster is idle now.[0m


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-07T23:52:53,520160465-05:00] STAGE: Start benchmarking of mode seq ...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-07T23:52:53,525130166-05:00] INFO: > Start system activity collection services[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-07T23:52:53,529047433-05:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:341 - rados_bench() > rm --force /mnt/sda8/ceph-research/scripts/output/sys_activity_4KB_seq.dat.rados_bench_host.1
# ./benchmarks/bench-rados:342 - rados_bench() > sar_pid_rados_bench_host=111038
# ./benchmarks/bench-rados:342 - rados_bench() > S_TIME_FORMAT=ISO
# ./benchmarks/bench-rados:342 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/output/sys_activity_4KB_seq.dat.rados_bench_host.1 2

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-07T23:52:53,534757680-05:00] INFO: >> for OSD host[0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-07T23:52:53,542803633-05:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.2 bash
10.10.2.2: b'8324\n'
[1] 23:52:53 [SUCCESS] ljishen@10.10.2.2
8324

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-07T23:52:53,728061897-05:00] INFO: > Run rados bench[0m
# ./benchmarks/bench-rados:379 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/output/rados_bench_4KB_seq.log.1
## ./benchmarks/bench-rados:376 - rados_bench() > count_cpus 32-63,96-127
# ./benchmarks/bench-rados:376 - rados_bench() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 128 --show-time
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-07T23:52:53,746451164-05:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-07T23:52:53,749019487-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'ad4e77b0-404e-11ec-aba1-c5f3c07a3dc7', '--', 'rados', 'bench', '99999999', 'seq', '--pool', 'bench_rados', '--concurrent-ios', '128', '--show-time'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid ad4e77b0-404e-11ec-aba1-c5f3c07a3dc7 -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 128 --show-time
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
hints = 1
2021-11-08T04:52:56.987911+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-11-08T04:52:56.987911+0000     0       0         0         0         0         0           -           0
2021-11-08T04:52:57.988012+0000     1     127     40973     40846   159.527   159.555  0.00315831  0.00312342
2021-11-08T04:52:58.988109+0000     2     127     83382     83255   162.586    165.66  0.00325553  0.00306752
2021-11-08T04:52:59.988192+0000     3     128    119560    119432   155.492   141.316  0.00363474  0.00320751
2021-11-08T04:53:00.988274+0000     4     127    153401    153274   149.665   132.195   0.0033342   0.0033339
2021-11-08T04:53:01.988353+0000     5     128    187104    186976    146.06   131.648  0.00325951  0.00341687
2021-11-08T04:53:02.988438+0000     6     127    220819    220692   143.665   131.703  0.00411767  0.00347457
2021-11-08T04:53:03.988511+0000     7     127    254792    254665   142.099   132.707  0.00409858  0.00351323
2021-11-08T04:53:04.988593+0000     8     127    288066    287939   140.582   129.977  0.00439455  0.00355135
2021-11-08T04:53:05.988722+0000     9     128    317620    317492   137.787   115.441  0.00411098   0.0036165
2021-11-08T04:53:06.988807+0000    10     127    347708    347581   135.761   117.535  0.00432759  0.00367775
2021-11-08T04:53:07.988879+0000    11     127    380703    380576   135.135   128.887  0.00431989  0.00369497
2021-11-08T04:53:08.988962+0000    12     127    414620    414493   134.913   132.488  0.00280195  0.00370129
2021-11-08T04:53:09.989055+0000    13     127    453229    453102   136.136   150.816  0.00397537   0.0036681
2021-11-08T04:53:10.989134+0000    14     128    490975    490847   136.942   147.441  0.00345346  0.00364644
2021-11-08T04:53:11.989210+0000    15     127    529364    529237    137.81   149.961   0.0033206  0.00362346
2021-11-08T04:53:12.989285+0000    16     128    566136    566008   138.173   143.637  0.00385785  0.00361408
2021-11-08T04:53:13.989370+0000    17     127    602677    602550   138.441   142.742  0.00300254  0.00360701
2021-11-08T04:53:14.989466+0000    18     127    642217    642090    139.33   154.453  0.00314882  0.00358408
2021-11-08T04:53:15.989544+0000    19     128    682211    682083   140.218   156.223  0.00244077   0.0035614
2021-11-08T04:53:16.989631+0000 min lat: 0.000236306 max lat: 0.0964894 avg lat: 0.00353994
2021-11-08T04:53:16.989631+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-11-08T04:53:16.989631+0000    20     127    722467    722340   141.069   157.254  0.00347183  0.00353994
2021-11-08T04:53:17.989716+0000    21     128    761738    761610   141.656   153.398  0.00342745  0.00352522
2021-11-08T04:53:18.989790+0000    22     128    800765    800637   142.146   152.449  0.00245217  0.00351305
2021-11-08T04:53:19.989857+0000    23     128    840573    840445   142.726     155.5  0.00311616  0.00349877
2021-11-08T04:53:20.989934+0000    24     128    878602    878474   142.968   148.551  0.00318881  0.00349273
2021-11-08T04:53:21.990055+0000 Total time run:       24.3827
Total reads made:     893678
Read size:            4096
Object size:          4096
Bandwidth (MB/sec):   143.172
Average IOPS:         36652
Stddev IOPS:          3451.22
Max IOPS:             42409
Min IOPS:             29553
Average Latency(s):   0.00348785
Max latency(s):       0.0964894
Min latency(s):       0.000236306

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-07T23:53:22,578501480-05:00] INFO: > Stop system activity collection services[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-07T23:53:22,583283587-05:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:391 - rados_bench() > kill -INT 111038

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-07T23:53:22,587927114-05:00] INFO: >> for OSD host[0m
# ./benchmarks/bench-rados:396 - rados_bench() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.2 kill -INT 8324
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-07T23:53:22,595074181-05:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.2 kill -INT 8324
[1] 23:53:22 [SUCCESS] ljishen@10.10.2.2

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-07T23:53:22,775149913-05:00] INFO: >>> Copy system activity file from OSD host[0m
## ./benchmarks/bench-rados:403 - rados_bench() > basename -- /tmp/bench-rados/sys_activity_4KB_seq.dat.osd_host.1
# ./benchmarks/bench-rados:403 - rados_bench() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.2:/tmp/bench-rados/sys_activity_4KB_seq.dat.osd_host.1 /mnt/sda8/ceph-research/scripts/output/sys_activity_4KB_seq.dat.osd_host.1


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-07T23:53:22,931973107-05:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-07T23:53:46,976444070-05:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 893.68k objects, 3.4 GiB
    usage:   11 GiB used, 89 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-07T23:53:46,982881459-05:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-07T23:53:56,046331127-05:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 893.68k objects, 3.4 GiB
    usage:   11 GiB used, 89 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-07T23:53:56,052609806-05:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-07T23:54:05,113749769-05:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 893.68k objects, 3.4 GiB
    usage:   11 GiB used, 89 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-07T23:54:05,120684436-05:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-07T23:54:14,247400594-05:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 893.68k objects, 3.4 GiB
    usage:   11 GiB used, 89 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-07T23:54:14,253913406-05:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-07T23:54:23,182314500-05:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 893.68k objects, 3.4 GiB
    usage:   11 GiB used, 89 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-07T23:54:23,188576990-05:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-07T23:54:23,193393532-05:00] INFO: > The cluster is idle now.[0m


[1;7;39;49m[2021-11-07T23:54:23,195175974-05:00][RUNNING][ROUND 2/1/21] object_size=4KB[0m


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-07T23:54:23,197971966-05:00] STAGE: Launch a Ceph cluster (CEPH_HOST: 10.10.2.3, OSD_HOST: 10.10.2.2) ...[0m
# ./benchmarks/bench-rados:183 - launch_ceph_cluster() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.3 bash -euo pipefail
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-07T23:54:23,206564739-05:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.3 bash -euo pipefail
10.10.2.3: b'\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-07T23:54:26,794669100-05:00] INFO: > Remove existing clusters\x1b[0m\n'
10.10.2.3: b'## bash:17 -  > basename -- /var/lib/ceph/ad4e77b0-404e-11ec-aba1-c5f3c07a3dc7\n'
10.10.2.3: b'# bash:17 -  > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm --host 10.10.2.3 --host 10.10.2.2 rm-cluster --force --fsid ad4e77b0-404e-11ec-aba1-c5f3c07a3dc7\n'
10.10.2.3: b"\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-07T23:54:26,804264858-05:00] DEBUG: HOST_PARAMS: ['--host', '10.10.2.3', '--host', '10.10.2.2']\x1b[0m\n"
10.10.2.3: b"\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-07T23:54:26,808132005-05:00] DEBUG: CEPHADM_PARAMS: ['rm-cluster', '--force', '--fsid', 'ad4e77b0-404e-11ec-aba1-c5f3c07a3dc7']\x1b[0m\n"
10.10.2.3: b"# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:63 -  > /tmp/bench-rados/ceph-research/scripts/parallel-ssh --host 10.10.2.3 --host 10.10.2.2 'sudo python3 -u - rm-cluster --force --fsid ad4e77b0-404e-11ec-aba1-c5f3c07a3dc7'\n"
10.10.2.3: b'\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-07T23:54:26,816232851-05:00] DEBUG: command from stdin\x1b[0m\n'
10.10.2.3: b"# /tmp/bench-rados/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host 10.10.2.3 --host 10.10.2.2 'sudo python3 -u - rm-cluster --force --fsid ad4e77b0-404e-11ec-aba1-c5f3c07a3dc7'\n"
10.10.2.3: b'[1] 23:54:32 [SUCCESS] 10.10.2.3\n[2] 23:54:35 [SUCCESS] 10.10.2.2\n'
10.10.2.3: b'\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-07T23:54:35,662809698-05:00] INFO: > Deploy a new cluster\x1b[0m\n'
10.10.2.3: b'# bash:24 -  > /tmp/bench-rados/ceph-research/scripts/ceph-deploy -m 10.10.2.3 -o node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/10.10.2.2:/dev/nvme0n1\n'
10.10.2.3: b'\n\n\x1b[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-07T23:54:35,673841325-05:00] STAGE: Verify passwordless SSH to hosts, passwordless sudo via ljishen, and the existence of devices on hosts...\x1b[0m\n'
10.10.2.3: b'\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-07T23:54:35,678234775-05:00] INFO: > Check the host for the monitor\x1b[0m\n# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:181 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@10.10.2.3 sudo --non-interactive --validate\n'
10.10.2.3: b'\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-07T23:54:35,826766923-05:00] INFO: > Check hosts for OSDs\x1b[0m\n'
10.10.2.3: b"\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-07T23:54:35,831484244-05:00] INFO: >> Check host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'\x1b[0m\n"
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:200 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us sudo --non-interactive --validate\n'
10.10.2.3: b"\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-07T23:54:35,978134241-05:00] INFO: >>> Check device '/dev/nvme0n1'\x1b[0m\n"
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:215 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us test -b /dev/nvme0n1\n'
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:226 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us lsblk --nodeps --noheadings --output TYPE /dev/nvme0n1\n'
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:227 - verify_parameters() > grep --word-regexp disk\n'
10.10.2.3: b'\n\n\x1b[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-07T23:54:36,254484049-05:00] STAGE: Reset devices on hosts...\x1b[0m\n'
10.10.2.3: b"\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-07T23:54:36,259560148-05:00] INFO: > Resetting device '/dev/nvme0n1' on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'\x1b[0m\n"
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:288 - reset_host_devices() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us sudo sh\n'
10.10.2.3: b'  Removing ceph--c437b0d6--582a--47c1--a77f--e992a9523275-osd--block--65ba1998--60df--4406--a3f6--cef650b47e9f (253:0)\n'
10.10.2.3: b'  Archiving volume group "ceph-c437b0d6-582a-47c1-a77f-e992a9523275" metadata (seqno 5).\n'
10.10.2.3: b'  Releasing logical volume "osd-block-65ba1998-60df-4406-a3f6-cef650b47e9f"\n'
10.10.2.3: b'  Creating volume group backup "/etc/lvm/backup/ceph-c437b0d6-582a-47c1-a77f-e992a9523275" (seqno 6).\n'
10.10.2.3: b'  Logical volume "osd-block-65ba1998-60df-4406-a3f6-cef650b47e9f" successfully removed\n'
10.10.2.3: b'  Removing physical volume "/dev/nvme0n1" from volume group "ceph-c437b0d6-582a-47c1-a77f-e992a9523275"\n'
10.10.2.3: b'  Volume group "ceph-c437b0d6-582a-47c1-a77f-e992a9523275" successfully removed\n'
10.10.2.3: b'\n\n\x1b[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-07T23:54:36,591641881-05:00] STAGE: Bootstrap a new cluster...\x1b[0m\n'
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:317 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.3 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults\n'
10.10.2.3: b'\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-07T23:54:36,600237261-05:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.3: b"\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-07T23:54:36,603737356-05:00] DEBUG: CEPHADM_PARAMS: ['bootstrap', '--config', '/tmp/bench-rados/ceph-research/scripts/ceph.conf', '--mon-ip', '10.10.2.3', '--output-dir', '/tmp/bench-rados/deployment_data_root/etc/ceph', '--ssh-user', 'ljishen', '--skip-dashboard', '--no-minimize-config', '--allow-overwrite', '--allow-fqdn-hostname', '--orphan-initial-daemons', '--skip-monitoring-stack', '--single-host-defaults']\x1b[0m\n"
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.3 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults\n'
10.10.2.3: b'Verifying podman|docker is present...\nVerifying lvm2 is present...\n'
10.10.2.3: b'Verifying time synchronization is in place...\n'
10.10.2.3: b'Unit ntp.service is enabled and running\nRepeating the final host check...\n'
10.10.2.3: b'docker (/usr/bin/docker) is present\n'
10.10.2.3: b'systemctl is present\n'
10.10.2.3: b'lvcreate is present\n'
10.10.2.3: b'Unit ntp.service is enabled and running\n'
10.10.2.3: b'Host looks OK\n'
10.10.2.3: b'Cluster fsid: f923c7de-404f-11ec-aba1-c5f3c07a3dc7\n'
10.10.2.3: b'Verifying IP 10.10.2.3 port 3300 ...\n'
10.10.2.3: b'Verifying IP 10.10.2.3 port 6789 ...\n'
10.10.2.3: b'Mon IP `10.10.2.3` is in CIDR network `10.10.2.0/24`\n- internal network (--cluster-network) has not been provided, OSD replication will default to the public_network\n'
10.10.2.3: b'Adjusting default settings to suit single-host cluster...\n'
10.10.2.3: b'Pulling container image quay.io/ceph/ceph:v16...\n'
10.10.2.3: b'Ceph version: ceph version 16.2.6 (ee28fb57e47e9f88813e24bbf4c14496ca299d31) pacific (stable)\n'
10.10.2.3: b'Extracting ceph user uid/gid from container image...\n'
10.10.2.3: b'Creating initial keys...\n'
10.10.2.3: b'Creating initial monmap...\n'
10.10.2.3: b'Creating mon...\n'
10.10.2.3: b'Waiting for mon to start...\n'
10.10.2.3: b'Waiting for mon...\n'
10.10.2.3: b'mon is available\n'
10.10.2.3: b'Setting mon public_network to 10.10.2.0/24\n'
10.10.2.3: b'Wrote config to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf\n'
10.10.2.3: b'Wrote keyring to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring\n'
10.10.2.3: b'Creating mgr...\n'
10.10.2.3: b'Verifying port 9283 ...\n'
10.10.2.3: b'Waiting for mgr to start...\nWaiting for mgr...\n'
10.10.2.3: b'mgr not available, waiting (1/15)...\n'
10.10.2.3: b'mgr not available, waiting (2/15)...\n'
10.10.2.3: b'mgr is available\n'
10.10.2.3: b'Enabling cephadm module...\n'
10.10.2.3: b'Waiting for the mgr to restart...\n'
10.10.2.3: b'Waiting for mgr epoch 5...\n'
10.10.2.3: b'mgr epoch 5 is available\nSetting orchestrator backend to cephadm...\n'
10.10.2.3: b'Generating ssh key...\n'
10.10.2.3: b'Wrote public SSH key to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub\n'
10.10.2.3: b'Adding key to ljishen@localhost authorized_keys...\n'
10.10.2.3: b'Adding host node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us...\n'
10.10.2.3: b'Deploying unmanaged mon service...\n'
10.10.2.3: b'Deploying unmanaged mgr service...\n'
10.10.2.3: b'You can access the Ceph CLI with:\n\n\tsudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --fsid f923c7de-404f-11ec-aba1-c5f3c07a3dc7 -c /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf -k /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring\n\n'
10.10.2.3: b'Please consider enabling telemetry to help improve Ceph:\n\n\tceph telemetry on\n\nFor more information see:\n\n\thttps://docs.ceph.com/docs/pacific/mgr/telemetry/\n\n'
10.10.2.3: b'Bootstrap complete.\n'
10.10.2.3: b'\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-07T23:55:42,708772757-05:00] INFO: Sleep for 20 seconds\x1b[0m\n'
10.10.2.3: b'\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-07T23:56:02,716258718-05:00] INFO: > Set the default number of monitor daemons to 1\x1b[0m\n'
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:337 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1\n'
10.10.2.3: b'\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-07T23:56:02,725529532-05:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.3: b"\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-07T23:56:02,729229624-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mon', '1']\x1b[0m\n"
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1\n'
10.10.2.3: b'Inferring fsid f923c7de-404f-11ec-aba1-c5f3c07a3dc7\n'
10.10.2.3: b'Inferring config /var/lib/ceph/f923c7de-404f-11ec-aba1-c5f3c07a3dc7/mon.node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.3: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.3: b'Scheduled mon update...\n'
10.10.2.3: b'\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-07T23:56:12,583774391-05:00] INFO: > Set the default number of manager daemons to 1\x1b[0m\n'
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:342 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1\n'
10.10.2.3: b'\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-07T23:56:12,593661167-05:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.3: b"\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-07T23:56:12,597411324-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mgr', '1']\x1b[0m\n"
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1\n'
10.10.2.3: b'Inferring fsid f923c7de-404f-11ec-aba1-c5f3c07a3dc7\n'
10.10.2.3: b'Inferring config /var/lib/ceph/f923c7de-404f-11ec-aba1-c5f3c07a3dc7/mon.node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.3: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.3: b'Scheduled mgr update...\n'
10.10.2.3: b'\n\n\x1b[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-07T23:56:22,610091382-05:00] STAGE: Add hosts to the cluster...\x1b[0m\n'
10.10.2.3: b"\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-07T23:56:22,615638429-05:00] INFO: > Adding host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'\x1b[0m\n"
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:361 - add_hosts() > ssh-copy-id -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -f -i /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\n'
10.10.2.3: b'/usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: "/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub"\n'
10.10.2.3: b'\nNumber of key(s) added: 1\n\nNow try logging into the machine, with:   "ssh -o \'GlobalKnownHostsFile=/dev/null\' -o \'LogLevel=ERROR\' -o \'PasswordAuthentication=no\' -o \'StrictHostKeyChecking=no\' -o \'UserKnownHostsFile=/dev/null\' \'ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\'"\nand check to make sure that only the key(s) you wanted were added.\n\n'
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:367 - add_hosts() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us 10.10.2.2\n'
10.10.2.3: b'\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-07T23:56:22,823813137-05:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.3: b"\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-07T23:56:22,826988458-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'host', 'add', 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us', '10.10.2.2']\x1b[0m\n"
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us 10.10.2.2\n'
10.10.2.3: b'Inferring fsid f923c7de-404f-11ec-aba1-c5f3c07a3dc7\n'
10.10.2.3: b'Inferring config /var/lib/ceph/f923c7de-404f-11ec-aba1-c5f3c07a3dc7/mon.node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.3: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.3: b"Added host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us' with addr '10.10.2.2'\n"
10.10.2.3: b'\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-07T23:56:33,030879452-05:00] INFO: Sleep for 20 seconds\x1b[0m\n'
10.10.2.3: b'\n\n\x1b[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-07T23:56:53,035865630-05:00] STAGE: Add OSDs to the cluster...\x1b[0m\n'
10.10.2.3: b"\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-07T23:56:53,042452450-05:00] INFO: > Adding devices ['/dev/nvme0n1'] on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us' as OSDs\x1b[0m\n"
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:388 - add_host_devices_as_osds() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1\n'
10.10.2.3: b'\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-07T23:56:53,052037046-05:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.3: b"\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-07T23:56:53,055591664-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'daemon', 'add', 'osd', 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1']\x1b[0m\n"
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1\n'
10.10.2.3: b'Inferring fsid f923c7de-404f-11ec-aba1-c5f3c07a3dc7\n'
10.10.2.3: b'Inferring config /var/lib/ceph/f923c7de-404f-11ec-aba1-c5f3c07a3dc7/mon.node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.3: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.3: b"Created osd(s) 0 on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'\n"
10.10.2.3: b'\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-07T23:57:18,893344154-05:00] INFO: > Sleep for 20 seconds\x1b[0m\n'
10.10.2.3: b'\n\n\x1b[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-07T23:57:38,899263783-05:00] STAGE: Show Cluster Status\x1b[0m\n'
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:408 - show_status() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status\n'
10.10.2.3: b'\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-07T23:57:38,909742256-05:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.3: b"\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-07T23:57:38,913705555-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', '--status']\x1b[0m\n"
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status\n'
10.10.2.3: b'Inferring fsid f923c7de-404f-11ec-aba1-c5f3c07a3dc7\n'
10.10.2.3: b'Inferring config /var/lib/ceph/f923c7de-404f-11ec-aba1-c5f3c07a3dc7/mon.node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.3: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.3: b'  cluster:\n    id:     f923c7de-404f-11ec-aba1-c5f3c07a3dc7\n    health: HEALTH_WARN\n            1 pool(s) have no replicas configured\n \n  services:\n    mon: 1 daemons, quorum node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us (age 2m)\n    mgr: node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us.hztvzi(active, since 2m)\n    osd: 1 osds: 1 up (since 19s), 1 in (since 42s)\n \n  data:\n    pools:   1 pools, 1 pgs\n    objects: 0 objects, 0 B\n    usage:   4.7 MiB used, 100 GiB / 100 GiB avail\n    pgs:     1 active+clean\n \n'
[1] 23:57:48 [SUCCESS] ljishen@10.10.2.3

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-07T23:54:26,794669100-05:00] INFO: > Remove existing clusters[0m
## bash:17 -  > basename -- /var/lib/ceph/ad4e77b0-404e-11ec-aba1-c5f3c07a3dc7
# bash:17 -  > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm --host 10.10.2.3 --host 10.10.2.2 rm-cluster --force --fsid ad4e77b0-404e-11ec-aba1-c5f3c07a3dc7
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-07T23:54:26,804264858-05:00] DEBUG: HOST_PARAMS: ['--host', '10.10.2.3', '--host', '10.10.2.2'][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-07T23:54:26,808132005-05:00] DEBUG: CEPHADM_PARAMS: ['rm-cluster', '--force', '--fsid', 'ad4e77b0-404e-11ec-aba1-c5f3c07a3dc7'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:63 -  > /tmp/bench-rados/ceph-research/scripts/parallel-ssh --host 10.10.2.3 --host 10.10.2.2 'sudo python3 -u - rm-cluster --force --fsid ad4e77b0-404e-11ec-aba1-c5f3c07a3dc7'
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-07T23:54:26,816232851-05:00] DEBUG: command from stdin[0m
# /tmp/bench-rados/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host 10.10.2.3 --host 10.10.2.2 'sudo python3 -u - rm-cluster --force --fsid ad4e77b0-404e-11ec-aba1-c5f3c07a3dc7'
[1] 23:54:32 [SUCCESS] 10.10.2.3
[2] 23:54:35 [SUCCESS] 10.10.2.2

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-07T23:54:35,662809698-05:00] INFO: > Deploy a new cluster[0m
# bash:24 -  > /tmp/bench-rados/ceph-research/scripts/ceph-deploy -m 10.10.2.3 -o node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/10.10.2.2:/dev/nvme0n1


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-07T23:54:35,673841325-05:00] STAGE: Verify passwordless SSH to hosts, passwordless sudo via ljishen, and the existence of devices on hosts...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-07T23:54:35,678234775-05:00] INFO: > Check the host for the monitor[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:181 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@10.10.2.3 sudo --non-interactive --validate

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-07T23:54:35,826766923-05:00] INFO: > Check hosts for OSDs[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-07T23:54:35,831484244-05:00] INFO: >> Check host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:200 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us sudo --non-interactive --validate

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-07T23:54:35,978134241-05:00] INFO: >>> Check device '/dev/nvme0n1'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:215 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us test -b /dev/nvme0n1
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:226 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us lsblk --nodeps --noheadings --output TYPE /dev/nvme0n1
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:227 - verify_parameters() > grep --word-regexp disk


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-07T23:54:36,254484049-05:00] STAGE: Reset devices on hosts...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-07T23:54:36,259560148-05:00] INFO: > Resetting device '/dev/nvme0n1' on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:288 - reset_host_devices() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us sudo sh
  Removing ceph--c437b0d6--582a--47c1--a77f--e992a9523275-osd--block--65ba1998--60df--4406--a3f6--cef650b47e9f (253:0)
  Archiving volume group "ceph-c437b0d6-582a-47c1-a77f-e992a9523275" metadata (seqno 5).
  Releasing logical volume "osd-block-65ba1998-60df-4406-a3f6-cef650b47e9f"
  Creating volume group backup "/etc/lvm/backup/ceph-c437b0d6-582a-47c1-a77f-e992a9523275" (seqno 6).
  Logical volume "osd-block-65ba1998-60df-4406-a3f6-cef650b47e9f" successfully removed
  Removing physical volume "/dev/nvme0n1" from volume group "ceph-c437b0d6-582a-47c1-a77f-e992a9523275"
  Volume group "ceph-c437b0d6-582a-47c1-a77f-e992a9523275" successfully removed


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-07T23:54:36,591641881-05:00] STAGE: Bootstrap a new cluster...[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:317 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.3 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-07T23:54:36,600237261-05:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-07T23:54:36,603737356-05:00] DEBUG: CEPHADM_PARAMS: ['bootstrap', '--config', '/tmp/bench-rados/ceph-research/scripts/ceph.conf', '--mon-ip', '10.10.2.3', '--output-dir', '/tmp/bench-rados/deployment_data_root/etc/ceph', '--ssh-user', 'ljishen', '--skip-dashboard', '--no-minimize-config', '--allow-overwrite', '--allow-fqdn-hostname', '--orphan-initial-daemons', '--skip-monitoring-stack', '--single-host-defaults'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.3 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults
Verifying podman|docker is present...
Verifying lvm2 is present...
Verifying time synchronization is in place...
Unit ntp.service is enabled and running
Repeating the final host check...
docker (/usr/bin/docker) is present
systemctl is present
lvcreate is present
Unit ntp.service is enabled and running
Host looks OK
Cluster fsid: f923c7de-404f-11ec-aba1-c5f3c07a3dc7
Verifying IP 10.10.2.3 port 3300 ...
Verifying IP 10.10.2.3 port 6789 ...
Mon IP `10.10.2.3` is in CIDR network `10.10.2.0/24`
- internal network (--cluster-network) has not been provided, OSD replication will default to the public_network
Adjusting default settings to suit single-host cluster...
Pulling container image quay.io/ceph/ceph:v16...
Ceph version: ceph version 16.2.6 (ee28fb57e47e9f88813e24bbf4c14496ca299d31) pacific (stable)
Extracting ceph user uid/gid from container image...
Creating initial keys...
Creating initial monmap...
Creating mon...
Waiting for mon to start...
Waiting for mon...
mon is available
Setting mon public_network to 10.10.2.0/24
Wrote config to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf
Wrote keyring to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring
Creating mgr...
Verifying port 9283 ...
Waiting for mgr to start...
Waiting for mgr...
mgr not available, waiting (1/15)...
mgr not available, waiting (2/15)...
mgr is available
Enabling cephadm module...
Waiting for the mgr to restart...
Waiting for mgr epoch 5...
mgr epoch 5 is available
Setting orchestrator backend to cephadm...
Generating ssh key...
Wrote public SSH key to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub
Adding key to ljishen@localhost authorized_keys...
Adding host node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us...
Deploying unmanaged mon service...
Deploying unmanaged mgr service...
You can access the Ceph CLI with:

	sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --fsid f923c7de-404f-11ec-aba1-c5f3c07a3dc7 -c /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf -k /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring

Please consider enabling telemetry to help improve Ceph:

	ceph telemetry on

For more information see:

	https://docs.ceph.com/docs/pacific/mgr/telemetry/

Bootstrap complete.

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-07T23:55:42,708772757-05:00] INFO: Sleep for 20 seconds[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-07T23:56:02,716258718-05:00] INFO: > Set the default number of monitor daemons to 1[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:337 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-07T23:56:02,725529532-05:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-07T23:56:02,729229624-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mon', '1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1
Inferring fsid f923c7de-404f-11ec-aba1-c5f3c07a3dc7
Inferring config /var/lib/ceph/f923c7de-404f-11ec-aba1-c5f3c07a3dc7/mon.node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Scheduled mon update...

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-07T23:56:12,583774391-05:00] INFO: > Set the default number of manager daemons to 1[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:342 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-07T23:56:12,593661167-05:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-07T23:56:12,597411324-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mgr', '1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1
Inferring fsid f923c7de-404f-11ec-aba1-c5f3c07a3dc7
Inferring config /var/lib/ceph/f923c7de-404f-11ec-aba1-c5f3c07a3dc7/mon.node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Scheduled mgr update...


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-07T23:56:22,610091382-05:00] STAGE: Add hosts to the cluster...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-07T23:56:22,615638429-05:00] INFO: > Adding host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:361 - add_hosts() > ssh-copy-id -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -f -i /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us
/usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: "/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub"

Number of key(s) added: 1

Now try logging into the machine, with:   "ssh -o 'GlobalKnownHostsFile=/dev/null' -o 'LogLevel=ERROR' -o 'PasswordAuthentication=no' -o 'StrictHostKeyChecking=no' -o 'UserKnownHostsFile=/dev/null' 'ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'"
and check to make sure that only the key(s) you wanted were added.

# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:367 - add_hosts() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us 10.10.2.2
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-07T23:56:22,823813137-05:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-07T23:56:22,826988458-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'host', 'add', 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us', '10.10.2.2'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us 10.10.2.2
Inferring fsid f923c7de-404f-11ec-aba1-c5f3c07a3dc7
Inferring config /var/lib/ceph/f923c7de-404f-11ec-aba1-c5f3c07a3dc7/mon.node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Added host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us' with addr '10.10.2.2'

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-07T23:56:33,030879452-05:00] INFO: Sleep for 20 seconds[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-07T23:56:53,035865630-05:00] STAGE: Add OSDs to the cluster...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-07T23:56:53,042452450-05:00] INFO: > Adding devices ['/dev/nvme0n1'] on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us' as OSDs[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:388 - add_host_devices_as_osds() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-07T23:56:53,052037046-05:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-07T23:56:53,055591664-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'daemon', 'add', 'osd', 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1
Inferring fsid f923c7de-404f-11ec-aba1-c5f3c07a3dc7
Inferring config /var/lib/ceph/f923c7de-404f-11ec-aba1-c5f3c07a3dc7/mon.node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Created osd(s) 0 on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-07T23:57:18,893344154-05:00] INFO: > Sleep for 20 seconds[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-07T23:57:38,899263783-05:00] STAGE: Show Cluster Status[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:408 - show_status() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-07T23:57:38,909742256-05:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-07T23:57:38,913705555-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', '--status'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status
Inferring fsid f923c7de-404f-11ec-aba1-c5f3c07a3dc7
Inferring config /var/lib/ceph/f923c7de-404f-11ec-aba1-c5f3c07a3dc7/mon.node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
  cluster:
    id:     f923c7de-404f-11ec-aba1-c5f3c07a3dc7
    health: HEALTH_WARN
            1 pool(s) have no replicas configured
 
  services:
    mon: 1 daemons, quorum node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us (age 2m)
    mgr: node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us.hztvzi(active, since 2m)
    osd: 1 osds: 1 up (since 19s), 1 in (since 42s)
 
  data:
    pools:   1 pools, 1 pgs
    objects: 0 objects, 0 B
    usage:   4.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     1 active+clean
 

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-07T23:57:48,934660540-05:00] INFO: > Copy configuration files to local[0m
# ./benchmarks/bench-rados:220 - launch_ceph_cluster() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.3 sudo chown --changes --recursive ljishen /tmp/bench-rados/deployment_data_root/etc/ceph
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-07T23:57:48,941826293-05:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.3 sudo chown --changes --recursive ljishen /tmp/bench-rados/deployment_data_root/etc/ceph
[1] 23:57:49 [SUCCESS] ljishen@10.10.2.3
# ./benchmarks/bench-rados:223 - launch_ceph_cluster() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.3:/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf
# ./benchmarks/bench-rados:226 - launch_ceph_cluster() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.3:/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-07T23:57:49,412250224-05:00] STAGE: Update local ceph.conf as a client...[0m


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-07T23:57:49,415223631-05:00] STAGE: Configure Ceph pool...[0m
# ./benchmarks/bench-rados:257 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_max_backfills 32
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-07T23:57:49,435938524-05:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-07T23:57:49,438582880-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'f923c7de-404f-11ec-aba1-c5f3c07a3dc7', '--', 'ceph', 'config', 'set', 'osd', 'osd_max_backfills', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid f923c7de-404f-11ec-aba1-c5f3c07a3dc7 -- ceph config set osd osd_max_backfills 32
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:258 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_active 32
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-07T23:57:53,555657848-05:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-07T23:57:53,558216262-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'f923c7de-404f-11ec-aba1-c5f3c07a3dc7', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_active', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid f923c7de-404f-11ec-aba1-c5f3c07a3dc7 -- ceph config set osd osd_recovery_max_active 32
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:259 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_single_start 8
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-07T23:57:57,724389216-05:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-07T23:57:57,727249539-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'f923c7de-404f-11ec-aba1-c5f3c07a3dc7', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_single_start', '8'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid f923c7de-404f-11ec-aba1-c5f3c07a3dc7 -- ceph config set osd osd_recovery_max_single_start 8
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:260 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_op_priority 63
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-07T23:58:01,943514493-05:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-07T23:58:01,946242908-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'f923c7de-404f-11ec-aba1-c5f3c07a3dc7', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_op_priority', '63'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid f923c7de-404f-11ec-aba1-c5f3c07a3dc7 -- ceph config set osd osd_recovery_op_priority 63
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:262 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config show-with-defaults osd.0
# ./benchmarks/bench-rados:263 - configure_ceph_pool() > grep 'osd_max_backfills\|osd_recovery'
# ./benchmarks/bench-rados:263 - configure_ceph_pool() > column -t -s ' '
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
osd_max_backfills                        32         mon
osd_recovery_cost                        20971520   default
osd_recovery_delay_start                 0.000000   default
osd_recovery_max_active                  32         mon
osd_recovery_max_active_hdd              3          default
osd_recovery_max_active_ssd              10         default
osd_recovery_max_chunk                   8388608    default
osd_recovery_max_omap_entries_per_chunk  8096       default
osd_recovery_max_single_start            8          mon
osd_recovery_op_priority                 3          default
osd_recovery_op_warn_multiple            16         default
osd_recovery_priority                    5          default
osd_recovery_retry_interval              30.000000  default
osd_recovery_sleep                       0.000000   default
osd_recovery_sleep_hdd                   0.100000   default
osd_recovery_sleep_hybrid                0.025000   default
osd_recovery_sleep_ssd                   0.000000   default
# ./benchmarks/bench-rados:265 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool create bench_rados 128 128 replicated --size 1 --pg-num-min 128
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-07T23:58:10,015416808-05:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-07T23:58:10,018353346-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'f923c7de-404f-11ec-aba1-c5f3c07a3dc7', '--', 'ceph', 'osd', 'pool', 'create', 'bench_rados', '128', '128', 'replicated', '--size', '1', '--pg-num-min', '128'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid f923c7de-404f-11ec-aba1-c5f3c07a3dc7 -- ceph osd pool create bench_rados 128 128 replicated --size 1 --pg-num-min 128
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
pool 'bench_rados' created
# ./benchmarks/bench-rados:267 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados min_size 1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-07T23:58:14,451969096-05:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-07T23:58:14,454759689-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'f923c7de-404f-11ec-aba1-c5f3c07a3dc7', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'min_size', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid f923c7de-404f-11ec-aba1-c5f3c07a3dc7 -- ceph osd pool set bench_rados min_size 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 min_size to 1
# ./benchmarks/bench-rados:268 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados pg_autoscale_mode off
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-07T23:58:18,827244594-05:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-07T23:58:18,830018905-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'f923c7de-404f-11ec-aba1-c5f3c07a3dc7', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'pg_autoscale_mode', 'off'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid f923c7de-404f-11ec-aba1-c5f3c07a3dc7 -- ceph osd pool set bench_rados pg_autoscale_mode off
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 pg_autoscale_mode to off
# ./benchmarks/bench-rados:269 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool application enable bench_rados benchmark
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-07T23:58:22,978270248-05:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-07T23:58:22,981256228-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'f923c7de-404f-11ec-aba1-c5f3c07a3dc7', '--', 'ceph', 'osd', 'pool', 'application', 'enable', 'bench_rados', 'benchmark'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid f923c7de-404f-11ec-aba1-c5f3c07a3dc7 -- ceph osd pool application enable bench_rados benchmark
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
enabled application 'benchmark' on pool 'bench_rados'
# ./benchmarks/bench-rados:270 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados noscrub 1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-07T23:58:27,673935084-05:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-07T23:58:27,676941914-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'f923c7de-404f-11ec-aba1-c5f3c07a3dc7', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'noscrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid f923c7de-404f-11ec-aba1-c5f3c07a3dc7 -- ceph osd pool set bench_rados noscrub 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 noscrub to 1
# ./benchmarks/bench-rados:271 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados nodeep-scrub 1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-07T23:58:32,782246319-05:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-07T23:58:32,785074382-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'f923c7de-404f-11ec-aba1-c5f3c07a3dc7', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'nodeep-scrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid f923c7de-404f-11ec-aba1-c5f3c07a3dc7 -- ceph osd pool set bench_rados nodeep-scrub 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 nodeep-scrub to 1
# ./benchmarks/bench-rados:272 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool ls detail
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-07T23:58:37,991305098-05:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-07T23:58:37,994304444-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'f923c7de-404f-11ec-aba1-c5f3c07a3dc7', '--', 'ceph', 'osd', 'pool', 'ls', 'detail'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid f923c7de-404f-11ec-aba1-c5f3c07a3dc7 -- ceph osd pool ls detail
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
pool 1 'device_health_metrics' replicated size 1 min_size 1 crush_rule 0 object_hash rjenkins pg_num 64 pgp_num 64 autoscale_mode on last_change 17 lfor 0/0/15 flags hashpspool stripe_width 0 pg_num_min 1 application mgr_devicehealth
pool 2 'bench_rados' replicated size 1 min_size 1 crush_rule 0 object_hash rjenkins pg_num 128 pgp_num 128 autoscale_mode off last_change 19 flags hashpspool,noscrub,nodeep-scrub stripe_width 0 pg_num_min 128 application benchmark

# ./benchmarks/bench-rados:273 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd df tree
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-07T23:58:42,012662055-05:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-07T23:58:42,015204500-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'f923c7de-404f-11ec-aba1-c5f3c07a3dc7', '--', 'ceph', 'osd', 'df', 'tree'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid f923c7de-404f-11ec-aba1-c5f3c07a3dc7 -- ceph osd df tree
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
ID  CLASS  WEIGHT   REWEIGHT  SIZE     RAW USE  DATA     OMAP  META     AVAIL    %USE  VAR   PGS  STATUS  TYPE NAME      
-1         0.09769         -  100 GiB  5.7 MiB  160 KiB   0 B  5.6 MiB  100 GiB  0.01  1.00    -          root default   
-3         0.09769         -  100 GiB  5.7 MiB  160 KiB   0 B  5.6 MiB  100 GiB  0.01  1.00    -              host node-1
 0    ssd  0.09769   1.00000  100 GiB  5.7 MiB  160 KiB   0 B  5.6 MiB  100 GiB  0.01  1.00  192      up          osd.0  
                       TOTAL  100 GiB  5.7 MiB  160 KiB   0 B  5.6 MiB  100 GiB  0.01                                    
MIN/MAX VAR: 1.00/1.00  STDDEV: 0


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-07T23:58:46,105139146-05:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-07T23:59:10,163331941-05:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-07T23:59:19,207922334-05:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-07T23:59:28,330213073-05:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-07T23:59:28,336534784-05:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-07T23:59:37,423556077-05:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-07T23:59:37,429909468-05:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-07T23:59:46,420850353-05:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-07T23:59:46,427502527-05:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-07T23:59:55,472266280-05:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-07T23:59:55,479045664-05:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:00:07,016072941-05:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:00:07,022334088-05:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:00:07,027111146-05:00] INFO: > The cluster is idle now.[0m


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:00:07,030163161-05:00] STAGE: Start benchmarking of mode write ...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:00:07,035699642-05:00] INFO: > Start system activity collection services[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:00:07,040158149-05:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:341 - rados_bench() > rm --force /mnt/sda8/ceph-research/scripts/output/sys_activity_4KB_write.dat.rados_bench_host.2
# ./benchmarks/bench-rados:342 - rados_bench() > sar_pid_rados_bench_host=121601
# ./benchmarks/bench-rados:342 - rados_bench() > S_TIME_FORMAT=ISO
# ./benchmarks/bench-rados:342 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/output/sys_activity_4KB_write.dat.rados_bench_host.2 2

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:00:07,046065739-05:00] INFO: >> for OSD host[0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:00:07,053870186-05:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.2 bash
10.10.2.2: b'11965\n'
[1] 00:00:07 [SUCCESS] ljishen@10.10.2.2
11965

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:00:07,236324085-05:00] INFO: > Run rados bench[0m
# ./benchmarks/bench-rados:370 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/output/rados_bench_4KB_write.log.2
## ./benchmarks/bench-rados:361 - rados_bench() > count_cpus 32-63,96-127
# ./benchmarks/bench-rados:361 - rados_bench() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 60 write --pool bench_rados -b 4096 -O 4096 --concurrent-ios 128 --show-time --write-object --write-omap --write-xattr --no-cleanup
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:00:07,255297975-05:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:00:07,257928986-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'f923c7de-404f-11ec-aba1-c5f3c07a3dc7', '--', 'rados', 'bench', '60', 'write', '--pool', 'bench_rados', '-b', '4096', '-O', '4096', '--concurrent-ios', '128', '--show-time', '--write-object', '--write-omap', '--write-xattr', '--no-cleanup'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid f923c7de-404f-11ec-aba1-c5f3c07a3dc7 -- rados bench 60 write --pool bench_rados -b 4096 -O 4096 --concurrent-ios 128 --show-time --write-object --write-omap --write-xattr --no-cleanup
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
hints = 1
2021-11-08T05:00:10.456623+0000 Maintaining 128 concurrent writes of 4096 bytes to objects of size 4096 for up to 60 seconds or 0 objects
2021-11-08T05:00:10.456632+0000 Object prefix: benchmark_data_node-0.bluefield2.ucsc-cmps10_7
2021-11-08T05:00:10.457181+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-11-08T05:00:10.457181+0000     0       0         0         0         0         0           -           0
2021-11-08T05:00:11.457275+0000     1     128     23208     23080   90.1509   90.1562  0.00602335  0.00551992
2021-11-08T05:00:12.457363+0000     2     127     45239     45112   88.1029   86.0625  0.00515539  0.00566491
2021-11-08T05:00:13.457450+0000     3     128     67220     67092   87.3526   85.8594  0.00714065  0.00571444
2021-11-08T05:00:14.457534+0000     4     128     87728     87600   85.5401   80.1094  0.00562935   0.0058373
2021-11-08T05:00:15.457616+0000     5     128    107838    107710   84.1417   78.5547   0.0164235  0.00593615
2021-11-08T05:00:16.457699+0000     6     127    124359    124232   80.8737   64.5391  0.00516726  0.00617791
2021-11-08T05:00:17.457773+0000     7     128    139742    139614   77.9034   60.0859  0.00796347  0.00641311
2021-11-08T05:00:18.457854+0000     8     128    153629    153501   74.9457   54.2461  0.00650283  0.00666606
2021-11-08T05:00:19.457940+0000     9     128    174595    174467   75.7174   81.8984  0.00642062  0.00659968
2021-11-08T05:00:20.458023+0000    10     127    195396    195269   76.2708   81.2578  0.00454629  0.00655216
2021-11-08T05:00:21.458098+0000    11     127    214947    214820   76.2794   76.3711   0.0105647  0.00654936
2021-11-08T05:00:22.458174+0000    12     128    220662    220534   71.7827   22.3203   0.0187607  0.00694825
2021-11-08T05:00:23.458247+0000    13     128    223030    222902   66.9725      9.25   0.0583647  0.00744785
2021-11-08T05:00:24.458325+0000    14     128    232939    232811   64.9533    38.707   0.0105484  0.00767585
2021-11-08T05:00:25.458411+0000    15     128    245796    245668   63.9709   50.2227   0.0089438  0.00781164
2021-11-08T05:00:26.458495+0000    16     127    262136    262009   63.9619    63.832  0.00524849  0.00781424
2021-11-08T05:00:27.458582+0000    17     127    283889    283762   65.1974   84.9727  0.00470354  0.00766652
2021-11-08T05:00:28.458668+0000    18     127    303189    303062   65.7634   75.3906  0.00643283   0.0075997
2021-11-08T05:00:29.458743+0000    19     128    322409    322281   66.2531   75.0742  0.00755438  0.00754399
2021-11-08T05:00:30.458822+0000 min lat: 0.00274614 max lat: 0.0877854 avg lat: 0.00758489
2021-11-08T05:00:30.458822+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-11-08T05:00:30.458822+0000    20     128    337516    337388   65.8908   59.0117  0.00882632  0.00758489
2021-11-08T05:00:31.458905+0000    21     127    354133    354006    65.844   64.9141  0.00489321  0.00759134
2021-11-08T05:00:32.458982+0000    22     127    374944    374817    66.546    81.293  0.00698303  0.00751117
2021-11-08T05:00:33.459060+0000    23     127    397596    397469   67.4995   88.4844  0.00704271  0.00740502
2021-11-08T05:00:34.459135+0000    24     127    419804    419677   68.3013     86.75  0.00523334  0.00731841
2021-11-08T05:00:35.459212+0000    25     128    426079    425951   66.5495   24.5078   0.0466579  0.00750323
2021-11-08T05:00:36.459302+0000    26     128    427999    427871   64.2783       7.5    0.062967  0.00776394
2021-11-08T05:00:37.459382+0000    27     128    439369    439241   63.5425   44.4141  0.00712273  0.00786593
2021-11-08T05:00:38.459455+0000    28     128    453628    453500   63.2622   55.6992  0.00812241  0.00790132
2021-11-08T05:00:39.459527+0000    29     127    468346    468219   63.0633   57.4961  0.00558194  0.00792622
2021-11-08T05:00:40.459597+0000    30     128    481866    481738   62.7213   52.8086  0.00897119  0.00796915
2021-11-08T05:00:41.459677+0000    31     128    495107    494979   62.3664   51.7227  0.00905156   0.0080143
2021-11-08T05:00:42.459757+0000    32     128    510854    510726   62.3395   61.5117  0.00801151  0.00801824
2021-11-08T05:00:43.459833+0000    33     127    529205    529078   62.6226   71.6875  0.00947556  0.00798134
2021-11-08T05:00:44.459924+0000    34     128    541548    541420   62.1986   48.2109   0.0120562  0.00803618
2021-11-08T05:00:45.460017+0000    35     128    559976    559848    62.478   71.9844  0.00552139  0.00800045
2021-11-08T05:00:46.460133+0000    36     128    579777    579649   62.8908   77.3477  0.00847088  0.00794813
2021-11-08T05:00:47.460208+0000    37     128    599203    599075   63.2418   75.8828  0.00475342   0.0079042
2021-11-08T05:00:48.460297+0000    38     128    619649    619521   63.6791   79.8672  0.00639486  0.00784976
2021-11-08T05:00:49.460388+0000    39     128    633301    633173   63.4136   53.3281   0.0827197  0.00787494
2021-11-08T05:00:50.460472+0000 min lat: 0.000738032 max lat: 0.271882 avg lat: 0.00805448
2021-11-08T05:00:50.460472+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-11-08T05:00:50.460472+0000    40     128    635413    635285   62.0345      8.25   0.0222928  0.00805448
2021-11-08T05:00:51.460560+0000    41     127    642838    642711   61.2289   29.0078  0.00861888  0.00816363
2021-11-08T05:00:52.460636+0000    42     127    655435    655308   60.9426    49.207  0.00887543  0.00820228
2021-11-08T05:00:53.460718+0000    43     128    669243    669115   60.7795   53.9336   0.0097787  0.00822412
2021-11-08T05:00:54.460802+0000    44     128    682569    682441   60.5811   52.0547  0.00943975  0.00825123
2021-11-08T05:00:55.460882+0000    45     128    695888    695760   60.3909   52.0273  0.00853579  0.00827747
2021-11-08T05:00:56.460976+0000    46     128    708491    708363   60.1482   49.2305   0.0105681   0.0083102
2021-11-08T05:00:57.461065+0000    47     128    720883    720755   59.8983   48.4062   0.0102709   0.0083451
2021-11-08T05:00:58.461139+0000    48     128    732808    732680   59.6208    46.582   0.0468419  0.00837571
2021-11-08T05:00:59.461214+0000    49     128    740308    740180   59.0019   29.2969  0.00897717  0.00847215
2021-11-08T05:01:00.461293+0000    50     127    755500    755373   59.0087   59.3477  0.00419432   0.0084716
2021-11-08T05:01:01.461367+0000    51     128    776915    776787   59.4917   83.6484  0.00659956  0.00840244
2021-11-08T05:01:02.461442+0000    52     127    799077    798950   60.0124   86.5742  0.00528612  0.00832971
2021-11-08T05:01:03.461525+0000    53     127    822372    822245   60.5969   90.9961  0.00470851   0.0082495
2021-11-08T05:01:04.461605+0000    54     128    840921    840793   60.8163   72.4531   0.0516609   0.0082132
2021-11-08T05:01:05.461682+0000    55     128    842969    842841    59.856         8   0.0354252  0.00834691
2021-11-08T05:01:06.461766+0000    56     128    847412    847284    59.097   17.3555    0.012678  0.00845848
2021-11-08T05:01:07.461847+0000    57     127    859504    859377   58.8889   47.2383  0.00806965   0.0084886
2021-11-08T05:01:08.461926+0000    58     128    873070    872942   58.7871   52.9883  0.00916259  0.00850321
2021-11-08T05:01:09.462004+0000    59     128    885900    885772   58.6401   50.1172   0.0106202  0.00852448
2021-11-08T05:01:10.462088+0000 min lat: 0.000738032 max lat: 0.271882 avg lat: 0.00853696
2021-11-08T05:01:10.462088+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-11-08T05:01:10.462088+0000    60      57    899522    899465   58.5542   53.4883  0.00855307  0.00853696
2021-11-08T05:01:11.462209+0000 Total time run:         60.0065
Total writes made:      899522
Write size:             4096
Object size:            4096
Bandwidth (MB/sec):     58.5563
Stddev Bandwidth:       22.6066
Max bandwidth (MB/sec): 90.9961
Min bandwidth (MB/sec): 7.5
Average IOPS:           14990
Stddev IOPS:            5787.3
Max IOPS:               23295
Min IOPS:               1920
Average Latency(s):     0.00853697
Stddev Latency(s):      0.00868966
Max latency(s):         0.271882
Min latency(s):         0.000738032

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:01:12,132524250-05:00] INFO: > Stop system activity collection services[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:01:12,137476860-05:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:391 - rados_bench() > kill -INT 121601

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:01:12,142311417-05:00] INFO: >> for OSD host[0m
# ./benchmarks/bench-rados:396 - rados_bench() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.2 kill -INT 11965
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:01:12,149544947-05:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.2 kill -INT 11965
[1] 00:01:12 [SUCCESS] ljishen@10.10.2.2

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:01:12,331526496-05:00] INFO: >>> Copy system activity file from OSD host[0m
## ./benchmarks/bench-rados:403 - rados_bench() > basename -- /tmp/bench-rados/sys_activity_4KB_write.dat.osd_host.2
# ./benchmarks/bench-rados:403 - rados_bench() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.2:/tmp/bench-rados/sys_activity_4KB_write.dat.osd_host.2 /mnt/sda8/ceph-research/scripts/output/sys_activity_4KB_write.dat.osd_host.2


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:01:12,502444722-05:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:01:36,432649570-05:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 899.52k objects, 3.4 GiB
    usage:   11 GiB used, 89 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:01:36,439103160-05:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:01:45,438109334-05:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 899.52k objects, 3.4 GiB
    usage:   11 GiB used, 89 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:01:45,444716594-05:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:01:54,380859599-05:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 899.52k objects, 3.4 GiB
    usage:   11 GiB used, 89 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:01:54,387472298-05:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:02:03,476702988-05:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 899.52k objects, 3.4 GiB
    usage:   11 GiB used, 89 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:02:03,483477994-05:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:02:12,605401157-05:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 899.52k objects, 3.4 GiB
    usage:   11 GiB used, 89 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:02:12,612202412-05:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:02:12,617150713-05:00] INFO: > The cluster is idle now.[0m


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:02:12,620011268-05:00] STAGE: Start benchmarking of mode seq ...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:02:12,625458420-05:00] INFO: > Start system activity collection services[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:02:12,630081758-05:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:341 - rados_bench() > rm --force /mnt/sda8/ceph-research/scripts/output/sys_activity_4KB_seq.dat.rados_bench_host.2
# ./benchmarks/bench-rados:342 - rados_bench() > sar_pid_rados_bench_host=124008
# ./benchmarks/bench-rados:342 - rados_bench() > S_TIME_FORMAT=ISO
# ./benchmarks/bench-rados:342 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/output/sys_activity_4KB_seq.dat.rados_bench_host.2 2

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:02:12,636000118-05:00] INFO: >> for OSD host[0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:02:12,644002369-05:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.2 bash
10.10.2.2: b'12082\n'
[1] 00:02:12 [SUCCESS] ljishen@10.10.2.2
12082

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:02:12,828772977-05:00] INFO: > Run rados bench[0m
# ./benchmarks/bench-rados:379 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/output/rados_bench_4KB_seq.log.2
## ./benchmarks/bench-rados:376 - rados_bench() > count_cpus 32-63,96-127
# ./benchmarks/bench-rados:376 - rados_bench() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 128 --show-time
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:02:12,846997925-05:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:02:12,849644977-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'f923c7de-404f-11ec-aba1-c5f3c07a3dc7', '--', 'rados', 'bench', '99999999', 'seq', '--pool', 'bench_rados', '--concurrent-ios', '128', '--show-time'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid f923c7de-404f-11ec-aba1-c5f3c07a3dc7 -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 128 --show-time
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
hints = 1
2021-11-08T05:02:15.875407+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-11-08T05:02:15.875407+0000     0       0         0         0         0         0           -           0
2021-11-08T05:02:16.875526+0000     1     127     41546     41419   161.763   161.793  0.00271742  0.00307986
2021-11-08T05:02:17.875612+0000     2     127     81219     81092   158.361   154.973  0.00348128  0.00314986
2021-11-08T05:02:18.875696+0000     3     127    123628    123501    160.79    165.66  0.00294799  0.00310285
2021-11-08T05:02:19.875762+0000     4     127    164606    164479   160.607    160.07  0.00265548   0.0031074
2021-11-08T05:02:20.875832+0000     5     127    202989    202862    158.47   149.934  0.00369744  0.00314954
2021-11-08T05:02:21.875898+0000     6     128    240865    240737   156.715   147.949  0.00306778   0.0031738
2021-11-08T05:02:22.875963+0000     7     127    277203    277076   154.605   141.949  0.00238834  0.00322901
2021-11-08T05:02:23.876029+0000     8     127    318496    318369    155.44   161.301  0.00357536  0.00321185
2021-11-08T05:02:24.876103+0000     9     127    359992    359865   156.178   162.094  0.00284913  0.00319665
2021-11-08T05:02:25.876219+0000    10     128    400004    399876   156.188   156.293  0.00291564  0.00319619
2021-11-08T05:02:26.876286+0000    11     128    440741    440613   156.454   159.129   0.0152993  0.00318778
2021-11-08T05:02:27.876352+0000    12     128    481016    480888   156.526   157.324  0.00246095  0.00318955
2021-11-08T05:02:28.876426+0000    13     128    522367    522239    156.91   161.527  0.00309797  0.00318174
2021-11-08T05:02:29.876501+0000    14     127    562591    562464   156.924   157.129  0.00363451  0.00318125
2021-11-08T05:02:30.876617+0000    15     128    602204    602076   156.777   154.734  0.00315758  0.00318458
2021-11-08T05:02:31.876690+0000    16     128    642451    642323   156.804   157.215  0.00354636  0.00318402
2021-11-08T05:02:32.876780+0000    17     128    681267    681139   156.498   151.625  0.00372737  0.00319011
2021-11-08T05:02:33.876860+0000    18     127    718285    718158   155.837   144.605  0.00315569  0.00320397
2021-11-08T05:02:34.876947+0000    19     127    758406    758279   155.883   156.723  0.00303649  0.00320292
2021-11-08T05:02:35.877020+0000 min lat: 0.000240965 max lat: 0.115296 avg lat: 0.00319792
2021-11-08T05:02:35.877020+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-11-08T05:02:35.877020+0000    20     128    799587    799459   156.131   160.859  0.00205018  0.00319792
2021-11-08T05:02:36.877094+0000    21     128    839819    839691   156.179   157.156  0.00291376  0.00319681
2021-11-08T05:02:37.877161+0000    22     128    877857    877729   155.834   148.586  0.00319904  0.00320398
2021-11-08T05:02:38.877257+0000 Total time run:       22.5285
Total reads made:     899522
Read size:            4096
Object size:          4096
Bandwidth (MB/sec):   155.969
Average IOPS:         39928
Stddev IOPS:          1564.39
Max IOPS:             42409
Min IOPS:             36339
Average Latency(s):   0.0032013
Max latency(s):       0.115296
Min latency(s):       0.000240965

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:02:39,521544236-05:00] INFO: > Stop system activity collection services[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:02:39,526567649-05:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:391 - rados_bench() > kill -INT 124008

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:02:39,531354906-05:00] INFO: >> for OSD host[0m
# ./benchmarks/bench-rados:396 - rados_bench() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.2 kill -INT 12082
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:02:39,538247324-05:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.2 kill -INT 12082
[1] 00:02:39 [SUCCESS] ljishen@10.10.2.2

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:02:39,720332478-05:00] INFO: >>> Copy system activity file from OSD host[0m
## ./benchmarks/bench-rados:403 - rados_bench() > basename -- /tmp/bench-rados/sys_activity_4KB_seq.dat.osd_host.2
# ./benchmarks/bench-rados:403 - rados_bench() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.2:/tmp/bench-rados/sys_activity_4KB_seq.dat.osd_host.2 /mnt/sda8/ceph-research/scripts/output/sys_activity_4KB_seq.dat.osd_host.2


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:02:39,875427026-05:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:03:03,711418882-05:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 899.52k objects, 3.4 GiB
    usage:   11 GiB used, 89 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:03:03,718036310-05:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:03:12,963284141-05:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 899.52k objects, 3.4 GiB
    usage:   11 GiB used, 89 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:03:12,969833161-05:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:03:22,006448749-05:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 899.52k objects, 3.4 GiB
    usage:   11 GiB used, 89 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:03:22,013019219-05:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:03:31,125208072-05:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 899.52k objects, 3.4 GiB
    usage:   11 GiB used, 89 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:03:31,131529723-05:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:03:40,367721495-05:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 899.52k objects, 3.4 GiB
    usage:   11 GiB used, 89 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:03:40,373982712-05:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:03:40,378785930-05:00] INFO: > The cluster is idle now.[0m


[1;7;39;49m[2021-11-08T00:03:40,380721290-05:00][RUNNING][ROUND 3/1/21] object_size=4KB[0m


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:03:40,383469532-05:00] STAGE: Launch a Ceph cluster (CEPH_HOST: 10.10.2.3, OSD_HOST: 10.10.2.2) ...[0m
# ./benchmarks/bench-rados:183 - launch_ceph_cluster() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.3 bash -euo pipefail
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:03:40,392116709-05:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.3 bash -euo pipefail
10.10.2.3: b'\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T00:03:40,782510438-05:00] INFO: > Remove existing clusters\x1b[0m\n'
10.10.2.3: b'## bash:17 -  > basename -- /var/lib/ceph/f923c7de-404f-11ec-aba1-c5f3c07a3dc7\n'
10.10.2.3: b'# bash:17 -  > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm --host 10.10.2.3 --host 10.10.2.2 rm-cluster --force --fsid f923c7de-404f-11ec-aba1-c5f3c07a3dc7\n'
10.10.2.3: b"\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T00:03:40,793212672-05:00] DEBUG: HOST_PARAMS: ['--host', '10.10.2.3', '--host', '10.10.2.2']\x1b[0m\n"
10.10.2.3: b"\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T00:03:40,796516536-05:00] DEBUG: CEPHADM_PARAMS: ['rm-cluster', '--force', '--fsid', 'f923c7de-404f-11ec-aba1-c5f3c07a3dc7']\x1b[0m\n"
10.10.2.3: b"# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:63 -  > /tmp/bench-rados/ceph-research/scripts/parallel-ssh --host 10.10.2.3 --host 10.10.2.2 'sudo python3 -u - rm-cluster --force --fsid f923c7de-404f-11ec-aba1-c5f3c07a3dc7'\n"
10.10.2.3: b'\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T00:03:40,804217608-05:00] DEBUG: command from stdin\x1b[0m\n'
10.10.2.3: b"# /tmp/bench-rados/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host 10.10.2.3 --host 10.10.2.2 'sudo python3 -u - rm-cluster --force --fsid f923c7de-404f-11ec-aba1-c5f3c07a3dc7'\n"
10.10.2.3: b'[1] 00:03:46 [SUCCESS] 10.10.2.3\n[2] 00:03:50 [SUCCESS] 10.10.2.2\n'
10.10.2.3: b'\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T00:03:50,625626065-05:00] INFO: > Deploy a new cluster\x1b[0m\n'
10.10.2.3: b'# bash:24 -  > /tmp/bench-rados/ceph-research/scripts/ceph-deploy -m 10.10.2.3 -o node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/10.10.2.2:/dev/nvme0n1\n'
10.10.2.3: b'\n\n\x1b[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T00:03:50,637140822-05:00] STAGE: Verify passwordless SSH to hosts, passwordless sudo via ljishen, and the existence of devices on hosts...\x1b[0m\n'
10.10.2.3: b'\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T00:03:50,641933145-05:00] INFO: > Check the host for the monitor\x1b[0m\n'
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:181 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@10.10.2.3 sudo --non-interactive --validate\n'
10.10.2.3: b'\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T00:03:50,789640429-05:00] INFO: > Check hosts for OSDs\x1b[0m\n'
10.10.2.3: b"\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T00:03:50,794636717-05:00] INFO: >> Check host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'\x1b[0m\n"
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:200 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us sudo --non-interactive --validate\n'
10.10.2.3: b"\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T00:03:50,941644923-05:00] INFO: >>> Check device '/dev/nvme0n1'\x1b[0m\n"
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:215 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us test -b /dev/nvme0n1\n'
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:226 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us lsblk --nodeps --noheadings --output TYPE /dev/nvme0n1\n# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:227 - verify_parameters() > grep --word-regexp disk\n'
10.10.2.3: b'\n\n\x1b[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T00:03:51,217851708-05:00] STAGE: Reset devices on hosts...\x1b[0m\n'
10.10.2.3: b"\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T00:03:51,222810735-05:00] INFO: > Resetting device '/dev/nvme0n1' on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'\x1b[0m\n"
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:288 - reset_host_devices() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us sudo sh\n'
10.10.2.3: b'  Removing ceph--96f2434f--1c60--44ed--8d10--ff54420f5588-osd--block--139419ee--1f7c--4153--b166--c1405df63b82 (253:0)\n'
10.10.2.3: b'  Archiving volume group "ceph-96f2434f-1c60-44ed-8d10-ff54420f5588" metadata (seqno 5).\n'
10.10.2.3: b'  Releasing logical volume "osd-block-139419ee-1f7c-4153-b166-c1405df63b82"\n'
10.10.2.3: b'  Creating volume group backup "/etc/lvm/backup/ceph-96f2434f-1c60-44ed-8d10-ff54420f5588" (seqno 6).\n'
10.10.2.3: b'  Logical volume "osd-block-139419ee-1f7c-4153-b166-c1405df63b82" successfully removed\n'
10.10.2.3: b'  Removing physical volume "/dev/nvme0n1" from volume group "ceph-96f2434f-1c60-44ed-8d10-ff54420f5588"\n'
10.10.2.3: b'  Volume group "ceph-96f2434f-1c60-44ed-8d10-ff54420f5588" successfully removed\n'
10.10.2.3: b'\n\n\x1b[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T00:03:51,623749552-05:00] STAGE: Bootstrap a new cluster...\x1b[0m\n'
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:317 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.3 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults\n'
10.10.2.3: b'\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T00:03:51,633341382-05:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.3: b"\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T00:03:51,636983584-05:00] DEBUG: CEPHADM_PARAMS: ['bootstrap', '--config', '/tmp/bench-rados/ceph-research/scripts/ceph.conf', '--mon-ip', '10.10.2.3', '--output-dir', '/tmp/bench-rados/deployment_data_root/etc/ceph', '--ssh-user', 'ljishen', '--skip-dashboard', '--no-minimize-config', '--allow-overwrite', '--allow-fqdn-hostname', '--orphan-initial-daemons', '--skip-monitoring-stack', '--single-host-defaults']\x1b[0m\n"
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.3 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults\n'
10.10.2.3: b'Verifying podman|docker is present...\nVerifying lvm2 is present...\n'
10.10.2.3: b'Verifying time synchronization is in place...\n'
10.10.2.3: b'Unit ntp.service is enabled and running\n'
10.10.2.3: b'Repeating the final host check...\n'
10.10.2.3: b'docker (/usr/bin/docker) is present\n'
10.10.2.3: b'systemctl is present\n'
10.10.2.3: b'lvcreate is present\n'
10.10.2.3: b'Unit ntp.service is enabled and running\n'
10.10.2.3: b'Host looks OK\n'
10.10.2.3: b'Cluster fsid: 43f824ca-4051-11ec-aba1-c5f3c07a3dc7\n'
10.10.2.3: b'Verifying IP 10.10.2.3 port 3300 ...\n'
10.10.2.3: b'Verifying IP 10.10.2.3 port 6789 ...\n'
10.10.2.3: b'Mon IP `10.10.2.3` is in CIDR network `10.10.2.0/24`\n'
10.10.2.3: b'- internal network (--cluster-network) has not been provided, OSD replication will default to the public_network\n'
10.10.2.3: b'Adjusting default settings to suit single-host cluster...\n'
10.10.2.3: b'Pulling container image quay.io/ceph/ceph:v16...\n'
10.10.2.3: b'Ceph version: ceph version 16.2.6 (ee28fb57e47e9f88813e24bbf4c14496ca299d31) pacific (stable)\n'
10.10.2.3: b'Extracting ceph user uid/gid from container image...\n'
10.10.2.3: b'Creating initial keys...\n'
10.10.2.3: b'Creating initial monmap...\n'
10.10.2.3: b'Creating mon...\n'
10.10.2.3: b'Waiting for mon to start...\n'
10.10.2.3: b'Waiting for mon...\n'
10.10.2.3: b'mon is available\nSetting mon public_network to 10.10.2.0/24\n'
10.10.2.3: b'Wrote config to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf\n'
10.10.2.3: b'Wrote keyring to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring\n'
10.10.2.3: b'Creating mgr...\n'
10.10.2.3: b'Verifying port 9283 ...\n'
10.10.2.3: b'Waiting for mgr to start...\nWaiting for mgr...\n'
10.10.2.3: b'mgr not available, waiting (1/15)...\n'
10.10.2.3: b'mgr not available, waiting (2/15)...\n'
10.10.2.3: b'mgr is available\n'
10.10.2.3: b'Enabling cephadm module...\n'
10.10.2.3: b'Waiting for the mgr to restart...\n'
10.10.2.3: b'Waiting for mgr epoch 5...\n'
10.10.2.3: b'mgr epoch 5 is available\nSetting orchestrator backend to cephadm...\n'
10.10.2.3: b'Generating ssh key...\n'
10.10.2.3: b'Wrote public SSH key to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub\n'
10.10.2.3: b'Adding key to ljishen@localhost authorized_keys...\n'
10.10.2.3: b'Adding host node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us...\n'
10.10.2.3: b'Deploying unmanaged mon service...\n'
10.10.2.3: b'Deploying unmanaged mgr service...\n'
10.10.2.3: b'You can access the Ceph CLI with:\n\n\tsudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --fsid 43f824ca-4051-11ec-aba1-c5f3c07a3dc7 -c /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf -k /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring\n\n'
10.10.2.3: b'Please consider enabling telemetry to help improve Ceph:\n\n\tceph telemetry on\n\nFor more information see:\n\n\thttps://docs.ceph.com/docs/pacific/mgr/telemetry/\n\n'
10.10.2.3: b'Bootstrap complete.\n'
10.10.2.3: b'\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T00:04:56,276595849-05:00] INFO: Sleep for 20 seconds\x1b[0m\n'
10.10.2.3: b'\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T00:05:16,284068346-05:00] INFO: > Set the default number of monitor daemons to 1\x1b[0m\n# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:337 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1\n'
10.10.2.3: b'\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T00:05:16,293568833-05:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.3: b"\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T00:05:16,297350599-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mon', '1']\x1b[0m\n"
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1\n'
10.10.2.3: b'Inferring fsid 43f824ca-4051-11ec-aba1-c5f3c07a3dc7\n'
10.10.2.3: b'Inferring config /var/lib/ceph/43f824ca-4051-11ec-aba1-c5f3c07a3dc7/mon.node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.3: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.3: b'Scheduled mon update...\n'
10.10.2.3: b'\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T00:05:26,072228847-05:00] INFO: > Set the default number of manager daemons to 1\x1b[0m\n'
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:342 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1\n'
10.10.2.3: b'\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T00:05:26,081852727-05:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.3: b"\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T00:05:26,085266949-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mgr', '1']\x1b[0m\n"
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1\n'
10.10.2.3: b'Inferring fsid 43f824ca-4051-11ec-aba1-c5f3c07a3dc7\n'
10.10.2.3: b'Inferring config /var/lib/ceph/43f824ca-4051-11ec-aba1-c5f3c07a3dc7/mon.node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.3: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.3: b'Scheduled mgr update...\n'
10.10.2.3: b'\n\n\x1b[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T00:05:36,333550596-05:00] STAGE: Add hosts to the cluster...\x1b[0m\n'
10.10.2.3: b"\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T00:05:36,339954130-05:00] INFO: > Adding host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'\x1b[0m\n"
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:361 - add_hosts() > ssh-copy-id -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -f -i /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\n'
10.10.2.3: b'/usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: "/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub"\n'
10.10.2.3: b'\nNumber of key(s) added: 1\n\nNow try logging into the machine, with:   "ssh -o \'GlobalKnownHostsFile=/dev/null\' -o \'LogLevel=ERROR\' -o \'PasswordAuthentication=no\' -o \'StrictHostKeyChecking=no\' -o \'UserKnownHostsFile=/dev/null\' \'ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\'"\nand check to make sure that only the key(s) you wanted were added.\n\n'
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:367 - add_hosts() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us 10.10.2.2\n'
10.10.2.3: b'\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T00:05:36,548055090-05:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.3: b"\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T00:05:36,551595300-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'host', 'add', 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us', '10.10.2.2']\x1b[0m\n"
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us 10.10.2.2\n'
10.10.2.3: b'Inferring fsid 43f824ca-4051-11ec-aba1-c5f3c07a3dc7\n'
10.10.2.3: b'Inferring config /var/lib/ceph/43f824ca-4051-11ec-aba1-c5f3c07a3dc7/mon.node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.3: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.3: b"Added host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us' with addr '10.10.2.2'\n"
10.10.2.3: b'\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T00:05:45,299852543-05:00] INFO: Sleep for 20 seconds\x1b[0m\n'
10.10.2.3: b'\n\n\x1b[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T00:06:05,304814566-05:00] STAGE: Add OSDs to the cluster...\x1b[0m\n'
10.10.2.3: b"\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T00:06:05,311365457-05:00] INFO: > Adding devices ['/dev/nvme0n1'] on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us' as OSDs\x1b[0m\n"
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:388 - add_host_devices_as_osds() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1\n'
10.10.2.3: b'\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T00:06:05,321388739-05:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.3: b"\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T00:06:05,325380471-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'daemon', 'add', 'osd', 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1']\x1b[0m\n"
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1\n'
10.10.2.3: b'Inferring fsid 43f824ca-4051-11ec-aba1-c5f3c07a3dc7\n'
10.10.2.3: b'Inferring config /var/lib/ceph/43f824ca-4051-11ec-aba1-c5f3c07a3dc7/mon.node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.3: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.3: b"Created osd(s) 0 on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'\n"
10.10.2.3: b'\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T00:06:31,812311237-05:00] INFO: > Sleep for 20 seconds\x1b[0m\n'
10.10.2.3: b'\n\n\x1b[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T00:06:51,818021526-05:00] STAGE: Show Cluster Status\x1b[0m\n'
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:408 - show_status() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status\n'
10.10.2.3: b'\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T00:06:51,828337522-05:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.3: b"\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T00:06:51,831800336-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', '--status']\x1b[0m\n"
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status\n'
10.10.2.3: b'Inferring fsid 43f824ca-4051-11ec-aba1-c5f3c07a3dc7\n'
10.10.2.3: b'Inferring config /var/lib/ceph/43f824ca-4051-11ec-aba1-c5f3c07a3dc7/mon.node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.3: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.3: b'  cluster:\n    id:     43f824ca-4051-11ec-aba1-c5f3c07a3dc7\n    health: HEALTH_WARN\n            1 pool(s) have no replicas configured\n \n  services:\n    mon: 1 daemons, quorum node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us (age 2m)\n    mgr: node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us.bcprtg(active, since 2m)\n    osd: 1 osds: 1 up (since 18s), 1 in (since 42s)\n \n  data:\n    pools:   1 pools, 1 pgs\n    objects: 0 objects, 0 B\n    usage:   4.7 MiB used, 100 GiB / 100 GiB avail\n    pgs:     1 active+clean\n \n'
[1] 00:07:01 [SUCCESS] ljishen@10.10.2.3

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:03:40,782510438-05:00] INFO: > Remove existing clusters[0m
## bash:17 -  > basename -- /var/lib/ceph/f923c7de-404f-11ec-aba1-c5f3c07a3dc7
# bash:17 -  > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm --host 10.10.2.3 --host 10.10.2.2 rm-cluster --force --fsid f923c7de-404f-11ec-aba1-c5f3c07a3dc7
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:03:40,793212672-05:00] DEBUG: HOST_PARAMS: ['--host', '10.10.2.3', '--host', '10.10.2.2'][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:03:40,796516536-05:00] DEBUG: CEPHADM_PARAMS: ['rm-cluster', '--force', '--fsid', 'f923c7de-404f-11ec-aba1-c5f3c07a3dc7'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:63 -  > /tmp/bench-rados/ceph-research/scripts/parallel-ssh --host 10.10.2.3 --host 10.10.2.2 'sudo python3 -u - rm-cluster --force --fsid f923c7de-404f-11ec-aba1-c5f3c07a3dc7'
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:03:40,804217608-05:00] DEBUG: command from stdin[0m
# /tmp/bench-rados/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host 10.10.2.3 --host 10.10.2.2 'sudo python3 -u - rm-cluster --force --fsid f923c7de-404f-11ec-aba1-c5f3c07a3dc7'
[1] 00:03:46 [SUCCESS] 10.10.2.3
[2] 00:03:50 [SUCCESS] 10.10.2.2

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:03:50,625626065-05:00] INFO: > Deploy a new cluster[0m
# bash:24 -  > /tmp/bench-rados/ceph-research/scripts/ceph-deploy -m 10.10.2.3 -o node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/10.10.2.2:/dev/nvme0n1


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:03:50,637140822-05:00] STAGE: Verify passwordless SSH to hosts, passwordless sudo via ljishen, and the existence of devices on hosts...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:03:50,641933145-05:00] INFO: > Check the host for the monitor[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:181 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@10.10.2.3 sudo --non-interactive --validate

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:03:50,789640429-05:00] INFO: > Check hosts for OSDs[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:03:50,794636717-05:00] INFO: >> Check host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:200 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us sudo --non-interactive --validate

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:03:50,941644923-05:00] INFO: >>> Check device '/dev/nvme0n1'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:215 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us test -b /dev/nvme0n1
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:226 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us lsblk --nodeps --noheadings --output TYPE /dev/nvme0n1
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:227 - verify_parameters() > grep --word-regexp disk


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:03:51,217851708-05:00] STAGE: Reset devices on hosts...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:03:51,222810735-05:00] INFO: > Resetting device '/dev/nvme0n1' on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:288 - reset_host_devices() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us sudo sh
  Removing ceph--96f2434f--1c60--44ed--8d10--ff54420f5588-osd--block--139419ee--1f7c--4153--b166--c1405df63b82 (253:0)
  Archiving volume group "ceph-96f2434f-1c60-44ed-8d10-ff54420f5588" metadata (seqno 5).
  Releasing logical volume "osd-block-139419ee-1f7c-4153-b166-c1405df63b82"
  Creating volume group backup "/etc/lvm/backup/ceph-96f2434f-1c60-44ed-8d10-ff54420f5588" (seqno 6).
  Logical volume "osd-block-139419ee-1f7c-4153-b166-c1405df63b82" successfully removed
  Removing physical volume "/dev/nvme0n1" from volume group "ceph-96f2434f-1c60-44ed-8d10-ff54420f5588"
  Volume group "ceph-96f2434f-1c60-44ed-8d10-ff54420f5588" successfully removed


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:03:51,623749552-05:00] STAGE: Bootstrap a new cluster...[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:317 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.3 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:03:51,633341382-05:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:03:51,636983584-05:00] DEBUG: CEPHADM_PARAMS: ['bootstrap', '--config', '/tmp/bench-rados/ceph-research/scripts/ceph.conf', '--mon-ip', '10.10.2.3', '--output-dir', '/tmp/bench-rados/deployment_data_root/etc/ceph', '--ssh-user', 'ljishen', '--skip-dashboard', '--no-minimize-config', '--allow-overwrite', '--allow-fqdn-hostname', '--orphan-initial-daemons', '--skip-monitoring-stack', '--single-host-defaults'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.3 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults
Verifying podman|docker is present...
Verifying lvm2 is present...
Verifying time synchronization is in place...
Unit ntp.service is enabled and running
Repeating the final host check...
docker (/usr/bin/docker) is present
systemctl is present
lvcreate is present
Unit ntp.service is enabled and running
Host looks OK
Cluster fsid: 43f824ca-4051-11ec-aba1-c5f3c07a3dc7
Verifying IP 10.10.2.3 port 3300 ...
Verifying IP 10.10.2.3 port 6789 ...
Mon IP `10.10.2.3` is in CIDR network `10.10.2.0/24`
- internal network (--cluster-network) has not been provided, OSD replication will default to the public_network
Adjusting default settings to suit single-host cluster...
Pulling container image quay.io/ceph/ceph:v16...
Ceph version: ceph version 16.2.6 (ee28fb57e47e9f88813e24bbf4c14496ca299d31) pacific (stable)
Extracting ceph user uid/gid from container image...
Creating initial keys...
Creating initial monmap...
Creating mon...
Waiting for mon to start...
Waiting for mon...
mon is available
Setting mon public_network to 10.10.2.0/24
Wrote config to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf
Wrote keyring to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring
Creating mgr...
Verifying port 9283 ...
Waiting for mgr to start...
Waiting for mgr...
mgr not available, waiting (1/15)...
mgr not available, waiting (2/15)...
mgr is available
Enabling cephadm module...
Waiting for the mgr to restart...
Waiting for mgr epoch 5...
mgr epoch 5 is available
Setting orchestrator backend to cephadm...
Generating ssh key...
Wrote public SSH key to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub
Adding key to ljishen@localhost authorized_keys...
Adding host node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us...
Deploying unmanaged mon service...
Deploying unmanaged mgr service...
You can access the Ceph CLI with:

	sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --fsid 43f824ca-4051-11ec-aba1-c5f3c07a3dc7 -c /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf -k /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring

Please consider enabling telemetry to help improve Ceph:

	ceph telemetry on

For more information see:

	https://docs.ceph.com/docs/pacific/mgr/telemetry/

Bootstrap complete.

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:04:56,276595849-05:00] INFO: Sleep for 20 seconds[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:05:16,284068346-05:00] INFO: > Set the default number of monitor daemons to 1[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:337 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:05:16,293568833-05:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:05:16,297350599-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mon', '1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1
Inferring fsid 43f824ca-4051-11ec-aba1-c5f3c07a3dc7
Inferring config /var/lib/ceph/43f824ca-4051-11ec-aba1-c5f3c07a3dc7/mon.node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Scheduled mon update...

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:05:26,072228847-05:00] INFO: > Set the default number of manager daemons to 1[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:342 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:05:26,081852727-05:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:05:26,085266949-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mgr', '1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1
Inferring fsid 43f824ca-4051-11ec-aba1-c5f3c07a3dc7
Inferring config /var/lib/ceph/43f824ca-4051-11ec-aba1-c5f3c07a3dc7/mon.node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Scheduled mgr update...


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:05:36,333550596-05:00] STAGE: Add hosts to the cluster...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:05:36,339954130-05:00] INFO: > Adding host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:361 - add_hosts() > ssh-copy-id -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -f -i /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us
/usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: "/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub"

Number of key(s) added: 1

Now try logging into the machine, with:   "ssh -o 'GlobalKnownHostsFile=/dev/null' -o 'LogLevel=ERROR' -o 'PasswordAuthentication=no' -o 'StrictHostKeyChecking=no' -o 'UserKnownHostsFile=/dev/null' 'ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'"
and check to make sure that only the key(s) you wanted were added.

# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:367 - add_hosts() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us 10.10.2.2
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:05:36,548055090-05:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:05:36,551595300-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'host', 'add', 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us', '10.10.2.2'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us 10.10.2.2
Inferring fsid 43f824ca-4051-11ec-aba1-c5f3c07a3dc7
Inferring config /var/lib/ceph/43f824ca-4051-11ec-aba1-c5f3c07a3dc7/mon.node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Added host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us' with addr '10.10.2.2'

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:05:45,299852543-05:00] INFO: Sleep for 20 seconds[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:06:05,304814566-05:00] STAGE: Add OSDs to the cluster...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:06:05,311365457-05:00] INFO: > Adding devices ['/dev/nvme0n1'] on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us' as OSDs[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:388 - add_host_devices_as_osds() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:06:05,321388739-05:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:06:05,325380471-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'daemon', 'add', 'osd', 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1
Inferring fsid 43f824ca-4051-11ec-aba1-c5f3c07a3dc7
Inferring config /var/lib/ceph/43f824ca-4051-11ec-aba1-c5f3c07a3dc7/mon.node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Created osd(s) 0 on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:06:31,812311237-05:00] INFO: > Sleep for 20 seconds[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:06:51,818021526-05:00] STAGE: Show Cluster Status[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:408 - show_status() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:06:51,828337522-05:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:06:51,831800336-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', '--status'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status
Inferring fsid 43f824ca-4051-11ec-aba1-c5f3c07a3dc7
Inferring config /var/lib/ceph/43f824ca-4051-11ec-aba1-c5f3c07a3dc7/mon.node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
  cluster:
    id:     43f824ca-4051-11ec-aba1-c5f3c07a3dc7
    health: HEALTH_WARN
            1 pool(s) have no replicas configured
 
  services:
    mon: 1 daemons, quorum node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us (age 2m)
    mgr: node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us.bcprtg(active, since 2m)
    osd: 1 osds: 1 up (since 18s), 1 in (since 42s)
 
  data:
    pools:   1 pools, 1 pgs
    objects: 0 objects, 0 B
    usage:   4.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     1 active+clean
 

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:07:01,424231014-05:00] INFO: > Copy configuration files to local[0m
# ./benchmarks/bench-rados:220 - launch_ceph_cluster() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.3 sudo chown --changes --recursive ljishen /tmp/bench-rados/deployment_data_root/etc/ceph
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:07:01,431256382-05:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.3 sudo chown --changes --recursive ljishen /tmp/bench-rados/deployment_data_root/etc/ceph
[1] 00:07:01 [SUCCESS] ljishen@10.10.2.3
# ./benchmarks/bench-rados:223 - launch_ceph_cluster() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.3:/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf
# ./benchmarks/bench-rados:226 - launch_ceph_cluster() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.3:/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:07:01,900094431-05:00] STAGE: Update local ceph.conf as a client...[0m


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:07:01,903169019-05:00] STAGE: Configure Ceph pool...[0m
# ./benchmarks/bench-rados:257 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_max_backfills 32
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:07:01,923421450-05:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:07:01,925990525-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '43f824ca-4051-11ec-aba1-c5f3c07a3dc7', '--', 'ceph', 'config', 'set', 'osd', 'osd_max_backfills', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 43f824ca-4051-11ec-aba1-c5f3c07a3dc7 -- ceph config set osd osd_max_backfills 32
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:258 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_active 32
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:07:05,978467321-05:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:07:05,981138759-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '43f824ca-4051-11ec-aba1-c5f3c07a3dc7', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_active', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 43f824ca-4051-11ec-aba1-c5f3c07a3dc7 -- ceph config set osd osd_recovery_max_active 32
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:259 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_single_start 8
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:07:10,146538640-05:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:07:10,149585466-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '43f824ca-4051-11ec-aba1-c5f3c07a3dc7', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_single_start', '8'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 43f824ca-4051-11ec-aba1-c5f3c07a3dc7 -- ceph config set osd osd_recovery_max_single_start 8
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:260 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_op_priority 63
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:07:14,339970884-05:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:07:14,342800700-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '43f824ca-4051-11ec-aba1-c5f3c07a3dc7', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_op_priority', '63'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 43f824ca-4051-11ec-aba1-c5f3c07a3dc7 -- ceph config set osd osd_recovery_op_priority 63
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:262 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config show-with-defaults osd.0
# ./benchmarks/bench-rados:263 - configure_ceph_pool() > grep 'osd_max_backfills\|osd_recovery'
# ./benchmarks/bench-rados:263 - configure_ceph_pool() > column -t -s ' '
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
osd_max_backfills                        32         mon
osd_recovery_cost                        20971520   default
osd_recovery_delay_start                 0.000000   default
osd_recovery_max_active                  32         mon
osd_recovery_max_active_hdd              3          default
osd_recovery_max_active_ssd              10         default
osd_recovery_max_chunk                   8388608    default
osd_recovery_max_omap_entries_per_chunk  8096       default
osd_recovery_max_single_start            8          mon
osd_recovery_op_priority                 3          default
osd_recovery_op_warn_multiple            16         default
osd_recovery_priority                    5          default
osd_recovery_retry_interval              30.000000  default
osd_recovery_sleep                       0.000000   default
osd_recovery_sleep_hdd                   0.100000   default
osd_recovery_sleep_hybrid                0.025000   default
osd_recovery_sleep_ssd                   0.000000   default
# ./benchmarks/bench-rados:265 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool create bench_rados 128 128 replicated --size 1 --pg-num-min 128
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:07:22,707561687-05:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:07:22,710559390-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '43f824ca-4051-11ec-aba1-c5f3c07a3dc7', '--', 'ceph', 'osd', 'pool', 'create', 'bench_rados', '128', '128', 'replicated', '--size', '1', '--pg-num-min', '128'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 43f824ca-4051-11ec-aba1-c5f3c07a3dc7 -- ceph osd pool create bench_rados 128 128 replicated --size 1 --pg-num-min 128
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
pool 'bench_rados' created
# ./benchmarks/bench-rados:267 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados min_size 1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:07:27,425648559-05:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:07:27,428660709-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '43f824ca-4051-11ec-aba1-c5f3c07a3dc7', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'min_size', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 43f824ca-4051-11ec-aba1-c5f3c07a3dc7 -- ceph osd pool set bench_rados min_size 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 min_size to 1
# ./benchmarks/bench-rados:268 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados pg_autoscale_mode off
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:07:31,704050216-05:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:07:31,707190117-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '43f824ca-4051-11ec-aba1-c5f3c07a3dc7', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'pg_autoscale_mode', 'off'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 43f824ca-4051-11ec-aba1-c5f3c07a3dc7 -- ceph osd pool set bench_rados pg_autoscale_mode off
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 pg_autoscale_mode to off
# ./benchmarks/bench-rados:269 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool application enable bench_rados benchmark
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:07:36,994300275-05:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:07:36,997162032-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '43f824ca-4051-11ec-aba1-c5f3c07a3dc7', '--', 'ceph', 'osd', 'pool', 'application', 'enable', 'bench_rados', 'benchmark'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 43f824ca-4051-11ec-aba1-c5f3c07a3dc7 -- ceph osd pool application enable bench_rados benchmark
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
enabled application 'benchmark' on pool 'bench_rados'
# ./benchmarks/bench-rados:270 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados noscrub 1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:07:41,414359834-05:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:07:41,417242530-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '43f824ca-4051-11ec-aba1-c5f3c07a3dc7', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'noscrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 43f824ca-4051-11ec-aba1-c5f3c07a3dc7 -- ceph osd pool set bench_rados noscrub 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 noscrub to 1
# ./benchmarks/bench-rados:271 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados nodeep-scrub 1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:07:45,818884720-05:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:07:45,821711290-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '43f824ca-4051-11ec-aba1-c5f3c07a3dc7', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'nodeep-scrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 43f824ca-4051-11ec-aba1-c5f3c07a3dc7 -- ceph osd pool set bench_rados nodeep-scrub 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 nodeep-scrub to 1
# ./benchmarks/bench-rados:272 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool ls detail
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:07:50,321612112-05:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:07:50,324573697-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '43f824ca-4051-11ec-aba1-c5f3c07a3dc7', '--', 'ceph', 'osd', 'pool', 'ls', 'detail'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 43f824ca-4051-11ec-aba1-c5f3c07a3dc7 -- ceph osd pool ls detail
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
pool 1 'device_health_metrics' replicated size 1 min_size 1 crush_rule 0 object_hash rjenkins pg_num 64 pgp_num 64 autoscale_mode on last_change 18 lfor 0/0/15 flags hashpspool stripe_width 0 pg_num_min 1 application mgr_devicehealth
pool 2 'bench_rados' replicated size 1 min_size 1 crush_rule 0 object_hash rjenkins pg_num 128 pgp_num 128 autoscale_mode off last_change 20 flags hashpspool,noscrub,nodeep-scrub stripe_width 0 pg_num_min 128 application benchmark

# ./benchmarks/bench-rados:273 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd df tree
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:07:54,223389533-05:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:07:54,226111095-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '43f824ca-4051-11ec-aba1-c5f3c07a3dc7', '--', 'ceph', 'osd', 'df', 'tree'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 43f824ca-4051-11ec-aba1-c5f3c07a3dc7 -- ceph osd df tree
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
ID  CLASS  WEIGHT   REWEIGHT  SIZE     RAW USE  DATA     OMAP  META     AVAIL    %USE  VAR   PGS  STATUS  TYPE NAME      
-1         0.09769         -  100 GiB  5.7 MiB  168 KiB   0 B  5.6 MiB  100 GiB  0.01  1.00    -          root default   
-3         0.09769         -  100 GiB  5.7 MiB  168 KiB   0 B  5.6 MiB  100 GiB  0.01  1.00    -              host node-1
 0    ssd  0.09769   1.00000  100 GiB  5.7 MiB  168 KiB   0 B  5.6 MiB  100 GiB  0.01  1.00  192      up          osd.0  
                       TOTAL  100 GiB  5.7 MiB  168 KiB   0 B  5.6 MiB  100 GiB  0.01                                    
MIN/MAX VAR: 1.00/1.00  STDDEV: 0


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:07:58,274266420-05:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:08:22,328520083-05:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:08:31,523114899-05:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:08:40,675846194-05:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:08:40,682369205-05:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:08:49,976495187-05:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:08:49,983384279-05:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:08:59,026255350-05:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:08:59,032951077-05:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:09:08,072110301-05:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:09:08,079294849-05:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:09:17,073002216-05:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:09:17,079415250-05:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:09:17,084639972-05:00] INFO: > The cluster is idle now.[0m


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:09:17,087957909-05:00] STAGE: Start benchmarking of mode write ...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:09:17,093533413-05:00] INFO: > Start system activity collection services[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:09:17,098154638-05:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:341 - rados_bench() > rm --force /mnt/sda8/ceph-research/scripts/output/sys_activity_4KB_write.dat.rados_bench_host.3
# ./benchmarks/bench-rados:342 - rados_bench() > sar_pid_rados_bench_host=133925
# ./benchmarks/bench-rados:342 - rados_bench() > S_TIME_FORMAT=ISO
# ./benchmarks/bench-rados:342 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/output/sys_activity_4KB_write.dat.rados_bench_host.3 2

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:09:17,104366943-05:00] INFO: >> for OSD host[0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:09:17,112520348-05:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.2 bash
10.10.2.2: b'15692\n'
[1] 00:09:17 [SUCCESS] ljishen@10.10.2.2
15692

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:09:17,300792332-05:00] INFO: > Run rados bench[0m
# ./benchmarks/bench-rados:370 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/output/rados_bench_4KB_write.log.3
## ./benchmarks/bench-rados:361 - rados_bench() > count_cpus 32-63,96-127
# ./benchmarks/bench-rados:361 - rados_bench() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 60 write --pool bench_rados -b 4096 -O 4096 --concurrent-ios 128 --show-time --write-object --write-omap --write-xattr --no-cleanup
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:09:17,319477698-05:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:09:17,322102087-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '43f824ca-4051-11ec-aba1-c5f3c07a3dc7', '--', 'rados', 'bench', '60', 'write', '--pool', 'bench_rados', '-b', '4096', '-O', '4096', '--concurrent-ios', '128', '--show-time', '--write-object', '--write-omap', '--write-xattr', '--no-cleanup'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 43f824ca-4051-11ec-aba1-c5f3c07a3dc7 -- rados bench 60 write --pool bench_rados -b 4096 -O 4096 --concurrent-ios 128 --show-time --write-object --write-omap --write-xattr --no-cleanup
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
hints = 1
2021-11-08T05:09:20.310380+0000 Maintaining 128 concurrent writes of 4096 bytes to objects of size 4096 for up to 60 seconds or 0 objects
2021-11-08T05:09:20.310389+0000 Object prefix: benchmark_data_node-0.bluefield2.ucsc-cmps10_7
2021-11-08T05:09:20.310943+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-11-08T05:09:20.310943+0000     0       0         0         0         0         0           -           0
2021-11-08T05:09:21.311057+0000     1     127     24080     23953   93.5611   93.5664  0.00595206    0.005319
2021-11-08T05:09:22.311149+0000     2     127     48320     48193     94.12   94.6875  0.00518495  0.00530219
2021-11-08T05:09:23.311219+0000     3     128     69889     69761    90.828     84.25  0.00488296  0.00549361
2021-11-08T05:09:24.311296+0000     4     127     88679     88552   86.4702   73.4023  0.00857542  0.00577545
2021-11-08T05:09:25.311378+0000     5     128    108316    108188   84.5155   76.7031  0.00958633  0.00590905
2021-11-08T05:09:26.311449+0000     6     127    123883    123756   80.5643   60.8125  0.00807734  0.00619906
2021-11-08T05:09:27.311520+0000     7     128    137675    137547   76.7504   53.8711  0.00931044  0.00650723
2021-11-08T05:09:28.311614+0000     8     128    152859    152731     74.57   59.3125  0.00602266  0.00670051
2021-11-08T05:09:29.311697+0000     9     128    175037    174909   75.9095   86.6328  0.00641068  0.00658154
2021-11-08T05:09:30.311777+0000    10     127    195911    195784   76.4722    81.543  0.00606636  0.00653436
2021-11-08T05:09:31.311852+0000    11     127    215946    215819   76.6343   78.2617  0.00850279  0.00652063
2021-11-08T05:09:32.311922+0000    12     128    219925    219797    71.543   15.5391    0.059357  0.00696704
2021-11-08T05:09:33.311999+0000    13     127    221998    221871   66.6628   8.10156   0.0144526  0.00749459
2021-11-08T05:09:34.312094+0000    14     128    233111    232983   65.0013   43.4062  0.00609751  0.00767172
2021-11-08T05:09:35.312180+0000    15     127    244896    244769   63.7369   46.0391  0.00942535  0.00784021
2021-11-08T05:09:36.312266+0000    16     128    261152    261024   63.7215   63.4961  0.00543391  0.00784287
2021-11-08T05:09:37.312351+0000    17     128    279731    279603   64.2419   72.5742  0.00576967  0.00777993
2021-11-08T05:09:38.312430+0000    18     127    300079    299952   65.0886   79.4883  0.00464584  0.00767907
2021-11-08T05:09:39.312503+0000    19     127    320453    320326   65.8513   79.5859   0.0091626  0.00758986
2021-11-08T05:09:40.312577+0000 min lat: 0.00279995 max lat: 0.102403 avg lat: 0.00765512
2021-11-08T05:09:40.312577+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-11-08T05:09:40.312577+0000    20     128    334409    334281   65.2841   54.5117   0.0104665  0.00765512
2021-11-08T05:09:41.312663+0000    21     128    349359    349231    64.956   58.3984  0.00336903  0.00769523
2021-11-08T05:09:42.312740+0000    22     128    368802    368674   65.4554   75.9492  0.00590292   0.0076364
2021-11-08T05:09:43.312816+0000    23     127    389408    389281   66.1091   80.4961  0.00865663  0.00755992
2021-11-08T05:09:44.312889+0000    24     128    410432    410304    66.776   82.1211  0.00573485  0.00748489
2021-11-08T05:09:45.312965+0000    25     128    426112    425984   66.5548     61.25   0.0489902  0.00750621
2021-11-08T05:09:46.313043+0000    26     128    428865    428737   64.4086   10.7539   0.0611366  0.00775088
2021-11-08T05:09:47.313116+0000    27     127    433771    433644   62.7329    19.168  0.00881497  0.00796769
2021-11-08T05:09:48.313189+0000    28     127    446844    446717   62.3161   51.0664  0.00970698  0.00802021
2021-11-08T05:09:49.313265+0000    29     128    460148    460020   61.9591   51.9648    0.011045  0.00806607
2021-11-08T05:09:50.313335+0000    30     128    468110    467982   60.9304   31.1016   0.0110465  0.00820345
2021-11-08T05:09:51.313412+0000    31     128    479115    478987   60.3515   42.9883   0.0117127  0.00828169
2021-11-08T05:09:52.313481+0000    32     128    490635    490507   59.8717        45   0.0104816  0.00834858
2021-11-08T05:09:53.313556+0000    33     127    506460    506333   59.9306   61.8203  0.00481636  0.00834085
2021-11-08T05:09:54.313636+0000    34     127    526418    526291   60.4607   77.9609  0.00842215  0.00826698
2021-11-08T05:09:55.313715+0000    35     128    541003    540875   60.3608   56.9688   0.0101477  0.00828086
2021-11-08T05:09:56.313786+0000    36     128    554426    554298   60.1405   52.4336  0.00562061  0.00831135
2021-11-08T05:09:57.313875+0000    37     127    572642    572515   60.4382   71.1602  0.00568931   0.0082708
2021-11-08T05:09:58.313960+0000    38     128    592337    592209    60.872   76.9297  0.00489772  0.00821179
2021-11-08T05:09:59.314034+0000    39     127    612791    612664   61.3598   79.9023  0.00550734  0.00814655
2021-11-08T05:10:00.314103+0000 min lat: 0.00279995 max lat: 0.342824 avg lat: 0.00810193
2021-11-08T05:10:00.314103+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-11-08T05:10:00.314103+0000    40     128    631934    631806    61.695   74.7734   0.0059334  0.00810193
2021-11-08T05:10:01.314182+0000    41     128    638096    637968   60.7773   24.0703   0.0544832  0.00821756
2021-11-08T05:10:02.314252+0000    42     128    640192    640064   59.5252    8.1875   0.0614253  0.00838645
2021-11-08T05:10:03.314334+0000    43     128    650649    650521   59.0907   40.8477   0.0090525   0.0084589
2021-11-08T05:10:04.314414+0000    44     127    664656    664529   58.9913   54.7188  0.00962399  0.00847325
2021-11-08T05:10:05.314491+0000    45     128    676929    676801   58.7455   47.9375   0.0106767  0.00850893
2021-11-08T05:10:06.314570+0000    46     128    689531    689403   58.5385   49.2266   0.0101864  0.00853884
2021-11-08T05:10:07.314649+0000    47     128    701991    701863   58.3285   48.6719   0.0117761  0.00856978
2021-11-08T05:10:08.314722+0000    48     128    714273    714145   58.1128   47.9766   0.0105799  0.00860173
2021-11-08T05:10:09.314796+0000    49     128    726191    726063   57.8768   46.5547    0.010014  0.00863632
2021-11-08T05:10:10.314869+0000    50     128    733662    733534   57.3029   29.1836   0.0567218  0.00871649
2021-11-08T05:10:11.314967+0000    51     128    743998    743870   56.9709    40.375   0.0107674  0.00877424
2021-11-08T05:10:12.315046+0000    52     127    763556    763429   57.3445   76.4023  0.00625841  0.00871713
2021-11-08T05:10:13.315122+0000    53     128    786684    786556   57.9669   90.3398  0.00623904   0.0086235
2021-11-08T05:10:14.315199+0000    54     127    809750    809623   58.5619   90.1055  0.00375956   0.0085362
2021-11-08T05:10:15.315277+0000    55     128    832598    832470   59.1197   89.2461  0.00521036  0.00845551
2021-11-08T05:10:16.315350+0000    56     128    841724    841596   58.7005   35.6484   0.0611422  0.00850976
2021-11-08T05:10:17.315425+0000    57     128    844028    843900   57.8286         9   0.0543726  0.00864265
2021-11-08T05:10:18.315508+0000    58     128    855038    854910    57.573   43.0078  0.00830384  0.00868251
2021-11-08T05:10:19.315587+0000    59     128    869935    869807   57.5834   58.1914  0.00869734  0.00868091
2021-11-08T05:10:20.315661+0000 min lat: 0.0022929 max lat: 0.342824 avg lat: 0.00869591
2021-11-08T05:10:20.315661+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-11-08T05:10:20.315661+0000    60      64    883090    883026   57.4842   51.6367  0.00798359  0.00869591
2021-11-08T05:10:21.315775+0000 Total time run:         60.0066
Total writes made:      883090
Write size:             4096
Object size:            4096
Bandwidth (MB/sec):     57.4865
Stddev Bandwidth:       23.1969
Max bandwidth (MB/sec): 94.6875
Min bandwidth (MB/sec): 8.10156
Average IOPS:           14716
Stddev IOPS:            5938.4
Max IOPS:               24240
Min IOPS:               2074
Average Latency(s):     0.00869589
Stddev Latency(s):      0.00899403
Max latency(s):         0.342824
Min latency(s):         0.0022929

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:10:21,883950094-05:00] INFO: > Stop system activity collection services[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:10:21,888919185-05:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:391 - rados_bench() > kill -INT 133925

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:10:21,893877565-05:00] INFO: >> for OSD host[0m
# ./benchmarks/bench-rados:396 - rados_bench() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.2 kill -INT 15692
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:10:21,901215874-05:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.2 kill -INT 15692
[1] 00:10:22 [SUCCESS] ljishen@10.10.2.2

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:10:22,079989687-05:00] INFO: >>> Copy system activity file from OSD host[0m
## ./benchmarks/bench-rados:403 - rados_bench() > basename -- /tmp/bench-rados/sys_activity_4KB_write.dat.osd_host.3
# ./benchmarks/bench-rados:403 - rados_bench() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.2:/tmp/bench-rados/sys_activity_4KB_write.dat.osd_host.3 /mnt/sda8/ceph-research/scripts/output/sys_activity_4KB_write.dat.osd_host.3


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:10:22,250028168-05:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:10:46,232215958-05:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 883.09k objects, 3.4 GiB
    usage:   11 GiB used, 89 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:10:46,238944556-05:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:10:55,211906712-05:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 883.09k objects, 3.4 GiB
    usage:   11 GiB used, 89 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:10:55,218480759-05:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:11:04,344342238-05:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 883.09k objects, 3.4 GiB
    usage:   11 GiB used, 89 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:11:04,350983892-05:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:11:13,773427275-05:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 883.09k objects, 3.4 GiB
    usage:   11 GiB used, 89 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:11:13,780357053-05:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:11:22,857807346-05:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 883.09k objects, 3.4 GiB
    usage:   11 GiB used, 89 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:11:22,864318825-05:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:11:22,869471491-05:00] INFO: > The cluster is idle now.[0m


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:11:22,872819124-05:00] STAGE: Start benchmarking of mode seq ...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:11:22,878308716-05:00] INFO: > Start system activity collection services[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:11:22,883246608-05:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:341 - rados_bench() > rm --force /mnt/sda8/ceph-research/scripts/output/sys_activity_4KB_seq.dat.rados_bench_host.3
# ./benchmarks/bench-rados:342 - rados_bench() > sar_pid_rados_bench_host=136143
# ./benchmarks/bench-rados:342 - rados_bench() > S_TIME_FORMAT=ISO
# ./benchmarks/bench-rados:342 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/output/sys_activity_4KB_seq.dat.rados_bench_host.3 2

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:11:22,889215724-05:00] INFO: >> for OSD host[0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:11:22,897224196-05:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.2 bash
10.10.2.2: b'15800\n'
[1] 00:11:23 [SUCCESS] ljishen@10.10.2.2
15800

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:11:23,080982760-05:00] INFO: > Run rados bench[0m
# ./benchmarks/bench-rados:379 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/output/rados_bench_4KB_seq.log.3
## ./benchmarks/bench-rados:376 - rados_bench() > count_cpus 32-63,96-127
# ./benchmarks/bench-rados:376 - rados_bench() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 128 --show-time
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:11:23,099677744-05:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:11:23,102225859-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '43f824ca-4051-11ec-aba1-c5f3c07a3dc7', '--', 'rados', 'bench', '99999999', 'seq', '--pool', 'bench_rados', '--concurrent-ios', '128', '--show-time'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 43f824ca-4051-11ec-aba1-c5f3c07a3dc7 -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 128 --show-time
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
hints = 1
2021-11-08T05:11:26.348482+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-11-08T05:11:26.348482+0000     0       0         0         0         0         0           -           0
2021-11-08T05:11:27.348579+0000     1     128     38758     38630   150.874   150.898  0.00301877  0.00330498
2021-11-08T05:11:28.348665+0000     2     127     79007     78880   154.043   157.227  0.00304803  0.00323832
2021-11-08T05:11:29.348758+0000     3     128    117458    117330   152.756   150.195  0.00311125  0.00326641
2021-11-08T05:11:30.348828+0000     4     128    157238    157110   153.412   155.391  0.00366122  0.00325265
2021-11-08T05:11:31.348891+0000     5     128    197454    197326   154.146   157.094  0.00345256  0.00323809
2021-11-08T05:11:32.348964+0000     6     128    236715    236587   154.014   153.363  0.00304539  0.00324084
2021-11-08T05:11:33.349042+0000     7     128    276878    276750   154.423   156.887  0.00302465  0.00323253
2021-11-08T05:11:34.349133+0000     8     128    317524    317396   154.965   158.773  0.00315659   0.0032215
2021-11-08T05:11:35.349213+0000     9     127    354603    354476   153.839   144.844  0.00319926  0.00324506
2021-11-08T05:11:36.349287+0000    10     127    395246    395119    154.33   158.762   0.0030766  0.00323472
2021-11-08T05:11:37.349360+0000    11     128    434782    434654   154.338   154.434   0.0020338   0.0032349
2021-11-08T05:11:38.349457+0000    12     127    474525    474398   154.413    155.25  0.00344077  0.00323316
2021-11-08T05:11:39.349541+0000    13     128    513989    513861   154.392   154.152  0.00312578  0.00323372
2021-11-08T05:11:40.349631+0000    14     128    554703    554575   154.723   159.039  0.00256876   0.0032266
2021-11-08T05:11:41.349742+0000    15     128    593445    593317   154.496   151.336  0.00214089  0.00323162
2021-11-08T05:11:42.349817+0000    16     128    632202    632074   154.301   151.395  0.00271302  0.00323576
2021-11-08T05:11:43.349901+0000    17     128    671307    671179    154.21   152.754  0.00240843  0.00323763
2021-11-08T05:11:44.350022+0000    18     128    711540    711412   154.373    157.16  0.00341921  0.00323431
2021-11-08T05:11:45.350122+0000    19     127    751777    751650   154.519    157.18  0.00340281  0.00323129
2021-11-08T05:11:46.350190+0000 min lat: 0.000234983 max lat: 0.087147 avg lat: 0.00322625
2021-11-08T05:11:46.350190+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-11-08T05:11:46.350190+0000    20     128    792564    792436   154.759    159.32  0.00314103  0.00322625
2021-11-08T05:11:47.350272+0000    21     128    830410    830282   154.429   147.836  0.00406739  0.00323296
2021-11-08T05:11:48.350353+0000    22     127    868755    868628   154.217   149.789  0.00305348   0.0032376
2021-11-08T05:11:49.350454+0000 Total time run:       22.3734
Total reads made:     883090
Read size:            4096
Object size:          4096
Bandwidth (MB/sec):   154.182
Average IOPS:         39470
Stddev IOPS:          1014.21
Max IOPS:             40786
Min IOPS:             37080
Average Latency(s):   0.00323842
Max latency(s):       0.087147
Min latency(s):       0.000234983

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:11:50,017189178-05:00] INFO: > Stop system activity collection services[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:11:50,022000150-05:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:391 - rados_bench() > kill -INT 136143

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:11:50,026557855-05:00] INFO: >> for OSD host[0m
# ./benchmarks/bench-rados:396 - rados_bench() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.2 kill -INT 15800
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:11:50,033667131-05:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.2 kill -INT 15800
[1] 00:11:50 [SUCCESS] ljishen@10.10.2.2

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:11:50,216198752-05:00] INFO: >>> Copy system activity file from OSD host[0m
## ./benchmarks/bench-rados:403 - rados_bench() > basename -- /tmp/bench-rados/sys_activity_4KB_seq.dat.osd_host.3
# ./benchmarks/bench-rados:403 - rados_bench() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.2:/tmp/bench-rados/sys_activity_4KB_seq.dat.osd_host.3 /mnt/sda8/ceph-research/scripts/output/sys_activity_4KB_seq.dat.osd_host.3


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:11:50,371433326-05:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:12:14,318869191-05:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 883.09k objects, 3.4 GiB
    usage:   11 GiB used, 89 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:12:14,325464378-05:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:12:23,375477769-05:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 883.09k objects, 3.4 GiB
    usage:   11 GiB used, 89 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:12:23,381890863-05:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:12:32,629170433-05:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 883.09k objects, 3.4 GiB
    usage:   11 GiB used, 89 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:12:32,635759479-05:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:12:41,673339599-05:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 883.09k objects, 3.4 GiB
    usage:   11 GiB used, 89 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:12:41,679919758-05:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:12:50,715154839-05:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 883.09k objects, 3.4 GiB
    usage:   11 GiB used, 89 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:12:50,721991220-05:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:12:50,727032036-05:00] INFO: > The cluster is idle now.[0m


[1;7;39;49m[2021-11-08T00:12:50,730210941-05:00][RUNNING][ROUND 1/2/21] object_size=16KB[0m


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:12:50,733231427-05:00] STAGE: Launch a Ceph cluster (CEPH_HOST: 10.10.2.3, OSD_HOST: 10.10.2.2) ...[0m
# ./benchmarks/bench-rados:183 - launch_ceph_cluster() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.3 bash -euo pipefail
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:12:50,741718081-05:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.3 bash -euo pipefail
10.10.2.3: b'\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T00:12:51,062583324-05:00] INFO: > Remove existing clusters\x1b[0m\n'
10.10.2.3: b'## bash:17 -  > basename -- /var/lib/ceph/43f824ca-4051-11ec-aba1-c5f3c07a3dc7\n'
10.10.2.3: b'# bash:17 -  > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm --host 10.10.2.3 --host 10.10.2.2 rm-cluster --force --fsid 43f824ca-4051-11ec-aba1-c5f3c07a3dc7\n'
10.10.2.3: b"\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T00:12:51,072793399-05:00] DEBUG: HOST_PARAMS: ['--host', '10.10.2.3', '--host', '10.10.2.2']\x1b[0m\n"
10.10.2.3: b"\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T00:12:51,076040716-05:00] DEBUG: CEPHADM_PARAMS: ['rm-cluster', '--force', '--fsid', '43f824ca-4051-11ec-aba1-c5f3c07a3dc7']\x1b[0m\n"
10.10.2.3: b"# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:63 -  > /tmp/bench-rados/ceph-research/scripts/parallel-ssh --host 10.10.2.3 --host 10.10.2.2 'sudo python3 -u - rm-cluster --force --fsid 43f824ca-4051-11ec-aba1-c5f3c07a3dc7'\n"
10.10.2.3: b'\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T00:12:51,084163162-05:00] DEBUG: command from stdin\x1b[0m\n'
10.10.2.3: b"# /tmp/bench-rados/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host 10.10.2.3 --host 10.10.2.2 'sudo python3 -u - rm-cluster --force --fsid 43f824ca-4051-11ec-aba1-c5f3c07a3dc7'\n"
10.10.2.3: b'[1] 00:12:56 [SUCCESS] 10.10.2.3\n[2] 00:12:59 [SUCCESS] 10.10.2.2\n'
10.10.2.3: b'\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T00:12:59,620811702-05:00] INFO: > Deploy a new cluster\x1b[0m\n'
10.10.2.3: b'# bash:24 -  > /tmp/bench-rados/ceph-research/scripts/ceph-deploy -m 10.10.2.3 -o node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/10.10.2.2:/dev/nvme0n1\n'
10.10.2.3: b'\n\n\x1b[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T00:12:59,632268971-05:00] STAGE: Verify passwordless SSH to hosts, passwordless sudo via ljishen, and the existence of devices on hosts...\x1b[0m\n'
10.10.2.3: b'\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T00:12:59,637414410-05:00] INFO: > Check the host for the monitor\x1b[0m\n'
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:181 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@10.10.2.3 sudo --non-interactive --validate\n'
10.10.2.3: b'\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T00:12:59,786602609-05:00] INFO: > Check hosts for OSDs\x1b[0m\n'
10.10.2.3: b"\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T00:12:59,791994223-05:00] INFO: >> Check host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'\x1b[0m\n"
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:200 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us sudo --non-interactive --validate\n'
10.10.2.3: b"\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T00:12:59,942091789-05:00] INFO: >>> Check device '/dev/nvme0n1'\x1b[0m\n"
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:215 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us test -b /dev/nvme0n1\n'
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:226 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us lsblk --nodeps --noheadings --output TYPE /dev/nvme0n1\n'
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:227 - verify_parameters() > grep --word-regexp disk\n'
10.10.2.3: b'\n\n\x1b[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T00:13:00,222309625-05:00] STAGE: Reset devices on hosts...\x1b[0m\n'
10.10.2.3: b"\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T00:13:00,227318507-05:00] INFO: > Resetting device '/dev/nvme0n1' on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'\x1b[0m\n"
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:288 - reset_host_devices() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us sudo sh\n'
10.10.2.3: b'  Removing ceph--ad8c206e--dcdf--40c4--94c5--fc70261a3ead-osd--block--ca7dc655--6ec2--41b2--8e07--8644b66e3705 (253:0)\n'
10.10.2.3: b'  Archiving volume group "ceph-ad8c206e-dcdf-40c4-94c5-fc70261a3ead" metadata (seqno 5).\n  Releasing logical volume "osd-block-ca7dc655-6ec2-41b2-8e07-8644b66e3705"\n'
10.10.2.3: b'  Creating volume group backup "/etc/lvm/backup/ceph-ad8c206e-dcdf-40c4-94c5-fc70261a3ead" (seqno 6).\n'
10.10.2.3: b'  Logical volume "osd-block-ca7dc655-6ec2-41b2-8e07-8644b66e3705" successfully removed\n'
10.10.2.3: b'  Removing physical volume "/dev/nvme0n1" from volume group "ceph-ad8c206e-dcdf-40c4-94c5-fc70261a3ead"\n'
10.10.2.3: b'  Volume group "ceph-ad8c206e-dcdf-40c4-94c5-fc70261a3ead" successfully removed\n'
10.10.2.3: b'\n\n\x1b[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T00:13:00,551615275-05:00] STAGE: Bootstrap a new cluster...\x1b[0m\n'
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:317 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.3 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults\n'
10.10.2.3: b'\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T00:13:00,561288157-05:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.3: b"\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T00:13:00,564809601-05:00] DEBUG: CEPHADM_PARAMS: ['bootstrap', '--config', '/tmp/bench-rados/ceph-research/scripts/ceph.conf', '--mon-ip', '10.10.2.3', '--output-dir', '/tmp/bench-rados/deployment_data_root/etc/ceph', '--ssh-user', 'ljishen', '--skip-dashboard', '--no-minimize-config', '--allow-overwrite', '--allow-fqdn-hostname', '--orphan-initial-daemons', '--skip-monitoring-stack', '--single-host-defaults']\x1b[0m\n"
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.3 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults\n'
10.10.2.3: b'Verifying podman|docker is present...\nVerifying lvm2 is present...\n'
10.10.2.3: b'Verifying time synchronization is in place...\n'
10.10.2.3: b'Unit ntp.service is enabled and running\n'
10.10.2.3: b'Repeating the final host check...\ndocker (/usr/bin/docker) is present\n'
10.10.2.3: b'systemctl is present\n'
10.10.2.3: b'lvcreate is present\n'
10.10.2.3: b'Unit ntp.service is enabled and running\n'
10.10.2.3: b'Host looks OK\n'
10.10.2.3: b'Cluster fsid: 8b275766-4052-11ec-aba1-c5f3c07a3dc7\n'
10.10.2.3: b'Verifying IP 10.10.2.3 port 3300 ...\n'
10.10.2.3: b'Verifying IP 10.10.2.3 port 6789 ...\n'
10.10.2.3: b'Mon IP `10.10.2.3` is in CIDR network `10.10.2.0/24`\n'
10.10.2.3: b'- internal network (--cluster-network) has not been provided, OSD replication will default to the public_network\n'
10.10.2.3: b'Adjusting default settings to suit single-host cluster...\n'
10.10.2.3: b'Pulling container image quay.io/ceph/ceph:v16...\n'
10.10.2.3: b'Ceph version: ceph version 16.2.6 (ee28fb57e47e9f88813e24bbf4c14496ca299d31) pacific (stable)\n'
10.10.2.3: b'Extracting ceph user uid/gid from container image...\n'
10.10.2.3: b'Creating initial keys...\n'
10.10.2.3: b'Creating initial monmap...\n'
10.10.2.3: b'Creating mon...\n'
10.10.2.3: b'Waiting for mon to start...\n'
10.10.2.3: b'Waiting for mon...\n'
10.10.2.3: b'mon is available\nSetting mon public_network to 10.10.2.0/24\n'
10.10.2.3: b'Wrote config to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf\n'
10.10.2.3: b'Wrote keyring to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring\n'
10.10.2.3: b'Creating mgr...\n'
10.10.2.3: b'Verifying port 9283 ...\n'
10.10.2.3: b'Waiting for mgr to start...\nWaiting for mgr...\n'
10.10.2.3: b'mgr not available, waiting (1/15)...\n'
10.10.2.3: b'mgr not available, waiting (2/15)...\n'
10.10.2.3: b'mgr is available\n'
10.10.2.3: b'Enabling cephadm module...\n'
10.10.2.3: b'Waiting for the mgr to restart...\n'
10.10.2.3: b'Waiting for mgr epoch 5...\n'
10.10.2.3: b'mgr epoch 5 is available\nSetting orchestrator backend to cephadm...\n'
10.10.2.3: b'Generating ssh key...\n'
10.10.2.3: b'Wrote public SSH key to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub\n'
10.10.2.3: b'Adding key to ljishen@localhost authorized_keys...\n'
10.10.2.3: b'Adding host node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us...\n'
10.10.2.3: b'Deploying unmanaged mon service...\n'
10.10.2.3: b'Deploying unmanaged mgr service...\n'
10.10.2.3: b'You can access the Ceph CLI with:\n\n\tsudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --fsid 8b275766-4052-11ec-aba1-c5f3c07a3dc7 -c /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf -k /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring\n\n'
10.10.2.3: b'Please consider enabling telemetry to help improve Ceph:\n\n\tceph telemetry on\n\nFor more information see:\n\n\thttps://docs.ceph.com/docs/pacific/mgr/telemetry/\n\n'
10.10.2.3: b'Bootstrap complete.\n'
10.10.2.3: b'\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T00:14:09,450581615-05:00] INFO: Sleep for 20 seconds\x1b[0m\n'
10.10.2.3: b'\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T00:14:29,457856052-05:00] INFO: > Set the default number of monitor daemons to 1\x1b[0m\n'
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:337 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1\n'
10.10.2.3: b'\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T00:14:29,468292084-05:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.3: b"\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T00:14:29,471918987-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mon', '1']\x1b[0m\n"
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1\n'
10.10.2.3: b'Inferring fsid 8b275766-4052-11ec-aba1-c5f3c07a3dc7\n'
10.10.2.3: b'Inferring config /var/lib/ceph/8b275766-4052-11ec-aba1-c5f3c07a3dc7/mon.node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.3: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.3: b'Scheduled mon update...\n'
10.10.2.3: b'\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T00:14:39,538767928-05:00] INFO: > Set the default number of manager daemons to 1\x1b[0m\n'
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:342 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1\n'
10.10.2.3: b'\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T00:14:39,548371118-05:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.3: b"\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T00:14:39,551955491-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mgr', '1']\x1b[0m\n"
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1\n'
10.10.2.3: b'Inferring fsid 8b275766-4052-11ec-aba1-c5f3c07a3dc7\n'
10.10.2.3: b'Inferring config /var/lib/ceph/8b275766-4052-11ec-aba1-c5f3c07a3dc7/mon.node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.3: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.3: b'Scheduled mgr update...\n'
10.10.2.3: b'\n\n\x1b[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T00:14:49,993845444-05:00] STAGE: Add hosts to the cluster...\x1b[0m\n'
10.10.2.3: b"\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T00:14:49,999631001-05:00] INFO: > Adding host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'\x1b[0m\n"
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:361 - add_hosts() > ssh-copy-id -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -f -i /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\n'
10.10.2.3: b'/usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: "/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub"\n'
10.10.2.3: b'\nNumber of key(s) added: 1\n\nNow try logging into the machine, with:   "ssh -o \'GlobalKnownHostsFile=/dev/null\' -o \'LogLevel=ERROR\' -o \'PasswordAuthentication=no\' -o \'StrictHostKeyChecking=no\' -o \'UserKnownHostsFile=/dev/null\' \'ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\'"\nand check to make sure that only the key(s) you wanted were added.\n\n'
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:367 - add_hosts() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us 10.10.2.2\n'
10.10.2.3: b'\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T00:14:50,205020953-05:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.3: b"\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T00:14:50,208628420-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'host', 'add', 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us', '10.10.2.2']\x1b[0m\n"
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us 10.10.2.2\n'
10.10.2.3: b'Inferring fsid 8b275766-4052-11ec-aba1-c5f3c07a3dc7\n'
10.10.2.3: b'Inferring config /var/lib/ceph/8b275766-4052-11ec-aba1-c5f3c07a3dc7/mon.node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.3: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.3: b"Added host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us' with addr '10.10.2.2'\n"
10.10.2.3: b'\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T00:14:59,898089744-05:00] INFO: Sleep for 20 seconds\x1b[0m\n'
10.10.2.3: b'\n\n\x1b[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T00:15:19,903873272-05:00] STAGE: Add OSDs to the cluster...\x1b[0m\n'
10.10.2.3: b"\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T00:15:19,910724150-05:00] INFO: > Adding devices ['/dev/nvme0n1'] on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us' as OSDs\x1b[0m\n"
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:388 - add_host_devices_as_osds() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1\n'
10.10.2.3: b'\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T00:15:19,920332970-05:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.3: b"\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T00:15:19,924338769-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'daemon', 'add', 'osd', 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1']\x1b[0m\n"
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1\n'
10.10.2.3: b'Inferring fsid 8b275766-4052-11ec-aba1-c5f3c07a3dc7\n'
10.10.2.3: b'Inferring config /var/lib/ceph/8b275766-4052-11ec-aba1-c5f3c07a3dc7/mon.node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.3: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.3: b"Created osd(s) 0 on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'\n"
10.10.2.3: b'\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T00:15:46,468437790-05:00] INFO: > Sleep for 20 seconds\x1b[0m\n'
10.10.2.3: b'\n\n\x1b[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T00:16:06,474046948-05:00] STAGE: Show Cluster Status\x1b[0m\n'
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:408 - show_status() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status\n'
10.10.2.3: b'\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T00:16:06,484488498-05:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.3: b"\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T00:16:06,488261246-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', '--status']\x1b[0m\n"
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status\n'
10.10.2.3: b'Inferring fsid 8b275766-4052-11ec-aba1-c5f3c07a3dc7\n'
10.10.2.3: b'Inferring config /var/lib/ceph/8b275766-4052-11ec-aba1-c5f3c07a3dc7/mon.node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.3: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.3: b'  cluster:\n    id:     8b275766-4052-11ec-aba1-c5f3c07a3dc7\n    health: HEALTH_WARN\n            1 pool(s) have no replicas configured\n \n  services:\n    mon: 1 daemons, quorum node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us (age 2m)\n    mgr: node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us.rkdxyy(active, since 2m)\n    osd: 1 osds: 1 up (since 19s), 1 in (since 42s)\n \n  data:\n    pools:   1 pools, 1 pgs\n    objects: 0 objects, 0 B\n    usage:   4.7 MiB used, 100 GiB / 100 GiB avail\n    pgs:     1 active+clean\n \n'
[1] 00:16:16 [SUCCESS] ljishen@10.10.2.3

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:12:51,062583324-05:00] INFO: > Remove existing clusters[0m
## bash:17 -  > basename -- /var/lib/ceph/43f824ca-4051-11ec-aba1-c5f3c07a3dc7
# bash:17 -  > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm --host 10.10.2.3 --host 10.10.2.2 rm-cluster --force --fsid 43f824ca-4051-11ec-aba1-c5f3c07a3dc7
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:12:51,072793399-05:00] DEBUG: HOST_PARAMS: ['--host', '10.10.2.3', '--host', '10.10.2.2'][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:12:51,076040716-05:00] DEBUG: CEPHADM_PARAMS: ['rm-cluster', '--force', '--fsid', '43f824ca-4051-11ec-aba1-c5f3c07a3dc7'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:63 -  > /tmp/bench-rados/ceph-research/scripts/parallel-ssh --host 10.10.2.3 --host 10.10.2.2 'sudo python3 -u - rm-cluster --force --fsid 43f824ca-4051-11ec-aba1-c5f3c07a3dc7'
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:12:51,084163162-05:00] DEBUG: command from stdin[0m
# /tmp/bench-rados/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host 10.10.2.3 --host 10.10.2.2 'sudo python3 -u - rm-cluster --force --fsid 43f824ca-4051-11ec-aba1-c5f3c07a3dc7'
[1] 00:12:56 [SUCCESS] 10.10.2.3
[2] 00:12:59 [SUCCESS] 10.10.2.2

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:12:59,620811702-05:00] INFO: > Deploy a new cluster[0m
# bash:24 -  > /tmp/bench-rados/ceph-research/scripts/ceph-deploy -m 10.10.2.3 -o node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/10.10.2.2:/dev/nvme0n1


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:12:59,632268971-05:00] STAGE: Verify passwordless SSH to hosts, passwordless sudo via ljishen, and the existence of devices on hosts...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:12:59,637414410-05:00] INFO: > Check the host for the monitor[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:181 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@10.10.2.3 sudo --non-interactive --validate

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:12:59,786602609-05:00] INFO: > Check hosts for OSDs[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:12:59,791994223-05:00] INFO: >> Check host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:200 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us sudo --non-interactive --validate

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:12:59,942091789-05:00] INFO: >>> Check device '/dev/nvme0n1'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:215 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us test -b /dev/nvme0n1
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:226 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us lsblk --nodeps --noheadings --output TYPE /dev/nvme0n1
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:227 - verify_parameters() > grep --word-regexp disk


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:13:00,222309625-05:00] STAGE: Reset devices on hosts...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:13:00,227318507-05:00] INFO: > Resetting device '/dev/nvme0n1' on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:288 - reset_host_devices() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us sudo sh
  Removing ceph--ad8c206e--dcdf--40c4--94c5--fc70261a3ead-osd--block--ca7dc655--6ec2--41b2--8e07--8644b66e3705 (253:0)
  Archiving volume group "ceph-ad8c206e-dcdf-40c4-94c5-fc70261a3ead" metadata (seqno 5).
  Releasing logical volume "osd-block-ca7dc655-6ec2-41b2-8e07-8644b66e3705"
  Creating volume group backup "/etc/lvm/backup/ceph-ad8c206e-dcdf-40c4-94c5-fc70261a3ead" (seqno 6).
  Logical volume "osd-block-ca7dc655-6ec2-41b2-8e07-8644b66e3705" successfully removed
  Removing physical volume "/dev/nvme0n1" from volume group "ceph-ad8c206e-dcdf-40c4-94c5-fc70261a3ead"
  Volume group "ceph-ad8c206e-dcdf-40c4-94c5-fc70261a3ead" successfully removed


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:13:00,551615275-05:00] STAGE: Bootstrap a new cluster...[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:317 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.3 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:13:00,561288157-05:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:13:00,564809601-05:00] DEBUG: CEPHADM_PARAMS: ['bootstrap', '--config', '/tmp/bench-rados/ceph-research/scripts/ceph.conf', '--mon-ip', '10.10.2.3', '--output-dir', '/tmp/bench-rados/deployment_data_root/etc/ceph', '--ssh-user', 'ljishen', '--skip-dashboard', '--no-minimize-config', '--allow-overwrite', '--allow-fqdn-hostname', '--orphan-initial-daemons', '--skip-monitoring-stack', '--single-host-defaults'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.3 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults
Verifying podman|docker is present...
Verifying lvm2 is present...
Verifying time synchronization is in place...
Unit ntp.service is enabled and running
Repeating the final host check...
docker (/usr/bin/docker) is present
systemctl is present
lvcreate is present
Unit ntp.service is enabled and running
Host looks OK
Cluster fsid: 8b275766-4052-11ec-aba1-c5f3c07a3dc7
Verifying IP 10.10.2.3 port 3300 ...
Verifying IP 10.10.2.3 port 6789 ...
Mon IP `10.10.2.3` is in CIDR network `10.10.2.0/24`
- internal network (--cluster-network) has not been provided, OSD replication will default to the public_network
Adjusting default settings to suit single-host cluster...
Pulling container image quay.io/ceph/ceph:v16...
Ceph version: ceph version 16.2.6 (ee28fb57e47e9f88813e24bbf4c14496ca299d31) pacific (stable)
Extracting ceph user uid/gid from container image...
Creating initial keys...
Creating initial monmap...
Creating mon...
Waiting for mon to start...
Waiting for mon...
mon is available
Setting mon public_network to 10.10.2.0/24
Wrote config to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf
Wrote keyring to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring
Creating mgr...
Verifying port 9283 ...
Waiting for mgr to start...
Waiting for mgr...
mgr not available, waiting (1/15)...
mgr not available, waiting (2/15)...
mgr is available
Enabling cephadm module...
Waiting for the mgr to restart...
Waiting for mgr epoch 5...
mgr epoch 5 is available
Setting orchestrator backend to cephadm...
Generating ssh key...
Wrote public SSH key to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub
Adding key to ljishen@localhost authorized_keys...
Adding host node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us...
Deploying unmanaged mon service...
Deploying unmanaged mgr service...
You can access the Ceph CLI with:

	sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --fsid 8b275766-4052-11ec-aba1-c5f3c07a3dc7 -c /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf -k /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring

Please consider enabling telemetry to help improve Ceph:

	ceph telemetry on

For more information see:

	https://docs.ceph.com/docs/pacific/mgr/telemetry/

Bootstrap complete.

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:14:09,450581615-05:00] INFO: Sleep for 20 seconds[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:14:29,457856052-05:00] INFO: > Set the default number of monitor daemons to 1[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:337 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:14:29,468292084-05:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:14:29,471918987-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mon', '1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1
Inferring fsid 8b275766-4052-11ec-aba1-c5f3c07a3dc7
Inferring config /var/lib/ceph/8b275766-4052-11ec-aba1-c5f3c07a3dc7/mon.node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Scheduled mon update...

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:14:39,538767928-05:00] INFO: > Set the default number of manager daemons to 1[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:342 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:14:39,548371118-05:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:14:39,551955491-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mgr', '1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1
Inferring fsid 8b275766-4052-11ec-aba1-c5f3c07a3dc7
Inferring config /var/lib/ceph/8b275766-4052-11ec-aba1-c5f3c07a3dc7/mon.node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Scheduled mgr update...


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:14:49,993845444-05:00] STAGE: Add hosts to the cluster...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:14:49,999631001-05:00] INFO: > Adding host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:361 - add_hosts() > ssh-copy-id -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -f -i /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us
/usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: "/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub"

Number of key(s) added: 1

Now try logging into the machine, with:   "ssh -o 'GlobalKnownHostsFile=/dev/null' -o 'LogLevel=ERROR' -o 'PasswordAuthentication=no' -o 'StrictHostKeyChecking=no' -o 'UserKnownHostsFile=/dev/null' 'ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'"
and check to make sure that only the key(s) you wanted were added.

# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:367 - add_hosts() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us 10.10.2.2
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:14:50,205020953-05:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:14:50,208628420-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'host', 'add', 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us', '10.10.2.2'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us 10.10.2.2
Inferring fsid 8b275766-4052-11ec-aba1-c5f3c07a3dc7
Inferring config /var/lib/ceph/8b275766-4052-11ec-aba1-c5f3c07a3dc7/mon.node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Added host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us' with addr '10.10.2.2'

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:14:59,898089744-05:00] INFO: Sleep for 20 seconds[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:15:19,903873272-05:00] STAGE: Add OSDs to the cluster...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:15:19,910724150-05:00] INFO: > Adding devices ['/dev/nvme0n1'] on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us' as OSDs[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:388 - add_host_devices_as_osds() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:15:19,920332970-05:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:15:19,924338769-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'daemon', 'add', 'osd', 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1
Inferring fsid 8b275766-4052-11ec-aba1-c5f3c07a3dc7
Inferring config /var/lib/ceph/8b275766-4052-11ec-aba1-c5f3c07a3dc7/mon.node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Created osd(s) 0 on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:15:46,468437790-05:00] INFO: > Sleep for 20 seconds[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:16:06,474046948-05:00] STAGE: Show Cluster Status[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:408 - show_status() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:16:06,484488498-05:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:16:06,488261246-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', '--status'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status
Inferring fsid 8b275766-4052-11ec-aba1-c5f3c07a3dc7
Inferring config /var/lib/ceph/8b275766-4052-11ec-aba1-c5f3c07a3dc7/mon.node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
  cluster:
    id:     8b275766-4052-11ec-aba1-c5f3c07a3dc7
    health: HEALTH_WARN
            1 pool(s) have no replicas configured
 
  services:
    mon: 1 daemons, quorum node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us (age 2m)
    mgr: node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us.rkdxyy(active, since 2m)
    osd: 1 osds: 1 up (since 19s), 1 in (since 42s)
 
  data:
    pools:   1 pools, 1 pgs
    objects: 0 objects, 0 B
    usage:   4.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     1 active+clean
 

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:16:16,503280869-05:00] INFO: > Copy configuration files to local[0m
# ./benchmarks/bench-rados:220 - launch_ceph_cluster() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.3 sudo chown --changes --recursive ljishen /tmp/bench-rados/deployment_data_root/etc/ceph
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:16:16,510474414-05:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.3 sudo chown --changes --recursive ljishen /tmp/bench-rados/deployment_data_root/etc/ceph
[1] 00:16:16 [SUCCESS] ljishen@10.10.2.3
# ./benchmarks/bench-rados:223 - launch_ceph_cluster() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.3:/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf
# ./benchmarks/bench-rados:226 - launch_ceph_cluster() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.3:/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:16:16,980467752-05:00] STAGE: Update local ceph.conf as a client...[0m


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:16:16,983330199-05:00] STAGE: Configure Ceph pool...[0m
# ./benchmarks/bench-rados:257 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_max_backfills 32
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:16:17,003517248-05:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:16:17,006259359-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '8b275766-4052-11ec-aba1-c5f3c07a3dc7', '--', 'ceph', 'config', 'set', 'osd', 'osd_max_backfills', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 8b275766-4052-11ec-aba1-c5f3c07a3dc7 -- ceph config set osd osd_max_backfills 32
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:258 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_active 32
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:16:21,130209209-05:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:16:21,132972791-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '8b275766-4052-11ec-aba1-c5f3c07a3dc7', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_active', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 8b275766-4052-11ec-aba1-c5f3c07a3dc7 -- ceph config set osd osd_recovery_max_active 32
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:259 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_single_start 8
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:16:25,392992397-05:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:16:25,395654868-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '8b275766-4052-11ec-aba1-c5f3c07a3dc7', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_single_start', '8'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 8b275766-4052-11ec-aba1-c5f3c07a3dc7 -- ceph config set osd osd_recovery_max_single_start 8
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:260 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_op_priority 63
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:16:29,546161620-05:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:16:29,549056750-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '8b275766-4052-11ec-aba1-c5f3c07a3dc7', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_op_priority', '63'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 8b275766-4052-11ec-aba1-c5f3c07a3dc7 -- ceph config set osd osd_recovery_op_priority 63
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:262 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config show-with-defaults osd.0
# ./benchmarks/bench-rados:263 - configure_ceph_pool() > grep 'osd_max_backfills\|osd_recovery'
# ./benchmarks/bench-rados:263 - configure_ceph_pool() > column -t -s ' '
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
osd_max_backfills                        32         mon
osd_recovery_cost                        20971520   default
osd_recovery_delay_start                 0.000000   default
osd_recovery_max_active                  32         mon
osd_recovery_max_active_hdd              3          default
osd_recovery_max_active_ssd              10         default
osd_recovery_max_chunk                   8388608    default
osd_recovery_max_omap_entries_per_chunk  8096       default
osd_recovery_max_single_start            8          mon
osd_recovery_op_priority                 63         mon
osd_recovery_op_warn_multiple            16         default
osd_recovery_priority                    5          default
osd_recovery_retry_interval              30.000000  default
osd_recovery_sleep                       0.000000   default
osd_recovery_sleep_hdd                   0.100000   default
osd_recovery_sleep_hybrid                0.025000   default
osd_recovery_sleep_ssd                   0.000000   default
# ./benchmarks/bench-rados:265 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool create bench_rados 128 128 replicated --size 1 --pg-num-min 128
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:16:37,818873645-05:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:16:37,821594155-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '8b275766-4052-11ec-aba1-c5f3c07a3dc7', '--', 'ceph', 'osd', 'pool', 'create', 'bench_rados', '128', '128', 'replicated', '--size', '1', '--pg-num-min', '128'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 8b275766-4052-11ec-aba1-c5f3c07a3dc7 -- ceph osd pool create bench_rados 128 128 replicated --size 1 --pg-num-min 128
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
pool 'bench_rados' created
# ./benchmarks/bench-rados:267 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados min_size 1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:16:42,489735547-05:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:16:42,492871220-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '8b275766-4052-11ec-aba1-c5f3c07a3dc7', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'min_size', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 8b275766-4052-11ec-aba1-c5f3c07a3dc7 -- ceph osd pool set bench_rados min_size 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 min_size to 1
# ./benchmarks/bench-rados:268 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados pg_autoscale_mode off
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:16:47,702050662-05:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:16:47,704918661-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '8b275766-4052-11ec-aba1-c5f3c07a3dc7', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'pg_autoscale_mode', 'off'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 8b275766-4052-11ec-aba1-c5f3c07a3dc7 -- ceph osd pool set bench_rados pg_autoscale_mode off
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 pg_autoscale_mode to off
# ./benchmarks/bench-rados:269 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool application enable bench_rados benchmark
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:16:52,649624820-05:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:16:52,652343336-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '8b275766-4052-11ec-aba1-c5f3c07a3dc7', '--', 'ceph', 'osd', 'pool', 'application', 'enable', 'bench_rados', 'benchmark'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 8b275766-4052-11ec-aba1-c5f3c07a3dc7 -- ceph osd pool application enable bench_rados benchmark
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
enabled application 'benchmark' on pool 'bench_rados'
# ./benchmarks/bench-rados:270 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados noscrub 1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:16:57,145115213-05:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:16:57,147955529-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '8b275766-4052-11ec-aba1-c5f3c07a3dc7', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'noscrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 8b275766-4052-11ec-aba1-c5f3c07a3dc7 -- ceph osd pool set bench_rados noscrub 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 noscrub to 1
# ./benchmarks/bench-rados:271 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados nodeep-scrub 1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:17:01,581112668-05:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:17:01,584007567-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '8b275766-4052-11ec-aba1-c5f3c07a3dc7', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'nodeep-scrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 8b275766-4052-11ec-aba1-c5f3c07a3dc7 -- ceph osd pool set bench_rados nodeep-scrub 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 nodeep-scrub to 1
# ./benchmarks/bench-rados:272 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool ls detail
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:17:06,585442375-05:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:17:06,588232737-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '8b275766-4052-11ec-aba1-c5f3c07a3dc7', '--', 'ceph', 'osd', 'pool', 'ls', 'detail'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 8b275766-4052-11ec-aba1-c5f3c07a3dc7 -- ceph osd pool ls detail
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
pool 1 'device_health_metrics' replicated size 1 min_size 1 crush_rule 0 object_hash rjenkins pg_num 64 pgp_num 64 autoscale_mode on last_change 17 lfor 0/0/14 flags hashpspool stripe_width 0 pg_num_min 1 application mgr_devicehealth
pool 2 'bench_rados' replicated size 1 min_size 1 crush_rule 0 object_hash rjenkins pg_num 128 pgp_num 128 autoscale_mode off last_change 20 flags hashpspool,noscrub,nodeep-scrub stripe_width 0 pg_num_min 128 application benchmark

# ./benchmarks/bench-rados:273 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd df tree
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:17:10,749854844-05:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:17:10,752733413-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '8b275766-4052-11ec-aba1-c5f3c07a3dc7', '--', 'ceph', 'osd', 'df', 'tree'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 8b275766-4052-11ec-aba1-c5f3c07a3dc7 -- ceph osd df tree
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
ID  CLASS  WEIGHT   REWEIGHT  SIZE     RAW USE  DATA     OMAP  META     AVAIL    %USE  VAR   PGS  STATUS  TYPE NAME      
-1         0.09769         -  100 GiB  5.7 MiB  168 KiB   0 B  5.6 MiB  100 GiB  0.01  1.00    -          root default   
-3         0.09769         -  100 GiB  5.7 MiB  168 KiB   0 B  5.6 MiB  100 GiB  0.01  1.00    -              host node-1
 0    ssd  0.09769   1.00000  100 GiB  5.7 MiB  168 KiB   0 B  5.6 MiB  100 GiB  0.01  1.00  192      up          osd.0  
                       TOTAL  100 GiB  5.7 MiB  168 KiB   0 B  5.6 MiB  100 GiB  0.01                                    
MIN/MAX VAR: 1.00/1.00  STDDEV: 0


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:17:14,706716422-05:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:17:38,657781884-05:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:17:47,770074980-05:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:17:47,776782249-05:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:17:56,740196407-05:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:17:56,746841498-05:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:18:05,726811148-05:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:18:05,733115116-05:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:18:14,764657924-05:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:18:14,771564969-05:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:18:23,901235806-05:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:18:23,907846883-05:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:18:23,912730433-05:00] INFO: > The cluster is idle now.[0m


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:18:23,915909638-05:00] STAGE: Start benchmarking of mode write ...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:18:23,921953405-05:00] INFO: > Start system activity collection services[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:18:23,926793642-05:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:341 - rados_bench() > rm --force /mnt/sda8/ceph-research/scripts/output/sys_activity_16KB_write.dat.rados_bench_host.1
# ./benchmarks/bench-rados:342 - rados_bench() > sar_pid_rados_bench_host=144240
# ./benchmarks/bench-rados:342 - rados_bench() > S_TIME_FORMAT=ISO
# ./benchmarks/bench-rados:342 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/output/sys_activity_16KB_write.dat.rados_bench_host.1 2

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:18:23,932505744-05:00] INFO: >> for OSD host[0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:18:23,940589669-05:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.2 bash
10.10.2.2: b'19433\n'
[1] 00:18:24 [SUCCESS] ljishen@10.10.2.2
19433

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:18:24,128860654-05:00] INFO: > Run rados bench[0m
# ./benchmarks/bench-rados:370 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/output/rados_bench_16KB_write.log.1
## ./benchmarks/bench-rados:361 - rados_bench() > count_cpus 32-63,96-127
# ./benchmarks/bench-rados:361 - rados_bench() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 60 write --pool bench_rados -b 16384 -O 16384 --concurrent-ios 128 --show-time --write-object --write-omap --write-xattr --no-cleanup
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:18:24,147966142-05:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:18:24,150760672-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '8b275766-4052-11ec-aba1-c5f3c07a3dc7', '--', 'rados', 'bench', '60', 'write', '--pool', 'bench_rados', '-b', '16384', '-O', '16384', '--concurrent-ios', '128', '--show-time', '--write-object', '--write-omap', '--write-xattr', '--no-cleanup'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 8b275766-4052-11ec-aba1-c5f3c07a3dc7 -- rados bench 60 write --pool bench_rados -b 16384 -O 16384 --concurrent-ios 128 --show-time --write-object --write-omap --write-xattr --no-cleanup
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
hints = 1
2021-11-08T05:18:27.499089+0000 Maintaining 128 concurrent writes of 16384 bytes to objects of size 16384 for up to 60 seconds or 0 objects
2021-11-08T05:18:27.499099+0000 Object prefix: benchmark_data_node-0.bluefield2.ucsc-cmps10_7
2021-11-08T05:18:27.500163+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-11-08T05:18:27.500163+0000     0       0         0         0         0         0           -           0
2021-11-08T05:18:28.500280+0000     1     128     14376     14248   222.612   222.625  0.00931371  0.00892863
2021-11-08T05:18:29.500371+0000     2     128     28308     28180    220.14   217.688  0.00835648  0.00905583
2021-11-08T05:18:30.500462+0000     3     128     35477     35349   184.095   112.016   0.0241289   0.0107918
2021-11-08T05:18:31.500536+0000     4     128     41985     41857   163.491   101.688  0.00894686   0.0122154
2021-11-08T05:18:32.500603+0000     5     127     55387     55260   172.674   209.422  0.00868949   0.0115683
2021-11-08T05:18:33.500683+0000     6     128     63716     63588   165.581   130.125   0.0948616   0.0119867
2021-11-08T05:18:34.500770+0000     7     128     65190     65062   145.216   23.0312   0.0869903   0.0137035
2021-11-08T05:18:35.500849+0000     8     128     66854     66726   130.314        26   0.0248473   0.0153112
2021-11-08T05:18:36.500925+0000     9     128     72292     72164   125.275   84.9688   0.0229402   0.0159485
2021-11-08T05:18:37.501001+0000    10     128     77540     77412   120.947        82   0.0239658    0.016518
2021-11-08T05:18:38.501082+0000    11     128     88679     88551   125.773   174.047   0.0102161   0.0158893
2021-11-08T05:18:39.501162+0000    12     128     97318     97190    126.54   134.984   0.0196317   0.0157937
2021-11-08T05:18:40.501252+0000    13     127    103533    103406   124.276    97.125   0.0208856   0.0160744
2021-11-08T05:18:41.501348+0000    14     127    114792    114665   127.964   175.922  0.00789147   0.0156226
2021-11-08T05:18:42.501465+0000    15     128    125555    125427   130.642   168.156   0.0332833   0.0152559
2021-11-08T05:18:43.501560+0000    16     128    127091    126963   123.977        24   0.0893364   0.0161033
2021-11-08T05:18:44.501668+0000    17     128    128692    128564   118.155   25.0156     0.03735   0.0169085
2021-11-08T05:18:45.501755+0000    18     128    134196    134068   116.369        86   0.0220248   0.0171743
2021-11-08T05:18:46.501849+0000    19     128    139891    139763   114.927   88.9844   0.0230788   0.0173909
2021-11-08T05:18:47.501940+0000 min lat: 0.00530017 max lat: 0.0997236 avg lat: 0.0175869
2021-11-08T05:18:47.501940+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-11-08T05:18:47.501940+0000    20     128    145588    145460   113.631   89.0156   0.0233075   0.0175869
2021-11-08T05:18:48.502048+0000    21     128    151411    151283   112.552   90.9844    0.021601   0.0177586
2021-11-08T05:18:49.502143+0000    22     128    157939    157811   112.072       102    0.022119   0.0178358
2021-11-08T05:18:50.502244+0000    23     128    163764    163636   111.156   91.0156   0.0243094   0.0179826
2021-11-08T05:18:51.502343+0000    24     128    174606    174478   113.582   169.406  0.00852765   0.0176028
2021-11-08T05:18:52.502475+0000    25     128    186980    186852   116.772   193.344   0.0233318   0.0171165
2021-11-08T05:18:53.502575+0000    26     128    188772    188644   113.358        28    0.130969   0.0175924
2021-11-08T05:18:54.502668+0000    27     128    190308    190180   110.048        24    0.103941   0.0181448
2021-11-08T05:18:55.502745+0000    28     128    195974    195846   109.279   88.5312   0.0182366   0.0182821
2021-11-08T05:18:56.502822+0000    29     128    201835    201707   108.669   91.5781   0.0215674   0.0183945
2021-11-08T05:18:57.502898+0000    30     128    207544    207416    108.02   89.2031   0.0235381   0.0185064
2021-11-08T05:18:58.502977+0000    31     128    213271    213143   107.421   89.4844   0.0214581   0.0186076
2021-11-08T05:18:59.503057+0000    32     128    217290    217162   106.027   62.7969   0.0825446   0.0188276
2021-11-08T05:19:00.503131+0000    33     128    218826    218698   103.541        24   0.0827156   0.0192875
2021-11-08T05:19:01.503205+0000    34     128    220362    220234   101.202        24     0.05757   0.0197521
2021-11-08T05:19:02.503306+0000    35     128    225738    225610    100.71        84   0.0223366   0.0198528
2021-11-08T05:19:03.503384+0000    36     128    231370    231242   100.357        88   0.0223874   0.0199198
2021-11-08T05:19:04.503469+0000    37     127    236794    236667   99.9351   84.7656   0.0230206   0.0200053
2021-11-08T05:19:05.503558+0000    38     128    242122    241994   99.4954   83.2344   0.0237606   0.0200944
2021-11-08T05:19:06.503631+0000    39     128    247743    247615   99.1961   87.8281    0.022829   0.0201542
2021-11-08T05:19:07.503704+0000 min lat: 0.00517892 max lat: 0.138852 avg lat: 0.0204553
2021-11-08T05:19:07.503704+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-11-08T05:19:07.503704+0000    40     128    250115    249987   97.6427   37.0625   0.0911578   0.0204553
2021-11-08T05:19:08.503794+0000    41     128    251651    251523   95.8464        24   0.0899544   0.0208445
2021-11-08T05:19:09.503870+0000    42     128    256575    256447   95.3961   76.9375    0.018412   0.0209577
2021-11-08T05:19:10.503946+0000    43     128    262591    262463   95.3635        94    0.024089   0.0209644
2021-11-08T05:19:11.504021+0000    44     128    268419    268291   95.2656   91.0625   0.0233073   0.0209871
2021-11-08T05:19:12.504112+0000    45     128    274239    274111   95.1692   90.9375   0.0237801   0.0210084
2021-11-08T05:19:13.504186+0000    46     128    278719    278591   94.6219        70   0.0832969   0.0211225
2021-11-08T05:19:14.504258+0000    47     128    280579    280451    93.227   29.0625   0.0862369   0.0214347
2021-11-08T05:19:15.504329+0000    48     128    282047    281919   91.7627   22.9375   0.0914989   0.0217747
2021-11-08T05:19:16.504399+0000    49     128    287107    286979   91.5034   79.0625   0.0231247    0.021851
2021-11-08T05:19:17.504472+0000    50     128    292799    292671   91.4519   88.9375   0.0240644   0.0218622
2021-11-08T05:19:18.504546+0000    51     128    298431    298303   91.3841        88    0.019341   0.0218784
2021-11-08T05:19:19.504621+0000    52     127    304020    303893   91.3063   87.3438    0.023114   0.0218972
2021-11-08T05:19:20.504694+0000    53     128    309183    309055   91.1052   80.6562   0.0877467   0.0219351
2021-11-08T05:19:21.504767+0000    54     128    310659    310531   89.8452   23.0625   0.0899665   0.0222379
2021-11-08T05:19:22.504859+0000    55     128    312195    312067   88.6479        24   0.0928275   0.0225473
2021-11-08T05:19:23.504931+0000    56     128    316910    316782   88.3804   73.6719   0.0079558   0.0226251
2021-11-08T05:19:24.505008+0000    57     128    322497    322369   88.3613   87.2969   0.0224733   0.0226261
2021-11-08T05:19:25.505083+0000    58     128    328203    328075   88.3749   89.1562   0.0233979   0.0226263
2021-11-08T05:19:26.505154+0000    59     127    333839    333712   88.3697   88.0781   0.0233541    0.022625
2021-11-08T05:19:27.505229+0000 min lat: 0.00429528 max lat: 0.138852 avg lat: 0.0226245
2021-11-08T05:19:27.505229+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-11-08T05:19:27.505229+0000    60     128    339521    339393   88.3762   88.7656   0.0229025   0.0226245
2021-11-08T05:19:28.505355+0000 Total time run:         60.0211
Total writes made:      339521
Write size:             16384
Object size:            16384
Bandwidth (MB/sec):     88.3859
Stddev Bandwidth:       50.4767
Max bandwidth (MB/sec): 222.625
Min bandwidth (MB/sec): 22.9375
Average IOPS:           5656
Stddev IOPS:            3230.51
Max IOPS:               14248
Min IOPS:               1468
Average Latency(s):     0.0226243
Stddev Latency(s):      0.0182955
Max latency(s):         0.138852
Min latency(s):         0.00429528

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:19:29,197642175-05:00] INFO: > Stop system activity collection services[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:19:29,202314396-05:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:391 - rados_bench() > kill -INT 144240

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:19:29,207312150-05:00] INFO: >> for OSD host[0m
# ./benchmarks/bench-rados:396 - rados_bench() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.2 kill -INT 19433
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:19:29,214423732-05:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.2 kill -INT 19433
[1] 00:19:29 [SUCCESS] ljishen@10.10.2.2

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:19:29,393363020-05:00] INFO: >>> Copy system activity file from OSD host[0m
## ./benchmarks/bench-rados:403 - rados_bench() > basename -- /tmp/bench-rados/sys_activity_16KB_write.dat.osd_host.1
# ./benchmarks/bench-rados:403 - rados_bench() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.2:/tmp/bench-rados/sys_activity_16KB_write.dat.osd_host.1 /mnt/sda8/ceph-research/scripts/output/sys_activity_16KB_write.dat.osd_host.1


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:19:29,566734650-05:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:19:53,474520582-05:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 339.52k objects, 5.2 GiB
    usage:   16 GiB used, 84 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:19:53,480772632-05:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:20:02,700940914-05:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 339.52k objects, 5.2 GiB
    usage:   16 GiB used, 84 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:20:02,708170437-05:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:20:11,771601009-05:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 339.52k objects, 5.2 GiB
    usage:   16 GiB used, 84 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:20:11,778363040-05:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:20:20,828035529-05:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 339.52k objects, 5.2 GiB
    usage:   16 GiB used, 84 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:20:20,835041631-05:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:20:29,847205490-05:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 339.52k objects, 5.2 GiB
    usage:   16 GiB used, 84 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:20:29,853862874-05:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:20:29,858632198-05:00] INFO: > The cluster is idle now.[0m


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:20:29,861817786-05:00] STAGE: Start benchmarking of mode seq ...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:20:29,867734233-05:00] INFO: > Start system activity collection services[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:20:29,872380755-05:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:341 - rados_bench() > rm --force /mnt/sda8/ceph-research/scripts/output/sys_activity_16KB_seq.dat.rados_bench_host.1
# ./benchmarks/bench-rados:342 - rados_bench() > sar_pid_rados_bench_host=146337
# ./benchmarks/bench-rados:342 - rados_bench() > S_TIME_FORMAT=ISO
# ./benchmarks/bench-rados:342 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/output/sys_activity_16KB_seq.dat.rados_bench_host.1 2

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:20:29,878176023-05:00] INFO: >> for OSD host[0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:20:29,886484712-05:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.2 bash
10.10.2.2: b'19535\n'
[1] 00:20:30 [SUCCESS] ljishen@10.10.2.2
19535

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:20:30,072776405-05:00] INFO: > Run rados bench[0m
# ./benchmarks/bench-rados:379 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/output/rados_bench_16KB_seq.log.1
## ./benchmarks/bench-rados:376 - rados_bench() > count_cpus 32-63,96-127
# ./benchmarks/bench-rados:376 - rados_bench() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 128 --show-time
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:20:30,091069802-05:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:20:30,093764404-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '8b275766-4052-11ec-aba1-c5f3c07a3dc7', '--', 'rados', 'bench', '99999999', 'seq', '--pool', 'bench_rados', '--concurrent-ios', '128', '--show-time'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 8b275766-4052-11ec-aba1-c5f3c07a3dc7 -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 128 --show-time
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
hints = 1
2021-11-08T05:20:33.211510+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-11-08T05:20:33.211510+0000     0       0         0         0         0         0           -           0
2021-11-08T05:20:34.211616+0000     1     127     30593     30466    475.95   476.031  0.00403221  0.00418424
2021-11-08T05:20:35.211701+0000     2     128     61412     61284    478.72   481.531  0.00449846  0.00416669
2021-11-08T05:20:36.211783+0000     3     128     92007     91879   478.483   478.047  0.00457756  0.00417046
2021-11-08T05:20:37.211852+0000     4     128    122598    122470    478.35   477.984  0.00504491  0.00417211
2021-11-08T05:20:38.211925+0000     5     128    153339    153211   478.738   480.328   0.0041525  0.00416947
2021-11-08T05:20:39.211997+0000     6     128    180779    180651   470.402    428.75  0.00356925  0.00424366
2021-11-08T05:20:40.212075+0000     7     128    211238    211110   471.185   475.922  0.00463762  0.00423676
2021-11-08T05:20:41.212145+0000     8     128    242060    241932   472.482   481.594  0.00457196  0.00422571
2021-11-08T05:20:42.212219+0000     9     127    272396    272269   472.649   474.016   0.0047522  0.00422451
2021-11-08T05:20:43.212307+0000    10     128    302842    302714    472.95   475.703  0.00363043  0.00422192
2021-11-08T05:20:44.212398+0000    11     128    335450    335322   476.268     509.5  0.00413266  0.00419227
2021-11-08T05:20:45.212512+0000 Total time run:       11.1478
Total reads made:     339521
Read size:            16384
Object size:          16384
Bandwidth (MB/sec):   475.878
Average IOPS:         30456
Stddev IOPS:          1188.15
Max IOPS:             32608
Min IOPS:             27440
Average Latency(s):   0.00419592
Max latency(s):       0.105764
Min latency(s):       0.000256303

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:20:45,888227722-05:00] INFO: > Stop system activity collection services[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:20:45,893472392-05:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:391 - rados_bench() > kill -INT 146337

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:20:45,898327799-05:00] INFO: >> for OSD host[0m
# ./benchmarks/bench-rados:396 - rados_bench() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.2 kill -INT 19535
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:20:45,905330394-05:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.2 kill -INT 19535
[1] 00:20:46 [SUCCESS] ljishen@10.10.2.2

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:20:46,087842250-05:00] INFO: >>> Copy system activity file from OSD host[0m
## ./benchmarks/bench-rados:403 - rados_bench() > basename -- /tmp/bench-rados/sys_activity_16KB_seq.dat.osd_host.1
# ./benchmarks/bench-rados:403 - rados_bench() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.2:/tmp/bench-rados/sys_activity_16KB_seq.dat.osd_host.1 /mnt/sda8/ceph-research/scripts/output/sys_activity_16KB_seq.dat.osd_host.1


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:20:46,239232477-05:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:21:10,183891024-05:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 339.52k objects, 5.2 GiB
    usage:   16 GiB used, 84 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:21:10,190811665-05:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:21:19,211631319-05:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 339.52k objects, 5.2 GiB
    usage:   16 GiB used, 84 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:21:19,218140505-05:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:21:28,320619703-05:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 339.52k objects, 5.2 GiB
    usage:   16 GiB used, 84 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:21:28,327434033-05:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:21:37,413837810-05:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 339.52k objects, 5.2 GiB
    usage:   16 GiB used, 84 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:21:37,420520522-05:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:21:46,606866809-05:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 339.52k objects, 5.2 GiB
    usage:   16 GiB used, 84 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:21:46,613298057-05:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:21:46,618232964-05:00] INFO: > The cluster is idle now.[0m


[1;7;39;49m[2021-11-08T00:21:46,620190365-05:00][RUNNING][ROUND 2/2/21] object_size=16KB[0m


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:21:46,623178540-05:00] STAGE: Launch a Ceph cluster (CEPH_HOST: 10.10.2.3, OSD_HOST: 10.10.2.2) ...[0m
# ./benchmarks/bench-rados:183 - launch_ceph_cluster() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.3 bash -euo pipefail
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:21:46,632007198-05:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.3 bash -euo pipefail
10.10.2.3: b'\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T00:21:47,077524929-05:00] INFO: > Remove existing clusters\x1b[0m\n'
10.10.2.3: b'## bash:17 -  > basename -- /var/lib/ceph/8b275766-4052-11ec-aba1-c5f3c07a3dc7\n'
10.10.2.3: b'# bash:17 -  > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm --host 10.10.2.3 --host 10.10.2.2 rm-cluster --force --fsid 8b275766-4052-11ec-aba1-c5f3c07a3dc7\n'
10.10.2.3: b"\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T00:21:47,088075596-05:00] DEBUG: HOST_PARAMS: ['--host', '10.10.2.3', '--host', '10.10.2.2']\x1b[0m\n"
10.10.2.3: b"\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T00:21:47,091399798-05:00] DEBUG: CEPHADM_PARAMS: ['rm-cluster', '--force', '--fsid', '8b275766-4052-11ec-aba1-c5f3c07a3dc7']\x1b[0m\n"
10.10.2.3: b"# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:63 -  > /tmp/bench-rados/ceph-research/scripts/parallel-ssh --host 10.10.2.3 --host 10.10.2.2 'sudo python3 -u - rm-cluster --force --fsid 8b275766-4052-11ec-aba1-c5f3c07a3dc7'\n"
10.10.2.3: b'\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T00:21:47,099878015-05:00] DEBUG: command from stdin\x1b[0m\n'
10.10.2.3: b"# /tmp/bench-rados/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host 10.10.2.3 --host 10.10.2.2 'sudo python3 -u - rm-cluster --force --fsid 8b275766-4052-11ec-aba1-c5f3c07a3dc7'\n"
10.10.2.3: b'[1] 00:21:53 [SUCCESS] 10.10.2.3\n[2] 00:21:58 [SUCCESS] 10.10.2.2\n'
10.10.2.3: b'\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T00:21:58,161230379-05:00] INFO: > Deploy a new cluster\x1b[0m\n'
10.10.2.3: b'# bash:24 -  > /tmp/bench-rados/ceph-research/scripts/ceph-deploy -m 10.10.2.3 -o node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/10.10.2.2:/dev/nvme0n1\n'
10.10.2.3: b'\n\n\x1b[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T00:21:58,173170618-05:00] STAGE: Verify passwordless SSH to hosts, passwordless sudo via ljishen, and the existence of devices on hosts...\x1b[0m\n'
10.10.2.3: b'\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T00:21:58,177782210-05:00] INFO: > Check the host for the monitor\x1b[0m\n'
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:181 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@10.10.2.3 sudo --non-interactive --validate\n'
10.10.2.3: b'\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T00:21:58,325964427-05:00] INFO: > Check hosts for OSDs\x1b[0m\n'
10.10.2.3: b"\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T00:21:58,330601647-05:00] INFO: >> Check host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'\x1b[0m\n"
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:200 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us sudo --non-interactive --validate\n'
10.10.2.3: b"\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T00:21:58,478233976-05:00] INFO: >>> Check device '/dev/nvme0n1'\x1b[0m\n"
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:215 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us test -b /dev/nvme0n1\n'
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:226 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us lsblk --nodeps --noheadings --output TYPE /dev/nvme0n1\n'
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:227 - verify_parameters() > grep --word-regexp disk\n'
10.10.2.3: b'\n\n\x1b[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T00:21:58,754678063-05:00] STAGE: Reset devices on hosts...\x1b[0m\n'
10.10.2.3: b"\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T00:21:58,759551319-05:00] INFO: > Resetting device '/dev/nvme0n1' on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'\x1b[0m\n"
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:288 - reset_host_devices() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us sudo sh\n'
10.10.2.3: b'  Removing ceph--c3428ab6--de67--4206--b584--a2f366204063-osd--block--7c5cb04a--6342--4508--a9dc--902cc6e64612 (253:0)\n'
10.10.2.3: b'  Archiving volume group "ceph-c3428ab6-de67-4206-b584-a2f366204063" metadata (seqno 5).\n'
10.10.2.3: b'  Releasing logical volume "osd-block-7c5cb04a-6342-4508-a9dc-902cc6e64612"\n'
10.10.2.3: b'  Creating volume group backup "/etc/lvm/backup/ceph-c3428ab6-de67-4206-b584-a2f366204063" (seqno 6).\n'
10.10.2.3: b'  Logical volume "osd-block-7c5cb04a-6342-4508-a9dc-902cc6e64612" successfully removed\n  Removing physical volume "/dev/nvme0n1" from volume group "ceph-c3428ab6-de67-4206-b584-a2f366204063"\n'
10.10.2.3: b'  Volume group "ceph-c3428ab6-de67-4206-b584-a2f366204063" successfully removed\n'
10.10.2.3: b'\n\n\x1b[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T00:21:59,075386401-05:00] STAGE: Bootstrap a new cluster...\x1b[0m\n'
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:317 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.3 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults\n'
10.10.2.3: b'\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T00:21:59,084501149-05:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.3: b"\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T00:21:59,088261434-05:00] DEBUG: CEPHADM_PARAMS: ['bootstrap', '--config', '/tmp/bench-rados/ceph-research/scripts/ceph.conf', '--mon-ip', '10.10.2.3', '--output-dir', '/tmp/bench-rados/deployment_data_root/etc/ceph', '--ssh-user', 'ljishen', '--skip-dashboard', '--no-minimize-config', '--allow-overwrite', '--allow-fqdn-hostname', '--orphan-initial-daemons', '--skip-monitoring-stack', '--single-host-defaults']\x1b[0m\n"
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.3 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults\n'
10.10.2.3: b'Verifying podman|docker is present...\nVerifying lvm2 is present...\n'
10.10.2.3: b'Verifying time synchronization is in place...\n'
10.10.2.3: b'Unit ntp.service is enabled and running\n'
10.10.2.3: b'Repeating the final host check...\ndocker (/usr/bin/docker) is present\n'
10.10.2.3: b'systemctl is present\n'
10.10.2.3: b'lvcreate is present\n'
10.10.2.3: b'Unit ntp.service is enabled and running\n'
10.10.2.3: b'Host looks OK\n'
10.10.2.3: b'Cluster fsid: cc237262-4053-11ec-aba1-c5f3c07a3dc7\n'
10.10.2.3: b'Verifying IP 10.10.2.3 port 3300 ...\n'
10.10.2.3: b'Verifying IP 10.10.2.3 port 6789 ...\n'
10.10.2.3: b'Mon IP `10.10.2.3` is in CIDR network `10.10.2.0/24`\n'
10.10.2.3: b'- internal network (--cluster-network) has not been provided, OSD replication will default to the public_network\n'
10.10.2.3: b'Adjusting default settings to suit single-host cluster...\n'
10.10.2.3: b'Pulling container image quay.io/ceph/ceph:v16...\n'
10.10.2.3: b'Ceph version: ceph version 16.2.6 (ee28fb57e47e9f88813e24bbf4c14496ca299d31) pacific (stable)\nExtracting ceph user uid/gid from container image...\n'
10.10.2.3: b'Creating initial keys...\n'
10.10.2.3: b'Creating initial monmap...\n'
10.10.2.3: b'Creating mon...\n'
10.10.2.3: b'Waiting for mon to start...\n'
10.10.2.3: b'Waiting for mon...\n'
10.10.2.3: b'mon is available\n'
10.10.2.3: b'Setting mon public_network to 10.10.2.0/24\n'
10.10.2.3: b'Wrote config to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf\n'
10.10.2.3: b'Wrote keyring to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring\n'
10.10.2.3: b'Creating mgr...\n'
10.10.2.3: b'Verifying port 9283 ...\n'
10.10.2.3: b'Waiting for mgr to start...\nWaiting for mgr...\n'
10.10.2.3: b'mgr not available, waiting (1/15)...\n'
10.10.2.3: b'mgr not available, waiting (2/15)...\n'
10.10.2.3: b'mgr is available\n'
10.10.2.3: b'Enabling cephadm module...\n'
10.10.2.3: b'Waiting for the mgr to restart...\n'
10.10.2.3: b'Waiting for mgr epoch 5...\n'
10.10.2.3: b'mgr epoch 5 is available\n'
10.10.2.3: b'Setting orchestrator backend to cephadm...\n'
10.10.2.3: b'Generating ssh key...\n'
10.10.2.3: b'Wrote public SSH key to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub\n'
10.10.2.3: b'Adding key to ljishen@localhost authorized_keys...\n'
10.10.2.3: b'Adding host node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us...\n'
10.10.2.3: b'Deploying unmanaged mon service...\n'
10.10.2.3: b'Deploying unmanaged mgr service...\n'
10.10.2.3: b'You can access the Ceph CLI with:\n\n\tsudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --fsid cc237262-4053-11ec-aba1-c5f3c07a3dc7 -c /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf -k /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring\n\nPlease consider enabling telemetry to help improve Ceph:\n\n\tceph telemetry on\n\nFor more information see:\n\n\thttps://docs.ceph.com/docs/pacific/mgr/telemetry/\n\n'
10.10.2.3: b'Bootstrap complete.\n'
10.10.2.3: b'\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T00:23:06,885521175-05:00] INFO: Sleep for 20 seconds\x1b[0m\n'
10.10.2.3: b'\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T00:23:26,893173414-05:00] INFO: > Set the default number of monitor daemons to 1\x1b[0m\n'
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:337 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1\n'
10.10.2.3: b'\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T00:23:26,904130839-05:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.3: b"\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T00:23:26,908038402-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mon', '1']\x1b[0m\n"
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1\n'
10.10.2.3: b'Inferring fsid cc237262-4053-11ec-aba1-c5f3c07a3dc7\n'
10.10.2.3: b'Inferring config /var/lib/ceph/cc237262-4053-11ec-aba1-c5f3c07a3dc7/mon.node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.3: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.3: b'Scheduled mon update...\n'
10.10.2.3: b'\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T00:23:36,445999228-05:00] INFO: > Set the default number of manager daemons to 1\x1b[0m\n'
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:342 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1\n'
10.10.2.3: b'\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T00:23:36,456156844-05:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.3: b"\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T00:23:36,460181288-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mgr', '1']\x1b[0m\n"
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1\n'
10.10.2.3: b'Inferring fsid cc237262-4053-11ec-aba1-c5f3c07a3dc7\n'
10.10.2.3: b'Inferring config /var/lib/ceph/cc237262-4053-11ec-aba1-c5f3c07a3dc7/mon.node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.3: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.3: b'Scheduled mgr update...\n'
10.10.2.3: b'\n\n\x1b[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T00:23:47,161069438-05:00] STAGE: Add hosts to the cluster...\x1b[0m\n'
10.10.2.3: b"\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T00:23:47,167048229-05:00] INFO: > Adding host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'\x1b[0m\n"
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:361 - add_hosts() > ssh-copy-id -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -f -i /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\n'
10.10.2.3: b'/usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: "/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub"\n'
10.10.2.3: b'\nNumber of key(s) added: 1\n\nNow try logging into the machine, with:   "ssh -o \'GlobalKnownHostsFile=/dev/null\' -o \'LogLevel=ERROR\' -o \'PasswordAuthentication=no\' -o \'StrictHostKeyChecking=no\' -o \'UserKnownHostsFile=/dev/null\' \'ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\'"\nand check to make sure that only the key(s) you wanted were added.\n\n'
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:367 - add_hosts() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us 10.10.2.2\n'
10.10.2.3: b'\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T00:23:47,372101970-05:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.3: b"\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T00:23:47,375561819-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'host', 'add', 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us', '10.10.2.2']\x1b[0m\n"
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us 10.10.2.2\n'
10.10.2.3: b'Inferring fsid cc237262-4053-11ec-aba1-c5f3c07a3dc7\n'
10.10.2.3: b'Inferring config /var/lib/ceph/cc237262-4053-11ec-aba1-c5f3c07a3dc7/mon.node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.3: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.3: b"Added host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us' with addr '10.10.2.2'\n"
10.10.2.3: b'\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T00:23:57,771972445-05:00] INFO: Sleep for 20 seconds\x1b[0m\n'
10.10.2.3: b'\n\n\x1b[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T00:24:17,776814683-05:00] STAGE: Add OSDs to the cluster...\x1b[0m\n'
10.10.2.3: b"\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T00:24:17,783620054-05:00] INFO: > Adding devices ['/dev/nvme0n1'] on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us' as OSDs\x1b[0m\n"
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:388 - add_host_devices_as_osds() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1\n'
10.10.2.3: b'\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T00:24:17,793095974-05:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.3: b"\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T00:24:17,796657825-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'daemon', 'add', 'osd', 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1']\x1b[0m\n"
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1\n'
10.10.2.3: b'Inferring fsid cc237262-4053-11ec-aba1-c5f3c07a3dc7\n'
10.10.2.3: b'Inferring config /var/lib/ceph/cc237262-4053-11ec-aba1-c5f3c07a3dc7/mon.node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.3: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.3: b"Created osd(s) 0 on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'\n"
10.10.2.3: b'\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T00:24:44,260398724-05:00] INFO: > Sleep for 20 seconds\x1b[0m\n'
10.10.2.3: b'\n\n\x1b[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T00:25:04,265944658-05:00] STAGE: Show Cluster Status\x1b[0m\n# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:408 - show_status() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status\n'
10.10.2.3: b'\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T00:25:04,276496728-05:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.3: b"\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T00:25:04,279936018-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', '--status']\x1b[0m\n"
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status\n'
10.10.2.3: b'Inferring fsid cc237262-4053-11ec-aba1-c5f3c07a3dc7\n'
10.10.2.3: b'Inferring config /var/lib/ceph/cc237262-4053-11ec-aba1-c5f3c07a3dc7/mon.node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.3: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.3: b'  cluster:\n    id:     cc237262-4053-11ec-aba1-c5f3c07a3dc7\n    health: HEALTH_WARN\n            1 pool(s) have no replicas configured\n \n  services:\n    mon: 1 daemons, quorum node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us (age 2m)\n    mgr: node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us.mnkrfb(active, since 2m)\n    osd: 1 osds: 1 up (since 18s), 1 in (since 41s)\n \n  data:\n    pools:   1 pools, 1 pgs\n    objects: 0 objects, 0 B\n    usage:   4.7 MiB used, 100 GiB / 100 GiB avail\n    pgs:     1 active+clean\n \n'
[1] 00:25:13 [SUCCESS] ljishen@10.10.2.3

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:21:47,077524929-05:00] INFO: > Remove existing clusters[0m
## bash:17 -  > basename -- /var/lib/ceph/8b275766-4052-11ec-aba1-c5f3c07a3dc7
# bash:17 -  > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm --host 10.10.2.3 --host 10.10.2.2 rm-cluster --force --fsid 8b275766-4052-11ec-aba1-c5f3c07a3dc7
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:21:47,088075596-05:00] DEBUG: HOST_PARAMS: ['--host', '10.10.2.3', '--host', '10.10.2.2'][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:21:47,091399798-05:00] DEBUG: CEPHADM_PARAMS: ['rm-cluster', '--force', '--fsid', '8b275766-4052-11ec-aba1-c5f3c07a3dc7'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:63 -  > /tmp/bench-rados/ceph-research/scripts/parallel-ssh --host 10.10.2.3 --host 10.10.2.2 'sudo python3 -u - rm-cluster --force --fsid 8b275766-4052-11ec-aba1-c5f3c07a3dc7'
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:21:47,099878015-05:00] DEBUG: command from stdin[0m
# /tmp/bench-rados/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host 10.10.2.3 --host 10.10.2.2 'sudo python3 -u - rm-cluster --force --fsid 8b275766-4052-11ec-aba1-c5f3c07a3dc7'
[1] 00:21:53 [SUCCESS] 10.10.2.3
[2] 00:21:58 [SUCCESS] 10.10.2.2

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:21:58,161230379-05:00] INFO: > Deploy a new cluster[0m
# bash:24 -  > /tmp/bench-rados/ceph-research/scripts/ceph-deploy -m 10.10.2.3 -o node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/10.10.2.2:/dev/nvme0n1


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:21:58,173170618-05:00] STAGE: Verify passwordless SSH to hosts, passwordless sudo via ljishen, and the existence of devices on hosts...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:21:58,177782210-05:00] INFO: > Check the host for the monitor[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:181 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@10.10.2.3 sudo --non-interactive --validate

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:21:58,325964427-05:00] INFO: > Check hosts for OSDs[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:21:58,330601647-05:00] INFO: >> Check host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:200 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us sudo --non-interactive --validate

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:21:58,478233976-05:00] INFO: >>> Check device '/dev/nvme0n1'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:215 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us test -b /dev/nvme0n1
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:226 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us lsblk --nodeps --noheadings --output TYPE /dev/nvme0n1
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:227 - verify_parameters() > grep --word-regexp disk


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:21:58,754678063-05:00] STAGE: Reset devices on hosts...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:21:58,759551319-05:00] INFO: > Resetting device '/dev/nvme0n1' on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:288 - reset_host_devices() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us sudo sh
  Removing ceph--c3428ab6--de67--4206--b584--a2f366204063-osd--block--7c5cb04a--6342--4508--a9dc--902cc6e64612 (253:0)
  Archiving volume group "ceph-c3428ab6-de67-4206-b584-a2f366204063" metadata (seqno 5).
  Releasing logical volume "osd-block-7c5cb04a-6342-4508-a9dc-902cc6e64612"
  Creating volume group backup "/etc/lvm/backup/ceph-c3428ab6-de67-4206-b584-a2f366204063" (seqno 6).
  Logical volume "osd-block-7c5cb04a-6342-4508-a9dc-902cc6e64612" successfully removed
  Removing physical volume "/dev/nvme0n1" from volume group "ceph-c3428ab6-de67-4206-b584-a2f366204063"
  Volume group "ceph-c3428ab6-de67-4206-b584-a2f366204063" successfully removed


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:21:59,075386401-05:00] STAGE: Bootstrap a new cluster...[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:317 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.3 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:21:59,084501149-05:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:21:59,088261434-05:00] DEBUG: CEPHADM_PARAMS: ['bootstrap', '--config', '/tmp/bench-rados/ceph-research/scripts/ceph.conf', '--mon-ip', '10.10.2.3', '--output-dir', '/tmp/bench-rados/deployment_data_root/etc/ceph', '--ssh-user', 'ljishen', '--skip-dashboard', '--no-minimize-config', '--allow-overwrite', '--allow-fqdn-hostname', '--orphan-initial-daemons', '--skip-monitoring-stack', '--single-host-defaults'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.3 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults
Verifying podman|docker is present...
Verifying lvm2 is present...
Verifying time synchronization is in place...
Unit ntp.service is enabled and running
Repeating the final host check...
docker (/usr/bin/docker) is present
systemctl is present
lvcreate is present
Unit ntp.service is enabled and running
Host looks OK
Cluster fsid: cc237262-4053-11ec-aba1-c5f3c07a3dc7
Verifying IP 10.10.2.3 port 3300 ...
Verifying IP 10.10.2.3 port 6789 ...
Mon IP `10.10.2.3` is in CIDR network `10.10.2.0/24`
- internal network (--cluster-network) has not been provided, OSD replication will default to the public_network
Adjusting default settings to suit single-host cluster...
Pulling container image quay.io/ceph/ceph:v16...
Ceph version: ceph version 16.2.6 (ee28fb57e47e9f88813e24bbf4c14496ca299d31) pacific (stable)
Extracting ceph user uid/gid from container image...
Creating initial keys...
Creating initial monmap...
Creating mon...
Waiting for mon to start...
Waiting for mon...
mon is available
Setting mon public_network to 10.10.2.0/24
Wrote config to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf
Wrote keyring to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring
Creating mgr...
Verifying port 9283 ...
Waiting for mgr to start...
Waiting for mgr...
mgr not available, waiting (1/15)...
mgr not available, waiting (2/15)...
mgr is available
Enabling cephadm module...
Waiting for the mgr to restart...
Waiting for mgr epoch 5...
mgr epoch 5 is available
Setting orchestrator backend to cephadm...
Generating ssh key...
Wrote public SSH key to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub
Adding key to ljishen@localhost authorized_keys...
Adding host node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us...
Deploying unmanaged mon service...
Deploying unmanaged mgr service...
You can access the Ceph CLI with:

	sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --fsid cc237262-4053-11ec-aba1-c5f3c07a3dc7 -c /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf -k /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring

Please consider enabling telemetry to help improve Ceph:

	ceph telemetry on

For more information see:

	https://docs.ceph.com/docs/pacific/mgr/telemetry/

Bootstrap complete.

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:23:06,885521175-05:00] INFO: Sleep for 20 seconds[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:23:26,893173414-05:00] INFO: > Set the default number of monitor daemons to 1[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:337 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:23:26,904130839-05:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:23:26,908038402-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mon', '1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1
Inferring fsid cc237262-4053-11ec-aba1-c5f3c07a3dc7
Inferring config /var/lib/ceph/cc237262-4053-11ec-aba1-c5f3c07a3dc7/mon.node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Scheduled mon update...

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:23:36,445999228-05:00] INFO: > Set the default number of manager daemons to 1[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:342 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:23:36,456156844-05:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:23:36,460181288-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mgr', '1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1
Inferring fsid cc237262-4053-11ec-aba1-c5f3c07a3dc7
Inferring config /var/lib/ceph/cc237262-4053-11ec-aba1-c5f3c07a3dc7/mon.node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Scheduled mgr update...


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:23:47,161069438-05:00] STAGE: Add hosts to the cluster...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:23:47,167048229-05:00] INFO: > Adding host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:361 - add_hosts() > ssh-copy-id -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -f -i /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us
/usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: "/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub"

Number of key(s) added: 1

Now try logging into the machine, with:   "ssh -o 'GlobalKnownHostsFile=/dev/null' -o 'LogLevel=ERROR' -o 'PasswordAuthentication=no' -o 'StrictHostKeyChecking=no' -o 'UserKnownHostsFile=/dev/null' 'ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'"
and check to make sure that only the key(s) you wanted were added.

# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:367 - add_hosts() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us 10.10.2.2
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:23:47,372101970-05:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:23:47,375561819-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'host', 'add', 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us', '10.10.2.2'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us 10.10.2.2
Inferring fsid cc237262-4053-11ec-aba1-c5f3c07a3dc7
Inferring config /var/lib/ceph/cc237262-4053-11ec-aba1-c5f3c07a3dc7/mon.node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Added host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us' with addr '10.10.2.2'

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:23:57,771972445-05:00] INFO: Sleep for 20 seconds[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:24:17,776814683-05:00] STAGE: Add OSDs to the cluster...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:24:17,783620054-05:00] INFO: > Adding devices ['/dev/nvme0n1'] on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us' as OSDs[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:388 - add_host_devices_as_osds() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:24:17,793095974-05:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:24:17,796657825-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'daemon', 'add', 'osd', 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1
Inferring fsid cc237262-4053-11ec-aba1-c5f3c07a3dc7
Inferring config /var/lib/ceph/cc237262-4053-11ec-aba1-c5f3c07a3dc7/mon.node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Created osd(s) 0 on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:24:44,260398724-05:00] INFO: > Sleep for 20 seconds[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:25:04,265944658-05:00] STAGE: Show Cluster Status[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:408 - show_status() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:25:04,276496728-05:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:25:04,279936018-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', '--status'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status
Inferring fsid cc237262-4053-11ec-aba1-c5f3c07a3dc7
Inferring config /var/lib/ceph/cc237262-4053-11ec-aba1-c5f3c07a3dc7/mon.node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
  cluster:
    id:     cc237262-4053-11ec-aba1-c5f3c07a3dc7
    health: HEALTH_WARN
            1 pool(s) have no replicas configured
 
  services:
    mon: 1 daemons, quorum node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us (age 2m)
    mgr: node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us.mnkrfb(active, since 2m)
    osd: 1 osds: 1 up (since 18s), 1 in (since 41s)
 
  data:
    pools:   1 pools, 1 pgs
    objects: 0 objects, 0 B
    usage:   4.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     1 active+clean
 

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:25:13,366599719-05:00] INFO: > Copy configuration files to local[0m
# ./benchmarks/bench-rados:220 - launch_ceph_cluster() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.3 sudo chown --changes --recursive ljishen /tmp/bench-rados/deployment_data_root/etc/ceph
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:25:13,373876199-05:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.3 sudo chown --changes --recursive ljishen /tmp/bench-rados/deployment_data_root/etc/ceph
[1] 00:25:13 [SUCCESS] ljishen@10.10.2.3
# ./benchmarks/bench-rados:223 - launch_ceph_cluster() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.3:/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf
# ./benchmarks/bench-rados:226 - launch_ceph_cluster() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.3:/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:25:13,852415163-05:00] STAGE: Update local ceph.conf as a client...[0m


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:25:13,855570483-05:00] STAGE: Configure Ceph pool...[0m
# ./benchmarks/bench-rados:257 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_max_backfills 32
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:25:13,876446075-05:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:25:13,879032101-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'cc237262-4053-11ec-aba1-c5f3c07a3dc7', '--', 'ceph', 'config', 'set', 'osd', 'osd_max_backfills', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid cc237262-4053-11ec-aba1-c5f3c07a3dc7 -- ceph config set osd osd_max_backfills 32
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:258 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_active 32
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:25:18,077831829-05:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:25:18,080489731-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'cc237262-4053-11ec-aba1-c5f3c07a3dc7', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_active', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid cc237262-4053-11ec-aba1-c5f3c07a3dc7 -- ceph config set osd osd_recovery_max_active 32
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:259 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_single_start 8
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:25:22,241413283-05:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:25:22,243972489-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'cc237262-4053-11ec-aba1-c5f3c07a3dc7', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_single_start', '8'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid cc237262-4053-11ec-aba1-c5f3c07a3dc7 -- ceph config set osd osd_recovery_max_single_start 8
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:260 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_op_priority 63
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:25:26,286216354-05:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:25:26,288982560-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'cc237262-4053-11ec-aba1-c5f3c07a3dc7', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_op_priority', '63'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid cc237262-4053-11ec-aba1-c5f3c07a3dc7 -- ceph config set osd osd_recovery_op_priority 63
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:262 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config show-with-defaults osd.0
# ./benchmarks/bench-rados:263 - configure_ceph_pool() > grep 'osd_max_backfills\|osd_recovery'
# ./benchmarks/bench-rados:263 - configure_ceph_pool() > column -t -s ' '
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
osd_max_backfills                        32         mon
osd_recovery_cost                        20971520   default
osd_recovery_delay_start                 0.000000   default
osd_recovery_max_active                  32         mon
osd_recovery_max_active_hdd              3          default
osd_recovery_max_active_ssd              10         default
osd_recovery_max_chunk                   8388608    default
osd_recovery_max_omap_entries_per_chunk  8096       default
osd_recovery_max_single_start            8          mon
osd_recovery_op_priority                 63         mon
osd_recovery_op_warn_multiple            16         default
osd_recovery_priority                    5          default
osd_recovery_retry_interval              30.000000  default
osd_recovery_sleep                       0.000000   default
osd_recovery_sleep_hdd                   0.100000   default
osd_recovery_sleep_hybrid                0.025000   default
osd_recovery_sleep_ssd                   0.000000   default
# ./benchmarks/bench-rados:265 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool create bench_rados 128 128 replicated --size 1 --pg-num-min 128
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:25:34,459556233-05:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:25:34,462272915-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'cc237262-4053-11ec-aba1-c5f3c07a3dc7', '--', 'ceph', 'osd', 'pool', 'create', 'bench_rados', '128', '128', 'replicated', '--size', '1', '--pg-num-min', '128'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid cc237262-4053-11ec-aba1-c5f3c07a3dc7 -- ceph osd pool create bench_rados 128 128 replicated --size 1 --pg-num-min 128
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
pool 'bench_rados' created
# ./benchmarks/bench-rados:267 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados min_size 1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:25:39,668258809-05:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:25:39,671142176-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'cc237262-4053-11ec-aba1-c5f3c07a3dc7', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'min_size', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid cc237262-4053-11ec-aba1-c5f3c07a3dc7 -- ceph osd pool set bench_rados min_size 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 min_size to 1
# ./benchmarks/bench-rados:268 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados pg_autoscale_mode off
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:25:43,936835794-05:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:25:43,939552256-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'cc237262-4053-11ec-aba1-c5f3c07a3dc7', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'pg_autoscale_mode', 'off'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid cc237262-4053-11ec-aba1-c5f3c07a3dc7 -- ceph osd pool set bench_rados pg_autoscale_mode off
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 pg_autoscale_mode to off
# ./benchmarks/bench-rados:269 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool application enable bench_rados benchmark
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:25:48,370726923-05:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:25:48,373371219-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'cc237262-4053-11ec-aba1-c5f3c07a3dc7', '--', 'ceph', 'osd', 'pool', 'application', 'enable', 'bench_rados', 'benchmark'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid cc237262-4053-11ec-aba1-c5f3c07a3dc7 -- ceph osd pool application enable bench_rados benchmark
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
enabled application 'benchmark' on pool 'bench_rados'
# ./benchmarks/bench-rados:270 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados noscrub 1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:25:52,598249236-05:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:25:52,601011354-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'cc237262-4053-11ec-aba1-c5f3c07a3dc7', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'noscrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid cc237262-4053-11ec-aba1-c5f3c07a3dc7 -- ceph osd pool set bench_rados noscrub 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 noscrub to 1
# ./benchmarks/bench-rados:271 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados nodeep-scrub 1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:25:57,375422951-05:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:25:57,378111982-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'cc237262-4053-11ec-aba1-c5f3c07a3dc7', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'nodeep-scrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid cc237262-4053-11ec-aba1-c5f3c07a3dc7 -- ceph osd pool set bench_rados nodeep-scrub 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 nodeep-scrub to 1
# ./benchmarks/bench-rados:272 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool ls detail
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:26:02,360373556-05:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:26:02,363266952-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'cc237262-4053-11ec-aba1-c5f3c07a3dc7', '--', 'ceph', 'osd', 'pool', 'ls', 'detail'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid cc237262-4053-11ec-aba1-c5f3c07a3dc7 -- ceph osd pool ls detail
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
pool 1 'device_health_metrics' replicated size 1 min_size 1 crush_rule 0 object_hash rjenkins pg_num 64 pgp_num 64 autoscale_mode on last_change 17 lfor 0/0/14 flags hashpspool stripe_width 0 pg_num_min 1 application mgr_devicehealth
pool 2 'bench_rados' replicated size 1 min_size 1 crush_rule 0 object_hash rjenkins pg_num 128 pgp_num 128 autoscale_mode off last_change 20 flags hashpspool,noscrub,nodeep-scrub stripe_width 0 pg_num_min 128 application benchmark

# ./benchmarks/bench-rados:273 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd df tree
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:26:06,338733843-05:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:26:06,341372769-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'cc237262-4053-11ec-aba1-c5f3c07a3dc7', '--', 'ceph', 'osd', 'df', 'tree'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid cc237262-4053-11ec-aba1-c5f3c07a3dc7 -- ceph osd df tree
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
ID  CLASS  WEIGHT   REWEIGHT  SIZE     RAW USE  DATA     OMAP  META     AVAIL    %USE  VAR   PGS  STATUS  TYPE NAME      
-1         0.09769         -  100 GiB  5.7 MiB  168 KiB   0 B  5.6 MiB  100 GiB  0.01  1.00    -          root default   
-3         0.09769         -  100 GiB  5.7 MiB  168 KiB   0 B  5.6 MiB  100 GiB  0.01  1.00    -              host node-1
 0    ssd  0.09769   1.00000  100 GiB  5.7 MiB  168 KiB   0 B  5.6 MiB  100 GiB  0.01  1.00  192      up          osd.0  
                       TOTAL  100 GiB  5.7 MiB  168 KiB   0 B  5.6 MiB  100 GiB  0.01                                    
MIN/MAX VAR: 1.00/1.00  STDDEV: 0


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:26:10,468287636-05:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:26:34,618682297-05:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:26:43,694251096-05:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:26:52,691522795-05:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:26:52,698309031-05:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:27:01,817106348-05:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:27:01,823797545-05:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:27:10,802807649-05:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:27:10,809907796-05:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:27:19,904478632-05:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:27:19,911171321-05:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:27:28,846769988-05:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:27:28,853625385-05:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:27:28,858547215-05:00] INFO: > The cluster is idle now.[0m


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:27:28,861467752-05:00] STAGE: Start benchmarking of mode write ...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:27:28,867077339-05:00] INFO: > Start system activity collection services[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:27:28,871911344-05:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:341 - rados_bench() > rm --force /mnt/sda8/ceph-research/scripts/output/sys_activity_16KB_write.dat.rados_bench_host.2
# ./benchmarks/bench-rados:342 - rados_bench() > sar_pid_rados_bench_host=155261
# ./benchmarks/bench-rados:342 - rados_bench() > S_TIME_FORMAT=ISO
# ./benchmarks/bench-rados:342 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/output/sys_activity_16KB_write.dat.rados_bench_host.2 2

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:27:28,877762467-05:00] INFO: >> for OSD host[0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:27:28,886129724-05:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.2 bash
10.10.2.2: b'23141\n'
[1] 00:27:29 [SUCCESS] ljishen@10.10.2.2
23141

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:27:29,072427962-05:00] INFO: > Run rados bench[0m
# ./benchmarks/bench-rados:370 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/output/rados_bench_16KB_write.log.2
## ./benchmarks/bench-rados:361 - rados_bench() > count_cpus 32-63,96-127
# ./benchmarks/bench-rados:361 - rados_bench() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 60 write --pool bench_rados -b 16384 -O 16384 --concurrent-ios 128 --show-time --write-object --write-omap --write-xattr --no-cleanup
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:27:29,091460689-05:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:27:29,094070130-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'cc237262-4053-11ec-aba1-c5f3c07a3dc7', '--', 'rados', 'bench', '60', 'write', '--pool', 'bench_rados', '-b', '16384', '-O', '16384', '--concurrent-ios', '128', '--show-time', '--write-object', '--write-omap', '--write-xattr', '--no-cleanup'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid cc237262-4053-11ec-aba1-c5f3c07a3dc7 -- rados bench 60 write --pool bench_rados -b 16384 -O 16384 --concurrent-ios 128 --show-time --write-object --write-omap --write-xattr --no-cleanup
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
hints = 1
2021-11-08T05:27:32.358134+0000 Maintaining 128 concurrent writes of 16384 bytes to objects of size 16384 for up to 60 seconds or 0 objects
2021-11-08T05:27:32.358146+0000 Object prefix: benchmark_data_node-0.bluefield2.ucsc-cmps10_7
2021-11-08T05:27:32.359255+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-11-08T05:27:32.359255+0000     0       0         0         0         0         0           -           0
2021-11-08T05:27:33.359390+0000     1     127     14373     14246   222.577   222.594  0.00862191   0.0089291
2021-11-08T05:27:34.359489+0000     2     128     28360     28232   220.544   218.531   0.0092839  0.00904473
2021-11-08T05:27:35.359569+0000     3     128     35588     35460   184.672   112.938   0.0240874   0.0107745
2021-11-08T05:27:36.359640+0000     4     127     42284     42157   164.662   104.641    0.008847   0.0121286
2021-11-08T05:27:37.359719+0000     5     127     56897     56770   177.392   228.328  0.00928708   0.0112577
2021-11-08T05:27:38.359790+0000     6     128     64150     64022   166.711   113.312   0.0910025   0.0118801
2021-11-08T05:27:39.359864+0000     7     128     65686     65558   146.323        24   0.0855344   0.0136195
2021-11-08T05:27:40.359935+0000     8     127     68616     68489   133.757   45.7969   0.0186591   0.0149369
2021-11-08T05:27:41.360008+0000     9     128     75088     74960   130.129   101.109   0.0213322    0.015348
2021-11-08T05:27:42.360081+0000    10     128     81856     81728    127.69    105.75  0.00913451    0.015655
2021-11-08T05:27:43.360157+0000    11     128     94243     94115   133.676   193.547   0.0172644   0.0149454
2021-11-08T05:27:44.360229+0000    12     128     99747     99619   129.702        86   0.0245771   0.0153952
2021-11-08T05:27:45.360303+0000    13     128    109756    109628   131.754   156.391  0.00838476   0.0151723
2021-11-08T05:27:46.360377+0000    14     127    121869    121742   135.862   189.281   0.0111216   0.0147138
2021-11-08T05:27:47.360459+0000    15     128    126431    126303   131.556   71.2656   0.0874925   0.0151493
2021-11-08T05:27:48.360533+0000    16     128    127905    127777   124.773   23.0312   0.0893503   0.0159791
2021-11-08T05:27:49.360604+0000    17     127    130695    130568   119.998   43.6094   0.0249151   0.0166541
2021-11-08T05:27:50.360676+0000    18     128    136128    136000   118.047    84.875   0.0204563   0.0169307
2021-11-08T05:27:51.360749+0000    19     128    141217    141089   116.018   79.5156   0.0253554   0.0172262
2021-11-08T05:27:52.360820+0000 min lat: 0.00509651 max lat: 0.0949325 avg lat: 0.0175067
2021-11-08T05:27:52.360820+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-11-08T05:27:52.360820+0000    20     127    146225    146098    114.13   78.2656   0.0305435   0.0175067
2021-11-08T05:27:53.360901+0000    21     128    151775    151647   112.824   86.7031   0.0152456   0.0177171
2021-11-08T05:27:54.360979+0000    22     128    160144    160016   113.639   130.766   0.0234443   0.0175881
2021-11-08T05:27:55.361058+0000    23     128    166928    166800   113.307       106   0.0178743   0.0176437
2021-11-08T05:27:56.361133+0000    24     128    176243    176115    114.65   145.547   0.0113266     0.01744
2021-11-08T05:27:57.361199+0000    25     128    187425    187297   117.052   174.719   0.0228207   0.0170778
2021-11-08T05:27:58.361277+0000    26     128    188961    188833   113.473        24    0.093008   0.0175805
2021-11-08T05:27:59.361352+0000    27     128    190497    190369   110.159        24   0.0829357   0.0181208
2021-11-08T05:28:00.361432+0000    28     128    197025    196897   109.867       102   0.0176686    0.018194
2021-11-08T05:28:01.361514+0000    29     128    203041    202913    109.32        94     0.02204   0.0182883
2021-11-08T05:28:02.361588+0000    30     128    208673    208545   108.609        88    0.023757   0.0184052
2021-11-08T05:28:03.361664+0000    31     128    214245    214117   107.914   87.0625   0.0228886   0.0185253
2021-11-08T05:28:04.361743+0000    32     128    217505    217377   106.133   50.9375   0.0813047   0.0188105
2021-11-08T05:28:05.361817+0000    33     128    219041    218913   103.644        24   0.0850574   0.0192583
2021-11-08T05:28:06.361894+0000    34     128    220961    220833   101.478        30   0.0741607   0.0197027
2021-11-08T05:28:07.361973+0000    35     128    226789    226661    101.18   91.0625   0.0231271   0.0197598
2021-11-08T05:28:08.362048+0000    36     128    232737    232609   100.951   92.9375   0.0231997   0.0198052
2021-11-08T05:28:09.362122+0000    37     128    238625    238497   100.709        92   0.0223144   0.0198523
2021-11-08T05:28:10.362196+0000    38     128    244385    244257   100.427        90    0.022673   0.0199098
2021-11-08T05:28:11.362265+0000    39     128    248865    248737   99.6467        70   0.0917362    0.020038
2021-11-08T05:28:12.362335+0000 min lat: 0.00325308 max lat: 0.0949325 avg lat: 0.0204354
2021-11-08T05:28:12.362335+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-11-08T05:28:12.362335+0000    40     128    250341    250213   97.7321   23.0625   0.0929489   0.0204354
2021-11-08T05:28:13.362413+0000    41     128    252261    252133     96.08        30    0.022134    0.020807
2021-11-08T05:28:14.362481+0000    42     128    257953    257825   95.9098   88.9375   0.0209891   0.0208468
2021-11-08T05:28:15.362552+0000    43     127    263848    263721   95.8216    92.125   0.0229151   0.0208626
2021-11-08T05:28:16.362628+0000    44     128    269729    269601   95.7318    91.875   0.0215542   0.0208843
2021-11-08T05:28:17.362702+0000    45     128    275489    275361   95.6043        90   0.0227706   0.0209137
2021-11-08T05:28:18.362774+0000    46     128    279397    279269   94.8533   61.0625   0.0893397   0.0210638
2021-11-08T05:28:19.362849+0000    47     128    280933    280805   93.3457        24    0.080682   0.0214092
2021-11-08T05:28:20.362920+0000    48     128    283041    282913   92.0872   32.9375   0.0225939    0.021711
2021-11-08T05:28:21.362991+0000    49     128    288407    288279   91.9188   83.8438    0.022515   0.0217502
2021-11-08T05:28:22.363062+0000    50     128    293911    293783   91.8003        86   0.0216936   0.0217821
2021-11-08T05:28:23.363129+0000    51     127    299448    299321   91.6969   86.5312   0.0229161   0.0218038
2021-11-08T05:28:24.363198+0000    52     127    304952    304825   91.5872        86   0.0228978   0.0218299
2021-11-08T05:28:25.363268+0000    53     128    309399    309271   91.1698   69.4688   0.0924377   0.0219213
2021-11-08T05:28:26.363340+0000    54     128    310885    310757   89.9114   23.2188    0.067536   0.0222257
2021-11-08T05:28:27.363414+0000    55     128    312343    312215   88.6908   22.7812   0.0876414   0.0225232
2021-11-08T05:28:28.363485+0000    56     128    317847    317719   88.6427        86   0.0180606   0.0225561
2021-11-08T05:28:29.363553+0000    57     127    323694    323567   88.6905    91.375   0.0221803    0.022544
2021-11-08T05:28:30.363626+0000    58     128    329445    329317   88.7103   89.8438   0.0228035   0.0225406
2021-11-08T05:28:31.363699+0000    59     128    335181    335053   88.7257    89.625  0.00932495   0.0225353
2021-11-08T05:28:32.363769+0000 min lat: 0.00325308 max lat: 0.0959462 avg lat: 0.02258
2021-11-08T05:28:32.363769+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-11-08T05:28:32.363769+0000    60      54    340045    339991   88.5328   77.1562   0.0900279     0.02258
2021-11-08T05:28:33.363880+0000 Total time run:         60.0384
Total writes made:      340045
Write size:             16384
Object size:            16384
Bandwidth (MB/sec):     88.4967
Stddev Bandwidth:       50.5081
Max bandwidth (MB/sec): 228.328
Min bandwidth (MB/sec): 22.7812
Average IOPS:           5663
Stddev IOPS:            3232.52
Max IOPS:               14613
Min IOPS:               1458
Average Latency(s):     0.0225904
Stddev Latency(s):      0.0182352
Max latency(s):         0.0959462
Min latency(s):         0.00325308

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:28:34,083142540-05:00] INFO: > Stop system activity collection services[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:28:34,088125706-05:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:391 - rados_bench() > kill -INT 155261

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:28:34,093127317-05:00] INFO: >> for OSD host[0m
# ./benchmarks/bench-rados:396 - rados_bench() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.2 kill -INT 23141
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:28:34,100259004-05:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.2 kill -INT 23141
[1] 00:28:34 [SUCCESS] ljishen@10.10.2.2

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:28:34,280882873-05:00] INFO: >>> Copy system activity file from OSD host[0m
## ./benchmarks/bench-rados:403 - rados_bench() > basename -- /tmp/bench-rados/sys_activity_16KB_write.dat.osd_host.2
# ./benchmarks/bench-rados:403 - rados_bench() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.2:/tmp/bench-rados/sys_activity_16KB_write.dat.osd_host.2 /mnt/sda8/ceph-research/scripts/output/sys_activity_16KB_write.dat.osd_host.2


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:28:34,450385623-05:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:28:58,457575019-05:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 340.05k objects, 5.2 GiB
    usage:   16 GiB used, 84 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:28:58,464133225-05:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:29:07,485014352-05:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 340.05k objects, 5.2 GiB
    usage:   16 GiB used, 84 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:29:07,491768848-05:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:29:16,478043828-05:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 340.05k objects, 5.2 GiB
    usage:   16 GiB used, 84 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:29:16,484607935-05:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:29:25,488813838-05:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 340.05k objects, 5.2 GiB
    usage:   16 GiB used, 84 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:29:25,495221119-05:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:29:34,464064408-05:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 340.05k objects, 5.2 GiB
    usage:   16 GiB used, 84 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:29:34,470661187-05:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:29:34,475595210-05:00] INFO: > The cluster is idle now.[0m


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:29:34,478703581-05:00] STAGE: Start benchmarking of mode seq ...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:29:34,484372550-05:00] INFO: > Start system activity collection services[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:29:34,489364904-05:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:341 - rados_bench() > rm --force /mnt/sda8/ceph-research/scripts/output/sys_activity_16KB_seq.dat.rados_bench_host.2
# ./benchmarks/bench-rados:342 - rados_bench() > sar_pid_rados_bench_host=157372
# ./benchmarks/bench-rados:342 - rados_bench() > S_TIME_FORMAT=ISO
# ./benchmarks/bench-rados:342 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/output/sys_activity_16KB_seq.dat.rados_bench_host.2 2

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:29:34,495457963-05:00] INFO: >> for OSD host[0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:29:34,503599314-05:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.2 bash
10.10.2.2: b'23245\n'
[1] 00:29:34 [SUCCESS] ljishen@10.10.2.2
23245

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:29:34,689525111-05:00] INFO: > Run rados bench[0m
# ./benchmarks/bench-rados:379 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/output/rados_bench_16KB_seq.log.2
## ./benchmarks/bench-rados:376 - rados_bench() > count_cpus 32-63,96-127
# ./benchmarks/bench-rados:376 - rados_bench() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 128 --show-time
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:29:34,709059675-05:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:29:34,711943482-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'cc237262-4053-11ec-aba1-c5f3c07a3dc7', '--', 'rados', 'bench', '99999999', 'seq', '--pool', 'bench_rados', '--concurrent-ios', '128', '--show-time'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid cc237262-4053-11ec-aba1-c5f3c07a3dc7 -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 128 --show-time
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
hints = 1
2021-11-08T05:29:38.058857+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-11-08T05:29:38.058857+0000     0       0         0         0         0         0           -           0
2021-11-08T05:29:39.058973+0000     1     128     29827     29699   463.954   464.047  0.00433581  0.00429228
2021-11-08T05:29:40.059063+0000     2     127     59836     59709   466.409   468.906  0.00419857  0.00427371
2021-11-08T05:29:41.059145+0000     3     128     90199     90071   469.062   474.406  0.00333659  0.00425204
2021-11-08T05:29:42.059210+0000     4     128    120146    120018   468.769   467.922  0.00437384  0.00425636
2021-11-08T05:29:43.059274+0000     5     128    150229    150101   469.019   470.047  0.00461891  0.00425431
2021-11-08T05:29:44.059339+0000     6     128    180372    180244   469.341   470.984  0.00370992  0.00425175
2021-11-08T05:29:45.059403+0000     7     128    210051    209923   468.536   463.734  0.00306523  0.00425915
2021-11-08T05:29:46.059469+0000     8     128    240005    239877   468.469   468.031  0.00414931  0.00425999
2021-11-08T05:29:47.059538+0000     9     127    270187    270060   468.814   471.609  0.00367633  0.00425739
2021-11-08T05:29:48.059611+0000    10     128    299330    299202   467.464   455.344  0.00434592  0.00426956
2021-11-08T05:29:49.059693+0000    11     127    328946    328819   467.033   462.766  0.00402981  0.00427352
2021-11-08T05:29:50.059789+0000 Total time run:       11.3771
Total reads made:     340045
Read size:            16384
Object size:          16384
Bandwidth (MB/sec):   467.007
Average IOPS:         29888
Stddev IOPS:          338.614
Max IOPS:             30362
Min IOPS:             29142
Average Latency(s):   0.00427421
Max latency(s):       0.0147251
Min latency(s):       0.000382591

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:29:50,747670025-05:00] INFO: > Stop system activity collection services[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:29:50,752622684-05:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:391 - rados_bench() > kill -INT 157372

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:29:50,757736997-05:00] INFO: >> for OSD host[0m
# ./benchmarks/bench-rados:396 - rados_bench() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.2 kill -INT 23245
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:29:50,765042222-05:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.2 kill -INT 23245
[1] 00:29:50 [SUCCESS] ljishen@10.10.2.2

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:29:50,944051046-05:00] INFO: >>> Copy system activity file from OSD host[0m
## ./benchmarks/bench-rados:403 - rados_bench() > basename -- /tmp/bench-rados/sys_activity_16KB_seq.dat.osd_host.2
# ./benchmarks/bench-rados:403 - rados_bench() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.2:/tmp/bench-rados/sys_activity_16KB_seq.dat.osd_host.2 /mnt/sda8/ceph-research/scripts/output/sys_activity_16KB_seq.dat.osd_host.2


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:29:51,095281282-05:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:30:15,202687936-05:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 340.05k objects, 5.2 GiB
    usage:   16 GiB used, 84 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:30:15,209084527-05:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:30:24,396507306-05:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 340.05k objects, 5.2 GiB
    usage:   16 GiB used, 84 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:30:24,403222589-05:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:30:33,532526113-05:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 340.05k objects, 5.2 GiB
    usage:   16 GiB used, 84 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:30:33,538853153-05:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:30:42,540379578-05:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 340.05k objects, 5.2 GiB
    usage:   16 GiB used, 84 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:30:42,547210368-05:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:30:51,775544841-05:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 340.05k objects, 5.2 GiB
    usage:   16 GiB used, 84 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:30:51,782247278-05:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:30:51,787778418-05:00] INFO: > The cluster is idle now.[0m


[1;7;39;49m[2021-11-08T00:30:51,789794089-05:00][RUNNING][ROUND 3/2/21] object_size=16KB[0m


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:30:51,792872343-05:00] STAGE: Launch a Ceph cluster (CEPH_HOST: 10.10.2.3, OSD_HOST: 10.10.2.2) ...[0m
# ./benchmarks/bench-rados:183 - launch_ceph_cluster() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.3 bash -euo pipefail
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:30:51,801405984-05:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.3 bash -euo pipefail
10.10.2.3: b'\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T00:30:52,125500081-05:00] INFO: > Remove existing clusters\x1b[0m\n'
10.10.2.3: b'## bash:17 -  > basename -- /var/lib/ceph/cc237262-4053-11ec-aba1-c5f3c07a3dc7\n'
10.10.2.3: b'# bash:17 -  > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm --host 10.10.2.3 --host 10.10.2.2 rm-cluster --force --fsid cc237262-4053-11ec-aba1-c5f3c07a3dc7\n'
10.10.2.3: b"\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T00:30:52,135669430-05:00] DEBUG: HOST_PARAMS: ['--host', '10.10.2.3', '--host', '10.10.2.2']\x1b[0m\n"
10.10.2.3: b"\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T00:30:52,139047654-05:00] DEBUG: CEPHADM_PARAMS: ['rm-cluster', '--force', '--fsid', 'cc237262-4053-11ec-aba1-c5f3c07a3dc7']\x1b[0m\n"
10.10.2.3: b"# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:63 -  > /tmp/bench-rados/ceph-research/scripts/parallel-ssh --host 10.10.2.3 --host 10.10.2.2 'sudo python3 -u - rm-cluster --force --fsid cc237262-4053-11ec-aba1-c5f3c07a3dc7'\n"
10.10.2.3: b'\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T00:30:52,147738313-05:00] DEBUG: command from stdin\x1b[0m\n'
10.10.2.3: b"# /tmp/bench-rados/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host 10.10.2.3 --host 10.10.2.2 'sudo python3 -u - rm-cluster --force --fsid cc237262-4053-11ec-aba1-c5f3c07a3dc7'\n"
10.10.2.3: b'[1] 00:30:57 [SUCCESS] 10.10.2.3\n[2] 00:31:02 [SUCCESS] 10.10.2.2\n'
10.10.2.3: b'\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T00:31:02,210934838-05:00] INFO: > Deploy a new cluster\x1b[0m\n'
10.10.2.3: b'# bash:24 -  > /tmp/bench-rados/ceph-research/scripts/ceph-deploy -m 10.10.2.3 -o node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/10.10.2.2:/dev/nvme0n1\n'
10.10.2.3: b'\n\n\x1b[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T00:31:02,222664652-05:00] STAGE: Verify passwordless SSH to hosts, passwordless sudo via ljishen, and the existence of devices on hosts...\x1b[0m\n'
10.10.2.3: b'\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T00:31:02,227570880-05:00] INFO: > Check the host for the monitor\x1b[0m\n'
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:181 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@10.10.2.3 sudo --non-interactive --validate\n'
10.10.2.3: b'\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T00:31:02,378468707-05:00] INFO: > Check hosts for OSDs\x1b[0m\n'
10.10.2.3: b"\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T00:31:02,382301840-05:00] INFO: >> Check host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'\x1b[0m\n# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:200 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us sudo --non-interactive --validate\n"
10.10.2.3: b"\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T00:31:02,529687530-05:00] INFO: >>> Check device '/dev/nvme0n1'\x1b[0m\n"
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:215 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us test -b /dev/nvme0n1\n'
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:226 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us lsblk --nodeps --noheadings --output TYPE /dev/nvme0n1\n'
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:227 - verify_parameters() > grep --word-regexp disk\n'
10.10.2.3: b'\n\n\x1b[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T00:31:02,810113981-05:00] STAGE: Reset devices on hosts...\x1b[0m\n'
10.10.2.3: b"\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T00:31:02,815035447-05:00] INFO: > Resetting device '/dev/nvme0n1' on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'\x1b[0m\n"
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:288 - reset_host_devices() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us sudo sh\n'
10.10.2.3: b'  Removing ceph--cbd822f8--c7fb--4476--bcac--9f35001907d7-osd--block--6be69dbb--9297--4048--b736--3dde0d9f1656 (253:0)\n'
10.10.2.3: b'  Archiving volume group "ceph-cbd822f8-c7fb-4476-bcac-9f35001907d7" metadata (seqno 5).\n  Releasing logical volume "osd-block-6be69dbb-9297-4048-b736-3dde0d9f1656"\n'
10.10.2.3: b'  Creating volume group backup "/etc/lvm/backup/ceph-cbd822f8-c7fb-4476-bcac-9f35001907d7" (seqno 6).\n'
10.10.2.3: b'  Logical volume "osd-block-6be69dbb-9297-4048-b736-3dde0d9f1656" successfully removed\n'
10.10.2.3: b'  Removing physical volume "/dev/nvme0n1" from volume group "ceph-cbd822f8-c7fb-4476-bcac-9f35001907d7"\n'
10.10.2.3: b'  Volume group "ceph-cbd822f8-c7fb-4476-bcac-9f35001907d7" successfully removed\n'
10.10.2.3: b'\n\n\x1b[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T00:31:03,123453829-05:00] STAGE: Bootstrap a new cluster...\x1b[0m\n'
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:317 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.3 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults\n'
10.10.2.3: b'\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T00:31:03,133176525-05:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.3: b"\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T00:31:03,136567573-05:00] DEBUG: CEPHADM_PARAMS: ['bootstrap', '--config', '/tmp/bench-rados/ceph-research/scripts/ceph.conf', '--mon-ip', '10.10.2.3', '--output-dir', '/tmp/bench-rados/deployment_data_root/etc/ceph', '--ssh-user', 'ljishen', '--skip-dashboard', '--no-minimize-config', '--allow-overwrite', '--allow-fqdn-hostname', '--orphan-initial-daemons', '--skip-monitoring-stack', '--single-host-defaults']\x1b[0m\n"
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.3 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults\n'
10.10.2.3: b'Verifying podman|docker is present...\nVerifying lvm2 is present...\n'
10.10.2.3: b'Verifying time synchronization is in place...\n'
10.10.2.3: b'Unit ntp.service is enabled and running\n'
10.10.2.3: b'Repeating the final host check...\n'
10.10.2.3: b'docker (/usr/bin/docker) is present\n'
10.10.2.3: b'systemctl is present\n'
10.10.2.3: b'lvcreate is present\n'
10.10.2.3: b'Unit ntp.service is enabled and running\n'
10.10.2.3: b'Host looks OK\n'
10.10.2.3: b'Cluster fsid: 106b29b4-4055-11ec-aba1-c5f3c07a3dc7\n'
10.10.2.3: b'Verifying IP 10.10.2.3 port 3300 ...\n'
10.10.2.3: b'Verifying IP 10.10.2.3 port 6789 ...\n'
10.10.2.3: b'Mon IP `10.10.2.3` is in CIDR network `10.10.2.0/24`\n'
10.10.2.3: b'- internal network (--cluster-network) has not been provided, OSD replication will default to the public_network\n'
10.10.2.3: b'Adjusting default settings to suit single-host cluster...\n'
10.10.2.3: b'Pulling container image quay.io/ceph/ceph:v16...\n'
10.10.2.3: b'Ceph version: ceph version 16.2.6 (ee28fb57e47e9f88813e24bbf4c14496ca299d31) pacific (stable)\n'
10.10.2.3: b'Extracting ceph user uid/gid from container image...\n'
10.10.2.3: b'Creating initial keys...\n'
10.10.2.3: b'Creating initial monmap...\n'
10.10.2.3: b'Creating mon...\n'
10.10.2.3: b'Waiting for mon to start...\n'
10.10.2.3: b'Waiting for mon...\n'
10.10.2.3: b'mon is available\nSetting mon public_network to 10.10.2.0/24\n'
10.10.2.3: b'Wrote config to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf\n'
10.10.2.3: b'Wrote keyring to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring\nCreating mgr...\nVerifying port 9283 ...\n'
10.10.2.3: b'Waiting for mgr to start...\nWaiting for mgr...\n'
10.10.2.3: b'mgr not available, waiting (1/15)...\n'
10.10.2.3: b'mgr not available, waiting (2/15)...\n'
10.10.2.3: b'mgr is available\n'
10.10.2.3: b'Enabling cephadm module...\n'
10.10.2.3: b'Waiting for the mgr to restart...\n'
10.10.2.3: b'Waiting for mgr epoch 5...\n'
10.10.2.3: b'mgr epoch 5 is available\n'
10.10.2.3: b'Setting orchestrator backend to cephadm...\n'
10.10.2.3: b'Generating ssh key...\n'
10.10.2.3: b'Wrote public SSH key to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub\n'
10.10.2.3: b'Adding key to ljishen@localhost authorized_keys...\n'
10.10.2.3: b'Adding host node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us...\n'
10.10.2.3: b'Deploying unmanaged mon service...\n'
10.10.2.3: b'Deploying unmanaged mgr service...\n'
10.10.2.3: b'You can access the Ceph CLI with:\n\n\tsudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --fsid 106b29b4-4055-11ec-aba1-c5f3c07a3dc7 -c /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf -k /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring\n\nPlease consider enabling telemetry to help improve Ceph:\n\n\tceph telemetry on\n\nFor more information see:\n\n\thttps://docs.ceph.com/docs/pacific/mgr/telemetry/\n\nBootstrap complete.\n'
10.10.2.3: b'\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T00:32:10,232146724-05:00] INFO: Sleep for 20 seconds\x1b[0m\n'
10.10.2.3: b'\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T00:32:30,239741708-05:00] INFO: > Set the default number of monitor daemons to 1\x1b[0m\n# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:337 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1\n'
10.10.2.3: b'\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T00:32:30,249240221-05:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.3: b"\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T00:32:30,252215074-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mon', '1']\x1b[0m\n"
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1\n'
10.10.2.3: b'Inferring fsid 106b29b4-4055-11ec-aba1-c5f3c07a3dc7\n'
10.10.2.3: b'Inferring config /var/lib/ceph/106b29b4-4055-11ec-aba1-c5f3c07a3dc7/mon.node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.3: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.3: b'Scheduled mon update...\n'
10.10.2.3: b'\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T00:32:39,887892947-05:00] INFO: > Set the default number of manager daemons to 1\x1b[0m\n'
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:342 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1\n'
10.10.2.3: b'\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T00:32:39,897161867-05:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.3: b"\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T00:32:39,900445994-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mgr', '1']\x1b[0m\n"
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1\n'
10.10.2.3: b'Inferring fsid 106b29b4-4055-11ec-aba1-c5f3c07a3dc7\n'
10.10.2.3: b'Inferring config /var/lib/ceph/106b29b4-4055-11ec-aba1-c5f3c07a3dc7/mon.node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.3: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.3: b'Scheduled mgr update...\n'
10.10.2.3: b'\n\n\x1b[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T00:32:49,839710071-05:00] STAGE: Add hosts to the cluster...\x1b[0m\n'
10.10.2.3: b"\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T00:32:49,845395780-05:00] INFO: > Adding host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'\x1b[0m\n"
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:361 - add_hosts() > ssh-copy-id -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -f -i /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\n'
10.10.2.3: b'/usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: "/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub"\n'
10.10.2.3: b'\nNumber of key(s) added: 1\n\nNow try logging into the machine, with:   "ssh -o \'GlobalKnownHostsFile=/dev/null\' -o \'LogLevel=ERROR\' -o \'PasswordAuthentication=no\' -o \'StrictHostKeyChecking=no\' -o \'UserKnownHostsFile=/dev/null\' \'ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\'"\nand check to make sure that only the key(s) you wanted were added.\n\n'
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:367 - add_hosts() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us 10.10.2.2\n'
10.10.2.3: b'\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T00:32:50,052551926-05:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.3: b"\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T00:32:50,056329233-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'host', 'add', 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us', '10.10.2.2']\x1b[0m\n"
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us 10.10.2.2\n'
10.10.2.3: b'Inferring fsid 106b29b4-4055-11ec-aba1-c5f3c07a3dc7\n'
10.10.2.3: b'Inferring config /var/lib/ceph/106b29b4-4055-11ec-aba1-c5f3c07a3dc7/mon.node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.3: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.3: b"Added host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us' with addr '10.10.2.2'\n"
10.10.2.3: b'\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T00:32:59,839738166-05:00] INFO: Sleep for 20 seconds\x1b[0m\n'
10.10.2.3: b'\n\n\x1b[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T00:33:19,844544094-05:00] STAGE: Add OSDs to the cluster...\x1b[0m\n'
10.10.2.3: b"\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T00:33:19,851258995-05:00] INFO: > Adding devices ['/dev/nvme0n1'] on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us' as OSDs\x1b[0m\n"
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:388 - add_host_devices_as_osds() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1\n'
10.10.2.3: b'\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T00:33:19,861115284-05:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.3: b"\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T00:33:19,864657759-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'daemon', 'add', 'osd', 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1']\x1b[0m\n"
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1\n'
10.10.2.3: b'Inferring fsid 106b29b4-4055-11ec-aba1-c5f3c07a3dc7\n'
10.10.2.3: b'Inferring config /var/lib/ceph/106b29b4-4055-11ec-aba1-c5f3c07a3dc7/mon.node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.3: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.3: b"Created osd(s) 0 on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'\n"
10.10.2.3: b'\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T00:33:46,095002227-05:00] INFO: > Sleep for 20 seconds\x1b[0m\n'
10.10.2.3: b'\n\n\x1b[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T00:34:06,101049324-05:00] STAGE: Show Cluster Status\x1b[0m\n'
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:408 - show_status() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status\n'
10.10.2.3: b'\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T00:34:06,111379725-05:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.3: b"\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T00:34:06,115140420-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', '--status']\x1b[0m\n"
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status\n'
10.10.2.3: b'Inferring fsid 106b29b4-4055-11ec-aba1-c5f3c07a3dc7\n'
10.10.2.3: b'Inferring config /var/lib/ceph/106b29b4-4055-11ec-aba1-c5f3c07a3dc7/mon.node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.3: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.3: b'  cluster:\n    id:     106b29b4-4055-11ec-aba1-c5f3c07a3dc7\n    health: HEALTH_WARN\n            1 pool(s) have no replicas configured\n \n  services:\n    mon: 1 daemons, quorum node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us (age 2m)\n    mgr: node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us.tkgdcw(active, since 2m)\n    osd: 1 osds: 1 up (since 18s), 1 in (since 42s)\n \n  data:\n    pools:   1 pools, 1 pgs\n    objects: 0 objects, 0 B\n    usage:   4.7 MiB used, 100 GiB / 100 GiB avail\n    pgs:     1 active+clean\n \n'
[1] 00:34:16 [SUCCESS] ljishen@10.10.2.3

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:30:52,125500081-05:00] INFO: > Remove existing clusters[0m
## bash:17 -  > basename -- /var/lib/ceph/cc237262-4053-11ec-aba1-c5f3c07a3dc7
# bash:17 -  > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm --host 10.10.2.3 --host 10.10.2.2 rm-cluster --force --fsid cc237262-4053-11ec-aba1-c5f3c07a3dc7
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:30:52,135669430-05:00] DEBUG: HOST_PARAMS: ['--host', '10.10.2.3', '--host', '10.10.2.2'][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:30:52,139047654-05:00] DEBUG: CEPHADM_PARAMS: ['rm-cluster', '--force', '--fsid', 'cc237262-4053-11ec-aba1-c5f3c07a3dc7'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:63 -  > /tmp/bench-rados/ceph-research/scripts/parallel-ssh --host 10.10.2.3 --host 10.10.2.2 'sudo python3 -u - rm-cluster --force --fsid cc237262-4053-11ec-aba1-c5f3c07a3dc7'
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:30:52,147738313-05:00] DEBUG: command from stdin[0m
# /tmp/bench-rados/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host 10.10.2.3 --host 10.10.2.2 'sudo python3 -u - rm-cluster --force --fsid cc237262-4053-11ec-aba1-c5f3c07a3dc7'
[1] 00:30:57 [SUCCESS] 10.10.2.3
[2] 00:31:02 [SUCCESS] 10.10.2.2

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:31:02,210934838-05:00] INFO: > Deploy a new cluster[0m
# bash:24 -  > /tmp/bench-rados/ceph-research/scripts/ceph-deploy -m 10.10.2.3 -o node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/10.10.2.2:/dev/nvme0n1


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:31:02,222664652-05:00] STAGE: Verify passwordless SSH to hosts, passwordless sudo via ljishen, and the existence of devices on hosts...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:31:02,227570880-05:00] INFO: > Check the host for the monitor[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:181 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@10.10.2.3 sudo --non-interactive --validate

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:31:02,378468707-05:00] INFO: > Check hosts for OSDs[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:31:02,382301840-05:00] INFO: >> Check host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:200 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us sudo --non-interactive --validate

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:31:02,529687530-05:00] INFO: >>> Check device '/dev/nvme0n1'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:215 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us test -b /dev/nvme0n1
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:226 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us lsblk --nodeps --noheadings --output TYPE /dev/nvme0n1
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:227 - verify_parameters() > grep --word-regexp disk


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:31:02,810113981-05:00] STAGE: Reset devices on hosts...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:31:02,815035447-05:00] INFO: > Resetting device '/dev/nvme0n1' on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:288 - reset_host_devices() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us sudo sh
  Removing ceph--cbd822f8--c7fb--4476--bcac--9f35001907d7-osd--block--6be69dbb--9297--4048--b736--3dde0d9f1656 (253:0)
  Archiving volume group "ceph-cbd822f8-c7fb-4476-bcac-9f35001907d7" metadata (seqno 5).
  Releasing logical volume "osd-block-6be69dbb-9297-4048-b736-3dde0d9f1656"
  Creating volume group backup "/etc/lvm/backup/ceph-cbd822f8-c7fb-4476-bcac-9f35001907d7" (seqno 6).
  Logical volume "osd-block-6be69dbb-9297-4048-b736-3dde0d9f1656" successfully removed
  Removing physical volume "/dev/nvme0n1" from volume group "ceph-cbd822f8-c7fb-4476-bcac-9f35001907d7"
  Volume group "ceph-cbd822f8-c7fb-4476-bcac-9f35001907d7" successfully removed


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:31:03,123453829-05:00] STAGE: Bootstrap a new cluster...[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:317 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.3 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:31:03,133176525-05:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:31:03,136567573-05:00] DEBUG: CEPHADM_PARAMS: ['bootstrap', '--config', '/tmp/bench-rados/ceph-research/scripts/ceph.conf', '--mon-ip', '10.10.2.3', '--output-dir', '/tmp/bench-rados/deployment_data_root/etc/ceph', '--ssh-user', 'ljishen', '--skip-dashboard', '--no-minimize-config', '--allow-overwrite', '--allow-fqdn-hostname', '--orphan-initial-daemons', '--skip-monitoring-stack', '--single-host-defaults'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.3 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults
Verifying podman|docker is present...
Verifying lvm2 is present...
Verifying time synchronization is in place...
Unit ntp.service is enabled and running
Repeating the final host check...
docker (/usr/bin/docker) is present
systemctl is present
lvcreate is present
Unit ntp.service is enabled and running
Host looks OK
Cluster fsid: 106b29b4-4055-11ec-aba1-c5f3c07a3dc7
Verifying IP 10.10.2.3 port 3300 ...
Verifying IP 10.10.2.3 port 6789 ...
Mon IP `10.10.2.3` is in CIDR network `10.10.2.0/24`
- internal network (--cluster-network) has not been provided, OSD replication will default to the public_network
Adjusting default settings to suit single-host cluster...
Pulling container image quay.io/ceph/ceph:v16...
Ceph version: ceph version 16.2.6 (ee28fb57e47e9f88813e24bbf4c14496ca299d31) pacific (stable)
Extracting ceph user uid/gid from container image...
Creating initial keys...
Creating initial monmap...
Creating mon...
Waiting for mon to start...
Waiting for mon...
mon is available
Setting mon public_network to 10.10.2.0/24
Wrote config to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf
Wrote keyring to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring
Creating mgr...
Verifying port 9283 ...
Waiting for mgr to start...
Waiting for mgr...
mgr not available, waiting (1/15)...
mgr not available, waiting (2/15)...
mgr is available
Enabling cephadm module...
Waiting for the mgr to restart...
Waiting for mgr epoch 5...
mgr epoch 5 is available
Setting orchestrator backend to cephadm...
Generating ssh key...
Wrote public SSH key to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub
Adding key to ljishen@localhost authorized_keys...
Adding host node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us...
Deploying unmanaged mon service...
Deploying unmanaged mgr service...
You can access the Ceph CLI with:

	sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --fsid 106b29b4-4055-11ec-aba1-c5f3c07a3dc7 -c /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf -k /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring

Please consider enabling telemetry to help improve Ceph:

	ceph telemetry on

For more information see:

	https://docs.ceph.com/docs/pacific/mgr/telemetry/

Bootstrap complete.

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:32:10,232146724-05:00] INFO: Sleep for 20 seconds[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:32:30,239741708-05:00] INFO: > Set the default number of monitor daemons to 1[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:337 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:32:30,249240221-05:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:32:30,252215074-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mon', '1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1
Inferring fsid 106b29b4-4055-11ec-aba1-c5f3c07a3dc7
Inferring config /var/lib/ceph/106b29b4-4055-11ec-aba1-c5f3c07a3dc7/mon.node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Scheduled mon update...

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:32:39,887892947-05:00] INFO: > Set the default number of manager daemons to 1[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:342 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:32:39,897161867-05:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:32:39,900445994-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mgr', '1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1
Inferring fsid 106b29b4-4055-11ec-aba1-c5f3c07a3dc7
Inferring config /var/lib/ceph/106b29b4-4055-11ec-aba1-c5f3c07a3dc7/mon.node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Scheduled mgr update...


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:32:49,839710071-05:00] STAGE: Add hosts to the cluster...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:32:49,845395780-05:00] INFO: > Adding host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:361 - add_hosts() > ssh-copy-id -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -f -i /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us
/usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: "/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub"

Number of key(s) added: 1

Now try logging into the machine, with:   "ssh -o 'GlobalKnownHostsFile=/dev/null' -o 'LogLevel=ERROR' -o 'PasswordAuthentication=no' -o 'StrictHostKeyChecking=no' -o 'UserKnownHostsFile=/dev/null' 'ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'"
and check to make sure that only the key(s) you wanted were added.

# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:367 - add_hosts() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us 10.10.2.2
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:32:50,052551926-05:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:32:50,056329233-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'host', 'add', 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us', '10.10.2.2'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us 10.10.2.2
Inferring fsid 106b29b4-4055-11ec-aba1-c5f3c07a3dc7
Inferring config /var/lib/ceph/106b29b4-4055-11ec-aba1-c5f3c07a3dc7/mon.node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Added host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us' with addr '10.10.2.2'

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:32:59,839738166-05:00] INFO: Sleep for 20 seconds[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:33:19,844544094-05:00] STAGE: Add OSDs to the cluster...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:33:19,851258995-05:00] INFO: > Adding devices ['/dev/nvme0n1'] on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us' as OSDs[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:388 - add_host_devices_as_osds() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:33:19,861115284-05:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:33:19,864657759-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'daemon', 'add', 'osd', 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1
Inferring fsid 106b29b4-4055-11ec-aba1-c5f3c07a3dc7
Inferring config /var/lib/ceph/106b29b4-4055-11ec-aba1-c5f3c07a3dc7/mon.node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Created osd(s) 0 on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:33:46,095002227-05:00] INFO: > Sleep for 20 seconds[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:34:06,101049324-05:00] STAGE: Show Cluster Status[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:408 - show_status() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:34:06,111379725-05:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:34:06,115140420-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', '--status'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status
Inferring fsid 106b29b4-4055-11ec-aba1-c5f3c07a3dc7
Inferring config /var/lib/ceph/106b29b4-4055-11ec-aba1-c5f3c07a3dc7/mon.node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
  cluster:
    id:     106b29b4-4055-11ec-aba1-c5f3c07a3dc7
    health: HEALTH_WARN
            1 pool(s) have no replicas configured
 
  services:
    mon: 1 daemons, quorum node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us (age 2m)
    mgr: node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us.tkgdcw(active, since 2m)
    osd: 1 osds: 1 up (since 18s), 1 in (since 42s)
 
  data:
    pools:   1 pools, 1 pgs
    objects: 0 objects, 0 B
    usage:   4.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     1 active+clean
 

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:34:16,017314161-05:00] INFO: > Copy configuration files to local[0m
# ./benchmarks/bench-rados:220 - launch_ceph_cluster() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.3 sudo chown --changes --recursive ljishen /tmp/bench-rados/deployment_data_root/etc/ceph
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:34:16,024690390-05:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.3 sudo chown --changes --recursive ljishen /tmp/bench-rados/deployment_data_root/etc/ceph
[1] 00:34:16 [SUCCESS] ljishen@10.10.2.3
# ./benchmarks/bench-rados:223 - launch_ceph_cluster() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.3:/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf
# ./benchmarks/bench-rados:226 - launch_ceph_cluster() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.3:/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:34:16,496415308-05:00] STAGE: Update local ceph.conf as a client...[0m


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:34:16,499674253-05:00] STAGE: Configure Ceph pool...[0m
# ./benchmarks/bench-rados:257 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_max_backfills 32
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:34:16,519911863-05:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:34:16,522502528-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '106b29b4-4055-11ec-aba1-c5f3c07a3dc7', '--', 'ceph', 'config', 'set', 'osd', 'osd_max_backfills', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 106b29b4-4055-11ec-aba1-c5f3c07a3dc7 -- ceph config set osd osd_max_backfills 32
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:258 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_active 32
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:34:20,535584121-05:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:34:20,538286256-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '106b29b4-4055-11ec-aba1-c5f3c07a3dc7', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_active', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 106b29b4-4055-11ec-aba1-c5f3c07a3dc7 -- ceph config set osd osd_recovery_max_active 32
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:259 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_single_start 8
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:34:24,871052303-05:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:34:24,873781339-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '106b29b4-4055-11ec-aba1-c5f3c07a3dc7', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_single_start', '8'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 106b29b4-4055-11ec-aba1-c5f3c07a3dc7 -- ceph config set osd osd_recovery_max_single_start 8
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:260 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_op_priority 63
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:34:29,080781104-05:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:34:29,083717000-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '106b29b4-4055-11ec-aba1-c5f3c07a3dc7', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_op_priority', '63'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 106b29b4-4055-11ec-aba1-c5f3c07a3dc7 -- ceph config set osd osd_recovery_op_priority 63
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:262 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config show-with-defaults osd.0
# ./benchmarks/bench-rados:263 - configure_ceph_pool() > grep 'osd_max_backfills\|osd_recovery'
# ./benchmarks/bench-rados:263 - configure_ceph_pool() > column -t -s ' '
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
osd_max_backfills                        32         mon
osd_recovery_cost                        20971520   default
osd_recovery_delay_start                 0.000000   default
osd_recovery_max_active                  32         mon
osd_recovery_max_active_hdd              3          default
osd_recovery_max_active_ssd              10         default
osd_recovery_max_chunk                   8388608    default
osd_recovery_max_omap_entries_per_chunk  8096       default
osd_recovery_max_single_start            8          mon
osd_recovery_op_priority                 3          default
osd_recovery_op_warn_multiple            16         default
osd_recovery_priority                    5          default
osd_recovery_retry_interval              30.000000  default
osd_recovery_sleep                       0.000000   default
osd_recovery_sleep_hdd                   0.100000   default
osd_recovery_sleep_hybrid                0.025000   default
osd_recovery_sleep_ssd                   0.000000   default
# ./benchmarks/bench-rados:265 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool create bench_rados 128 128 replicated --size 1 --pg-num-min 128
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:34:37,306119178-05:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:34:37,309135797-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '106b29b4-4055-11ec-aba1-c5f3c07a3dc7', '--', 'ceph', 'osd', 'pool', 'create', 'bench_rados', '128', '128', 'replicated', '--size', '1', '--pg-num-min', '128'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 106b29b4-4055-11ec-aba1-c5f3c07a3dc7 -- ceph osd pool create bench_rados 128 128 replicated --size 1 --pg-num-min 128
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
pool 'bench_rados' created
# ./benchmarks/bench-rados:267 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados min_size 1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:34:41,741725052-05:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:34:41,744701514-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '106b29b4-4055-11ec-aba1-c5f3c07a3dc7', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'min_size', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 106b29b4-4055-11ec-aba1-c5f3c07a3dc7 -- ceph osd pool set bench_rados min_size 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 min_size to 1
# ./benchmarks/bench-rados:268 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados pg_autoscale_mode off
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:34:46,127706667-05:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:34:46,130517246-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '106b29b4-4055-11ec-aba1-c5f3c07a3dc7', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'pg_autoscale_mode', 'off'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 106b29b4-4055-11ec-aba1-c5f3c07a3dc7 -- ceph osd pool set bench_rados pg_autoscale_mode off
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 pg_autoscale_mode to off
# ./benchmarks/bench-rados:269 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool application enable bench_rados benchmark
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:34:51,487339837-05:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:34:51,490048274-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '106b29b4-4055-11ec-aba1-c5f3c07a3dc7', '--', 'ceph', 'osd', 'pool', 'application', 'enable', 'bench_rados', 'benchmark'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 106b29b4-4055-11ec-aba1-c5f3c07a3dc7 -- ceph osd pool application enable bench_rados benchmark
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
enabled application 'benchmark' on pool 'bench_rados'
# ./benchmarks/bench-rados:270 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados noscrub 1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:34:55,832453573-05:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:34:55,835568377-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '106b29b4-4055-11ec-aba1-c5f3c07a3dc7', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'noscrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 106b29b4-4055-11ec-aba1-c5f3c07a3dc7 -- ceph osd pool set bench_rados noscrub 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 noscrub to 1
# ./benchmarks/bench-rados:271 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados nodeep-scrub 1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:35:00,352904063-05:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:35:00,355772261-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '106b29b4-4055-11ec-aba1-c5f3c07a3dc7', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'nodeep-scrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 106b29b4-4055-11ec-aba1-c5f3c07a3dc7 -- ceph osd pool set bench_rados nodeep-scrub 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 nodeep-scrub to 1
# ./benchmarks/bench-rados:272 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool ls detail
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:35:05,720326836-05:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:35:05,723270947-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '106b29b4-4055-11ec-aba1-c5f3c07a3dc7', '--', 'ceph', 'osd', 'pool', 'ls', 'detail'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 106b29b4-4055-11ec-aba1-c5f3c07a3dc7 -- ceph osd pool ls detail
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
pool 1 'device_health_metrics' replicated size 1 min_size 1 crush_rule 0 object_hash rjenkins pg_num 64 pgp_num 64 autoscale_mode on last_change 18 lfor 0/0/15 flags hashpspool stripe_width 0 pg_num_min 1 application mgr_devicehealth
pool 2 'bench_rados' replicated size 1 min_size 1 crush_rule 0 object_hash rjenkins pg_num 128 pgp_num 128 autoscale_mode off last_change 21 flags hashpspool,noscrub,nodeep-scrub stripe_width 0 pg_num_min 128 application benchmark

# ./benchmarks/bench-rados:273 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd df tree
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:35:09,948652731-05:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:35:09,951322675-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '106b29b4-4055-11ec-aba1-c5f3c07a3dc7', '--', 'ceph', 'osd', 'df', 'tree'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 106b29b4-4055-11ec-aba1-c5f3c07a3dc7 -- ceph osd df tree
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
ID  CLASS  WEIGHT   REWEIGHT  SIZE     RAW USE  DATA     OMAP  META     AVAIL    %USE  VAR   PGS  STATUS  TYPE NAME      
-1         0.09769         -  100 GiB  5.7 MiB  176 KiB   0 B  5.6 MiB  100 GiB  0.01  1.00    -          root default   
-3         0.09769         -  100 GiB  5.7 MiB  176 KiB   0 B  5.6 MiB  100 GiB  0.01  1.00    -              host node-1
 0    ssd  0.09769   1.00000  100 GiB  5.7 MiB  176 KiB   0 B  5.6 MiB  100 GiB  0.01  1.00  192      up          osd.0  
                       TOTAL  100 GiB  5.7 MiB  176 KiB   0 B  5.6 MiB  100 GiB  0.01                                    
MIN/MAX VAR: 1.00/1.00  STDDEV: 0


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:35:14,035507080-05:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:35:37,995230482-05:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:35:47,120675555-05:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:35:56,305259644-05:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:35:56,311734814-05:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:36:05,275222740-05:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:36:05,281896805-05:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:36:14,252478722-05:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:36:14,259606402-05:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:36:23,345658882-05:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:36:23,352341813-05:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:36:32,364030207-05:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:36:32,370597451-05:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:36:32,375383936-05:00] INFO: > The cluster is idle now.[0m


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:36:32,378783997-05:00] STAGE: Start benchmarking of mode write ...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:36:32,384555830-05:00] INFO: > Start system activity collection services[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:36:32,389554045-05:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:341 - rados_bench() > rm --force /mnt/sda8/ceph-research/scripts/output/sys_activity_16KB_write.dat.rados_bench_host.3
# ./benchmarks/bench-rados:342 - rados_bench() > sar_pid_rados_bench_host=164959
# ./benchmarks/bench-rados:342 - rados_bench() > S_TIME_FORMAT=ISO
# ./benchmarks/bench-rados:342 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/output/sys_activity_16KB_write.dat.rados_bench_host.3 2

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:36:32,395783802-05:00] INFO: >> for OSD host[0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:36:32,403985377-05:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.2 bash
10.10.2.2: b'26869\n'
[1] 00:36:32 [SUCCESS] ljishen@10.10.2.2
26869

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:36:32,593260483-05:00] INFO: > Run rados bench[0m
# ./benchmarks/bench-rados:370 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/output/rados_bench_16KB_write.log.3
## ./benchmarks/bench-rados:361 - rados_bench() > count_cpus 32-63,96-127
# ./benchmarks/bench-rados:361 - rados_bench() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 60 write --pool bench_rados -b 16384 -O 16384 --concurrent-ios 128 --show-time --write-object --write-omap --write-xattr --no-cleanup
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:36:32,612367170-05:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:36:32,615009492-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '106b29b4-4055-11ec-aba1-c5f3c07a3dc7', '--', 'rados', 'bench', '60', 'write', '--pool', 'bench_rados', '-b', '16384', '-O', '16384', '--concurrent-ios', '128', '--show-time', '--write-object', '--write-omap', '--write-xattr', '--no-cleanup'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 106b29b4-4055-11ec-aba1-c5f3c07a3dc7 -- rados bench 60 write --pool bench_rados -b 16384 -O 16384 --concurrent-ios 128 --show-time --write-object --write-omap --write-xattr --no-cleanup
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
hints = 1
2021-11-08T05:36:35.774581+0000 Maintaining 128 concurrent writes of 16384 bytes to objects of size 16384 for up to 60 seconds or 0 objects
2021-11-08T05:36:35.774590+0000 Object prefix: benchmark_data_node-0.bluefield2.ucsc-cmps10_7
2021-11-08T05:36:35.775729+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-11-08T05:36:35.775729+0000     0       0         0         0         0         0           -           0
2021-11-08T05:36:36.775825+0000     1     128     14143     14015   218.971   218.984  0.00907001  0.00907932
2021-11-08T05:36:37.775902+0000     2     128     27901     27773   216.962   214.969  0.00964912  0.00919405
2021-11-08T05:36:38.775984+0000     3     128     35389     35261   183.638       117   0.0222772   0.0108548
2021-11-08T05:36:39.776056+0000     4     128     41085     40957   159.977        89  0.00938614   0.0124845
2021-11-08T05:36:40.776120+0000     5     127     54699     54572   170.525   212.734  0.00926658   0.0117158
2021-11-08T05:36:41.776189+0000     6     128     63933     63805   166.147   144.266   0.0869005   0.0119165
2021-11-08T05:36:42.776267+0000     7     128     65469     65341    145.84        24   0.0834189   0.0136577
2021-11-08T05:36:43.776338+0000     8     128     67709     67581   131.985        35   0.0206209   0.0150852
2021-11-08T05:36:44.776410+0000     9     128     73725     73597   127.763        94   0.0206206   0.0156323
2021-11-08T05:36:45.776480+0000    10     128     79549     79421   124.086        91   0.0214697   0.0160961
2021-11-08T05:36:46.776570+0000    11     128     93105     92977    132.06   211.812   0.0114245   0.0151236
2021-11-08T05:36:47.776647+0000    12     128     98609     98481   128.221        86   0.0232102   0.0155801
2021-11-08T05:36:48.776718+0000    13     128    106829    106701   128.237   128.438   0.0092478   0.0155884
2021-11-08T05:36:49.776790+0000    14     127    119083    118956   132.754   191.484    0.010084   0.0150578
2021-11-08T05:36:50.776860+0000    15     128    125898    125770   131.001   106.469   0.0923754   0.0152257
2021-11-08T05:36:51.776933+0000    16     128    127370    127242   124.251        23   0.0651564   0.0160639
2021-11-08T05:36:52.777007+0000    17     127    129847    129720   119.219   38.7188    0.022634   0.0167649
2021-11-08T05:36:53.777077+0000    18     128    135370    135242   117.389   86.2812   0.0228353   0.0170268
2021-11-08T05:36:54.777149+0000    19     128    140874    140746   115.737        86   0.0226437   0.0172694
2021-11-08T05:36:55.777214+0000 min lat: 0.00511613 max lat: 0.0992706 avg lat: 0.0174993
2021-11-08T05:36:55.777214+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-11-08T05:36:55.777214+0000    20     128    146314    146186     114.2        85   0.0219697   0.0174993
2021-11-08T05:36:56.777300+0000    21     128    151306    151178   112.475        78   0.0288364   0.0177703
2021-11-08T05:36:57.777381+0000    22     128    157514    157386   111.772        97   0.0198009   0.0178819
2021-11-08T05:36:58.777451+0000    23     128    163594    163466   111.042        95   0.0222367   0.0179974
2021-11-08T05:36:59.777522+0000    24     128    171210    171082   111.373       119  0.00998684   0.0179529
2021-11-08T05:37:00.777591+0000    25     128    183493    183365   114.595   191.922   0.0101781    0.017447
2021-11-08T05:37:01.777665+0000    26     127    188155    188028   112.989   72.8594    0.086617   0.0176792
2021-11-08T05:37:02.777740+0000    27     128    189635    189507    109.66   23.1094   0.0828442   0.0182019
2021-11-08T05:37:03.777813+0000    28     128    193219    193091   107.744        56    0.021075   0.0185541
2021-11-08T05:37:04.777885+0000    29     128    198319    198191   106.776   79.6875   0.0237776   0.0187227
2021-11-08T05:37:05.777952+0000    30     128    203866    203738   106.106   86.6719   0.0231303   0.0188418
2021-11-08T05:37:06.778032+0000    31     128    209194    209066   105.368     83.25   0.0256963   0.0189705
2021-11-08T05:37:07.778108+0000    32     128    214442    214314   104.638        82   0.0234454   0.0191039
2021-11-08T05:37:08.778190+0000    33     128    217562    217434   102.944     48.75   0.0862232   0.0194105
2021-11-08T05:37:09.778261+0000    34     128    219050    218922     100.6     23.25   0.0931306   0.0198513
2021-11-08T05:37:10.778337+0000    35     127    221031    220904   98.6106   30.9688   0.0222183   0.0202708
2021-11-08T05:37:11.778423+0000    36     128    226650    226522   98.3096   87.7812    0.021962   0.0203355
2021-11-08T05:37:12.778510+0000    37     128    232282    232154   98.0307        88   0.0282584   0.0203958
2021-11-08T05:37:13.778583+0000    38     128    237530    237402   97.6087        82   0.0244076   0.0204824
2021-11-08T05:37:14.778658+0000    39     128    243034    242906   97.3109        86   0.0222812   0.0205475
2021-11-08T05:37:15.778721+0000 min lat: 0.00511613 max lat: 0.15285 avg lat: 0.0205218
2021-11-08T05:37:15.778721+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-11-08T05:37:15.778721+0000    40     128    249270    249142   97.3139   97.4375   0.0912267   0.0205218
2021-11-08T05:37:16.778800+0000    41     128    250743    250615   95.5017   23.0156   0.0860734   0.0209179
2021-11-08T05:37:17.778870+0000    42     128    253238    253110    94.156   38.9844   0.0223181   0.0212354
2021-11-08T05:37:18.778944+0000    43     128    259447    259319   94.2223   97.0156   0.0210113   0.0212211
2021-11-08T05:37:19.779019+0000    44     128    265207    265079   94.1262        90   0.0233362   0.0212403
2021-11-08T05:37:20.779085+0000    45     128    270774    270646   93.9674   86.9844   0.0228274   0.0212748
2021-11-08T05:37:21.779153+0000    46     128    276406    276278   93.8375        88   0.0219367   0.0213056
2021-11-08T05:37:22.779228+0000    47     128    279606    279478   92.9047        50   0.0912963   0.0215049
2021-11-08T05:37:23.779300+0000    48     128    281142    281014   91.4692        24    0.084046   0.0218457
2021-11-08T05:37:24.779374+0000    49     128    283639    283511   90.3986   39.0156    0.021494   0.0221177
2021-11-08T05:37:25.779459+0000    50     127    289631    289504   90.4633   93.6406   0.0199787   0.0221028
2021-11-08T05:37:26.779535+0000    51     128    295222    295094    90.402   87.3438   0.0245356    0.022118
2021-11-08T05:37:27.779611+0000    52     128    300535    300407   90.2599   83.0156   0.0257043   0.0221506
2021-11-08T05:37:28.779686+0000    53     128    305846    305718   90.1225   82.9844   0.0230924   0.0221859
2021-11-08T05:37:29.779757+0000    54     128    309623    309495   89.5463   59.0156   0.0823683    0.022322
2021-11-08T05:37:30.779830+0000    55     128    311094    310966   88.3361   22.9844   0.0880829   0.0226207
2021-11-08T05:37:31.779905+0000    56     128    312567    312439   87.1696   23.0156   0.0904477   0.0229296
2021-11-08T05:37:32.779981+0000    57     128    318390    318262   87.2364   90.9844    0.026743   0.0229195
2021-11-08T05:37:33.780053+0000    58     128    323959    323831   87.2325   87.0156   0.0254756   0.0229219
2021-11-08T05:37:34.780128+0000    59     128    329526    329398   87.2282   86.9844   0.0216654   0.0229206
2021-11-08T05:37:35.780194+0000 min lat: 0.00511613 max lat: 0.15285 avg lat: 0.0229208
2021-11-08T05:37:35.780194+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-11-08T05:37:35.780194+0000    60      65    335095    335030    87.241        88   0.0232822   0.0229208
2021-11-08T05:37:36.780325+0000 Total time run:         60.0146
Total writes made:      335095
Write size:             16384
Object size:            16384
Bandwidth (MB/sec):     87.2431
Stddev Bandwidth:       49.6056
Max bandwidth (MB/sec): 218.984
Min bandwidth (MB/sec): 22.9844
Average IOPS:           5583
Stddev IOPS:            3174.76
Max IOPS:               14015
Min IOPS:               1471
Average Latency(s):     0.0229206
Stddev Latency(s):      0.0184447
Max latency(s):         0.15285
Min latency(s):         0.00511613

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:37:37,457639473-05:00] INFO: > Stop system activity collection services[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:37:37,462746693-05:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:391 - rados_bench() > kill -INT 164959

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:37:37,467837712-05:00] INFO: >> for OSD host[0m
# ./benchmarks/bench-rados:396 - rados_bench() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.2 kill -INT 26869
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:37:37,475109905-05:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.2 kill -INT 26869
[1] 00:37:37 [SUCCESS] ljishen@10.10.2.2

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:37:37,656085832-05:00] INFO: >>> Copy system activity file from OSD host[0m
## ./benchmarks/bench-rados:403 - rados_bench() > basename -- /tmp/bench-rados/sys_activity_16KB_write.dat.osd_host.3
# ./benchmarks/bench-rados:403 - rados_bench() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.2:/tmp/bench-rados/sys_activity_16KB_write.dat.osd_host.3 /mnt/sda8/ceph-research/scripts/output/sys_activity_16KB_write.dat.osd_host.3


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:37:37,826421136-05:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:38:01,742288358-05:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 335.10k objects, 5.1 GiB
    usage:   16 GiB used, 84 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:38:01,748952894-05:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:38:10,918343710-05:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 335.10k objects, 5.1 GiB
    usage:   16 GiB used, 84 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:38:10,925687348-05:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:38:19,978159425-05:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 335.10k objects, 5.1 GiB
    usage:   16 GiB used, 84 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:38:19,984745895-05:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:38:29,169690938-05:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 335.10k objects, 5.1 GiB
    usage:   16 GiB used, 84 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:38:29,176397324-05:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:38:38,163447030-05:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 335.10k objects, 5.1 GiB
    usage:   16 GiB used, 84 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:38:38,170335690-05:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:38:38,175285383-05:00] INFO: > The cluster is idle now.[0m


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:38:38,178637413-05:00] STAGE: Start benchmarking of mode seq ...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:38:38,184688543-05:00] INFO: > Start system activity collection services[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:38:38,189526085-05:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:341 - rados_bench() > rm --force /mnt/sda8/ceph-research/scripts/output/sys_activity_16KB_seq.dat.rados_bench_host.3
# ./benchmarks/bench-rados:342 - rados_bench() > sar_pid_rados_bench_host=166301
# ./benchmarks/bench-rados:342 - rados_bench() > S_TIME_FORMAT=ISO
# ./benchmarks/bench-rados:342 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/output/sys_activity_16KB_seq.dat.rados_bench_host.3 2

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:38:38,195896467-05:00] INFO: >> for OSD host[0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:38:38,204384492-05:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.2 bash
10.10.2.2: b'26965\n'
[1] 00:38:38 [SUCCESS] ljishen@10.10.2.2
26965

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:38:38,388843047-05:00] INFO: > Run rados bench[0m
# ./benchmarks/bench-rados:379 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/output/rados_bench_16KB_seq.log.3
## ./benchmarks/bench-rados:376 - rados_bench() > count_cpus 32-63,96-127
# ./benchmarks/bench-rados:376 - rados_bench() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 128 --show-time
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:38:38,407274492-05:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:38:38,409946540-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '106b29b4-4055-11ec-aba1-c5f3c07a3dc7', '--', 'rados', 'bench', '99999999', 'seq', '--pool', 'bench_rados', '--concurrent-ios', '128', '--show-time'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 106b29b4-4055-11ec-aba1-c5f3c07a3dc7 -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 128 --show-time
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
hints = 1
2021-11-08T05:38:41.641543+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-11-08T05:38:41.641543+0000     0       0         0         0         0         0           -           0
2021-11-08T05:38:42.641657+0000     1     128     32213     32085   501.232   501.328  0.00394094  0.00397497
2021-11-08T05:38:43.641744+0000     2     127     64876     64749   505.781   510.375  0.00407609  0.00394366
2021-11-08T05:38:44.641826+0000     3     127     96856     96729   503.736   499.688  0.00415258  0.00396081
2021-11-08T05:38:45.641893+0000     4     127    129328    129201   504.637   507.375  0.00354743  0.00395384
2021-11-08T05:38:46.641966+0000     5     128    162311    162183   506.771   515.344  0.00274975  0.00393833
2021-11-08T05:38:47.642031+0000     6     128    195441    195313    508.58   517.656  0.00383339  0.00392411
2021-11-08T05:38:48.642108+0000     7     128    228483    228355   509.674   516.281  0.00412206  0.00391623
2021-11-08T05:38:49.642185+0000     8     127    260688    260561   508.862   503.219  0.00414151  0.00392267
2021-11-08T05:38:50.642263+0000     9     128    293280    293152   508.899   509.234   0.0041443  0.00392204
2021-11-08T05:38:51.642342+0000    10     127    326499    326372   509.912   519.062   0.0040426  0.00391435
2021-11-08T05:38:52.642443+0000 Total time run:       10.2636
Total reads made:     335095
Read size:            16384
Object size:          16384
Bandwidth (MB/sec):   510.14
Average IOPS:         32648
Stddev IOPS:          449.465
Max IOPS:             33220
Min IOPS:             31980
Average Latency(s):   0.00391303
Max latency(s):       0.0262183
Min latency(s):       0.000308692

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:38:53,294213345-05:00] INFO: > Stop system activity collection services[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:38:53,299280711-05:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:391 - rados_bench() > kill -INT 166301

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:38:53,304374265-05:00] INFO: >> for OSD host[0m
# ./benchmarks/bench-rados:396 - rados_bench() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.2 kill -INT 26965
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:38:53,312197085-05:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.2 kill -INT 26965
[1] 00:38:53 [SUCCESS] ljishen@10.10.2.2

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:38:53,495962484-05:00] INFO: >>> Copy system activity file from OSD host[0m
## ./benchmarks/bench-rados:403 - rados_bench() > basename -- /tmp/bench-rados/sys_activity_16KB_seq.dat.osd_host.3
# ./benchmarks/bench-rados:403 - rados_bench() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.2:/tmp/bench-rados/sys_activity_16KB_seq.dat.osd_host.3 /mnt/sda8/ceph-research/scripts/output/sys_activity_16KB_seq.dat.osd_host.3


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:38:53,646683373-05:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:39:17,602524578-05:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 335.10k objects, 5.1 GiB
    usage:   16 GiB used, 84 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:39:17,609398419-05:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:39:26,780249089-05:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 335.10k objects, 5.1 GiB
    usage:   16 GiB used, 84 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:39:26,787151344-05:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:39:35,805354915-05:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 335.10k objects, 5.1 GiB
    usage:   16 GiB used, 84 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:39:35,812246209-05:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:39:44,923372649-05:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 335.10k objects, 5.1 GiB
    usage:   16 GiB used, 84 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:39:44,930096036-05:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:39:53,978122148-05:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 335.10k objects, 5.1 GiB
    usage:   16 GiB used, 84 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:39:53,984705782-05:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:39:53,990058545-05:00] INFO: > The cluster is idle now.[0m


[1;7;39;49m[2021-11-08T00:39:53,993218984-05:00][RUNNING][ROUND 1/3/21] object_size=64KB[0m


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:39:53,996220884-05:00] STAGE: Launch a Ceph cluster (CEPH_HOST: 10.10.2.3, OSD_HOST: 10.10.2.2) ...[0m
# ./benchmarks/bench-rados:183 - launch_ceph_cluster() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.3 bash -euo pipefail
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:39:54,004696266-05:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.3 bash -euo pipefail
10.10.2.3: b'\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T00:39:54,326331060-05:00] INFO: > Remove existing clusters\x1b[0m\n'
10.10.2.3: b'## bash:17 -  > basename -- /var/lib/ceph/106b29b4-4055-11ec-aba1-c5f3c07a3dc7\n'
10.10.2.3: b'# bash:17 -  > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm --host 10.10.2.3 --host 10.10.2.2 rm-cluster --force --fsid 106b29b4-4055-11ec-aba1-c5f3c07a3dc7\n'
10.10.2.3: b"\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T00:39:54,336496899-05:00] DEBUG: HOST_PARAMS: ['--host', '10.10.2.3', '--host', '10.10.2.2']\x1b[0m\n"
10.10.2.3: b"\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T00:39:54,339612057-05:00] DEBUG: CEPHADM_PARAMS: ['rm-cluster', '--force', '--fsid', '106b29b4-4055-11ec-aba1-c5f3c07a3dc7']\x1b[0m\n"
10.10.2.3: b"# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:63 -  > /tmp/bench-rados/ceph-research/scripts/parallel-ssh --host 10.10.2.3 --host 10.10.2.2 'sudo python3 -u - rm-cluster --force --fsid 106b29b4-4055-11ec-aba1-c5f3c07a3dc7'\n"
10.10.2.3: b'\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T00:39:54,347928797-05:00] DEBUG: command from stdin\x1b[0m\n'
10.10.2.3: b"# /tmp/bench-rados/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host 10.10.2.3 --host 10.10.2.2 'sudo python3 -u - rm-cluster --force --fsid 106b29b4-4055-11ec-aba1-c5f3c07a3dc7'\n"
10.10.2.3: b'[1] 00:40:00 [SUCCESS] 10.10.2.3\n[2] 00:40:04 [SUCCESS] 10.10.2.2\n'
10.10.2.3: b'\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T00:40:04,441297907-05:00] INFO: > Deploy a new cluster\x1b[0m\n'
10.10.2.3: b'# bash:24 -  > /tmp/bench-rados/ceph-research/scripts/ceph-deploy -m 10.10.2.3 -o node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/10.10.2.2:/dev/nvme0n1\n'
10.10.2.3: b'\n\n\x1b[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T00:40:04,453218527-05:00] STAGE: Verify passwordless SSH to hosts, passwordless sudo via ljishen, and the existence of devices on hosts...\x1b[0m\n'
10.10.2.3: b'\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T00:40:04,457854784-05:00] INFO: > Check the host for the monitor\x1b[0m\n'
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:181 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@10.10.2.3 sudo --non-interactive --validate\n'
10.10.2.3: b'\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T00:40:04,610325434-05:00] INFO: > Check hosts for OSDs\x1b[0m\n'
10.10.2.3: b"\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T00:40:04,615210040-05:00] INFO: >> Check host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'\x1b[0m\n"
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:200 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us sudo --non-interactive --validate\n'
10.10.2.3: b"\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T00:40:04,761911556-05:00] INFO: >>> Check device '/dev/nvme0n1'\x1b[0m\n"
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:215 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us test -b /dev/nvme0n1\n'
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:226 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us lsblk --nodeps --noheadings --output TYPE /dev/nvme0n1\n'
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:227 - verify_parameters() > grep --word-regexp disk\n'
10.10.2.3: b'\n\n\x1b[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T00:40:05,046715523-05:00] STAGE: Reset devices on hosts...\x1b[0m\n'
10.10.2.3: b"\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T00:40:05,051797672-05:00] INFO: > Resetting device '/dev/nvme0n1' on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'\x1b[0m\n"
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:288 - reset_host_devices() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us sudo sh\n'
10.10.2.3: b'  Removing ceph--509955b5--1e47--4adf--acfe--e10a99fd354d-osd--block--79eb690a--a5c9--4dbb--b00e--7b5f782054a5 (253:0)\n'
10.10.2.3: b'  Archiving volume group "ceph-509955b5-1e47-4adf-acfe-e10a99fd354d" metadata (seqno 5).\n  Releasing logical volume "osd-block-79eb690a-a5c9-4dbb-b00e-7b5f782054a5"\n'
10.10.2.3: b'  Creating volume group backup "/etc/lvm/backup/ceph-509955b5-1e47-4adf-acfe-e10a99fd354d" (seqno 6).\n'
10.10.2.3: b'  Logical volume "osd-block-79eb690a-a5c9-4dbb-b00e-7b5f782054a5" successfully removed\n'
10.10.2.3: b'  Removing physical volume "/dev/nvme0n1" from volume group "ceph-509955b5-1e47-4adf-acfe-e10a99fd354d"\n'
10.10.2.3: b'  Volume group "ceph-509955b5-1e47-4adf-acfe-e10a99fd354d" successfully removed\n'
10.10.2.3: b'\n\n\x1b[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T00:40:05,363718030-05:00] STAGE: Bootstrap a new cluster...\x1b[0m\n'
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:317 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.3 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults\n'
10.10.2.3: b'\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T00:40:05,373328000-05:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.3: b"\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T00:40:05,376882527-05:00] DEBUG: CEPHADM_PARAMS: ['bootstrap', '--config', '/tmp/bench-rados/ceph-research/scripts/ceph.conf', '--mon-ip', '10.10.2.3', '--output-dir', '/tmp/bench-rados/deployment_data_root/etc/ceph', '--ssh-user', 'ljishen', '--skip-dashboard', '--no-minimize-config', '--allow-overwrite', '--allow-fqdn-hostname', '--orphan-initial-daemons', '--skip-monitoring-stack', '--single-host-defaults']\x1b[0m\n"
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.3 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults\n'
10.10.2.3: b'Verifying podman|docker is present...\n'
10.10.2.3: b'Verifying lvm2 is present...\nVerifying time synchronization is in place...\n'
10.10.2.3: b'Unit ntp.service is enabled and running\n'
10.10.2.3: b'Repeating the final host check...\n'
10.10.2.3: b'docker (/usr/bin/docker) is present\n'
10.10.2.3: b'systemctl is present\nlvcreate is present\n'
10.10.2.3: b'Unit ntp.service is enabled and running\nHost looks OK\n'
10.10.2.3: b'Cluster fsid: 539e2faa-4056-11ec-aba1-c5f3c07a3dc7\n'
10.10.2.3: b'Verifying IP 10.10.2.3 port 3300 ...\n'
10.10.2.3: b'Verifying IP 10.10.2.3 port 6789 ...\n'
10.10.2.3: b'Mon IP `10.10.2.3` is in CIDR network `10.10.2.0/24`\n'
10.10.2.3: b'- internal network (--cluster-network) has not been provided, OSD replication will default to the public_network\n'
10.10.2.3: b'Adjusting default settings to suit single-host cluster...\nPulling container image quay.io/ceph/ceph:v16...\n'
10.10.2.3: b'Ceph version: ceph version 16.2.6 (ee28fb57e47e9f88813e24bbf4c14496ca299d31) pacific (stable)\n'
10.10.2.3: b'Extracting ceph user uid/gid from container image...\n'
10.10.2.3: b'Creating initial keys...\n'
10.10.2.3: b'Creating initial monmap...\n'
10.10.2.3: b'Creating mon...\n'
10.10.2.3: b'Waiting for mon to start...\n'
10.10.2.3: b'Waiting for mon...\n'
10.10.2.3: b'mon is available\nSetting mon public_network to 10.10.2.0/24\n'
10.10.2.3: b'Wrote config to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf\n'
10.10.2.3: b'Wrote keyring to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring\n'
10.10.2.3: b'Creating mgr...\n'
10.10.2.3: b'Verifying port 9283 ...\n'
10.10.2.3: b'Waiting for mgr to start...\nWaiting for mgr...\n'
10.10.2.3: b'mgr not available, waiting (1/15)...\n'
10.10.2.3: b'mgr not available, waiting (2/15)...\n'
10.10.2.3: b'mgr is available\n'
10.10.2.3: b'Enabling cephadm module...\n'
10.10.2.3: b'Waiting for the mgr to restart...\n'
10.10.2.3: b'Waiting for mgr epoch 5...\n'
10.10.2.3: b'mgr epoch 5 is available\n'
10.10.2.3: b'Setting orchestrator backend to cephadm...\n'
10.10.2.3: b'Generating ssh key...\n'
10.10.2.3: b'Wrote public SSH key to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub\n'
10.10.2.3: b'Adding key to ljishen@localhost authorized_keys...\n'
10.10.2.3: b'Adding host node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us...\n'
10.10.2.3: b'Deploying unmanaged mon service...\n'
10.10.2.3: b'Deploying unmanaged mgr service...\n'
10.10.2.3: b'You can access the Ceph CLI with:\n\n\tsudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --fsid 539e2faa-4056-11ec-aba1-c5f3c07a3dc7 -c /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf -k /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring\n\n'
10.10.2.3: b'Please consider enabling telemetry to help improve Ceph:\n\n\tceph telemetry on\n\nFor more information see:\n\n\thttps://docs.ceph.com/docs/pacific/mgr/telemetry/\n\n'
10.10.2.3: b'Bootstrap complete.\n'
10.10.2.3: b'\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T00:41:14,805789407-05:00] INFO: Sleep for 20 seconds\x1b[0m\n'
10.10.2.3: b'\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T00:41:34,813470944-05:00] INFO: > Set the default number of monitor daemons to 1\x1b[0m\n# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:337 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1\n'
10.10.2.3: b'\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T00:41:34,823746340-05:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.3: b"\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T00:41:34,827567580-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mon', '1']\x1b[0m\n"
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1\n'
10.10.2.3: b'Inferring fsid 539e2faa-4056-11ec-aba1-c5f3c07a3dc7\n'
10.10.2.3: b'Inferring config /var/lib/ceph/539e2faa-4056-11ec-aba1-c5f3c07a3dc7/mon.node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.3: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.3: b'Scheduled mon update...\n'
10.10.2.3: b'\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T00:41:44,105467616-05:00] INFO: > Set the default number of manager daemons to 1\x1b[0m\n'
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:342 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1\n'
10.10.2.3: b'\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T00:41:44,115889470-05:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.3: b"\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T00:41:44,119509760-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mgr', '1']\x1b[0m\n"
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1\n'
10.10.2.3: b'Inferring fsid 539e2faa-4056-11ec-aba1-c5f3c07a3dc7\n'
10.10.2.3: b'Inferring config /var/lib/ceph/539e2faa-4056-11ec-aba1-c5f3c07a3dc7/mon.node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.3: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.3: b'Scheduled mgr update...\n'
10.10.2.3: b'\n\n\x1b[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T00:41:54,025905794-05:00] STAGE: Add hosts to the cluster...\x1b[0m\n'
10.10.2.3: b"\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T00:41:54,032144153-05:00] INFO: > Adding host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'\x1b[0m\n"
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:361 - add_hosts() > ssh-copy-id -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -f -i /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\n'
10.10.2.3: b'/usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: "/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub"\n'
10.10.2.3: b'\nNumber of key(s) added: 1\n\nNow try logging into the machine, with:   "ssh -o \'GlobalKnownHostsFile=/dev/null\' -o \'LogLevel=ERROR\' -o \'PasswordAuthentication=no\' -o \'StrictHostKeyChecking=no\' -o \'UserKnownHostsFile=/dev/null\' \'ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\'"\nand check to make sure that only the key(s) you wanted were added.\n\n'
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:367 - add_hosts() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us 10.10.2.2\n'
10.10.2.3: b'\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T00:41:54,244710045-05:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.3: b"\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T00:41:54,248403494-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'host', 'add', 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us', '10.10.2.2']\x1b[0m\n"
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us 10.10.2.2\n'
10.10.2.3: b'Inferring fsid 539e2faa-4056-11ec-aba1-c5f3c07a3dc7\n'
10.10.2.3: b'Inferring config /var/lib/ceph/539e2faa-4056-11ec-aba1-c5f3c07a3dc7/mon.node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.3: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.3: b"Added host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us' with addr '10.10.2.2'\n"
10.10.2.3: b'\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T00:42:04,585394579-05:00] INFO: Sleep for 20 seconds\x1b[0m\n'
10.10.2.3: b'\n\n\x1b[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T00:42:24,590327303-05:00] STAGE: Add OSDs to the cluster...\x1b[0m\n'
10.10.2.3: b"\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T00:42:24,596715105-05:00] INFO: > Adding devices ['/dev/nvme0n1'] on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us' as OSDs\x1b[0m\n"
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:388 - add_host_devices_as_osds() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1\n'
10.10.2.3: b'\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T00:42:24,606673604-05:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.3: b"\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T00:42:24,610276272-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'daemon', 'add', 'osd', 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1']\x1b[0m\n"
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1\n'
10.10.2.3: b'Inferring fsid 539e2faa-4056-11ec-aba1-c5f3c07a3dc7\n'
10.10.2.3: b'Inferring config /var/lib/ceph/539e2faa-4056-11ec-aba1-c5f3c07a3dc7/mon.node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.3: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.3: b"Created osd(s) 0 on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'\n"
10.10.2.3: b'\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T00:42:50,740846377-05:00] INFO: > Sleep for 20 seconds\x1b[0m\n'
10.10.2.3: b'\n\n\x1b[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T00:43:10,746869746-05:00] STAGE: Show Cluster Status\x1b[0m\n'
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:408 - show_status() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status\n'
10.10.2.3: b'\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T00:43:10,757187694-05:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.3: b"\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T00:43:10,760845205-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', '--status']\x1b[0m\n"
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status\n'
10.10.2.3: b'Inferring fsid 539e2faa-4056-11ec-aba1-c5f3c07a3dc7\n'
10.10.2.3: b'Inferring config /var/lib/ceph/539e2faa-4056-11ec-aba1-c5f3c07a3dc7/mon.node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.3: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.3: b'  cluster:\n    id:     539e2faa-4056-11ec-aba1-c5f3c07a3dc7\n    health: HEALTH_WARN\n            1 pool(s) have no replicas configured\n \n  services:\n    mon: 1 daemons, quorum node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us (age 2m)\n    mgr: node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us.jdduqo(active, since 2m)\n    osd: 1 osds: 1 up (since 19s), 1 in (since 42s)\n \n  data:\n    pools:   1 pools, 1 pgs\n    objects: 0 objects, 0 B\n    usage:   4.7 MiB used, 100 GiB / 100 GiB avail\n    pgs:     1 active+clean\n \n'
[1] 00:43:20 [SUCCESS] ljishen@10.10.2.3

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:39:54,326331060-05:00] INFO: > Remove existing clusters[0m
## bash:17 -  > basename -- /var/lib/ceph/106b29b4-4055-11ec-aba1-c5f3c07a3dc7
# bash:17 -  > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm --host 10.10.2.3 --host 10.10.2.2 rm-cluster --force --fsid 106b29b4-4055-11ec-aba1-c5f3c07a3dc7
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:39:54,336496899-05:00] DEBUG: HOST_PARAMS: ['--host', '10.10.2.3', '--host', '10.10.2.2'][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:39:54,339612057-05:00] DEBUG: CEPHADM_PARAMS: ['rm-cluster', '--force', '--fsid', '106b29b4-4055-11ec-aba1-c5f3c07a3dc7'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:63 -  > /tmp/bench-rados/ceph-research/scripts/parallel-ssh --host 10.10.2.3 --host 10.10.2.2 'sudo python3 -u - rm-cluster --force --fsid 106b29b4-4055-11ec-aba1-c5f3c07a3dc7'
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:39:54,347928797-05:00] DEBUG: command from stdin[0m
# /tmp/bench-rados/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host 10.10.2.3 --host 10.10.2.2 'sudo python3 -u - rm-cluster --force --fsid 106b29b4-4055-11ec-aba1-c5f3c07a3dc7'
[1] 00:40:00 [SUCCESS] 10.10.2.3
[2] 00:40:04 [SUCCESS] 10.10.2.2

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:40:04,441297907-05:00] INFO: > Deploy a new cluster[0m
# bash:24 -  > /tmp/bench-rados/ceph-research/scripts/ceph-deploy -m 10.10.2.3 -o node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/10.10.2.2:/dev/nvme0n1


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:40:04,453218527-05:00] STAGE: Verify passwordless SSH to hosts, passwordless sudo via ljishen, and the existence of devices on hosts...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:40:04,457854784-05:00] INFO: > Check the host for the monitor[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:181 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@10.10.2.3 sudo --non-interactive --validate

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:40:04,610325434-05:00] INFO: > Check hosts for OSDs[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:40:04,615210040-05:00] INFO: >> Check host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:200 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us sudo --non-interactive --validate

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:40:04,761911556-05:00] INFO: >>> Check device '/dev/nvme0n1'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:215 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us test -b /dev/nvme0n1
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:226 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us lsblk --nodeps --noheadings --output TYPE /dev/nvme0n1
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:227 - verify_parameters() > grep --word-regexp disk


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:40:05,046715523-05:00] STAGE: Reset devices on hosts...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:40:05,051797672-05:00] INFO: > Resetting device '/dev/nvme0n1' on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:288 - reset_host_devices() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us sudo sh
  Removing ceph--509955b5--1e47--4adf--acfe--e10a99fd354d-osd--block--79eb690a--a5c9--4dbb--b00e--7b5f782054a5 (253:0)
  Archiving volume group "ceph-509955b5-1e47-4adf-acfe-e10a99fd354d" metadata (seqno 5).
  Releasing logical volume "osd-block-79eb690a-a5c9-4dbb-b00e-7b5f782054a5"
  Creating volume group backup "/etc/lvm/backup/ceph-509955b5-1e47-4adf-acfe-e10a99fd354d" (seqno 6).
  Logical volume "osd-block-79eb690a-a5c9-4dbb-b00e-7b5f782054a5" successfully removed
  Removing physical volume "/dev/nvme0n1" from volume group "ceph-509955b5-1e47-4adf-acfe-e10a99fd354d"
  Volume group "ceph-509955b5-1e47-4adf-acfe-e10a99fd354d" successfully removed


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:40:05,363718030-05:00] STAGE: Bootstrap a new cluster...[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:317 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.3 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:40:05,373328000-05:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:40:05,376882527-05:00] DEBUG: CEPHADM_PARAMS: ['bootstrap', '--config', '/tmp/bench-rados/ceph-research/scripts/ceph.conf', '--mon-ip', '10.10.2.3', '--output-dir', '/tmp/bench-rados/deployment_data_root/etc/ceph', '--ssh-user', 'ljishen', '--skip-dashboard', '--no-minimize-config', '--allow-overwrite', '--allow-fqdn-hostname', '--orphan-initial-daemons', '--skip-monitoring-stack', '--single-host-defaults'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.3 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults
Verifying podman|docker is present...
Verifying lvm2 is present...
Verifying time synchronization is in place...
Unit ntp.service is enabled and running
Repeating the final host check...
docker (/usr/bin/docker) is present
systemctl is present
lvcreate is present
Unit ntp.service is enabled and running
Host looks OK
Cluster fsid: 539e2faa-4056-11ec-aba1-c5f3c07a3dc7
Verifying IP 10.10.2.3 port 3300 ...
Verifying IP 10.10.2.3 port 6789 ...
Mon IP `10.10.2.3` is in CIDR network `10.10.2.0/24`
- internal network (--cluster-network) has not been provided, OSD replication will default to the public_network
Adjusting default settings to suit single-host cluster...
Pulling container image quay.io/ceph/ceph:v16...
Ceph version: ceph version 16.2.6 (ee28fb57e47e9f88813e24bbf4c14496ca299d31) pacific (stable)
Extracting ceph user uid/gid from container image...
Creating initial keys...
Creating initial monmap...
Creating mon...
Waiting for mon to start...
Waiting for mon...
mon is available
Setting mon public_network to 10.10.2.0/24
Wrote config to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf
Wrote keyring to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring
Creating mgr...
Verifying port 9283 ...
Waiting for mgr to start...
Waiting for mgr...
mgr not available, waiting (1/15)...
mgr not available, waiting (2/15)...
mgr is available
Enabling cephadm module...
Waiting for the mgr to restart...
Waiting for mgr epoch 5...
mgr epoch 5 is available
Setting orchestrator backend to cephadm...
Generating ssh key...
Wrote public SSH key to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub
Adding key to ljishen@localhost authorized_keys...
Adding host node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us...
Deploying unmanaged mon service...
Deploying unmanaged mgr service...
You can access the Ceph CLI with:

	sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --fsid 539e2faa-4056-11ec-aba1-c5f3c07a3dc7 -c /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf -k /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring

Please consider enabling telemetry to help improve Ceph:

	ceph telemetry on

For more information see:

	https://docs.ceph.com/docs/pacific/mgr/telemetry/

Bootstrap complete.

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:41:14,805789407-05:00] INFO: Sleep for 20 seconds[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:41:34,813470944-05:00] INFO: > Set the default number of monitor daemons to 1[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:337 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:41:34,823746340-05:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:41:34,827567580-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mon', '1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1
Inferring fsid 539e2faa-4056-11ec-aba1-c5f3c07a3dc7
Inferring config /var/lib/ceph/539e2faa-4056-11ec-aba1-c5f3c07a3dc7/mon.node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Scheduled mon update...

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:41:44,105467616-05:00] INFO: > Set the default number of manager daemons to 1[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:342 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:41:44,115889470-05:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:41:44,119509760-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mgr', '1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1
Inferring fsid 539e2faa-4056-11ec-aba1-c5f3c07a3dc7
Inferring config /var/lib/ceph/539e2faa-4056-11ec-aba1-c5f3c07a3dc7/mon.node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Scheduled mgr update...


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:41:54,025905794-05:00] STAGE: Add hosts to the cluster...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:41:54,032144153-05:00] INFO: > Adding host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:361 - add_hosts() > ssh-copy-id -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -f -i /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us
/usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: "/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub"

Number of key(s) added: 1

Now try logging into the machine, with:   "ssh -o 'GlobalKnownHostsFile=/dev/null' -o 'LogLevel=ERROR' -o 'PasswordAuthentication=no' -o 'StrictHostKeyChecking=no' -o 'UserKnownHostsFile=/dev/null' 'ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'"
and check to make sure that only the key(s) you wanted were added.

# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:367 - add_hosts() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us 10.10.2.2
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:41:54,244710045-05:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:41:54,248403494-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'host', 'add', 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us', '10.10.2.2'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us 10.10.2.2
Inferring fsid 539e2faa-4056-11ec-aba1-c5f3c07a3dc7
Inferring config /var/lib/ceph/539e2faa-4056-11ec-aba1-c5f3c07a3dc7/mon.node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Added host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us' with addr '10.10.2.2'

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:42:04,585394579-05:00] INFO: Sleep for 20 seconds[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:42:24,590327303-05:00] STAGE: Add OSDs to the cluster...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:42:24,596715105-05:00] INFO: > Adding devices ['/dev/nvme0n1'] on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us' as OSDs[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:388 - add_host_devices_as_osds() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:42:24,606673604-05:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:42:24,610276272-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'daemon', 'add', 'osd', 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1
Inferring fsid 539e2faa-4056-11ec-aba1-c5f3c07a3dc7
Inferring config /var/lib/ceph/539e2faa-4056-11ec-aba1-c5f3c07a3dc7/mon.node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Created osd(s) 0 on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:42:50,740846377-05:00] INFO: > Sleep for 20 seconds[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:43:10,746869746-05:00] STAGE: Show Cluster Status[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:408 - show_status() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:43:10,757187694-05:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:43:10,760845205-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', '--status'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status
Inferring fsid 539e2faa-4056-11ec-aba1-c5f3c07a3dc7
Inferring config /var/lib/ceph/539e2faa-4056-11ec-aba1-c5f3c07a3dc7/mon.node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
  cluster:
    id:     539e2faa-4056-11ec-aba1-c5f3c07a3dc7
    health: HEALTH_WARN
            1 pool(s) have no replicas configured
 
  services:
    mon: 1 daemons, quorum node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us (age 2m)
    mgr: node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us.jdduqo(active, since 2m)
    osd: 1 osds: 1 up (since 19s), 1 in (since 42s)
 
  data:
    pools:   1 pools, 1 pgs
    objects: 0 objects, 0 B
    usage:   4.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     1 active+clean
 

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:43:20,549148157-05:00] INFO: > Copy configuration files to local[0m
# ./benchmarks/bench-rados:220 - launch_ceph_cluster() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.3 sudo chown --changes --recursive ljishen /tmp/bench-rados/deployment_data_root/etc/ceph
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:43:20,556490701-05:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.3 sudo chown --changes --recursive ljishen /tmp/bench-rados/deployment_data_root/etc/ceph
[1] 00:43:20 [SUCCESS] ljishen@10.10.2.3
# ./benchmarks/bench-rados:223 - launch_ceph_cluster() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.3:/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf
# ./benchmarks/bench-rados:226 - launch_ceph_cluster() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.3:/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:43:21,024077608-05:00] STAGE: Update local ceph.conf as a client...[0m


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:43:21,027257504-05:00] STAGE: Configure Ceph pool...[0m
# ./benchmarks/bench-rados:257 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_max_backfills 32
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:43:21,047811668-05:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:43:21,050357068-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '539e2faa-4056-11ec-aba1-c5f3c07a3dc7', '--', 'ceph', 'config', 'set', 'osd', 'osd_max_backfills', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 539e2faa-4056-11ec-aba1-c5f3c07a3dc7 -- ceph config set osd osd_max_backfills 32
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:258 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_active 32
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:43:25,034629981-05:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:43:25,037478762-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '539e2faa-4056-11ec-aba1-c5f3c07a3dc7', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_active', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 539e2faa-4056-11ec-aba1-c5f3c07a3dc7 -- ceph config set osd osd_recovery_max_active 32
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:259 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_single_start 8
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:43:29,178568379-05:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:43:29,181292365-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '539e2faa-4056-11ec-aba1-c5f3c07a3dc7', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_single_start', '8'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 539e2faa-4056-11ec-aba1-c5f3c07a3dc7 -- ceph config set osd osd_recovery_max_single_start 8
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:260 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_op_priority 63
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:43:33,271094914-05:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:43:33,273755610-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '539e2faa-4056-11ec-aba1-c5f3c07a3dc7', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_op_priority', '63'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 539e2faa-4056-11ec-aba1-c5f3c07a3dc7 -- ceph config set osd osd_recovery_op_priority 63
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:262 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config show-with-defaults osd.0
# ./benchmarks/bench-rados:263 - configure_ceph_pool() > grep 'osd_max_backfills\|osd_recovery'
# ./benchmarks/bench-rados:263 - configure_ceph_pool() > column -t -s ' '
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
osd_max_backfills                        32         mon
osd_recovery_cost                        20971520   default
osd_recovery_delay_start                 0.000000   default
osd_recovery_max_active                  32         mon
osd_recovery_max_active_hdd              3          default
osd_recovery_max_active_ssd              10         default
osd_recovery_max_chunk                   8388608    default
osd_recovery_max_omap_entries_per_chunk  8096       default
osd_recovery_max_single_start            8          mon
osd_recovery_op_priority                 3          default
osd_recovery_op_warn_multiple            16         default
osd_recovery_priority                    5          default
osd_recovery_retry_interval              30.000000  default
osd_recovery_sleep                       0.000000   default
osd_recovery_sleep_hdd                   0.100000   default
osd_recovery_sleep_hybrid                0.025000   default
osd_recovery_sleep_ssd                   0.000000   default
# ./benchmarks/bench-rados:265 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool create bench_rados 128 128 replicated --size 1 --pg-num-min 128
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:43:41,321887322-05:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:43:41,324702790-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '539e2faa-4056-11ec-aba1-c5f3c07a3dc7', '--', 'ceph', 'osd', 'pool', 'create', 'bench_rados', '128', '128', 'replicated', '--size', '1', '--pg-num-min', '128'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 539e2faa-4056-11ec-aba1-c5f3c07a3dc7 -- ceph osd pool create bench_rados 128 128 replicated --size 1 --pg-num-min 128
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
pool 'bench_rados' created
# ./benchmarks/bench-rados:267 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados min_size 1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:43:46,404215644-05:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:43:46,407019501-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '539e2faa-4056-11ec-aba1-c5f3c07a3dc7', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'min_size', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 539e2faa-4056-11ec-aba1-c5f3c07a3dc7 -- ceph osd pool set bench_rados min_size 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 min_size to 1
# ./benchmarks/bench-rados:268 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados pg_autoscale_mode off
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:43:50,923402206-05:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:43:50,926135379-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '539e2faa-4056-11ec-aba1-c5f3c07a3dc7', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'pg_autoscale_mode', 'off'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 539e2faa-4056-11ec-aba1-c5f3c07a3dc7 -- ceph osd pool set bench_rados pg_autoscale_mode off
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 pg_autoscale_mode to off
# ./benchmarks/bench-rados:269 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool application enable bench_rados benchmark
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:43:55,526536509-05:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:43:55,529175775-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '539e2faa-4056-11ec-aba1-c5f3c07a3dc7', '--', 'ceph', 'osd', 'pool', 'application', 'enable', 'bench_rados', 'benchmark'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 539e2faa-4056-11ec-aba1-c5f3c07a3dc7 -- ceph osd pool application enable bench_rados benchmark
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
enabled application 'benchmark' on pool 'bench_rados'
# ./benchmarks/bench-rados:270 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados noscrub 1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:43:59,940201015-05:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:43:59,943060146-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '539e2faa-4056-11ec-aba1-c5f3c07a3dc7', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'noscrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 539e2faa-4056-11ec-aba1-c5f3c07a3dc7 -- ceph osd pool set bench_rados noscrub 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 noscrub to 1
# ./benchmarks/bench-rados:271 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados nodeep-scrub 1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:44:04,257165558-05:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:44:04,259910163-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '539e2faa-4056-11ec-aba1-c5f3c07a3dc7', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'nodeep-scrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 539e2faa-4056-11ec-aba1-c5f3c07a3dc7 -- ceph osd pool set bench_rados nodeep-scrub 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 nodeep-scrub to 1
# ./benchmarks/bench-rados:272 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool ls detail
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:44:08,477837797-05:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:44:08,480624972-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '539e2faa-4056-11ec-aba1-c5f3c07a3dc7', '--', 'ceph', 'osd', 'pool', 'ls', 'detail'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 539e2faa-4056-11ec-aba1-c5f3c07a3dc7 -- ceph osd pool ls detail
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
pool 1 'device_health_metrics' replicated size 1 min_size 1 crush_rule 0 object_hash rjenkins pg_num 64 pgp_num 64 autoscale_mode on last_change 18 lfor 0/0/15 flags hashpspool stripe_width 0 pg_num_min 1 application mgr_devicehealth
pool 2 'bench_rados' replicated size 1 min_size 1 crush_rule 0 object_hash rjenkins pg_num 128 pgp_num 128 autoscale_mode off last_change 21 flags hashpspool,noscrub,nodeep-scrub stripe_width 0 pg_num_min 128 application benchmark

# ./benchmarks/bench-rados:273 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd df tree
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:44:12,689074978-05:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:44:12,692074754-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '539e2faa-4056-11ec-aba1-c5f3c07a3dc7', '--', 'ceph', 'osd', 'df', 'tree'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 539e2faa-4056-11ec-aba1-c5f3c07a3dc7 -- ceph osd df tree
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
ID  CLASS  WEIGHT   REWEIGHT  SIZE     RAW USE  DATA     OMAP  META     AVAIL    %USE  VAR   PGS  STATUS  TYPE NAME      
-1         0.09769         -  100 GiB  5.7 MiB  176 KiB   0 B  5.6 MiB  100 GiB  0.01  1.00    -          root default   
-3         0.09769         -  100 GiB  5.7 MiB  176 KiB   0 B  5.6 MiB  100 GiB  0.01  1.00    -              host node-1
 0    ssd  0.09769   1.00000  100 GiB  5.7 MiB  176 KiB   0 B  5.6 MiB  100 GiB  0.01  1.00  192      up          osd.0  
                       TOTAL  100 GiB  5.7 MiB  176 KiB   0 B  5.6 MiB  100 GiB  0.01                                    
MIN/MAX VAR: 1.00/1.00  STDDEV: 0


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:44:16,731991827-05:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:44:40,777418451-05:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:44:49,999182509-05:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:44:59,040525866-05:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:44:59,047284299-05:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:45:08,081947539-05:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:45:08,088967204-05:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:45:17,289685972-05:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:45:17,296340278-05:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:45:26,400184522-05:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:45:26,407210990-05:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:45:35,536940680-05:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:45:35,543853033-05:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:45:35,549261841-05:00] INFO: > The cluster is idle now.[0m


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:45:35,552279170-05:00] STAGE: Start benchmarking of mode write ...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:45:35,557934423-05:00] INFO: > Start system activity collection services[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:45:35,562996257-05:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:341 - rados_bench() > rm --force /mnt/sda8/ceph-research/scripts/output/sys_activity_64KB_write.dat.rados_bench_host.1
# ./benchmarks/bench-rados:342 - rados_bench() > sar_pid_rados_bench_host=171884
# ./benchmarks/bench-rados:342 - rados_bench() > S_TIME_FORMAT=ISO
# ./benchmarks/bench-rados:342 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/output/sys_activity_64KB_write.dat.rados_bench_host.1 2

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:45:35,569388540-05:00] INFO: >> for OSD host[0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:45:35,577523869-05:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.2 bash
10.10.2.2: b'30592\n'
[1] 00:45:35 [SUCCESS] ljishen@10.10.2.2
30592

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:45:35,764501439-05:00] INFO: > Run rados bench[0m
# ./benchmarks/bench-rados:370 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/output/rados_bench_64KB_write.log.1
## ./benchmarks/bench-rados:361 - rados_bench() > count_cpus 32-63,96-127
# ./benchmarks/bench-rados:361 - rados_bench() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 60 write --pool bench_rados -b 65536 -O 65536 --concurrent-ios 128 --show-time --write-object --write-omap --write-xattr --no-cleanup
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:45:35,783713623-05:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:45:35,786392244-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '539e2faa-4056-11ec-aba1-c5f3c07a3dc7', '--', 'rados', 'bench', '60', 'write', '--pool', 'bench_rados', '-b', '65536', '-O', '65536', '--concurrent-ios', '128', '--show-time', '--write-object', '--write-omap', '--write-xattr', '--no-cleanup'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 539e2faa-4056-11ec-aba1-c5f3c07a3dc7 -- rados bench 60 write --pool bench_rados -b 65536 -O 65536 --concurrent-ios 128 --show-time --write-object --write-omap --write-xattr --no-cleanup
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
hints = 1
2021-11-08T05:45:39.062203+0000 Maintaining 128 concurrent writes of 65536 bytes to objects of size 65536 for up to 60 seconds or 0 objects
2021-11-08T05:45:39.062215+0000 Object prefix: benchmark_data_node-0.bluefield2.ucsc-cmps10_7
2021-11-08T05:45:39.065592+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-11-08T05:45:39.065592+0000     0       0         0         0         0         0           -           0
2021-11-08T05:45:40.065685+0000     1     128      4480      4352   271.984       272   0.0289353   0.0289661
2021-11-08T05:45:41.065763+0000     2     127      8468      8341   260.639   249.312   0.0705601    0.030163
2021-11-08T05:45:42.065842+0000     3     128     10164     10036   209.068   105.938   0.0715051    0.037597
2021-11-08T05:45:43.065926+0000     4     127     13320     13193   206.125   197.312   0.0248074   0.0386681
2021-11-08T05:45:44.065999+0000     5     128     16896     16768   209.584   223.438     0.12136   0.0378976
2021-11-08T05:45:45.066096+0000     6     128     17844     17716   184.527     59.25    0.130619   0.0427818
2021-11-08T05:45:46.066170+0000     7     128     18868     18740   167.308        64    0.130685   0.0475659
2021-11-08T05:45:47.066245+0000     8     128     20788     20660   161.394       120   0.0600293   0.0494655
2021-11-08T05:45:48.066314+0000     9     128     22656     22528   156.433    116.75   0.0673102   0.0509835
2021-11-08T05:45:49.066382+0000    10     128     25012     24884   155.513    147.25   0.0729162   0.0512105
2021-11-08T05:45:50.066452+0000    11     128     26752     26624   151.261    108.75   0.0765617    0.052746
2021-11-08T05:45:51.066526+0000    12     128     30464     30336   157.988       232   0.0288159   0.0505836
2021-11-08T05:45:52.066598+0000    13     128     33152     33024   158.757       168    0.127897   0.0500855
2021-11-08T05:45:53.066673+0000    14     128     34176     34048   151.989        64    0.129659   0.0524605
2021-11-08T05:45:54.066736+0000    15     128     35200     35072   146.123        64    0.106063   0.0546079
2021-11-08T05:45:55.066809+0000    16     128     37248     37120   144.989       128   0.0663135   0.0550414
2021-11-08T05:45:56.066877+0000    17     128     39168     39040   143.519       120   0.0646941   0.0556349
2021-11-08T05:45:57.066953+0000    18     128     40756     40628   141.059     99.25    0.127313   0.0564348
2021-11-08T05:45:58.067028+0000    19     128     41780     41652   137.003        64    0.130082   0.0582267
2021-11-08T05:45:59.067093+0000 min lat: 0.0195072 max lat: 0.13565 avg lat: 0.059842
2021-11-08T05:45:59.067093+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-11-08T05:45:59.067093+0000    20     128     42752     42624    133.19     60.75    0.124067    0.059842
2021-11-08T05:46:00.067172+0000    21     128     45876     45748   136.145    195.25   0.0285218   0.0587306
2021-11-08T05:46:01.067248+0000    22     128     49152     49024   139.262    204.75    0.102297   0.0572549
2021-11-08T05:46:02.067321+0000    23     128     50176     50048    135.99        64    0.130974   0.0587292
2021-11-08T05:46:03.067399+0000    24     128     51124     50996   132.792     59.25    0.130041   0.0600338
2021-11-08T05:46:04.067468+0000    25     128     53044     52916    132.28       120   0.0583251    0.060355
2021-11-08T05:46:05.067538+0000    26     128     54964     54836   131.808       120    0.071412   0.0606255
2021-11-08T05:46:06.067608+0000    27     128     56756     56628   131.074       112    0.121666   0.0609381
2021-11-08T05:46:07.067675+0000    28     128     57728     57600   128.562     60.75    0.128492   0.0620681
2021-11-08T05:46:08.067747+0000    29     128     58752     58624   126.336        64    0.130803   0.0632451
2021-11-08T05:46:09.067816+0000    30     128     59956     59828   124.633     75.25   0.0714748   0.0640711
2021-11-08T05:46:10.067890+0000    31     128     61876     61748   124.483       120   0.0688897   0.0642052
2021-11-08T05:46:11.067963+0000    32     128     63744     63616   124.241    116.75   0.0639246   0.0643301
2021-11-08T05:46:12.068038+0000    33     128     65204     65076   123.241     91.25    0.127563   0.0648438
2021-11-08T05:46:13.068117+0000    34     128     66176     66048   121.403     60.75    0.131468    0.065819
2021-11-08T05:46:14.068188+0000    35     128     67124     66996   119.627     59.25    0.132571   0.0667851
2021-11-08T05:46:15.068267+0000    36     128     68916     68788   119.415       112   0.0597174   0.0669476
2021-11-08T05:46:16.068339+0000    37     128     70836     70708    119.43       120   0.0646357   0.0668897
2021-11-08T05:46:17.068414+0000    38     128     72756     72628   119.445       120   0.0678636   0.0668735
2021-11-08T05:46:18.068489+0000    39     128     73780     73652   118.023        64    0.126844   0.0676922
2021-11-08T05:46:19.068557+0000 min lat: 0.0195072 max lat: 0.139868 avg lat: 0.0684776
2021-11-08T05:46:19.068557+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-11-08T05:46:19.068557+0000    40     128     74752     74624   116.591     60.75     0.12839   0.0684776
2021-11-08T05:46:20.068632+0000    41     128     76032     75904   115.699        80   0.0634722   0.0690955
2021-11-08T05:46:21.068702+0000    42     128     78004     77876   115.878    123.25    0.065154   0.0689815
2021-11-08T05:46:22.068778+0000    43     128     80000     79872   116.085    124.75   0.0604077   0.0688859
2021-11-08T05:46:23.068851+0000    44     128     81408     81280   115.446        88     0.10942    0.069229
2021-11-08T05:46:24.068917+0000    45     128     82432     82304   114.303        64    0.129717   0.0699356
2021-11-08T05:46:25.068992+0000    46     128     83380     83252   113.106     59.25    0.120833   0.0706081
2021-11-08T05:46:26.069066+0000    47     128     85300     85172   113.252       120   0.0632899   0.0705705
2021-11-08T05:46:27.069139+0000    48     128     87220     87092   113.393       120   0.0720078    0.070491
2021-11-08T05:46:28.069212+0000    49     128     89012     88884   113.364       112    0.109964   0.0704888
2021-11-08T05:46:29.069283+0000    50     128     89984     89856   112.312     60.75    0.132813   0.0711285
2021-11-08T05:46:30.069359+0000    51     128     90932     90804   111.271     59.25    0.133562   0.0717364
2021-11-08T05:46:31.069429+0000    52     128     92341     92213   110.825   88.0625   0.0864954   0.0720963
2021-11-08T05:46:32.069510+0000    53     127     94467     94340   111.242   132.938   0.0672743   0.0718568
2021-11-08T05:46:33.069581+0000    54     128     96436     96308   111.459       123   0.0610609   0.0717403
2021-11-08T05:46:34.069645+0000    55     128     97716     97588   110.887        80     0.12763   0.0720829
2021-11-08T05:46:35.069717+0000    56     128     98688     98560   109.992     60.75    0.135517   0.0726512
2021-11-08T05:46:36.069785+0000    57     128     99643     99515   109.109   59.6875    0.108896   0.0731778
2021-11-08T05:46:37.069859+0000    58     128    101684    101556   109.427   127.562   0.0654776   0.0730835
2021-11-08T05:46:38.069939+0000    59     127    103684    103557   109.692   125.062   0.0600185   0.0728854
2021-11-08T05:46:39.070004+0000 min lat: 0.0195072 max lat: 0.148149 avg lat: 0.072907
2021-11-08T05:46:39.070004+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-11-08T05:46:39.070004+0000    60     128    105396    105268   109.646   106.938    0.131409    0.072907
2021-11-08T05:46:40.070122+0000 Total time run:         60.1182
Total writes made:      105396
Write size:             65536
Object size:            65536
Bandwidth (MB/sec):     109.572
Stddev Bandwidth:       51.4741
Max bandwidth (MB/sec): 272
Min bandwidth (MB/sec): 59.25
Average IOPS:           1753
Stddev IOPS:            823.586
Max IOPS:               4352
Min IOPS:               948
Average Latency(s):     0.0729743
Stddev Latency(s):      0.0355503
Max latency(s):         0.148149
Min latency(s):         0.0195072

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:46:40,754129236-05:00] INFO: > Stop system activity collection services[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:46:40,758976135-05:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:391 - rados_bench() > kill -INT 171884

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:46:40,763673792-05:00] INFO: >> for OSD host[0m
# ./benchmarks/bench-rados:396 - rados_bench() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.2 kill -INT 30592
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:46:40,770875060-05:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.2 kill -INT 30592
[1] 00:46:40 [SUCCESS] ljishen@10.10.2.2

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:46:40,951862077-05:00] INFO: >>> Copy system activity file from OSD host[0m
## ./benchmarks/bench-rados:403 - rados_bench() > basename -- /tmp/bench-rados/sys_activity_64KB_write.dat.osd_host.1
# ./benchmarks/bench-rados:403 - rados_bench() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.2:/tmp/bench-rados/sys_activity_64KB_write.dat.osd_host.1 /mnt/sda8/ceph-research/scripts/output/sys_activity_64KB_write.dat.osd_host.1


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:46:41,122443945-05:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:47:05,232583139-05:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 105.40k objects, 6.4 GiB
    usage:   20 GiB used, 80 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:47:05,239361079-05:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:47:14,382561333-05:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 105.40k objects, 6.4 GiB
    usage:   20 GiB used, 80 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:47:14,389167328-05:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:47:23,442546303-05:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 105.40k objects, 6.4 GiB
    usage:   20 GiB used, 80 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:47:23,449711412-05:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:47:32,476403265-05:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 105.40k objects, 6.4 GiB
    usage:   20 GiB used, 80 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:47:32,483234224-05:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:47:41,568052741-05:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 105.40k objects, 6.4 GiB
    usage:   20 GiB used, 80 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:47:41,574722567-05:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:47:41,580065891-05:00] INFO: > The cluster is idle now.[0m


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:47:41,583241209-05:00] STAGE: Start benchmarking of mode seq ...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:47:41,589789655-05:00] INFO: > Start system activity collection services[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:47:41,594670348-05:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:341 - rados_bench() > rm --force /mnt/sda8/ceph-research/scripts/output/sys_activity_64KB_seq.dat.rados_bench_host.1
# ./benchmarks/bench-rados:342 - rados_bench() > sar_pid_rados_bench_host=173272
# ./benchmarks/bench-rados:342 - rados_bench() > S_TIME_FORMAT=ISO
# ./benchmarks/bench-rados:342 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/output/sys_activity_64KB_seq.dat.rados_bench_host.1 2

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:47:41,601079332-05:00] INFO: >> for OSD host[0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:47:41,609278782-05:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.2 bash
10.10.2.2: b'30692\n'
[1] 00:47:41 [SUCCESS] ljishen@10.10.2.2
30692

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:47:41,797105515-05:00] INFO: > Run rados bench[0m
# ./benchmarks/bench-rados:379 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/output/rados_bench_64KB_seq.log.1
## ./benchmarks/bench-rados:376 - rados_bench() > count_cpus 32-63,96-127
# ./benchmarks/bench-rados:376 - rados_bench() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 128 --show-time
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:47:41,815470602-05:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:47:41,818223062-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '539e2faa-4056-11ec-aba1-c5f3c07a3dc7', '--', 'rados', 'bench', '99999999', 'seq', '--pool', 'bench_rados', '--concurrent-ios', '128', '--show-time'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 539e2faa-4056-11ec-aba1-c5f3c07a3dc7 -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 128 --show-time
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
hints = 1
2021-11-08T05:47:45.038958+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-11-08T05:47:45.038958+0000     0       0         0         0         0         0           -           0
2021-11-08T05:47:46.039057+0000     1     127     17369     17242   1077.44   1077.62  0.00719676  0.00738673
2021-11-08T05:47:47.039134+0000     2     128     34675     34547   1079.46   1081.56  0.00863834  0.00738837
2021-11-08T05:47:48.039224+0000     3     127     52009     51882   1080.75   1083.44  0.00711067  0.00738317
2021-11-08T05:47:49.039294+0000     4     128     69343     69215   1081.37   1083.31  0.00675657  0.00738186
2021-11-08T05:47:50.039382+0000     5     127     86725     86598   1082.37   1086.44  0.00638081  0.00737654
2021-11-08T05:47:51.039457+0000     6     128    104036    103908   1082.27   1081.88  0.00678647  0.00737811
2021-11-08T05:47:52.039574+0000 Total time run:       6.08498
Total reads made:     105396
Read size:            65536
Object size:          65536
Bandwidth (MB/sec):   1082.54
Average IOPS:         17320
Stddev IOPS:          46.3638
Max IOPS:             17383
Min IOPS:             17242
Average Latency(s):   0.00737783
Max latency(s):       0.0121336
Min latency(s):       0.00256888

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:47:52,720646492-05:00] INFO: > Stop system activity collection services[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:47:52,725783528-05:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:391 - rados_bench() > kill -INT 173272

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:47:52,731283177-05:00] INFO: >> for OSD host[0m
# ./benchmarks/bench-rados:396 - rados_bench() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.2 kill -INT 30692
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:47:52,738527957-05:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.2 kill -INT 30692
[1] 00:47:52 [SUCCESS] ljishen@10.10.2.2

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:47:52,919881997-05:00] INFO: >>> Copy system activity file from OSD host[0m
## ./benchmarks/bench-rados:403 - rados_bench() > basename -- /tmp/bench-rados/sys_activity_64KB_seq.dat.osd_host.1
# ./benchmarks/bench-rados:403 - rados_bench() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.2:/tmp/bench-rados/sys_activity_64KB_seq.dat.osd_host.1 /mnt/sda8/ceph-research/scripts/output/sys_activity_64KB_seq.dat.osd_host.1


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:47:53,069294652-05:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:48:17,135123963-05:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 105.40k objects, 6.4 GiB
    usage:   20 GiB used, 80 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:48:17,142060462-05:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:48:26,197052737-05:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 105.40k objects, 6.4 GiB
    usage:   20 GiB used, 80 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:48:26,203768239-05:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:48:35,271491445-05:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 105.40k objects, 6.4 GiB
    usage:   20 GiB used, 80 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:48:35,278370675-05:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:48:44,364814816-05:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 105.40k objects, 6.4 GiB
    usage:   20 GiB used, 80 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:48:44,371942626-05:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:48:53,584685498-05:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 105.40k objects, 6.4 GiB
    usage:   20 GiB used, 80 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:48:53,591742764-05:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:48:53,596997322-05:00] INFO: > The cluster is idle now.[0m


[1;7;39;49m[2021-11-08T00:48:53,598926209-05:00][RUNNING][ROUND 2/3/21] object_size=64KB[0m


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:48:53,602198829-05:00] STAGE: Launch a Ceph cluster (CEPH_HOST: 10.10.2.3, OSD_HOST: 10.10.2.2) ...[0m
# ./benchmarks/bench-rados:183 - launch_ceph_cluster() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.3 bash -euo pipefail
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:48:53,611261005-05:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.3 bash -euo pipefail
10.10.2.3: b'\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T00:48:53,930213174-05:00] INFO: > Remove existing clusters\x1b[0m\n'
10.10.2.3: b'## bash:17 -  > basename -- /var/lib/ceph/539e2faa-4056-11ec-aba1-c5f3c07a3dc7\n'
10.10.2.3: b'# bash:17 -  > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm --host 10.10.2.3 --host 10.10.2.2 rm-cluster --force --fsid 539e2faa-4056-11ec-aba1-c5f3c07a3dc7\n'
10.10.2.3: b"\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T00:48:53,940360962-05:00] DEBUG: HOST_PARAMS: ['--host', '10.10.2.3', '--host', '10.10.2.2']\x1b[0m\n"
10.10.2.3: b"\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T00:48:53,944084498-05:00] DEBUG: CEPHADM_PARAMS: ['rm-cluster', '--force', '--fsid', '539e2faa-4056-11ec-aba1-c5f3c07a3dc7']\x1b[0m\n"
10.10.2.3: b"# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:63 -  > /tmp/bench-rados/ceph-research/scripts/parallel-ssh --host 10.10.2.3 --host 10.10.2.2 'sudo python3 -u - rm-cluster --force --fsid 539e2faa-4056-11ec-aba1-c5f3c07a3dc7'\n"
10.10.2.3: b'\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T00:48:53,952304448-05:00] DEBUG: command from stdin\x1b[0m\n'
10.10.2.3: b"# /tmp/bench-rados/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host 10.10.2.3 --host 10.10.2.2 'sudo python3 -u - rm-cluster --force --fsid 539e2faa-4056-11ec-aba1-c5f3c07a3dc7'\n"
10.10.2.3: b'[1] 00:48:59 [SUCCESS] 10.10.2.3\n[2] 00:49:05 [SUCCESS] 10.10.2.2\n'
10.10.2.3: b'\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T00:49:05,301392394-05:00] INFO: > Deploy a new cluster\x1b[0m\n'
10.10.2.3: b'# bash:24 -  > /tmp/bench-rados/ceph-research/scripts/ceph-deploy -m 10.10.2.3 -o node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/10.10.2.2:/dev/nvme0n1\n'
10.10.2.3: b'\n\n\x1b[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T00:49:05,312738502-05:00] STAGE: Verify passwordless SSH to hosts, passwordless sudo via ljishen, and the existence of devices on hosts...\x1b[0m\n'
10.10.2.3: b'\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T00:49:05,317320819-05:00] INFO: > Check the host for the monitor\x1b[0m\n'
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:181 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@10.10.2.3 sudo --non-interactive --validate\n'
10.10.2.3: b'\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T00:49:05,466661894-05:00] INFO: > Check hosts for OSDs\x1b[0m\n'
10.10.2.3: b"\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T00:49:05,471329000-05:00] INFO: >> Check host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'\x1b[0m\n"
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:200 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us sudo --non-interactive --validate\n'
10.10.2.3: b"\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T00:49:05,618064870-05:00] INFO: >>> Check device '/dev/nvme0n1'\x1b[0m\n"
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:215 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us test -b /dev/nvme0n1\n'
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:226 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us lsblk --nodeps --noheadings --output TYPE /dev/nvme0n1\n'
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:227 - verify_parameters() > grep --word-regexp disk\n'
10.10.2.3: b'\n\n\x1b[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T00:49:05,894608889-05:00] STAGE: Reset devices on hosts...\x1b[0m\n'
10.10.2.3: b"\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T00:49:05,899597302-05:00] INFO: > Resetting device '/dev/nvme0n1' on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'\x1b[0m\n"
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:288 - reset_host_devices() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us sudo sh\n'
10.10.2.3: b'  Removing ceph--86fb29eb--2b1d--46fa--a9a1--ed73c2dd68df-osd--block--472c4031--4742--4393--ba38--08e5effb6db6 (253:0)\n'
10.10.2.3: b'  Archiving volume group "ceph-86fb29eb-2b1d-46fa-a9a1-ed73c2dd68df" metadata (seqno 5).\n'
10.10.2.3: b'  Releasing logical volume "osd-block-472c4031-4742-4393-ba38-08e5effb6db6"\n'
10.10.2.3: b'  Creating volume group backup "/etc/lvm/backup/ceph-86fb29eb-2b1d-46fa-a9a1-ed73c2dd68df" (seqno 6).\n'
10.10.2.3: b'  Logical volume "osd-block-472c4031-4742-4393-ba38-08e5effb6db6" successfully removed\n'
10.10.2.3: b'  Removing physical volume "/dev/nvme0n1" from volume group "ceph-86fb29eb-2b1d-46fa-a9a1-ed73c2dd68df"\n'
10.10.2.3: b'  Volume group "ceph-86fb29eb-2b1d-46fa-a9a1-ed73c2dd68df" successfully removed\n'
10.10.2.3: b'\n\n\x1b[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T00:49:06,315746807-05:00] STAGE: Bootstrap a new cluster...\x1b[0m\n'
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:317 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.3 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults\n'
10.10.2.3: b'\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T00:49:06,325462710-05:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.3: b"\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T00:49:06,329246068-05:00] DEBUG: CEPHADM_PARAMS: ['bootstrap', '--config', '/tmp/bench-rados/ceph-research/scripts/ceph.conf', '--mon-ip', '10.10.2.3', '--output-dir', '/tmp/bench-rados/deployment_data_root/etc/ceph', '--ssh-user', 'ljishen', '--skip-dashboard', '--no-minimize-config', '--allow-overwrite', '--allow-fqdn-hostname', '--orphan-initial-daemons', '--skip-monitoring-stack', '--single-host-defaults']\x1b[0m\n"
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.3 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults\n'
10.10.2.3: b'Verifying podman|docker is present...\nVerifying lvm2 is present...\n'
10.10.2.3: b'Verifying time synchronization is in place...\n'
10.10.2.3: b'Unit ntp.service is enabled and running\n'
10.10.2.3: b'Repeating the final host check...\n'
10.10.2.3: b'docker (/usr/bin/docker) is present\n'
10.10.2.3: b'systemctl is present\n'
10.10.2.3: b'lvcreate is present\n'
10.10.2.3: b'Unit ntp.service is enabled and running\n'
10.10.2.3: b'Host looks OK\n'
10.10.2.3: b'Cluster fsid: 960cedbc-4057-11ec-aba1-c5f3c07a3dc7\n'
10.10.2.3: b'Verifying IP 10.10.2.3 port 3300 ...\n'
10.10.2.3: b'Verifying IP 10.10.2.3 port 6789 ...\n'
10.10.2.3: b'Mon IP `10.10.2.3` is in CIDR network `10.10.2.0/24`\n'
10.10.2.3: b'- internal network (--cluster-network) has not been provided, OSD replication will default to the public_network\n'
10.10.2.3: b'Adjusting default settings to suit single-host cluster...\n'
10.10.2.3: b'Pulling container image quay.io/ceph/ceph:v16...\n'
10.10.2.3: b'Ceph version: ceph version 16.2.6 (ee28fb57e47e9f88813e24bbf4c14496ca299d31) pacific (stable)\n'
10.10.2.3: b'Extracting ceph user uid/gid from container image...\n'
10.10.2.3: b'Creating initial keys...\n'
10.10.2.3: b'Creating initial monmap...\n'
10.10.2.3: b'Creating mon...\n'
10.10.2.3: b'Waiting for mon to start...\n'
10.10.2.3: b'Waiting for mon...\n'
10.10.2.3: b'mon is available\nSetting mon public_network to 10.10.2.0/24\n'
10.10.2.3: b'Wrote config to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf\n'
10.10.2.3: b'Wrote keyring to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring\n'
10.10.2.3: b'Creating mgr...\n'
10.10.2.3: b'Verifying port 9283 ...\n'
10.10.2.3: b'Waiting for mgr to start...\nWaiting for mgr...\n'
10.10.2.3: b'mgr not available, waiting (1/15)...\n'
10.10.2.3: b'mgr not available, waiting (2/15)...\n'
10.10.2.3: b'mgr is available\n'
10.10.2.3: b'Enabling cephadm module...\n'
10.10.2.3: b'Waiting for the mgr to restart...\n'
10.10.2.3: b'Waiting for mgr epoch 5...\n'
10.10.2.3: b'mgr epoch 5 is available\nSetting orchestrator backend to cephadm...\n'
10.10.2.3: b'Generating ssh key...\n'
10.10.2.3: b'Wrote public SSH key to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub\n'
10.10.2.3: b'Adding key to ljishen@localhost authorized_keys...\n'
10.10.2.3: b'Adding host node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us...\n'
10.10.2.3: b'Deploying unmanaged mon service...\n'
10.10.2.3: b'Deploying unmanaged mgr service...\n'
10.10.2.3: b'You can access the Ceph CLI with:\n\n\tsudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --fsid 960cedbc-4057-11ec-aba1-c5f3c07a3dc7 -c /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf -k /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring\n\n'
10.10.2.3: b'Please consider enabling telemetry to help improve Ceph:\n\n\tceph telemetry on\n\nFor more information see:\n\n\thttps://docs.ceph.com/docs/pacific/mgr/telemetry/\n\n'
10.10.2.3: b'Bootstrap complete.\n'
10.10.2.3: b'\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T00:50:14,122853722-05:00] INFO: Sleep for 20 seconds\x1b[0m\n'
10.10.2.3: b'\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T00:50:34,130125499-05:00] INFO: > Set the default number of monitor daemons to 1\x1b[0m\n# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:337 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1\n'
10.10.2.3: b'\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T00:50:34,140282795-05:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.3: b"\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T00:50:34,144123111-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mon', '1']\x1b[0m\n"
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1\n'
10.10.2.3: b'Inferring fsid 960cedbc-4057-11ec-aba1-c5f3c07a3dc7\n'
10.10.2.3: b'Inferring config /var/lib/ceph/960cedbc-4057-11ec-aba1-c5f3c07a3dc7/mon.node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.3: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.3: b'Scheduled mon update...\n'
10.10.2.3: b'\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T00:50:43,595747333-05:00] INFO: > Set the default number of manager daemons to 1\x1b[0m\n'
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:342 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1\n'
10.10.2.3: b'\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T00:50:43,605214075-05:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.3: b"\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T00:50:43,608836170-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mgr', '1']\x1b[0m\n"
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1\n'
10.10.2.3: b'Inferring fsid 960cedbc-4057-11ec-aba1-c5f3c07a3dc7\n'
10.10.2.3: b'Inferring config /var/lib/ceph/960cedbc-4057-11ec-aba1-c5f3c07a3dc7/mon.node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.3: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.3: b'Scheduled mgr update...\n'
10.10.2.3: b'\n\n\x1b[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T00:50:54,459847771-05:00] STAGE: Add hosts to the cluster...\x1b[0m\n'
10.10.2.3: b"\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T00:50:54,465284800-05:00] INFO: > Adding host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'\x1b[0m\n"
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:361 - add_hosts() > ssh-copy-id -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -f -i /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\n'
10.10.2.3: b'/usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: "/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub"\n'
10.10.2.3: b'\nNumber of key(s) added: 1\n\nNow try logging into the machine, with:   "ssh -o \'GlobalKnownHostsFile=/dev/null\' -o \'LogLevel=ERROR\' -o \'PasswordAuthentication=no\' -o \'StrictHostKeyChecking=no\' -o \'UserKnownHostsFile=/dev/null\' \'ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\'"\nand check to make sure that only the key(s) you wanted were added.\n\n'
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:367 - add_hosts() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us 10.10.2.2\n'
10.10.2.3: b'\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T00:50:54,672626458-05:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.3: b"\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T00:50:54,676072882-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'host', 'add', 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us', '10.10.2.2']\x1b[0m\n"
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us 10.10.2.2\n'
10.10.2.3: b'Inferring fsid 960cedbc-4057-11ec-aba1-c5f3c07a3dc7\n'
10.10.2.3: b'Inferring config /var/lib/ceph/960cedbc-4057-11ec-aba1-c5f3c07a3dc7/mon.node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.3: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.3: b"Added host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us' with addr '10.10.2.2'\n"
10.10.2.3: b'\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T00:51:05,021297764-05:00] INFO: Sleep for 20 seconds\x1b[0m\n'
10.10.2.3: b'\n\n\x1b[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T00:51:25,025981019-05:00] STAGE: Add OSDs to the cluster...\x1b[0m\n'
10.10.2.3: b"\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T00:51:25,032496422-05:00] INFO: > Adding devices ['/dev/nvme0n1'] on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us' as OSDs\x1b[0m\n"
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:388 - add_host_devices_as_osds() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1\n'
10.10.2.3: b'\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T00:51:25,042024100-05:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.3: b"\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T00:51:25,046203707-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'daemon', 'add', 'osd', 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1']\x1b[0m\n"
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1\n'
10.10.2.3: b'Inferring fsid 960cedbc-4057-11ec-aba1-c5f3c07a3dc7\n'
10.10.2.3: b'Inferring config /var/lib/ceph/960cedbc-4057-11ec-aba1-c5f3c07a3dc7/mon.node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.3: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.3: b"Created osd(s) 0 on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'\n"
10.10.2.3: b'\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T00:51:51,133237102-05:00] INFO: > Sleep for 20 seconds\x1b[0m\n'
10.10.2.3: b'\n\n\x1b[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T00:52:11,139253203-05:00] STAGE: Show Cluster Status\x1b[0m\n'
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:408 - show_status() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status\n'
10.10.2.3: b'\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T00:52:11,149067942-05:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.3: b"\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T00:52:11,152570711-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', '--status']\x1b[0m\n"
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status\n'
10.10.2.3: b'Inferring fsid 960cedbc-4057-11ec-aba1-c5f3c07a3dc7\n'
10.10.2.3: b'Inferring config /var/lib/ceph/960cedbc-4057-11ec-aba1-c5f3c07a3dc7/mon.node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.3: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.3: b'  cluster:\n    id:     960cedbc-4057-11ec-aba1-c5f3c07a3dc7\n    health: HEALTH_WARN\n            1 pool(s) have no replicas configured\n \n  services:\n    mon: 1 daemons, quorum node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us (age 2m)\n    mgr: node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us.uazliw(active, since 2m)\n    osd: 1 osds: 1 up (since 18s), 1 in (since 41s)\n \n  data:\n    pools:   1 pools, 1 pgs\n    objects: 0 objects, 0 B\n    usage:   4.7 MiB used, 100 GiB / 100 GiB avail\n    pgs:     1 active+clean\n \n'
[1] 00:52:20 [SUCCESS] ljishen@10.10.2.3

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:48:53,930213174-05:00] INFO: > Remove existing clusters[0m
## bash:17 -  > basename -- /var/lib/ceph/539e2faa-4056-11ec-aba1-c5f3c07a3dc7
# bash:17 -  > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm --host 10.10.2.3 --host 10.10.2.2 rm-cluster --force --fsid 539e2faa-4056-11ec-aba1-c5f3c07a3dc7
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:48:53,940360962-05:00] DEBUG: HOST_PARAMS: ['--host', '10.10.2.3', '--host', '10.10.2.2'][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:48:53,944084498-05:00] DEBUG: CEPHADM_PARAMS: ['rm-cluster', '--force', '--fsid', '539e2faa-4056-11ec-aba1-c5f3c07a3dc7'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:63 -  > /tmp/bench-rados/ceph-research/scripts/parallel-ssh --host 10.10.2.3 --host 10.10.2.2 'sudo python3 -u - rm-cluster --force --fsid 539e2faa-4056-11ec-aba1-c5f3c07a3dc7'
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:48:53,952304448-05:00] DEBUG: command from stdin[0m
# /tmp/bench-rados/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host 10.10.2.3 --host 10.10.2.2 'sudo python3 -u - rm-cluster --force --fsid 539e2faa-4056-11ec-aba1-c5f3c07a3dc7'
[1] 00:48:59 [SUCCESS] 10.10.2.3
[2] 00:49:05 [SUCCESS] 10.10.2.2

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:49:05,301392394-05:00] INFO: > Deploy a new cluster[0m
# bash:24 -  > /tmp/bench-rados/ceph-research/scripts/ceph-deploy -m 10.10.2.3 -o node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/10.10.2.2:/dev/nvme0n1


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:49:05,312738502-05:00] STAGE: Verify passwordless SSH to hosts, passwordless sudo via ljishen, and the existence of devices on hosts...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:49:05,317320819-05:00] INFO: > Check the host for the monitor[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:181 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@10.10.2.3 sudo --non-interactive --validate

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:49:05,466661894-05:00] INFO: > Check hosts for OSDs[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:49:05,471329000-05:00] INFO: >> Check host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:200 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us sudo --non-interactive --validate

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:49:05,618064870-05:00] INFO: >>> Check device '/dev/nvme0n1'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:215 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us test -b /dev/nvme0n1
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:226 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us lsblk --nodeps --noheadings --output TYPE /dev/nvme0n1
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:227 - verify_parameters() > grep --word-regexp disk


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:49:05,894608889-05:00] STAGE: Reset devices on hosts...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:49:05,899597302-05:00] INFO: > Resetting device '/dev/nvme0n1' on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:288 - reset_host_devices() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us sudo sh
  Removing ceph--86fb29eb--2b1d--46fa--a9a1--ed73c2dd68df-osd--block--472c4031--4742--4393--ba38--08e5effb6db6 (253:0)
  Archiving volume group "ceph-86fb29eb-2b1d-46fa-a9a1-ed73c2dd68df" metadata (seqno 5).
  Releasing logical volume "osd-block-472c4031-4742-4393-ba38-08e5effb6db6"
  Creating volume group backup "/etc/lvm/backup/ceph-86fb29eb-2b1d-46fa-a9a1-ed73c2dd68df" (seqno 6).
  Logical volume "osd-block-472c4031-4742-4393-ba38-08e5effb6db6" successfully removed
  Removing physical volume "/dev/nvme0n1" from volume group "ceph-86fb29eb-2b1d-46fa-a9a1-ed73c2dd68df"
  Volume group "ceph-86fb29eb-2b1d-46fa-a9a1-ed73c2dd68df" successfully removed


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:49:06,315746807-05:00] STAGE: Bootstrap a new cluster...[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:317 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.3 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:49:06,325462710-05:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:49:06,329246068-05:00] DEBUG: CEPHADM_PARAMS: ['bootstrap', '--config', '/tmp/bench-rados/ceph-research/scripts/ceph.conf', '--mon-ip', '10.10.2.3', '--output-dir', '/tmp/bench-rados/deployment_data_root/etc/ceph', '--ssh-user', 'ljishen', '--skip-dashboard', '--no-minimize-config', '--allow-overwrite', '--allow-fqdn-hostname', '--orphan-initial-daemons', '--skip-monitoring-stack', '--single-host-defaults'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.3 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults
Verifying podman|docker is present...
Verifying lvm2 is present...
Verifying time synchronization is in place...
Unit ntp.service is enabled and running
Repeating the final host check...
docker (/usr/bin/docker) is present
systemctl is present
lvcreate is present
Unit ntp.service is enabled and running
Host looks OK
Cluster fsid: 960cedbc-4057-11ec-aba1-c5f3c07a3dc7
Verifying IP 10.10.2.3 port 3300 ...
Verifying IP 10.10.2.3 port 6789 ...
Mon IP `10.10.2.3` is in CIDR network `10.10.2.0/24`
- internal network (--cluster-network) has not been provided, OSD replication will default to the public_network
Adjusting default settings to suit single-host cluster...
Pulling container image quay.io/ceph/ceph:v16...
Ceph version: ceph version 16.2.6 (ee28fb57e47e9f88813e24bbf4c14496ca299d31) pacific (stable)
Extracting ceph user uid/gid from container image...
Creating initial keys...
Creating initial monmap...
Creating mon...
Waiting for mon to start...
Waiting for mon...
mon is available
Setting mon public_network to 10.10.2.0/24
Wrote config to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf
Wrote keyring to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring
Creating mgr...
Verifying port 9283 ...
Waiting for mgr to start...
Waiting for mgr...
mgr not available, waiting (1/15)...
mgr not available, waiting (2/15)...
mgr is available
Enabling cephadm module...
Waiting for the mgr to restart...
Waiting for mgr epoch 5...
mgr epoch 5 is available
Setting orchestrator backend to cephadm...
Generating ssh key...
Wrote public SSH key to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub
Adding key to ljishen@localhost authorized_keys...
Adding host node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us...
Deploying unmanaged mon service...
Deploying unmanaged mgr service...
You can access the Ceph CLI with:

	sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --fsid 960cedbc-4057-11ec-aba1-c5f3c07a3dc7 -c /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf -k /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring

Please consider enabling telemetry to help improve Ceph:

	ceph telemetry on

For more information see:

	https://docs.ceph.com/docs/pacific/mgr/telemetry/

Bootstrap complete.

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:50:14,122853722-05:00] INFO: Sleep for 20 seconds[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:50:34,130125499-05:00] INFO: > Set the default number of monitor daemons to 1[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:337 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:50:34,140282795-05:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:50:34,144123111-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mon', '1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1
Inferring fsid 960cedbc-4057-11ec-aba1-c5f3c07a3dc7
Inferring config /var/lib/ceph/960cedbc-4057-11ec-aba1-c5f3c07a3dc7/mon.node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Scheduled mon update...

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:50:43,595747333-05:00] INFO: > Set the default number of manager daemons to 1[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:342 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:50:43,605214075-05:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:50:43,608836170-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mgr', '1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1
Inferring fsid 960cedbc-4057-11ec-aba1-c5f3c07a3dc7
Inferring config /var/lib/ceph/960cedbc-4057-11ec-aba1-c5f3c07a3dc7/mon.node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Scheduled mgr update...


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:50:54,459847771-05:00] STAGE: Add hosts to the cluster...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:50:54,465284800-05:00] INFO: > Adding host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:361 - add_hosts() > ssh-copy-id -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -f -i /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us
/usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: "/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub"

Number of key(s) added: 1

Now try logging into the machine, with:   "ssh -o 'GlobalKnownHostsFile=/dev/null' -o 'LogLevel=ERROR' -o 'PasswordAuthentication=no' -o 'StrictHostKeyChecking=no' -o 'UserKnownHostsFile=/dev/null' 'ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'"
and check to make sure that only the key(s) you wanted were added.

# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:367 - add_hosts() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us 10.10.2.2
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:50:54,672626458-05:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:50:54,676072882-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'host', 'add', 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us', '10.10.2.2'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us 10.10.2.2
Inferring fsid 960cedbc-4057-11ec-aba1-c5f3c07a3dc7
Inferring config /var/lib/ceph/960cedbc-4057-11ec-aba1-c5f3c07a3dc7/mon.node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Added host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us' with addr '10.10.2.2'

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:51:05,021297764-05:00] INFO: Sleep for 20 seconds[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:51:25,025981019-05:00] STAGE: Add OSDs to the cluster...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:51:25,032496422-05:00] INFO: > Adding devices ['/dev/nvme0n1'] on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us' as OSDs[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:388 - add_host_devices_as_osds() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:51:25,042024100-05:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:51:25,046203707-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'daemon', 'add', 'osd', 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1
Inferring fsid 960cedbc-4057-11ec-aba1-c5f3c07a3dc7
Inferring config /var/lib/ceph/960cedbc-4057-11ec-aba1-c5f3c07a3dc7/mon.node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Created osd(s) 0 on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:51:51,133237102-05:00] INFO: > Sleep for 20 seconds[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:52:11,139253203-05:00] STAGE: Show Cluster Status[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:408 - show_status() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:52:11,149067942-05:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:52:11,152570711-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', '--status'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status
Inferring fsid 960cedbc-4057-11ec-aba1-c5f3c07a3dc7
Inferring config /var/lib/ceph/960cedbc-4057-11ec-aba1-c5f3c07a3dc7/mon.node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
  cluster:
    id:     960cedbc-4057-11ec-aba1-c5f3c07a3dc7
    health: HEALTH_WARN
            1 pool(s) have no replicas configured
 
  services:
    mon: 1 daemons, quorum node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us (age 2m)
    mgr: node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us.uazliw(active, since 2m)
    osd: 1 osds: 1 up (since 18s), 1 in (since 41s)
 
  data:
    pools:   1 pools, 1 pgs
    objects: 0 objects, 0 B
    usage:   4.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     1 active+clean
 

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:52:20,116023942-05:00] INFO: > Copy configuration files to local[0m
# ./benchmarks/bench-rados:220 - launch_ceph_cluster() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.3 sudo chown --changes --recursive ljishen /tmp/bench-rados/deployment_data_root/etc/ceph
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:52:20,123295623-05:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.3 sudo chown --changes --recursive ljishen /tmp/bench-rados/deployment_data_root/etc/ceph
[1] 00:52:20 [SUCCESS] ljishen@10.10.2.3
# ./benchmarks/bench-rados:223 - launch_ceph_cluster() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.3:/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf
# ./benchmarks/bench-rados:226 - launch_ceph_cluster() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.3:/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:52:20,600583279-05:00] STAGE: Update local ceph.conf as a client...[0m


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:52:20,603661252-05:00] STAGE: Configure Ceph pool...[0m
# ./benchmarks/bench-rados:257 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_max_backfills 32
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:52:20,624735397-05:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:52:20,627322706-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '960cedbc-4057-11ec-aba1-c5f3c07a3dc7', '--', 'ceph', 'config', 'set', 'osd', 'osd_max_backfills', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 960cedbc-4057-11ec-aba1-c5f3c07a3dc7 -- ceph config set osd osd_max_backfills 32
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:258 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_active 32
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:52:24,831784087-05:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:52:24,834569619-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '960cedbc-4057-11ec-aba1-c5f3c07a3dc7', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_active', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 960cedbc-4057-11ec-aba1-c5f3c07a3dc7 -- ceph config set osd osd_recovery_max_active 32
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:259 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_single_start 8
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:52:29,017456001-05:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:52:29,020261611-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '960cedbc-4057-11ec-aba1-c5f3c07a3dc7', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_single_start', '8'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 960cedbc-4057-11ec-aba1-c5f3c07a3dc7 -- ceph config set osd osd_recovery_max_single_start 8
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:260 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_op_priority 63
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:52:33,205182542-05:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:52:33,207856774-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '960cedbc-4057-11ec-aba1-c5f3c07a3dc7', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_op_priority', '63'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 960cedbc-4057-11ec-aba1-c5f3c07a3dc7 -- ceph config set osd osd_recovery_op_priority 63
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:262 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config show-with-defaults osd.0
# ./benchmarks/bench-rados:263 - configure_ceph_pool() > grep 'osd_max_backfills\|osd_recovery'
# ./benchmarks/bench-rados:263 - configure_ceph_pool() > column -t -s ' '
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
osd_max_backfills                        32         mon
osd_recovery_cost                        20971520   default
osd_recovery_delay_start                 0.000000   default
osd_recovery_max_active                  32         mon
osd_recovery_max_active_hdd              3          default
osd_recovery_max_active_ssd              10         default
osd_recovery_max_chunk                   8388608    default
osd_recovery_max_omap_entries_per_chunk  8096       default
osd_recovery_max_single_start            8          mon
osd_recovery_op_priority                 3          default
osd_recovery_op_warn_multiple            16         default
osd_recovery_priority                    5          default
osd_recovery_retry_interval              30.000000  default
osd_recovery_sleep                       0.000000   default
osd_recovery_sleep_hdd                   0.100000   default
osd_recovery_sleep_hybrid                0.025000   default
osd_recovery_sleep_ssd                   0.000000   default
# ./benchmarks/bench-rados:265 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool create bench_rados 128 128 replicated --size 1 --pg-num-min 128
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:52:41,361376561-05:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:52:41,364379533-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '960cedbc-4057-11ec-aba1-c5f3c07a3dc7', '--', 'ceph', 'osd', 'pool', 'create', 'bench_rados', '128', '128', 'replicated', '--size', '1', '--pg-num-min', '128'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 960cedbc-4057-11ec-aba1-c5f3c07a3dc7 -- ceph osd pool create bench_rados 128 128 replicated --size 1 --pg-num-min 128
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
pool 'bench_rados' created
# ./benchmarks/bench-rados:267 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados min_size 1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:52:46,555912795-05:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:52:46,558715920-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '960cedbc-4057-11ec-aba1-c5f3c07a3dc7', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'min_size', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 960cedbc-4057-11ec-aba1-c5f3c07a3dc7 -- ceph osd pool set bench_rados min_size 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 min_size to 1
# ./benchmarks/bench-rados:268 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados pg_autoscale_mode off
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:52:50,866947577-05:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:52:50,869757456-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '960cedbc-4057-11ec-aba1-c5f3c07a3dc7', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'pg_autoscale_mode', 'off'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 960cedbc-4057-11ec-aba1-c5f3c07a3dc7 -- ceph osd pool set bench_rados pg_autoscale_mode off
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 pg_autoscale_mode to off
# ./benchmarks/bench-rados:269 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool application enable bench_rados benchmark
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:52:56,317757209-05:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:52:56,320555935-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '960cedbc-4057-11ec-aba1-c5f3c07a3dc7', '--', 'ceph', 'osd', 'pool', 'application', 'enable', 'bench_rados', 'benchmark'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 960cedbc-4057-11ec-aba1-c5f3c07a3dc7 -- ceph osd pool application enable bench_rados benchmark
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
enabled application 'benchmark' on pool 'bench_rados'
# ./benchmarks/bench-rados:270 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados noscrub 1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:53:00,569448101-05:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:53:00,572177527-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '960cedbc-4057-11ec-aba1-c5f3c07a3dc7', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'noscrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 960cedbc-4057-11ec-aba1-c5f3c07a3dc7 -- ceph osd pool set bench_rados noscrub 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 noscrub to 1
# ./benchmarks/bench-rados:271 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados nodeep-scrub 1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:53:05,499011976-05:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:53:05,501675157-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '960cedbc-4057-11ec-aba1-c5f3c07a3dc7', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'nodeep-scrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 960cedbc-4057-11ec-aba1-c5f3c07a3dc7 -- ceph osd pool set bench_rados nodeep-scrub 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 nodeep-scrub to 1
# ./benchmarks/bench-rados:272 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool ls detail
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:53:10,357924809-05:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:53:10,360837060-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '960cedbc-4057-11ec-aba1-c5f3c07a3dc7', '--', 'ceph', 'osd', 'pool', 'ls', 'detail'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 960cedbc-4057-11ec-aba1-c5f3c07a3dc7 -- ceph osd pool ls detail
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
pool 1 'device_health_metrics' replicated size 1 min_size 1 crush_rule 0 object_hash rjenkins pg_num 64 pgp_num 64 autoscale_mode on last_change 18 lfor 0/0/15 flags hashpspool stripe_width 0 pg_num_min 1 application mgr_devicehealth
pool 2 'bench_rados' replicated size 1 min_size 1 crush_rule 0 object_hash rjenkins pg_num 128 pgp_num 128 autoscale_mode off last_change 21 flags hashpspool,noscrub,nodeep-scrub stripe_width 0 pg_num_min 128 application benchmark

# ./benchmarks/bench-rados:273 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd df tree
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:53:14,541305836-05:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:53:14,544044460-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '960cedbc-4057-11ec-aba1-c5f3c07a3dc7', '--', 'ceph', 'osd', 'df', 'tree'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 960cedbc-4057-11ec-aba1-c5f3c07a3dc7 -- ceph osd df tree
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
ID  CLASS  WEIGHT   REWEIGHT  SIZE     RAW USE  DATA     OMAP  META     AVAIL    %USE  VAR   PGS  STATUS  TYPE NAME      
-1         0.09769         -  100 GiB  5.7 MiB  176 KiB   0 B  5.6 MiB  100 GiB  0.01  1.00    -          root default   
-3         0.09769         -  100 GiB  5.7 MiB  176 KiB   0 B  5.6 MiB  100 GiB  0.01  1.00    -              host node-1
 0    ssd  0.09769   1.00000  100 GiB  5.7 MiB  176 KiB   0 B  5.6 MiB  100 GiB  0.01  1.00  192      up          osd.0  
                       TOTAL  100 GiB  5.7 MiB  176 KiB   0 B  5.6 MiB  100 GiB  0.01                                    
MIN/MAX VAR: 1.00/1.00  STDDEV: 0


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:53:18,450456213-05:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:53:42,469053789-05:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:53:51,583032283-05:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:54:00,797162370-05:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:54:00,804639678-05:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:54:09,932057724-05:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:54:09,938634695-05:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:54:19,002161959-05:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:54:19,008855249-05:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:54:28,091105923-05:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:54:28,097989501-05:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:54:37,485763908-05:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:54:37,492561655-05:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:54:37,497855346-05:00] INFO: > The cluster is idle now.[0m


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:54:37,501114842-05:00] STAGE: Start benchmarking of mode write ...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:54:37,507104556-05:00] INFO: > Start system activity collection services[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:54:37,512036705-05:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:341 - rados_bench() > rm --force /mnt/sda8/ceph-research/scripts/output/sys_activity_64KB_write.dat.rados_bench_host.2
# ./benchmarks/bench-rados:342 - rados_bench() > sar_pid_rados_bench_host=178925
# ./benchmarks/bench-rados:342 - rados_bench() > S_TIME_FORMAT=ISO
# ./benchmarks/bench-rados:342 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/output/sys_activity_64KB_write.dat.rados_bench_host.2 2

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:54:37,518258847-05:00] INFO: >> for OSD host[0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:54:37,526686437-05:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.2 bash
10.10.2.2: b'34297\n'
[1] 00:54:37 [SUCCESS] ljishen@10.10.2.2
34297

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:54:37,712682204-05:00] INFO: > Run rados bench[0m
# ./benchmarks/bench-rados:370 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/output/rados_bench_64KB_write.log.2
## ./benchmarks/bench-rados:361 - rados_bench() > count_cpus 32-63,96-127
# ./benchmarks/bench-rados:361 - rados_bench() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 60 write --pool bench_rados -b 65536 -O 65536 --concurrent-ios 128 --show-time --write-object --write-omap --write-xattr --no-cleanup
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:54:37,731423441-05:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:54:37,733992655-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '960cedbc-4057-11ec-aba1-c5f3c07a3dc7', '--', 'rados', 'bench', '60', 'write', '--pool', 'bench_rados', '-b', '65536', '-O', '65536', '--concurrent-ios', '128', '--show-time', '--write-object', '--write-omap', '--write-xattr', '--no-cleanup'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 960cedbc-4057-11ec-aba1-c5f3c07a3dc7 -- rados bench 60 write --pool bench_rados -b 65536 -O 65536 --concurrent-ios 128 --show-time --write-object --write-omap --write-xattr --no-cleanup
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
hints = 1
2021-11-08T05:54:41.146111+0000 Maintaining 128 concurrent writes of 65536 bytes to objects of size 65536 for up to 60 seconds or 0 objects
2021-11-08T05:54:41.146122+0000 Object prefix: benchmark_data_node-0.bluefield2.ucsc-cmps10_7
2021-11-08T05:54:41.149240+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-11-08T05:54:41.149240+0000     0       0         0         0         0         0           -           0
2021-11-08T05:54:42.149360+0000     1     128      4513      4385   274.048   274.062   0.0285762   0.0286534
2021-11-08T05:54:43.149457+0000     2     128      8535      8407   262.699   251.375   0.0717239   0.0301666
2021-11-08T05:54:44.149540+0000     3     128     10273     10145   211.338   108.625   0.0693711   0.0374713
2021-11-08T05:54:45.149617+0000     4     128     13217     13089     204.5       184   0.0290147   0.0389625
2021-11-08T05:54:46.149689+0000     5     128     16801     16673   208.397       224   0.0745274   0.0380683
2021-11-08T05:54:47.149765+0000     6     128     17751     17623   183.559    59.375    0.125471   0.0428894
2021-11-08T05:54:48.149839+0000     7     128     18775     18647   166.478        64     0.13298   0.0476189
2021-11-08T05:54:49.149915+0000     8     128     20641     20513   160.246   116.625   0.0585238   0.0497452
2021-11-08T05:54:50.149983+0000     9     128     22615     22487   156.148   123.375   0.0647797   0.0510863
2021-11-08T05:54:51.150049+0000    10     128     24919     24791   154.932       144   0.0703514     0.05139
2021-11-08T05:54:52.150126+0000    11     128     26711     26583   151.029       112   0.0751948   0.0528483
2021-11-08T05:54:53.150214+0000    12     128     30369     30241   157.493   228.625   0.0287642   0.0507545
2021-11-08T05:54:54.150295+0000    13     128     33111     32983    158.56   171.375    0.129939   0.0500677
2021-11-08T05:54:55.150371+0000    14     128     34135     34007   151.805        64    0.134642   0.0525611
2021-11-08T05:54:56.150443+0000    15     128     35105     34977   145.726    60.625     0.12793    0.054702
2021-11-08T05:54:57.150514+0000    16     128     37207     37079   144.829   131.375   0.0655637   0.0551305
2021-11-08T05:54:58.150587+0000    17     128     39127     38999   143.368       120   0.0660951   0.0556743
2021-11-08T05:54:59.150662+0000    18     128     40791     40663    141.18       104    0.132381   0.0564781
2021-11-08T05:55:00.150736+0000    19     128     41761     41633    136.94    60.625     0.12936   0.0582392
2021-11-08T05:55:01.150812+0000 min lat: 0.0201462 max lat: 0.146093 avg lat: 0.0599037
2021-11-08T05:55:01.150812+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-11-08T05:55:01.150812+0000    20     128     42711     42583   133.062    59.375    0.130389   0.0599037
2021-11-08T05:55:02.150900+0000    21     127     45546     45419   135.165    177.25   0.0282895   0.0591381
2021-11-08T05:55:03.150999+0000    22     128     49045     48917   138.958   218.625   0.0739619   0.0574557
2021-11-08T05:55:04.151072+0000    23     128     50069     49941   135.699        64    0.122752    0.058792
2021-11-08T05:55:05.151143+0000    24     128     51093     50965   132.711        64    0.129416   0.0601756
2021-11-08T05:55:06.151213+0000    25     128     52951     52823   132.047   116.125   0.0587208   0.0605189
2021-11-08T05:55:07.151287+0000    26     128     54871     54743   131.584       120   0.0662963   0.0606963
2021-11-08T05:55:08.151362+0000    27     128     56725     56597   131.002   115.875   0.0956967   0.0609407
2021-11-08T05:55:09.151432+0000    28     127     57687     57560   128.472   60.1875    0.126665   0.0620652
2021-11-08T05:55:10.151504+0000    29     128     58711     58583   126.247   63.9375    0.131092   0.0632186
2021-11-08T05:55:11.151575+0000    30     128     60053     59925   124.834    83.875   0.0633998   0.0639934
2021-11-08T05:55:12.151648+0000    31     128     62039     61911   124.811   124.125   0.0658775   0.0640283
2021-11-08T05:55:13.151724+0000    32     128     63959     63831   124.661       120   0.0680524   0.0641124
2021-11-08T05:55:14.151794+0000    33     128     65301     65173   123.424    83.875    0.126641   0.0646519
2021-11-08T05:55:15.151874+0000    34     128     66325     66197   121.676        64    0.135517    0.065657
2021-11-08T05:55:16.151959+0000    35     128     67287     67159   119.918    60.125    0.130027   0.0665842
2021-11-08T05:55:17.152030+0000    36     128     69269     69141   120.027   123.875   0.0573723   0.0666156
2021-11-08T05:55:18.152100+0000    37     128     71061     70933    119.81       112   0.0680387   0.0666743
2021-11-08T05:55:19.152172+0000    38     128     72853     72725   119.604       112    0.107528   0.0667588
2021-11-08T05:55:20.152247+0000    39     128     73815     73687   118.079    60.125    0.130376   0.0675886
2021-11-08T05:55:21.152321+0000 min lat: 0.0201462 max lat: 0.146093 avg lat: 0.0684412
2021-11-08T05:55:21.152321+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-11-08T05:55:21.152321+0000    40     128     74839     74711   116.727        64    0.130416   0.0684412
2021-11-08T05:55:22.152408+0000    41     128     76181     76053   115.926    83.875   0.0593492   0.0689611
2021-11-08T05:55:23.152485+0000    42     128     78229     78101   116.213       128   0.0649365    0.068756
2021-11-08T05:55:24.152563+0000    43     128     80215     80087   116.397   124.125    0.063413   0.0686624
2021-11-08T05:55:25.152638+0000    44     128     81557     81429   115.657    83.875    0.127884   0.0690918
2021-11-08T05:55:26.152709+0000    45     128     82519     82391   114.423    60.125    0.124214   0.0697927
2021-11-08T05:55:27.152780+0000    46     128     83605     83477   113.411    67.875    0.106691   0.0704785
2021-11-08T05:55:28.152851+0000    47     128     85719     85591   113.809   132.125   0.0805503    0.070215
2021-11-08T05:55:29.152922+0000    48     128     87701     87573   114.019   123.875   0.0759838   0.0700899
2021-11-08T05:55:30.152999+0000    49     128     89303     89175   113.735   100.125    0.129383   0.0702734
2021-11-08T05:55:31.153081+0000    50     128     90261     90133   112.658    59.875    0.131814   0.0708798
2021-11-08T05:55:32.153152+0000    51     128     91285     91157   111.704        64    0.126516   0.0715403
2021-11-08T05:55:33.153223+0000    52     128     93015     92887   111.635   108.125   0.0583447   0.0715924
2021-11-08T05:55:34.153295+0000    53     128     94997     94869   111.865   123.875   0.0667335    0.071466
2021-11-08T05:55:35.153369+0000    54     128     96917     96789   112.016       120   0.0655512   0.0713603
2021-11-08T05:55:36.153443+0000    55     128     97941     97813   111.143        64    0.127271   0.0718913
2021-11-08T05:55:37.153515+0000    56     128     98903     98775   110.232    60.125    0.129432   0.0724575
2021-11-08T05:55:38.153585+0000    57     128    100245    100117   109.769    83.875   0.0555279   0.0728503
2021-11-08T05:55:39.153661+0000    58     128    102293    102165   110.083       128   0.0657972   0.0726266
2021-11-08T05:55:40.153733+0000    59     128    104213    104085   110.251       120   0.0655625   0.0724983
2021-11-08T05:55:41.153811+0000 min lat: 0.0201462 max lat: 0.146093 avg lat: 0.0727314
2021-11-08T05:55:41.153811+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-11-08T05:55:41.153811+0000    60     128    105621    105493    109.88        88     0.12342   0.0727314
2021-11-08T05:55:42.153942+0000 Total time run:         60.1103
Total writes made:      105621
Write size:             65536
Object size:            65536
Bandwidth (MB/sec):     109.82
Stddev Bandwidth:       51.069
Max bandwidth (MB/sec): 274.062
Min bandwidth (MB/sec): 59.375
Average IOPS:           1757
Stddev IOPS:            817.104
Max IOPS:               4385
Min IOPS:               950
Average Latency(s):     0.0728025
Stddev Latency(s):      0.0355392
Max latency(s):         0.146093
Min latency(s):         0.0201462

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:55:42,780609502-05:00] INFO: > Stop system activity collection services[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:55:42,785760213-05:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:391 - rados_bench() > kill -INT 178925

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:55:42,790916095-05:00] INFO: >> for OSD host[0m
# ./benchmarks/bench-rados:396 - rados_bench() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.2 kill -INT 34297
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:55:42,798277455-05:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.2 kill -INT 34297
[1] 00:55:42 [SUCCESS] ljishen@10.10.2.2

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:55:42,981748472-05:00] INFO: >>> Copy system activity file from OSD host[0m
## ./benchmarks/bench-rados:403 - rados_bench() > basename -- /tmp/bench-rados/sys_activity_64KB_write.dat.osd_host.2
# ./benchmarks/bench-rados:403 - rados_bench() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.2:/tmp/bench-rados/sys_activity_64KB_write.dat.osd_host.2 /mnt/sda8/ceph-research/scripts/output/sys_activity_64KB_write.dat.osd_host.2


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:55:43,151100146-05:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:56:07,246814072-05:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 105.62k objects, 6.4 GiB
    usage:   20 GiB used, 80 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:56:07,253715885-05:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:56:16,440834531-05:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 105.62k objects, 6.4 GiB
    usage:   20 GiB used, 80 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:56:16,447821205-05:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:56:25,501327828-05:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 105.62k objects, 6.4 GiB
    usage:   20 GiB used, 80 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:56:25,508377019-05:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:56:34,592887967-05:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 105.62k objects, 6.4 GiB
    usage:   20 GiB used, 80 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:56:34,599698949-05:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:56:43,722274052-05:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 105.62k objects, 6.4 GiB
    usage:   20 GiB used, 80 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:56:43,729004953-05:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:56:43,734249983-05:00] INFO: > The cluster is idle now.[0m


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:56:43,737538684-05:00] STAGE: Start benchmarking of mode seq ...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:56:43,744028670-05:00] INFO: > Start system activity collection services[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:56:43,749139407-05:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:341 - rados_bench() > rm --force /mnt/sda8/ceph-research/scripts/output/sys_activity_64KB_seq.dat.rados_bench_host.2
# ./benchmarks/bench-rados:342 - rados_bench() > sar_pid_rados_bench_host=180290
# ./benchmarks/bench-rados:342 - rados_bench() > S_TIME_FORMAT=ISO
# ./benchmarks/bench-rados:342 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/output/sys_activity_64KB_seq.dat.rados_bench_host.2 2

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:56:43,755595650-05:00] INFO: >> for OSD host[0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:56:43,764188803-05:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.2 bash
10.10.2.2: b'34402\n'
[1] 00:56:43 [SUCCESS] ljishen@10.10.2.2
34402

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:56:43,948302472-05:00] INFO: > Run rados bench[0m
# ./benchmarks/bench-rados:379 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/output/rados_bench_64KB_seq.log.2
## ./benchmarks/bench-rados:376 - rados_bench() > count_cpus 32-63,96-127
# ./benchmarks/bench-rados:376 - rados_bench() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 128 --show-time
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:56:43,966859803-05:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:56:43,969451930-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '960cedbc-4057-11ec-aba1-c5f3c07a3dc7', '--', 'rados', 'bench', '99999999', 'seq', '--pool', 'bench_rados', '--concurrent-ios', '128', '--show-time'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 960cedbc-4057-11ec-aba1-c5f3c07a3dc7 -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 128 --show-time
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
hints = 1
2021-11-08T05:56:47.074672+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-11-08T05:56:47.074672+0000     0       0         0         0         0         0           -           0
2021-11-08T05:56:48.074812+0000     1     127     17402     17275   1079.46   1079.69  0.00838769  0.00737476
2021-11-08T05:56:49.074909+0000     2     127     34809     34682   1083.65   1087.94  0.00734801  0.00735906
2021-11-08T05:56:50.074996+0000     3     128     52171     52043   1084.09   1085.06  0.00809413  0.00735963
2021-11-08T05:56:51.075072+0000     4     128     69576     69448      1085   1087.81  0.00807589  0.00735646
2021-11-08T05:56:52.075148+0000     5     127     86839     86712   1083.78      1079  0.00747348  0.00736793
2021-11-08T05:56:53.075233+0000     6     127    103903    103776   1080.89    1066.5  0.00718037   0.0073876
2021-11-08T05:56:54.075346+0000 Total time run:       6.10718
Total reads made:     105621
Read size:            65536
Object size:          65536
Bandwidth (MB/sec):   1080.91
Average IOPS:         17294
Stddev IOPS:          129.396
Max IOPS:             17407
Min IOPS:             17064
Average Latency(s):   0.00738852
Max latency(s):       0.0127152
Min latency(s):       0.00166473

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:56:54,804732143-05:00] INFO: > Stop system activity collection services[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:56:54,810369593-05:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:391 - rados_bench() > kill -INT 180290

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:56:54,815704242-05:00] INFO: >> for OSD host[0m
# ./benchmarks/bench-rados:396 - rados_bench() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.2 kill -INT 34402
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:56:54,822970973-05:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.2 kill -INT 34402
[1] 00:56:54 [SUCCESS] ljishen@10.10.2.2

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:56:55,008527282-05:00] INFO: >>> Copy system activity file from OSD host[0m
## ./benchmarks/bench-rados:403 - rados_bench() > basename -- /tmp/bench-rados/sys_activity_64KB_seq.dat.osd_host.2
# ./benchmarks/bench-rados:403 - rados_bench() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.2:/tmp/bench-rados/sys_activity_64KB_seq.dat.osd_host.2 /mnt/sda8/ceph-research/scripts/output/sys_activity_64KB_seq.dat.osd_host.2


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:56:55,157284176-05:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:57:19,167405913-05:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 105.62k objects, 6.4 GiB
    usage:   20 GiB used, 80 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:57:19,174755220-05:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:57:28,253048896-05:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 105.62k objects, 6.4 GiB
    usage:   20 GiB used, 80 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:57:28,260552404-05:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:57:37,444549220-05:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 105.62k objects, 6.4 GiB
    usage:   20 GiB used, 80 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:57:37,451457947-05:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:57:46,514389095-05:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 105.62k objects, 6.4 GiB
    usage:   20 GiB used, 80 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:57:46,521209325-05:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:57:55,521616389-05:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 105.62k objects, 6.4 GiB
    usage:   20 GiB used, 80 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:57:55,528509506-05:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:57:55,533928784-05:00] INFO: > The cluster is idle now.[0m


[1;7;39;49m[2021-11-08T00:57:55,535990101-05:00][RUNNING][ROUND 3/3/21] object_size=64KB[0m


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:57:55,539064397-05:00] STAGE: Launch a Ceph cluster (CEPH_HOST: 10.10.2.3, OSD_HOST: 10.10.2.2) ...[0m
# ./benchmarks/bench-rados:183 - launch_ceph_cluster() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.3 bash -euo pipefail
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:57:55,547985228-05:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.3 bash -euo pipefail
10.10.2.3: b'\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T00:57:55,902292977-05:00] INFO: > Remove existing clusters\x1b[0m\n'
10.10.2.3: b'## bash:17 -  > basename -- /var/lib/ceph/960cedbc-4057-11ec-aba1-c5f3c07a3dc7\n'
10.10.2.3: b'# bash:17 -  > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm --host 10.10.2.3 --host 10.10.2.2 rm-cluster --force --fsid 960cedbc-4057-11ec-aba1-c5f3c07a3dc7\n'
10.10.2.3: b"\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T00:57:55,912540792-05:00] DEBUG: HOST_PARAMS: ['--host', '10.10.2.3', '--host', '10.10.2.2']\x1b[0m\n"
10.10.2.3: b"\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T00:57:55,916231024-05:00] DEBUG: CEPHADM_PARAMS: ['rm-cluster', '--force', '--fsid', '960cedbc-4057-11ec-aba1-c5f3c07a3dc7']\x1b[0m\n"
10.10.2.3: b"# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:63 -  > /tmp/bench-rados/ceph-research/scripts/parallel-ssh --host 10.10.2.3 --host 10.10.2.2 'sudo python3 -u - rm-cluster --force --fsid 960cedbc-4057-11ec-aba1-c5f3c07a3dc7'\n"
10.10.2.3: b'\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T00:57:55,924484977-05:00] DEBUG: command from stdin\x1b[0m\n'
10.10.2.3: b"# /tmp/bench-rados/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host 10.10.2.3 --host 10.10.2.2 'sudo python3 -u - rm-cluster --force --fsid 960cedbc-4057-11ec-aba1-c5f3c07a3dc7'\n"
10.10.2.3: b'[1] 00:58:01 [SUCCESS] 10.10.2.3\n[2] 00:58:07 [SUCCESS] 10.10.2.2\n'
10.10.2.3: b'\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T00:58:07,202473880-05:00] INFO: > Deploy a new cluster\x1b[0m\n'
10.10.2.3: b'# bash:24 -  > /tmp/bench-rados/ceph-research/scripts/ceph-deploy -m 10.10.2.3 -o node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/10.10.2.2:/dev/nvme0n1\n'
10.10.2.3: b'\n\n\x1b[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T00:58:07,213337848-05:00] STAGE: Verify passwordless SSH to hosts, passwordless sudo via ljishen, and the existence of devices on hosts...\x1b[0m\n'
10.10.2.3: b'\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T00:58:07,217683877-05:00] INFO: > Check the host for the monitor\x1b[0m\n'
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:181 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@10.10.2.3 sudo --non-interactive --validate\n'
10.10.2.3: b'\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T00:58:07,366376891-05:00] INFO: > Check hosts for OSDs\x1b[0m\n'
10.10.2.3: b"\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T00:58:07,370896017-05:00] INFO: >> Check host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'\x1b[0m\n"
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:200 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us sudo --non-interactive --validate\n'
10.10.2.3: b"\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T00:58:07,521891204-05:00] INFO: >>> Check device '/dev/nvme0n1'\x1b[0m\n"
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:215 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us test -b /dev/nvme0n1\n'
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:226 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us lsblk --nodeps --noheadings --output TYPE /dev/nvme0n1\n'
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:227 - verify_parameters() > grep --word-regexp disk\n'
10.10.2.3: b'\n\n\x1b[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T00:58:07,802237952-05:00] STAGE: Reset devices on hosts...\x1b[0m\n'
10.10.2.3: b"\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T00:58:07,807093022-05:00] INFO: > Resetting device '/dev/nvme0n1' on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'\x1b[0m\n"
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:288 - reset_host_devices() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us sudo sh\n'
10.10.2.3: b'  Removing ceph--a48e4ee1--a773--400d--9f88--4950e097a143-osd--block--b76f6e6c--64c1--4660--a353--f00f7a44bbea (253:0)\n'
10.10.2.3: b'  Archiving volume group "ceph-a48e4ee1-a773-400d-9f88-4950e097a143" metadata (seqno 5).\n  Releasing logical volume "osd-block-b76f6e6c-64c1-4660-a353-f00f7a44bbea"\n'
10.10.2.3: b'  Creating volume group backup "/etc/lvm/backup/ceph-a48e4ee1-a773-400d-9f88-4950e097a143" (seqno 6).\n'
10.10.2.3: b'  Logical volume "osd-block-b76f6e6c-64c1-4660-a353-f00f7a44bbea" successfully removed\n'
10.10.2.3: b'  Removing physical volume "/dev/nvme0n1" from volume group "ceph-a48e4ee1-a773-400d-9f88-4950e097a143"\n'
10.10.2.3: b'  Volume group "ceph-a48e4ee1-a773-400d-9f88-4950e097a143" successfully removed\n'
10.10.2.3: b'\n\n\x1b[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T00:58:08,187610155-05:00] STAGE: Bootstrap a new cluster...\x1b[0m\n'
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:317 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.3 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults\n'
10.10.2.3: b'\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T00:58:08,197220657-05:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.3: b"\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T00:58:08,200499804-05:00] DEBUG: CEPHADM_PARAMS: ['bootstrap', '--config', '/tmp/bench-rados/ceph-research/scripts/ceph.conf', '--mon-ip', '10.10.2.3', '--output-dir', '/tmp/bench-rados/deployment_data_root/etc/ceph', '--ssh-user', 'ljishen', '--skip-dashboard', '--no-minimize-config', '--allow-overwrite', '--allow-fqdn-hostname', '--orphan-initial-daemons', '--skip-monitoring-stack', '--single-host-defaults']\x1b[0m\n"
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.3 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults\n'
10.10.2.3: b'Verifying podman|docker is present...\nVerifying lvm2 is present...\n'
10.10.2.3: b'Verifying time synchronization is in place...\n'
10.10.2.3: b'Unit ntp.service is enabled and running\n'
10.10.2.3: b'Repeating the final host check...\n'
10.10.2.3: b'docker (/usr/bin/docker) is present\n'
10.10.2.3: b'systemctl is present\n'
10.10.2.3: b'lvcreate is present\n'
10.10.2.3: b'Unit ntp.service is enabled and running\n'
10.10.2.3: b'Host looks OK\n'
10.10.2.3: b'Cluster fsid: d90784d2-4058-11ec-aba1-c5f3c07a3dc7\n'
10.10.2.3: b'Verifying IP 10.10.2.3 port 3300 ...\n'
10.10.2.3: b'Verifying IP 10.10.2.3 port 6789 ...\n'
10.10.2.3: b'Mon IP `10.10.2.3` is in CIDR network `10.10.2.0/24`\n'
10.10.2.3: b'- internal network (--cluster-network) has not been provided, OSD replication will default to the public_network\n'
10.10.2.3: b'Adjusting default settings to suit single-host cluster...\n'
10.10.2.3: b'Pulling container image quay.io/ceph/ceph:v16...\n'
10.10.2.3: b'Ceph version: ceph version 16.2.6 (ee28fb57e47e9f88813e24bbf4c14496ca299d31) pacific (stable)\n'
10.10.2.3: b'Extracting ceph user uid/gid from container image...\n'
10.10.2.3: b'Creating initial keys...\n'
10.10.2.3: b'Creating initial monmap...\n'
10.10.2.3: b'Creating mon...\n'
10.10.2.3: b'Waiting for mon to start...\n'
10.10.2.3: b'Waiting for mon...\n'
10.10.2.3: b'mon is available\n'
10.10.2.3: b'Setting mon public_network to 10.10.2.0/24\n'
10.10.2.3: b'Wrote config to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf\n'
10.10.2.3: b'Wrote keyring to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring\n'
10.10.2.3: b'Creating mgr...\n'
10.10.2.3: b'Verifying port 9283 ...\n'
10.10.2.3: b'Waiting for mgr to start...\nWaiting for mgr...\n'
10.10.2.3: b'mgr not available, waiting (1/15)...\n'
10.10.2.3: b'mgr not available, waiting (2/15)...\n'
10.10.2.3: b'mgr is available\n'
10.10.2.3: b'Enabling cephadm module...\n'
10.10.2.3: b'Waiting for the mgr to restart...\n'
10.10.2.3: b'Waiting for mgr epoch 5...\n'
10.10.2.3: b'mgr epoch 5 is available\n'
10.10.2.3: b'Setting orchestrator backend to cephadm...\n'
10.10.2.3: b'Generating ssh key...\n'
10.10.2.3: b'Wrote public SSH key to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub\n'
10.10.2.3: b'Adding key to ljishen@localhost authorized_keys...\n'
10.10.2.3: b'Adding host node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us...\n'
10.10.2.3: b'Deploying unmanaged mon service...\n'
10.10.2.3: b'Deploying unmanaged mgr service...\n'
10.10.2.3: b'You can access the Ceph CLI with:\n\n\tsudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --fsid d90784d2-4058-11ec-aba1-c5f3c07a3dc7 -c /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf -k /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring\n\n'
10.10.2.3: b'Please consider enabling telemetry to help improve Ceph:\n\n\tceph telemetry on\n\nFor more information see:\n\n\thttps://docs.ceph.com/docs/pacific/mgr/telemetry/\n\n'
10.10.2.3: b'Bootstrap complete.\n'
10.10.2.3: b'\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T00:59:14,570019467-05:00] INFO: Sleep for 20 seconds\x1b[0m\n'
10.10.2.3: b'\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T00:59:34,577812983-05:00] INFO: > Set the default number of monitor daemons to 1\x1b[0m\n'
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:337 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1\n'
10.10.2.3: b'\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T00:59:34,588690325-05:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.3: b"\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T00:59:34,592388102-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mon', '1']\x1b[0m\n"
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1\n'
10.10.2.3: b'Inferring fsid d90784d2-4058-11ec-aba1-c5f3c07a3dc7\n'
10.10.2.3: b'Inferring config /var/lib/ceph/d90784d2-4058-11ec-aba1-c5f3c07a3dc7/mon.node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.3: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.3: b'Scheduled mon update...\n'
10.10.2.3: b'\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T00:59:44,215673618-05:00] INFO: > Set the default number of manager daemons to 1\x1b[0m\n'
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:342 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1\n'
10.10.2.3: b'\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T00:59:44,225263398-05:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.3: b"\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T00:59:44,228175561-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mgr', '1']\x1b[0m\n"
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1\n'
10.10.2.3: b'Inferring fsid d90784d2-4058-11ec-aba1-c5f3c07a3dc7\n'
10.10.2.3: b'Inferring config /var/lib/ceph/d90784d2-4058-11ec-aba1-c5f3c07a3dc7/mon.node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.3: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.3: b'Scheduled mgr update...\n'
10.10.2.3: b'\n\n\x1b[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T00:59:54,394394674-05:00] STAGE: Add hosts to the cluster...\x1b[0m\n'
10.10.2.3: b"\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T00:59:54,400060852-05:00] INFO: > Adding host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'\x1b[0m\n"
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:361 - add_hosts() > ssh-copy-id -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -f -i /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\n'
10.10.2.3: b'/usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: "/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub"\n'
10.10.2.3: b'\nNumber of key(s) added: 1\n\nNow try logging into the machine, with:   "ssh -o \'GlobalKnownHostsFile=/dev/null\' -o \'LogLevel=ERROR\' -o \'PasswordAuthentication=no\' -o \'StrictHostKeyChecking=no\' -o \'UserKnownHostsFile=/dev/null\' \'ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\'"\nand check to make sure that only the key(s) you wanted were added.\n\n'
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:367 - add_hosts() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us 10.10.2.2\n'
10.10.2.3: b'\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T00:59:54,604388067-05:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.3: b"\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T00:59:54,608062689-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'host', 'add', 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us', '10.10.2.2']\x1b[0m\n"
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us 10.10.2.2\n'
10.10.2.3: b'Inferring fsid d90784d2-4058-11ec-aba1-c5f3c07a3dc7\n'
10.10.2.3: b'Inferring config /var/lib/ceph/d90784d2-4058-11ec-aba1-c5f3c07a3dc7/mon.node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.3: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.3: b"Added host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us' with addr '10.10.2.2'\n"
10.10.2.3: b'\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T01:00:04,693204569-05:00] INFO: Sleep for 20 seconds\x1b[0m\n'
10.10.2.3: b'\n\n\x1b[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T01:00:24,698224595-05:00] STAGE: Add OSDs to the cluster...\x1b[0m\n'
10.10.2.3: b"\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T01:00:24,705115714-05:00] INFO: > Adding devices ['/dev/nvme0n1'] on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us' as OSDs\x1b[0m\n"
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:388 - add_host_devices_as_osds() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1\n'
10.10.2.3: b'\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T01:00:24,715548975-05:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.3: b"\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T01:00:24,719752604-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'daemon', 'add', 'osd', 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1']\x1b[0m\n"
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1\n'
10.10.2.3: b'Inferring fsid d90784d2-4058-11ec-aba1-c5f3c07a3dc7\n'
10.10.2.3: b'Inferring config /var/lib/ceph/d90784d2-4058-11ec-aba1-c5f3c07a3dc7/mon.node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.3: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.3: b"Created osd(s) 0 on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'\n"
10.10.2.3: b'\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T01:00:50,188651309-05:00] INFO: > Sleep for 20 seconds\x1b[0m\n'
10.10.2.3: b'\n\n\x1b[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T01:01:10,194532937-05:00] STAGE: Show Cluster Status\x1b[0m\n'
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:408 - show_status() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status\n'
10.10.2.3: b'\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T01:01:10,204516851-05:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.3: b"\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T01:01:10,208060636-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', '--status']\x1b[0m\n"
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status\n'
10.10.2.3: b'Inferring fsid d90784d2-4058-11ec-aba1-c5f3c07a3dc7\n'
10.10.2.3: b'Inferring config /var/lib/ceph/d90784d2-4058-11ec-aba1-c5f3c07a3dc7/mon.node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.3: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.3: b'  cluster:\n    id:     d90784d2-4058-11ec-aba1-c5f3c07a3dc7\n    health: HEALTH_WARN\n            1 pool(s) have no replicas configured\n \n  services:\n    mon: 1 daemons, quorum node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us (age 2m)\n    mgr: node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us.vmrdek(active, since 2m)\n    osd: 1 osds: 1 up (since 18s), 1 in (since 41s)\n \n  data:\n    pools:   1 pools, 1 pgs\n    objects: 0 objects, 0 B\n    usage:   4.7 MiB used, 100 GiB / 100 GiB avail\n    pgs:     1 active+clean\n \n'
[1] 01:01:19 [SUCCESS] ljishen@10.10.2.3

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:57:55,902292977-05:00] INFO: > Remove existing clusters[0m
## bash:17 -  > basename -- /var/lib/ceph/960cedbc-4057-11ec-aba1-c5f3c07a3dc7
# bash:17 -  > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm --host 10.10.2.3 --host 10.10.2.2 rm-cluster --force --fsid 960cedbc-4057-11ec-aba1-c5f3c07a3dc7
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:57:55,912540792-05:00] DEBUG: HOST_PARAMS: ['--host', '10.10.2.3', '--host', '10.10.2.2'][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:57:55,916231024-05:00] DEBUG: CEPHADM_PARAMS: ['rm-cluster', '--force', '--fsid', '960cedbc-4057-11ec-aba1-c5f3c07a3dc7'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:63 -  > /tmp/bench-rados/ceph-research/scripts/parallel-ssh --host 10.10.2.3 --host 10.10.2.2 'sudo python3 -u - rm-cluster --force --fsid 960cedbc-4057-11ec-aba1-c5f3c07a3dc7'
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:57:55,924484977-05:00] DEBUG: command from stdin[0m
# /tmp/bench-rados/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host 10.10.2.3 --host 10.10.2.2 'sudo python3 -u - rm-cluster --force --fsid 960cedbc-4057-11ec-aba1-c5f3c07a3dc7'
[1] 00:58:01 [SUCCESS] 10.10.2.3
[2] 00:58:07 [SUCCESS] 10.10.2.2

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:58:07,202473880-05:00] INFO: > Deploy a new cluster[0m
# bash:24 -  > /tmp/bench-rados/ceph-research/scripts/ceph-deploy -m 10.10.2.3 -o node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/10.10.2.2:/dev/nvme0n1


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:58:07,213337848-05:00] STAGE: Verify passwordless SSH to hosts, passwordless sudo via ljishen, and the existence of devices on hosts...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:58:07,217683877-05:00] INFO: > Check the host for the monitor[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:181 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@10.10.2.3 sudo --non-interactive --validate

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:58:07,366376891-05:00] INFO: > Check hosts for OSDs[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:58:07,370896017-05:00] INFO: >> Check host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:200 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us sudo --non-interactive --validate

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:58:07,521891204-05:00] INFO: >>> Check device '/dev/nvme0n1'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:215 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us test -b /dev/nvme0n1
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:226 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us lsblk --nodeps --noheadings --output TYPE /dev/nvme0n1
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:227 - verify_parameters() > grep --word-regexp disk


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:58:07,802237952-05:00] STAGE: Reset devices on hosts...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:58:07,807093022-05:00] INFO: > Resetting device '/dev/nvme0n1' on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:288 - reset_host_devices() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us sudo sh
  Removing ceph--a48e4ee1--a773--400d--9f88--4950e097a143-osd--block--b76f6e6c--64c1--4660--a353--f00f7a44bbea (253:0)
  Archiving volume group "ceph-a48e4ee1-a773-400d-9f88-4950e097a143" metadata (seqno 5).
  Releasing logical volume "osd-block-b76f6e6c-64c1-4660-a353-f00f7a44bbea"
  Creating volume group backup "/etc/lvm/backup/ceph-a48e4ee1-a773-400d-9f88-4950e097a143" (seqno 6).
  Logical volume "osd-block-b76f6e6c-64c1-4660-a353-f00f7a44bbea" successfully removed
  Removing physical volume "/dev/nvme0n1" from volume group "ceph-a48e4ee1-a773-400d-9f88-4950e097a143"
  Volume group "ceph-a48e4ee1-a773-400d-9f88-4950e097a143" successfully removed


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:58:08,187610155-05:00] STAGE: Bootstrap a new cluster...[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:317 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.3 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:58:08,197220657-05:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:58:08,200499804-05:00] DEBUG: CEPHADM_PARAMS: ['bootstrap', '--config', '/tmp/bench-rados/ceph-research/scripts/ceph.conf', '--mon-ip', '10.10.2.3', '--output-dir', '/tmp/bench-rados/deployment_data_root/etc/ceph', '--ssh-user', 'ljishen', '--skip-dashboard', '--no-minimize-config', '--allow-overwrite', '--allow-fqdn-hostname', '--orphan-initial-daemons', '--skip-monitoring-stack', '--single-host-defaults'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.3 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults
Verifying podman|docker is present...
Verifying lvm2 is present...
Verifying time synchronization is in place...
Unit ntp.service is enabled and running
Repeating the final host check...
docker (/usr/bin/docker) is present
systemctl is present
lvcreate is present
Unit ntp.service is enabled and running
Host looks OK
Cluster fsid: d90784d2-4058-11ec-aba1-c5f3c07a3dc7
Verifying IP 10.10.2.3 port 3300 ...
Verifying IP 10.10.2.3 port 6789 ...
Mon IP `10.10.2.3` is in CIDR network `10.10.2.0/24`
- internal network (--cluster-network) has not been provided, OSD replication will default to the public_network
Adjusting default settings to suit single-host cluster...
Pulling container image quay.io/ceph/ceph:v16...
Ceph version: ceph version 16.2.6 (ee28fb57e47e9f88813e24bbf4c14496ca299d31) pacific (stable)
Extracting ceph user uid/gid from container image...
Creating initial keys...
Creating initial monmap...
Creating mon...
Waiting for mon to start...
Waiting for mon...
mon is available
Setting mon public_network to 10.10.2.0/24
Wrote config to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf
Wrote keyring to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring
Creating mgr...
Verifying port 9283 ...
Waiting for mgr to start...
Waiting for mgr...
mgr not available, waiting (1/15)...
mgr not available, waiting (2/15)...
mgr is available
Enabling cephadm module...
Waiting for the mgr to restart...
Waiting for mgr epoch 5...
mgr epoch 5 is available
Setting orchestrator backend to cephadm...
Generating ssh key...
Wrote public SSH key to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub
Adding key to ljishen@localhost authorized_keys...
Adding host node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us...
Deploying unmanaged mon service...
Deploying unmanaged mgr service...
You can access the Ceph CLI with:

	sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --fsid d90784d2-4058-11ec-aba1-c5f3c07a3dc7 -c /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf -k /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring

Please consider enabling telemetry to help improve Ceph:

	ceph telemetry on

For more information see:

	https://docs.ceph.com/docs/pacific/mgr/telemetry/

Bootstrap complete.

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:59:14,570019467-05:00] INFO: Sleep for 20 seconds[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:59:34,577812983-05:00] INFO: > Set the default number of monitor daemons to 1[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:337 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:59:34,588690325-05:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:59:34,592388102-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mon', '1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1
Inferring fsid d90784d2-4058-11ec-aba1-c5f3c07a3dc7
Inferring config /var/lib/ceph/d90784d2-4058-11ec-aba1-c5f3c07a3dc7/mon.node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Scheduled mon update...

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:59:44,215673618-05:00] INFO: > Set the default number of manager daemons to 1[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:342 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:59:44,225263398-05:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:59:44,228175561-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mgr', '1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1
Inferring fsid d90784d2-4058-11ec-aba1-c5f3c07a3dc7
Inferring config /var/lib/ceph/d90784d2-4058-11ec-aba1-c5f3c07a3dc7/mon.node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Scheduled mgr update...


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:59:54,394394674-05:00] STAGE: Add hosts to the cluster...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:59:54,400060852-05:00] INFO: > Adding host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:361 - add_hosts() > ssh-copy-id -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -f -i /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us
/usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: "/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub"

Number of key(s) added: 1

Now try logging into the machine, with:   "ssh -o 'GlobalKnownHostsFile=/dev/null' -o 'LogLevel=ERROR' -o 'PasswordAuthentication=no' -o 'StrictHostKeyChecking=no' -o 'UserKnownHostsFile=/dev/null' 'ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'"
and check to make sure that only the key(s) you wanted were added.

# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:367 - add_hosts() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us 10.10.2.2
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:59:54,604388067-05:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T00:59:54,608062689-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'host', 'add', 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us', '10.10.2.2'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us 10.10.2.2
Inferring fsid d90784d2-4058-11ec-aba1-c5f3c07a3dc7
Inferring config /var/lib/ceph/d90784d2-4058-11ec-aba1-c5f3c07a3dc7/mon.node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Added host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us' with addr '10.10.2.2'

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:00:04,693204569-05:00] INFO: Sleep for 20 seconds[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:00:24,698224595-05:00] STAGE: Add OSDs to the cluster...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:00:24,705115714-05:00] INFO: > Adding devices ['/dev/nvme0n1'] on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us' as OSDs[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:388 - add_host_devices_as_osds() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:00:24,715548975-05:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:00:24,719752604-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'daemon', 'add', 'osd', 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1
Inferring fsid d90784d2-4058-11ec-aba1-c5f3c07a3dc7
Inferring config /var/lib/ceph/d90784d2-4058-11ec-aba1-c5f3c07a3dc7/mon.node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Created osd(s) 0 on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:00:50,188651309-05:00] INFO: > Sleep for 20 seconds[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:01:10,194532937-05:00] STAGE: Show Cluster Status[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:408 - show_status() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:01:10,204516851-05:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:01:10,208060636-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', '--status'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status
Inferring fsid d90784d2-4058-11ec-aba1-c5f3c07a3dc7
Inferring config /var/lib/ceph/d90784d2-4058-11ec-aba1-c5f3c07a3dc7/mon.node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
  cluster:
    id:     d90784d2-4058-11ec-aba1-c5f3c07a3dc7
    health: HEALTH_WARN
            1 pool(s) have no replicas configured
 
  services:
    mon: 1 daemons, quorum node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us (age 2m)
    mgr: node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us.vmrdek(active, since 2m)
    osd: 1 osds: 1 up (since 18s), 1 in (since 41s)
 
  data:
    pools:   1 pools, 1 pgs
    objects: 0 objects, 0 B
    usage:   4.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     1 active+clean
 

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:01:19,441534759-05:00] INFO: > Copy configuration files to local[0m
# ./benchmarks/bench-rados:220 - launch_ceph_cluster() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.3 sudo chown --changes --recursive ljishen /tmp/bench-rados/deployment_data_root/etc/ceph
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:01:19,448893725-05:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.3 sudo chown --changes --recursive ljishen /tmp/bench-rados/deployment_data_root/etc/ceph
[1] 01:01:19 [SUCCESS] ljishen@10.10.2.3
# ./benchmarks/bench-rados:223 - launch_ceph_cluster() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.3:/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf
# ./benchmarks/bench-rados:226 - launch_ceph_cluster() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.3:/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:01:19,924166637-05:00] STAGE: Update local ceph.conf as a client...[0m


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:01:19,927448926-05:00] STAGE: Configure Ceph pool...[0m
# ./benchmarks/bench-rados:257 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_max_backfills 32
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:01:19,948653478-05:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:01:19,951314015-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'd90784d2-4058-11ec-aba1-c5f3c07a3dc7', '--', 'ceph', 'config', 'set', 'osd', 'osd_max_backfills', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid d90784d2-4058-11ec-aba1-c5f3c07a3dc7 -- ceph config set osd osd_max_backfills 32
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:258 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_active 32
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:01:24,127946905-05:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:01:24,130669619-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'd90784d2-4058-11ec-aba1-c5f3c07a3dc7', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_active', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid d90784d2-4058-11ec-aba1-c5f3c07a3dc7 -- ceph config set osd osd_recovery_max_active 32
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:259 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_single_start 8
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:01:28,566370271-05:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:01:28,569160152-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'd90784d2-4058-11ec-aba1-c5f3c07a3dc7', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_single_start', '8'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid d90784d2-4058-11ec-aba1-c5f3c07a3dc7 -- ceph config set osd osd_recovery_max_single_start 8
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:260 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_op_priority 63
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:01:32,594568646-05:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:01:32,597474024-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'd90784d2-4058-11ec-aba1-c5f3c07a3dc7', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_op_priority', '63'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid d90784d2-4058-11ec-aba1-c5f3c07a3dc7 -- ceph config set osd osd_recovery_op_priority 63
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:262 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config show-with-defaults osd.0
# ./benchmarks/bench-rados:263 - configure_ceph_pool() > grep 'osd_max_backfills\|osd_recovery'
# ./benchmarks/bench-rados:263 - configure_ceph_pool() > column -t -s ' '
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
osd_max_backfills                        32         mon
osd_recovery_cost                        20971520   default
osd_recovery_delay_start                 0.000000   default
osd_recovery_max_active                  32         mon
osd_recovery_max_active_hdd              3          default
osd_recovery_max_active_ssd              10         default
osd_recovery_max_chunk                   8388608    default
osd_recovery_max_omap_entries_per_chunk  8096       default
osd_recovery_max_single_start            8          mon
osd_recovery_op_priority                 3          default
osd_recovery_op_warn_multiple            16         default
osd_recovery_priority                    5          default
osd_recovery_retry_interval              30.000000  default
osd_recovery_sleep                       0.000000   default
osd_recovery_sleep_hdd                   0.100000   default
osd_recovery_sleep_hybrid                0.025000   default
osd_recovery_sleep_ssd                   0.000000   default
# ./benchmarks/bench-rados:265 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool create bench_rados 128 128 replicated --size 1 --pg-num-min 128
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:01:41,007455411-05:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:01:41,010361741-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'd90784d2-4058-11ec-aba1-c5f3c07a3dc7', '--', 'ceph', 'osd', 'pool', 'create', 'bench_rados', '128', '128', 'replicated', '--size', '1', '--pg-num-min', '128'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid d90784d2-4058-11ec-aba1-c5f3c07a3dc7 -- ceph osd pool create bench_rados 128 128 replicated --size 1 --pg-num-min 128
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
pool 'bench_rados' created
# ./benchmarks/bench-rados:267 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados min_size 1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:01:46,084000071-05:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:01:46,086728295-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'd90784d2-4058-11ec-aba1-c5f3c07a3dc7', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'min_size', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid d90784d2-4058-11ec-aba1-c5f3c07a3dc7 -- ceph osd pool set bench_rados min_size 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 min_size to 1
# ./benchmarks/bench-rados:268 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados pg_autoscale_mode off
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:01:50,443554362-05:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:01:50,446347889-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'd90784d2-4058-11ec-aba1-c5f3c07a3dc7', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'pg_autoscale_mode', 'off'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid d90784d2-4058-11ec-aba1-c5f3c07a3dc7 -- ceph osd pool set bench_rados pg_autoscale_mode off
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 pg_autoscale_mode to off
# ./benchmarks/bench-rados:269 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool application enable bench_rados benchmark
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:01:54,747927018-05:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:01:54,750755510-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'd90784d2-4058-11ec-aba1-c5f3c07a3dc7', '--', 'ceph', 'osd', 'pool', 'application', 'enable', 'bench_rados', 'benchmark'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid d90784d2-4058-11ec-aba1-c5f3c07a3dc7 -- ceph osd pool application enable bench_rados benchmark
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
enabled application 'benchmark' on pool 'bench_rados'
# ./benchmarks/bench-rados:270 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados noscrub 1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:01:59,322925228-05:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:01:59,325667769-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'd90784d2-4058-11ec-aba1-c5f3c07a3dc7', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'noscrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid d90784d2-4058-11ec-aba1-c5f3c07a3dc7 -- ceph osd pool set bench_rados noscrub 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 noscrub to 1
# ./benchmarks/bench-rados:271 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados nodeep-scrub 1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:02:03,836000261-05:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:02:03,838753222-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'd90784d2-4058-11ec-aba1-c5f3c07a3dc7', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'nodeep-scrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid d90784d2-4058-11ec-aba1-c5f3c07a3dc7 -- ceph osd pool set bench_rados nodeep-scrub 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 nodeep-scrub to 1
# ./benchmarks/bench-rados:272 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool ls detail
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:02:08,777689112-05:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:02:08,780610501-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'd90784d2-4058-11ec-aba1-c5f3c07a3dc7', '--', 'ceph', 'osd', 'pool', 'ls', 'detail'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid d90784d2-4058-11ec-aba1-c5f3c07a3dc7 -- ceph osd pool ls detail
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
pool 1 'device_health_metrics' replicated size 1 min_size 1 crush_rule 0 object_hash rjenkins pg_num 64 pgp_num 64 autoscale_mode on last_change 17 lfor 0/0/15 flags hashpspool stripe_width 0 pg_num_min 1 application mgr_devicehealth
pool 2 'bench_rados' replicated size 1 min_size 1 crush_rule 0 object_hash rjenkins pg_num 128 pgp_num 128 autoscale_mode off last_change 20 flags hashpspool,noscrub,nodeep-scrub stripe_width 0 pg_num_min 128 application benchmark

# ./benchmarks/bench-rados:273 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd df tree
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:02:12,991292964-05:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:02:12,994021549-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'd90784d2-4058-11ec-aba1-c5f3c07a3dc7', '--', 'ceph', 'osd', 'df', 'tree'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid d90784d2-4058-11ec-aba1-c5f3c07a3dc7 -- ceph osd df tree
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
ID  CLASS  WEIGHT   REWEIGHT  SIZE     RAW USE  DATA     OMAP  META     AVAIL    %USE  VAR   PGS  STATUS  TYPE NAME      
-1         0.09769         -  100 GiB  5.7 MiB  168 KiB   0 B  5.6 MiB  100 GiB  0.01  1.00    -          root default   
-3         0.09769         -  100 GiB  5.7 MiB  168 KiB   0 B  5.6 MiB  100 GiB  0.01  1.00    -              host node-1
 0    ssd  0.09769   1.00000  100 GiB  5.7 MiB  168 KiB   0 B  5.6 MiB  100 GiB  0.01  1.00  192      up          osd.0  
                       TOTAL  100 GiB  5.7 MiB  168 KiB   0 B  5.6 MiB  100 GiB  0.01                                    
MIN/MAX VAR: 1.00/1.00  STDDEV: 0


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:02:17,035116561-05:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:02:41,062724647-05:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:02:50,117606521-05:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:02:59,110956820-05:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:02:59,118148571-05:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:03:08,186941094-05:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:03:08,193681473-05:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:03:17,307611408-05:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:03:17,314688753-05:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:03:26,521142361-05:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:03:26,528480667-05:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:03:35,734351230-05:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:03:35,741383530-05:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:03:35,746616888-05:00] INFO: > The cluster is idle now.[0m


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:03:35,749703718-05:00] STAGE: Start benchmarking of mode write ...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:03:35,755692199-05:00] INFO: > Start system activity collection services[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:03:35,760784802-05:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:341 - rados_bench() > rm --force /mnt/sda8/ceph-research/scripts/output/sys_activity_64KB_write.dat.rados_bench_host.3
# ./benchmarks/bench-rados:342 - rados_bench() > sar_pid_rados_bench_host=185860
# ./benchmarks/bench-rados:342 - rados_bench() > S_TIME_FORMAT=ISO
# ./benchmarks/bench-rados:342 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/output/sys_activity_64KB_write.dat.rados_bench_host.3 2

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:03:35,766998478-05:00] INFO: >> for OSD host[0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:03:35,775372768-05:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.2 bash
10.10.2.2: b'38037\n'
[1] 01:03:35 [SUCCESS] ljishen@10.10.2.2
38037

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:03:35,961045602-05:00] INFO: > Run rados bench[0m
# ./benchmarks/bench-rados:370 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/output/rados_bench_64KB_write.log.3
## ./benchmarks/bench-rados:361 - rados_bench() > count_cpus 32-63,96-127
# ./benchmarks/bench-rados:361 - rados_bench() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 60 write --pool bench_rados -b 65536 -O 65536 --concurrent-ios 128 --show-time --write-object --write-omap --write-xattr --no-cleanup
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:03:35,980397490-05:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:03:35,983075500-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'd90784d2-4058-11ec-aba1-c5f3c07a3dc7', '--', 'rados', 'bench', '60', 'write', '--pool', 'bench_rados', '-b', '65536', '-O', '65536', '--concurrent-ios', '128', '--show-time', '--write-object', '--write-omap', '--write-xattr', '--no-cleanup'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid d90784d2-4058-11ec-aba1-c5f3c07a3dc7 -- rados bench 60 write --pool bench_rados -b 65536 -O 65536 --concurrent-ios 128 --show-time --write-object --write-omap --write-xattr --no-cleanup
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
hints = 1
2021-11-08T06:03:39.237206+0000 Maintaining 128 concurrent writes of 65536 bytes to objects of size 65536 for up to 60 seconds or 0 objects
2021-11-08T06:03:39.237218+0000 Object prefix: benchmark_data_node-0.bluefield2.ucsc-cmps10_7
2021-11-08T06:03:39.239658+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-11-08T06:03:39.239658+0000     0       0         0         0         0         0           -           0
2021-11-08T06:03:40.239779+0000     1     128      4594      4466   279.111   279.125   0.0282665   0.0283963
2021-11-08T06:03:41.239850+0000     2     128      8492      8364   261.359   243.625   0.0783557   0.0298716
2021-11-08T06:03:42.239918+0000     3     128     10226     10098   210.362   108.375   0.0728946   0.0374158
2021-11-08T06:03:43.239987+0000     4     128     13228     13100   204.674   187.625    0.047098   0.0389215
2021-11-08T06:03:44.240059+0000     5     128     16812     16684   208.536       224   0.0835063   0.0380014
2021-11-08T06:03:45.240127+0000     6     128     17778     17650   183.842    60.375    0.126141   0.0431215
2021-11-08T06:03:46.240201+0000     7     128     18732     18604   166.096    59.625     0.13212    0.047594
2021-11-08T06:03:47.240276+0000     8     128     20524     20396   159.333       112   0.0589597   0.0498622
2021-11-08T06:03:48.240343+0000     9     128     22444     22316   154.962       120    0.062668   0.0513815
2021-11-08T06:03:49.240411+0000    10     128     24818     24690   154.302   148.375   0.0732067   0.0516644
2021-11-08T06:03:50.240480+0000    11     128     26540     26412   150.058   107.625   0.0753168   0.0531884
2021-11-08T06:03:51.240548+0000    12     128     29810     29682   154.583   204.375   0.0256093   0.0516977
2021-11-08T06:03:52.240617+0000    13     128     33010     32882   158.076       200   0.0966128   0.0502638
2021-11-08T06:03:53.240685+0000    14     128     34034     33906   151.356        64    0.131469   0.0526196
2021-11-08T06:03:54.240752+0000    15     128     35058     34930   145.532        64    0.132197   0.0548115
2021-11-08T06:03:55.240820+0000    16     128     37106     36978   144.435       128   0.0606182   0.0553185
2021-11-08T06:03:56.240895+0000    17     128     39154     39026   143.468       128   0.0645897   0.0556901
2021-11-08T06:03:57.240969+0000    18     128     40748     40620   141.032    99.625    0.128001   0.0564303
2021-11-08T06:03:58.241039+0000    19     128     41772     41644   136.977        64    0.127027   0.0581748
2021-11-08T06:03:59.241110+0000 min lat: 0.0185789 max lat: 0.140038 avg lat: 0.0598086
2021-11-08T06:03:59.241110+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-11-08T06:03:59.241110+0000    20     128     42796     42668   133.328        64    0.127981   0.0598086
2021-11-08T06:04:00.241188+0000    21     128     45484     45356   134.979       168   0.0252322   0.0592155
2021-11-08T06:04:01.241262+0000    22     128     49068     48940   139.024       224   0.0696758   0.0573443
2021-11-08T06:04:02.241337+0000    23     128     50092     49964   135.762        64    0.129828   0.0588114
2021-11-08T06:04:03.241408+0000    24     128     51058     50930   132.621    60.375    0.130216   0.0601525
2021-11-08T06:04:04.241478+0000    25     128     52978     52850   132.116       120   0.0566951   0.0604697
2021-11-08T06:04:05.241544+0000    26     128     55084     54956   132.097   131.625   0.0661174   0.0605014
2021-11-08T06:04:06.241630+0000    27     128     56818     56690   131.218   108.375    0.123919   0.0608278
2021-11-08T06:04:07.241717+0000    28     127     57803     57676   128.732    61.625    0.127404   0.0619885
2021-11-08T06:04:08.241791+0000    29     128     58738     58610   126.306    58.375    0.135423   0.0631326
2021-11-08T06:04:09.241868+0000    30     128     60204     60076   125.149    91.625   0.0604056    0.063842
2021-11-08T06:04:10.241940+0000    31     128     62194     62066   125.124   124.375   0.0660299   0.0638855
2021-11-08T06:04:11.242011+0000    32     128     64044     63916   124.827   115.625   0.0678164   0.0640133
2021-11-08T06:04:12.242085+0000    33     128     65324     65196   123.468        80    0.129708   0.0646318
2021-11-08T06:04:13.242160+0000    34     128     66348     66220   121.719        64    0.132572   0.0656342
2021-11-08T06:04:14.242231+0000    35     128     67314     67186   119.966    60.375    0.132783   0.0665595
2021-11-08T06:04:15.242308+0000    36     128     69164     69036   119.846   115.625   0.0611473   0.0667146
2021-11-08T06:04:16.242387+0000    37     128     71026     70898   119.752   116.375   0.0652484   0.0667244
2021-11-08T06:04:17.242465+0000    38     128     72876     72748   119.643   115.625    0.107851   0.0667809
2021-11-08T06:04:18.242538+0000    39     128     73842     73714   118.123    60.375     0.13008     0.06758
2021-11-08T06:04:19.242610+0000 min lat: 0.0185789 max lat: 0.144032 avg lat: 0.0684232
2021-11-08T06:04:19.242610+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-11-08T06:04:19.242610+0000    40     128     74866     74738    116.77        64     0.13034   0.0684232
2021-11-08T06:04:20.242688+0000    41     128     76274     76146   116.068        88   0.0577543   0.0688772
2021-11-08T06:04:21.242761+0000    42     128     78252     78124   116.248   123.625   0.0683016   0.0687373
2021-11-08T06:04:22.242837+0000    43     128     80242     80114   116.436   124.375   0.0635544   0.0686503
2021-11-08T06:04:23.242915+0000    44     128     81522     81394   115.608        80    0.120066   0.0690615
2021-11-08T06:04:24.242992+0000    45     128     82546     82418   114.461        64    0.129495   0.0698083
2021-11-08T06:04:25.243069+0000    46     128     83570     83442   113.364        64    0.125001   0.0704909
2021-11-08T06:04:26.243143+0000    47     128     85618     85490   113.675       128   0.0670247   0.0703185
2021-11-08T06:04:27.243220+0000    48     128     87538     87410   113.807       120   0.0650411   0.0702478
2021-11-08T06:04:28.243301+0000    49     128     89202     89074   113.607       104    0.123134   0.0703525
2021-11-08T06:04:29.243385+0000    50     128     90156     90028   112.527    59.625    0.128379   0.0709612
2021-11-08T06:04:30.243463+0000    51     128     91180     91052   111.575        64    0.129353   0.0716151
2021-11-08T06:04:31.243537+0000    52     128     92786     92658    111.36   100.375   0.0570145   0.0717899
2021-11-08T06:04:32.243610+0000    53     128     94962     94834   111.824       136   0.0611919   0.0714965
2021-11-08T06:04:33.243681+0000    54     128     96882     96754   111.976       120   0.0680269   0.0713928
2021-11-08T06:04:34.243752+0000    55     128     97906     97778   111.103        64    0.130857   0.0719016
2021-11-08T06:04:35.243825+0000    56     128     98930     98802   110.262        64    0.128033   0.0724853
2021-11-08T06:04:36.243897+0000    57     128    100210    100082   109.731        80   0.0574478   0.0728702
2021-11-08T06:04:37.243967+0000    58     128    102316    102188   110.108   131.625   0.0636138   0.0726078
2021-11-08T06:04:38.244036+0000    59     128    104364    104236   110.411       128   0.0643161   0.0724188
2021-11-08T06:04:39.244107+0000 min lat: 0.0185789 max lat: 0.144032 avg lat: 0.0726588
2021-11-08T06:04:39.244107+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-11-08T06:04:39.244107+0000    60     128    105714    105586   109.977    84.375    0.126259   0.0726588
2021-11-08T06:04:40.244224+0000 Total time run:         60.0951
Total writes made:      105714
Write size:             65536
Object size:            65536
Bandwidth (MB/sec):     109.944
Stddev Bandwidth:       51.1088
Max bandwidth (MB/sec): 279.125
Min bandwidth (MB/sec): 58.375
Average IOPS:           1759
Stddev IOPS:            817.74
Max IOPS:               4466
Min IOPS:               934
Average Latency(s):     0.0727235
Stddev Latency(s):      0.0357606
Max latency(s):         0.144032
Min latency(s):         0.0185789

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:04:40,837943741-05:00] INFO: > Stop system activity collection services[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:04:40,843196706-05:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:391 - rados_bench() > kill -INT 185860

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:04:40,848352798-05:00] INFO: >> for OSD host[0m
# ./benchmarks/bench-rados:396 - rados_bench() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.2 kill -INT 38037
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:04:40,855518980-05:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.2 kill -INT 38037
[1] 01:04:41 [SUCCESS] ljishen@10.10.2.2

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:04:41,036083092-05:00] INFO: >>> Copy system activity file from OSD host[0m
## ./benchmarks/bench-rados:403 - rados_bench() > basename -- /tmp/bench-rados/sys_activity_64KB_write.dat.osd_host.3
# ./benchmarks/bench-rados:403 - rados_bench() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.2:/tmp/bench-rados/sys_activity_64KB_write.dat.osd_host.3 /mnt/sda8/ceph-research/scripts/output/sys_activity_64KB_write.dat.osd_host.3


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:04:41,206285307-05:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:05:05,472669905-05:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 105.72k objects, 6.5 GiB
    usage:   20 GiB used, 80 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:05:05,479733864-05:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:05:14,719988631-05:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 105.72k objects, 6.5 GiB
    usage:   20 GiB used, 80 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:05:14,727400267-05:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:05:23,721058329-05:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 105.72k objects, 6.5 GiB
    usage:   20 GiB used, 80 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:05:23,727859803-05:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:05:32,874864221-05:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 105.72k objects, 6.5 GiB
    usage:   20 GiB used, 80 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:05:32,882330419-05:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:05:42,086975376-05:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 105.72k objects, 6.5 GiB
    usage:   20 GiB used, 80 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:05:42,094166835-05:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:05:42,099675832-05:00] INFO: > The cluster is idle now.[0m


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:05:42,102999419-05:00] STAGE: Start benchmarking of mode seq ...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:05:42,109107276-05:00] INFO: > Start system activity collection services[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:05:42,114250514-05:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:341 - rados_bench() > rm --force /mnt/sda8/ceph-research/scripts/output/sys_activity_64KB_seq.dat.rados_bench_host.3
# ./benchmarks/bench-rados:342 - rados_bench() > sar_pid_rados_bench_host=187230
# ./benchmarks/bench-rados:342 - rados_bench() > S_TIME_FORMAT=ISO
# ./benchmarks/bench-rados:342 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/output/sys_activity_64KB_seq.dat.rados_bench_host.3 2

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:05:42,120591340-05:00] INFO: >> for OSD host[0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:05:42,128800949-05:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.2 bash
10.10.2.2: b'38156\n'
[1] 01:05:42 [SUCCESS] ljishen@10.10.2.2
38156

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:05:42,317561897-05:00] INFO: > Run rados bench[0m
# ./benchmarks/bench-rados:379 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/output/rados_bench_64KB_seq.log.3
## ./benchmarks/bench-rados:376 - rados_bench() > count_cpus 32-63,96-127
# ./benchmarks/bench-rados:376 - rados_bench() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 128 --show-time
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:05:42,337036016-05:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:05:42,339661938-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'd90784d2-4058-11ec-aba1-c5f3c07a3dc7', '--', 'rados', 'bench', '99999999', 'seq', '--pool', 'bench_rados', '--concurrent-ios', '128', '--show-time'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid d90784d2-4058-11ec-aba1-c5f3c07a3dc7 -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 128 --show-time
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
hints = 1
2021-11-08T06:05:45.578793+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-11-08T06:05:45.578793+0000     0       0         0         0         0         0           -           0
2021-11-08T06:05:46.578912+0000     1     128     17464     17336   1083.29    1083.5  0.00758577   0.0073458
2021-11-08T06:05:47.579007+0000     2     128     34716     34588   1080.72   1078.25  0.00741771  0.00737677
2021-11-08T06:05:48.579106+0000     3     127     52203     52076   1084.78      1093  0.00643964  0.00735603
2021-11-08T06:05:49.579182+0000     4     128     69709     69581   1087.08   1094.06  0.00732089  0.00734265
2021-11-08T06:05:50.579257+0000     5     127     87158     87031   1087.77   1090.62  0.00697171  0.00733983
2021-11-08T06:05:51.579336+0000     6     127    104566    104439   1087.79      1088  0.00736279  0.00734072
2021-11-08T06:05:52.579429+0000 Total time run:       6.07108
Total reads made:     105714
Read size:            65536
Object size:          65536
Bandwidth (MB/sec):   1088.3
Average IOPS:         17412
Stddev IOPS:          97.0356
Max IOPS:             17505
Min IOPS:             17252
Average Latency(s):   0.0073387
Max latency(s):       0.0129833
Min latency(s):       0.00186706

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:05:53,281024524-05:00] INFO: > Stop system activity collection services[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:05:53,286420699-05:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:391 - rados_bench() > kill -INT 187230

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:05:53,291859224-05:00] INFO: >> for OSD host[0m
# ./benchmarks/bench-rados:396 - rados_bench() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.2 kill -INT 38156
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:05:53,299332425-05:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.2 kill -INT 38156
[1] 01:05:53 [SUCCESS] ljishen@10.10.2.2

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:05:53,480460531-05:00] INFO: >>> Copy system activity file from OSD host[0m
## ./benchmarks/bench-rados:403 - rados_bench() > basename -- /tmp/bench-rados/sys_activity_64KB_seq.dat.osd_host.3
# ./benchmarks/bench-rados:403 - rados_bench() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.2:/tmp/bench-rados/sys_activity_64KB_seq.dat.osd_host.3 /mnt/sda8/ceph-research/scripts/output/sys_activity_64KB_seq.dat.osd_host.3


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:05:53,628868571-05:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:06:17,731529975-05:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 105.72k objects, 6.5 GiB
    usage:   20 GiB used, 80 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:06:17,738328494-05:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:06:26,641184536-05:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 105.72k objects, 6.5 GiB
    usage:   20 GiB used, 80 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:06:26,647902613-05:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:06:35,770257294-05:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 105.72k objects, 6.5 GiB
    usage:   20 GiB used, 80 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:06:35,777086260-05:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:06:44,957734401-05:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 105.72k objects, 6.5 GiB
    usage:   20 GiB used, 80 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:06:44,964388989-05:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:06:53,987482557-05:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 105.72k objects, 6.5 GiB
    usage:   20 GiB used, 80 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:06:53,994143457-05:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:06:53,999600376-05:00] INFO: > The cluster is idle now.[0m


[1;7;39;49m[2021-11-08T01:06:54,002872225-05:00][RUNNING][ROUND 1/4/21] object_size=256KB[0m


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:06:54,005967822-05:00] STAGE: Launch a Ceph cluster (CEPH_HOST: 10.10.2.3, OSD_HOST: 10.10.2.2) ...[0m
# ./benchmarks/bench-rados:183 - launch_ceph_cluster() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.3 bash -euo pipefail
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:06:54,014724964-05:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.3 bash -euo pipefail
10.10.2.3: b'\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T01:06:54,343376970-05:00] INFO: > Remove existing clusters\x1b[0m\n'
10.10.2.3: b'## bash:17 -  > basename -- /var/lib/ceph/d90784d2-4058-11ec-aba1-c5f3c07a3dc7\n'
10.10.2.3: b'# bash:17 -  > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm --host 10.10.2.3 --host 10.10.2.2 rm-cluster --force --fsid d90784d2-4058-11ec-aba1-c5f3c07a3dc7\n'
10.10.2.3: b"\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T01:06:54,353537228-05:00] DEBUG: HOST_PARAMS: ['--host', '10.10.2.3', '--host', '10.10.2.2']\x1b[0m\n"
10.10.2.3: b"\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T01:06:54,357178497-05:00] DEBUG: CEPHADM_PARAMS: ['rm-cluster', '--force', '--fsid', 'd90784d2-4058-11ec-aba1-c5f3c07a3dc7']\x1b[0m\n"
10.10.2.3: b"# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:63 -  > /tmp/bench-rados/ceph-research/scripts/parallel-ssh --host 10.10.2.3 --host 10.10.2.2 'sudo python3 -u - rm-cluster --force --fsid d90784d2-4058-11ec-aba1-c5f3c07a3dc7'\n"
10.10.2.3: b'\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T01:06:54,365957709-05:00] DEBUG: command from stdin\x1b[0m\n'
10.10.2.3: b"# /tmp/bench-rados/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host 10.10.2.3 --host 10.10.2.2 'sudo python3 -u - rm-cluster --force --fsid d90784d2-4058-11ec-aba1-c5f3c07a3dc7'\n"
10.10.2.3: b'[1] 01:07:00 [SUCCESS] 10.10.2.3\n[2] 01:07:05 [SUCCESS] 10.10.2.2\n'
10.10.2.3: b'\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T01:07:05,749491021-05:00] INFO: > Deploy a new cluster\x1b[0m\n'
10.10.2.3: b'# bash:24 -  > /tmp/bench-rados/ceph-research/scripts/ceph-deploy -m 10.10.2.3 -o node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/10.10.2.2:/dev/nvme0n1\n'
10.10.2.3: b'\n\n\x1b[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T01:07:05,761086115-05:00] STAGE: Verify passwordless SSH to hosts, passwordless sudo via ljishen, and the existence of devices on hosts...\x1b[0m\n'
10.10.2.3: b'\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T01:07:05,765725437-05:00] INFO: > Check the host for the monitor\x1b[0m\n'
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:181 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@10.10.2.3 sudo --non-interactive --validate\n'
10.10.2.3: b'\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T01:07:05,913864475-05:00] INFO: > Check hosts for OSDs\x1b[0m\n'
10.10.2.3: b"\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T01:07:05,918106949-05:00] INFO: >> Check host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'\x1b[0m\n"
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:200 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us sudo --non-interactive --validate\n'
10.10.2.3: b"\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T01:07:06,070176763-05:00] INFO: >>> Check device '/dev/nvme0n1'\x1b[0m\n"
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:215 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us test -b /dev/nvme0n1\n'
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:226 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us lsblk --nodeps --noheadings --output TYPE /dev/nvme0n1\n'
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:227 - verify_parameters() > grep --word-regexp disk\n'
10.10.2.3: b'\n\n\x1b[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T01:07:06,346186163-05:00] STAGE: Reset devices on hosts...\x1b[0m\n'
10.10.2.3: b"\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T01:07:06,351217184-05:00] INFO: > Resetting device '/dev/nvme0n1' on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'\x1b[0m\n"
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:288 - reset_host_devices() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us sudo sh\n'
10.10.2.3: b'  Removing ceph--3ef7566a--7509--4a6d--b39c--6a6737096ffd-osd--block--7fb22e98--28d2--4eda--9581--81e8800f519d (253:0)\n'
10.10.2.3: b'  Archiving volume group "ceph-3ef7566a-7509-4a6d-b39c-6a6737096ffd" metadata (seqno 5).\n  Releasing logical volume "osd-block-7fb22e98-28d2-4eda-9581-81e8800f519d"\n'
10.10.2.3: b'  Creating volume group backup "/etc/lvm/backup/ceph-3ef7566a-7509-4a6d-b39c-6a6737096ffd" (seqno 6).\n'
10.10.2.3: b'  Logical volume "osd-block-7fb22e98-28d2-4eda-9581-81e8800f519d" successfully removed\n'
10.10.2.3: b'  Removing physical volume "/dev/nvme0n1" from volume group "ceph-3ef7566a-7509-4a6d-b39c-6a6737096ffd"\n'
10.10.2.3: b'  Volume group "ceph-3ef7566a-7509-4a6d-b39c-6a6737096ffd" successfully removed\n'
10.10.2.3: b'\n\n\x1b[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T01:07:06,763678494-05:00] STAGE: Bootstrap a new cluster...\x1b[0m\n'
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:317 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.3 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults\n'
10.10.2.3: b'\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T01:07:06,772527939-05:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.3: b"\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T01:07:06,776215896-05:00] DEBUG: CEPHADM_PARAMS: ['bootstrap', '--config', '/tmp/bench-rados/ceph-research/scripts/ceph.conf', '--mon-ip', '10.10.2.3', '--output-dir', '/tmp/bench-rados/deployment_data_root/etc/ceph', '--ssh-user', 'ljishen', '--skip-dashboard', '--no-minimize-config', '--allow-overwrite', '--allow-fqdn-hostname', '--orphan-initial-daemons', '--skip-monitoring-stack', '--single-host-defaults']\x1b[0m\n"
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.3 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults\n'
10.10.2.3: b'Verifying podman|docker is present...\nVerifying lvm2 is present...\n'
10.10.2.3: b'Verifying time synchronization is in place...\n'
10.10.2.3: b'Unit ntp.service is enabled and running\n'
10.10.2.3: b'Repeating the final host check...\n'
10.10.2.3: b'docker (/usr/bin/docker) is present\n'
10.10.2.3: b'systemctl is present\n'
10.10.2.3: b'lvcreate is present\n'
10.10.2.3: b'Unit ntp.service is enabled and running\n'
10.10.2.3: b'Host looks OK\n'
10.10.2.3: b'Cluster fsid: 1a0b4120-405a-11ec-aba1-c5f3c07a3dc7\n'
10.10.2.3: b'Verifying IP 10.10.2.3 port 3300 ...\n'
10.10.2.3: b'Verifying IP 10.10.2.3 port 6789 ...\n'
10.10.2.3: b'Mon IP `10.10.2.3` is in CIDR network `10.10.2.0/24`\n'
10.10.2.3: b'- internal network (--cluster-network) has not been provided, OSD replication will default to the public_network\n'
10.10.2.3: b'Adjusting default settings to suit single-host cluster...\n'
10.10.2.3: b'Pulling container image quay.io/ceph/ceph:v16...\n'
10.10.2.3: b'Ceph version: ceph version 16.2.6 (ee28fb57e47e9f88813e24bbf4c14496ca299d31) pacific (stable)\nExtracting ceph user uid/gid from container image...\n'
10.10.2.3: b'Creating initial keys...\n'
10.10.2.3: b'Creating initial monmap...\n'
10.10.2.3: b'Creating mon...\n'
10.10.2.3: b'Waiting for mon to start...\n'
10.10.2.3: b'Waiting for mon...\n'
10.10.2.3: b'mon is available\n'
10.10.2.3: b'Setting mon public_network to 10.10.2.0/24\n'
10.10.2.3: b'Wrote config to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf\n'
10.10.2.3: b'Wrote keyring to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring\n'
10.10.2.3: b'Creating mgr...\n'
10.10.2.3: b'Verifying port 9283 ...\n'
10.10.2.3: b'Waiting for mgr to start...\nWaiting for mgr...\n'
10.10.2.3: b'mgr not available, waiting (1/15)...\n'
10.10.2.3: b'mgr not available, waiting (2/15)...\n'
10.10.2.3: b'mgr is available\n'
10.10.2.3: b'Enabling cephadm module...\n'
10.10.2.3: b'Waiting for the mgr to restart...\n'
10.10.2.3: b'Waiting for mgr epoch 5...\n'
10.10.2.3: b'mgr epoch 5 is available\n'
10.10.2.3: b'Setting orchestrator backend to cephadm...\n'
10.10.2.3: b'Generating ssh key...\n'
10.10.2.3: b'Wrote public SSH key to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub\n'
10.10.2.3: b'Adding key to ljishen@localhost authorized_keys...\n'
10.10.2.3: b'Adding host node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us...\n'
10.10.2.3: b'Deploying unmanaged mon service...\n'
10.10.2.3: b'Deploying unmanaged mgr service...\n'
10.10.2.3: b'You can access the Ceph CLI with:\n\n\tsudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --fsid 1a0b4120-405a-11ec-aba1-c5f3c07a3dc7 -c /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf -k /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring\n\n'
10.10.2.3: b'Please consider enabling telemetry to help improve Ceph:\n\n\tceph telemetry on\n\nFor more information see:\n\n\thttps://docs.ceph.com/docs/pacific/mgr/telemetry/\n\n'
10.10.2.3: b'Bootstrap complete.\n'
10.10.2.3: b'\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T01:08:14,581358149-05:00] INFO: Sleep for 20 seconds\x1b[0m\n'
10.10.2.3: b'\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T01:08:34,588730840-05:00] INFO: > Set the default number of monitor daemons to 1\x1b[0m\n'
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:337 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1\n'
10.10.2.3: b'\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T01:08:34,598687134-05:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.3: b"\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T01:08:34,602320949-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mon', '1']\x1b[0m\n"
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1\n'
10.10.2.3: b'Inferring fsid 1a0b4120-405a-11ec-aba1-c5f3c07a3dc7\n'
10.10.2.3: b'Inferring config /var/lib/ceph/1a0b4120-405a-11ec-aba1-c5f3c07a3dc7/mon.node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.3: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.3: b'Scheduled mon update...\n'
10.10.2.3: b'\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T01:08:44,240151126-05:00] INFO: > Set the default number of manager daemons to 1\x1b[0m\n'
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:342 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1\n'
10.10.2.3: b'\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T01:08:44,248922334-05:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.3: b"\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T01:08:44,252438768-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mgr', '1']\x1b[0m\n"
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1\n'
10.10.2.3: b'Inferring fsid 1a0b4120-405a-11ec-aba1-c5f3c07a3dc7\n'
10.10.2.3: b'Inferring config /var/lib/ceph/1a0b4120-405a-11ec-aba1-c5f3c07a3dc7/mon.node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.3: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.3: b'Scheduled mgr update...\n'
10.10.2.3: b'\n\n\x1b[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T01:08:54,225477078-05:00] STAGE: Add hosts to the cluster...\x1b[0m\n'
10.10.2.3: b"\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T01:08:54,231278303-05:00] INFO: > Adding host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'\x1b[0m\n"
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:361 - add_hosts() > ssh-copy-id -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -f -i /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\n'
10.10.2.3: b'/usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: "/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub"\n'
10.10.2.3: b'\nNumber of key(s) added: 1\n\nNow try logging into the machine, with:   "ssh -o \'GlobalKnownHostsFile=/dev/null\' -o \'LogLevel=ERROR\' -o \'PasswordAuthentication=no\' -o \'StrictHostKeyChecking=no\' -o \'UserKnownHostsFile=/dev/null\' \'ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\'"\nand check to make sure that only the key(s) you wanted were added.\n\n'
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:367 - add_hosts() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us 10.10.2.2\n'
10.10.2.3: b'\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T01:08:54,440487245-05:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.3: b"\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T01:08:54,444409345-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'host', 'add', 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us', '10.10.2.2']\x1b[0m\n"
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us 10.10.2.2\n'
10.10.2.3: b'Inferring fsid 1a0b4120-405a-11ec-aba1-c5f3c07a3dc7\n'
10.10.2.3: b'Inferring config /var/lib/ceph/1a0b4120-405a-11ec-aba1-c5f3c07a3dc7/mon.node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.3: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.3: b"Added host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us' with addr '10.10.2.2'\n"
10.10.2.3: b'\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T01:09:04,586406859-05:00] INFO: Sleep for 20 seconds\x1b[0m\n'
10.10.2.3: b'\n\n\x1b[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T01:09:24,591623187-05:00] STAGE: Add OSDs to the cluster...\x1b[0m\n'
10.10.2.3: b"\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T01:09:24,598424088-05:00] INFO: > Adding devices ['/dev/nvme0n1'] on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us' as OSDs\x1b[0m\n"
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:388 - add_host_devices_as_osds() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1\n'
10.10.2.3: b'\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T01:09:24,608271336-05:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.3: b"\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T01:09:24,611464781-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'daemon', 'add', 'osd', 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1']\x1b[0m\n"
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1\n'
10.10.2.3: b'Inferring fsid 1a0b4120-405a-11ec-aba1-c5f3c07a3dc7\n'
10.10.2.3: b'Inferring config /var/lib/ceph/1a0b4120-405a-11ec-aba1-c5f3c07a3dc7/mon.node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.3: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.3: b"Created osd(s) 0 on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'\n"
10.10.2.3: b'\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T01:09:49,898232025-05:00] INFO: > Sleep for 20 seconds\x1b[0m\n'
10.10.2.3: b'\n\n\x1b[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T01:10:09,903812126-05:00] STAGE: Show Cluster Status\x1b[0m\n'
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:408 - show_status() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status\n'
10.10.2.3: b'\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T01:10:09,913765835-05:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.3: b"\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T01:10:09,917292238-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', '--status']\x1b[0m\n"
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status\n'
10.10.2.3: b'Inferring fsid 1a0b4120-405a-11ec-aba1-c5f3c07a3dc7\n'
10.10.2.3: b'Inferring config /var/lib/ceph/1a0b4120-405a-11ec-aba1-c5f3c07a3dc7/mon.node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.3: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.3: b'  cluster:\n    id:     1a0b4120-405a-11ec-aba1-c5f3c07a3dc7\n    health: HEALTH_WARN\n            1 pool(s) have no replicas configured\n \n  services:\n    mon: 1 daemons, quorum node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us (age 2m)\n    mgr: node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us.dtnwbf(active, since 2m)\n    osd: 1 osds: 1 up (since 17s), 1 in (since 41s)\n \n  data:\n    pools:   1 pools, 1 pgs\n    objects: 0 objects, 0 B\n    usage:   4.7 MiB used, 100 GiB / 100 GiB avail\n    pgs:     1 active+clean\n \n'
[1] 01:10:19 [SUCCESS] ljishen@10.10.2.3

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:06:54,343376970-05:00] INFO: > Remove existing clusters[0m
## bash:17 -  > basename -- /var/lib/ceph/d90784d2-4058-11ec-aba1-c5f3c07a3dc7
# bash:17 -  > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm --host 10.10.2.3 --host 10.10.2.2 rm-cluster --force --fsid d90784d2-4058-11ec-aba1-c5f3c07a3dc7
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:06:54,353537228-05:00] DEBUG: HOST_PARAMS: ['--host', '10.10.2.3', '--host', '10.10.2.2'][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:06:54,357178497-05:00] DEBUG: CEPHADM_PARAMS: ['rm-cluster', '--force', '--fsid', 'd90784d2-4058-11ec-aba1-c5f3c07a3dc7'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:63 -  > /tmp/bench-rados/ceph-research/scripts/parallel-ssh --host 10.10.2.3 --host 10.10.2.2 'sudo python3 -u - rm-cluster --force --fsid d90784d2-4058-11ec-aba1-c5f3c07a3dc7'
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:06:54,365957709-05:00] DEBUG: command from stdin[0m
# /tmp/bench-rados/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host 10.10.2.3 --host 10.10.2.2 'sudo python3 -u - rm-cluster --force --fsid d90784d2-4058-11ec-aba1-c5f3c07a3dc7'
[1] 01:07:00 [SUCCESS] 10.10.2.3
[2] 01:07:05 [SUCCESS] 10.10.2.2

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:07:05,749491021-05:00] INFO: > Deploy a new cluster[0m
# bash:24 -  > /tmp/bench-rados/ceph-research/scripts/ceph-deploy -m 10.10.2.3 -o node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/10.10.2.2:/dev/nvme0n1


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:07:05,761086115-05:00] STAGE: Verify passwordless SSH to hosts, passwordless sudo via ljishen, and the existence of devices on hosts...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:07:05,765725437-05:00] INFO: > Check the host for the monitor[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:181 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@10.10.2.3 sudo --non-interactive --validate

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:07:05,913864475-05:00] INFO: > Check hosts for OSDs[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:07:05,918106949-05:00] INFO: >> Check host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:200 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us sudo --non-interactive --validate

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:07:06,070176763-05:00] INFO: >>> Check device '/dev/nvme0n1'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:215 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us test -b /dev/nvme0n1
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:226 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us lsblk --nodeps --noheadings --output TYPE /dev/nvme0n1
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:227 - verify_parameters() > grep --word-regexp disk


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:07:06,346186163-05:00] STAGE: Reset devices on hosts...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:07:06,351217184-05:00] INFO: > Resetting device '/dev/nvme0n1' on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:288 - reset_host_devices() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us sudo sh
  Removing ceph--3ef7566a--7509--4a6d--b39c--6a6737096ffd-osd--block--7fb22e98--28d2--4eda--9581--81e8800f519d (253:0)
  Archiving volume group "ceph-3ef7566a-7509-4a6d-b39c-6a6737096ffd" metadata (seqno 5).
  Releasing logical volume "osd-block-7fb22e98-28d2-4eda-9581-81e8800f519d"
  Creating volume group backup "/etc/lvm/backup/ceph-3ef7566a-7509-4a6d-b39c-6a6737096ffd" (seqno 6).
  Logical volume "osd-block-7fb22e98-28d2-4eda-9581-81e8800f519d" successfully removed
  Removing physical volume "/dev/nvme0n1" from volume group "ceph-3ef7566a-7509-4a6d-b39c-6a6737096ffd"
  Volume group "ceph-3ef7566a-7509-4a6d-b39c-6a6737096ffd" successfully removed


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:07:06,763678494-05:00] STAGE: Bootstrap a new cluster...[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:317 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.3 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:07:06,772527939-05:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:07:06,776215896-05:00] DEBUG: CEPHADM_PARAMS: ['bootstrap', '--config', '/tmp/bench-rados/ceph-research/scripts/ceph.conf', '--mon-ip', '10.10.2.3', '--output-dir', '/tmp/bench-rados/deployment_data_root/etc/ceph', '--ssh-user', 'ljishen', '--skip-dashboard', '--no-minimize-config', '--allow-overwrite', '--allow-fqdn-hostname', '--orphan-initial-daemons', '--skip-monitoring-stack', '--single-host-defaults'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.3 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults
Verifying podman|docker is present...
Verifying lvm2 is present...
Verifying time synchronization is in place...
Unit ntp.service is enabled and running
Repeating the final host check...
docker (/usr/bin/docker) is present
systemctl is present
lvcreate is present
Unit ntp.service is enabled and running
Host looks OK
Cluster fsid: 1a0b4120-405a-11ec-aba1-c5f3c07a3dc7
Verifying IP 10.10.2.3 port 3300 ...
Verifying IP 10.10.2.3 port 6789 ...
Mon IP `10.10.2.3` is in CIDR network `10.10.2.0/24`
- internal network (--cluster-network) has not been provided, OSD replication will default to the public_network
Adjusting default settings to suit single-host cluster...
Pulling container image quay.io/ceph/ceph:v16...
Ceph version: ceph version 16.2.6 (ee28fb57e47e9f88813e24bbf4c14496ca299d31) pacific (stable)
Extracting ceph user uid/gid from container image...
Creating initial keys...
Creating initial monmap...
Creating mon...
Waiting for mon to start...
Waiting for mon...
mon is available
Setting mon public_network to 10.10.2.0/24
Wrote config to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf
Wrote keyring to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring
Creating mgr...
Verifying port 9283 ...
Waiting for mgr to start...
Waiting for mgr...
mgr not available, waiting (1/15)...
mgr not available, waiting (2/15)...
mgr is available
Enabling cephadm module...
Waiting for the mgr to restart...
Waiting for mgr epoch 5...
mgr epoch 5 is available
Setting orchestrator backend to cephadm...
Generating ssh key...
Wrote public SSH key to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub
Adding key to ljishen@localhost authorized_keys...
Adding host node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us...
Deploying unmanaged mon service...
Deploying unmanaged mgr service...
You can access the Ceph CLI with:

	sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --fsid 1a0b4120-405a-11ec-aba1-c5f3c07a3dc7 -c /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf -k /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring

Please consider enabling telemetry to help improve Ceph:

	ceph telemetry on

For more information see:

	https://docs.ceph.com/docs/pacific/mgr/telemetry/

Bootstrap complete.

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:08:14,581358149-05:00] INFO: Sleep for 20 seconds[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:08:34,588730840-05:00] INFO: > Set the default number of monitor daemons to 1[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:337 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:08:34,598687134-05:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:08:34,602320949-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mon', '1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1
Inferring fsid 1a0b4120-405a-11ec-aba1-c5f3c07a3dc7
Inferring config /var/lib/ceph/1a0b4120-405a-11ec-aba1-c5f3c07a3dc7/mon.node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Scheduled mon update...

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:08:44,240151126-05:00] INFO: > Set the default number of manager daemons to 1[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:342 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:08:44,248922334-05:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:08:44,252438768-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mgr', '1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1
Inferring fsid 1a0b4120-405a-11ec-aba1-c5f3c07a3dc7
Inferring config /var/lib/ceph/1a0b4120-405a-11ec-aba1-c5f3c07a3dc7/mon.node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Scheduled mgr update...


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:08:54,225477078-05:00] STAGE: Add hosts to the cluster...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:08:54,231278303-05:00] INFO: > Adding host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:361 - add_hosts() > ssh-copy-id -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -f -i /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us
/usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: "/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub"

Number of key(s) added: 1

Now try logging into the machine, with:   "ssh -o 'GlobalKnownHostsFile=/dev/null' -o 'LogLevel=ERROR' -o 'PasswordAuthentication=no' -o 'StrictHostKeyChecking=no' -o 'UserKnownHostsFile=/dev/null' 'ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'"
and check to make sure that only the key(s) you wanted were added.

# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:367 - add_hosts() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us 10.10.2.2
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:08:54,440487245-05:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:08:54,444409345-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'host', 'add', 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us', '10.10.2.2'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us 10.10.2.2
Inferring fsid 1a0b4120-405a-11ec-aba1-c5f3c07a3dc7
Inferring config /var/lib/ceph/1a0b4120-405a-11ec-aba1-c5f3c07a3dc7/mon.node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Added host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us' with addr '10.10.2.2'

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:09:04,586406859-05:00] INFO: Sleep for 20 seconds[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:09:24,591623187-05:00] STAGE: Add OSDs to the cluster...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:09:24,598424088-05:00] INFO: > Adding devices ['/dev/nvme0n1'] on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us' as OSDs[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:388 - add_host_devices_as_osds() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:09:24,608271336-05:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:09:24,611464781-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'daemon', 'add', 'osd', 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1
Inferring fsid 1a0b4120-405a-11ec-aba1-c5f3c07a3dc7
Inferring config /var/lib/ceph/1a0b4120-405a-11ec-aba1-c5f3c07a3dc7/mon.node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Created osd(s) 0 on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:09:49,898232025-05:00] INFO: > Sleep for 20 seconds[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:10:09,903812126-05:00] STAGE: Show Cluster Status[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:408 - show_status() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:10:09,913765835-05:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:10:09,917292238-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', '--status'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status
Inferring fsid 1a0b4120-405a-11ec-aba1-c5f3c07a3dc7
Inferring config /var/lib/ceph/1a0b4120-405a-11ec-aba1-c5f3c07a3dc7/mon.node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
  cluster:
    id:     1a0b4120-405a-11ec-aba1-c5f3c07a3dc7
    health: HEALTH_WARN
            1 pool(s) have no replicas configured
 
  services:
    mon: 1 daemons, quorum node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us (age 2m)
    mgr: node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us.dtnwbf(active, since 2m)
    osd: 1 osds: 1 up (since 17s), 1 in (since 41s)
 
  data:
    pools:   1 pools, 1 pgs
    objects: 0 objects, 0 B
    usage:   4.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     1 active+clean
 

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:10:19,052166582-05:00] INFO: > Copy configuration files to local[0m
# ./benchmarks/bench-rados:220 - launch_ceph_cluster() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.3 sudo chown --changes --recursive ljishen /tmp/bench-rados/deployment_data_root/etc/ceph
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:10:19,059843007-05:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.3 sudo chown --changes --recursive ljishen /tmp/bench-rados/deployment_data_root/etc/ceph
[1] 01:10:19 [SUCCESS] ljishen@10.10.2.3
# ./benchmarks/bench-rados:223 - launch_ceph_cluster() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.3:/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf
# ./benchmarks/bench-rados:226 - launch_ceph_cluster() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.3:/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:10:19,536211379-05:00] STAGE: Update local ceph.conf as a client...[0m


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:10:19,539459835-05:00] STAGE: Configure Ceph pool...[0m
# ./benchmarks/bench-rados:257 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_max_backfills 32
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:10:19,559944049-05:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:10:19,562427651-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '1a0b4120-405a-11ec-aba1-c5f3c07a3dc7', '--', 'ceph', 'config', 'set', 'osd', 'osd_max_backfills', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 1a0b4120-405a-11ec-aba1-c5f3c07a3dc7 -- ceph config set osd osd_max_backfills 32
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:258 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_active 32
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:10:23,669949872-05:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:10:23,672619866-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '1a0b4120-405a-11ec-aba1-c5f3c07a3dc7', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_active', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 1a0b4120-405a-11ec-aba1-c5f3c07a3dc7 -- ceph config set osd osd_recovery_max_active 32
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:259 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_single_start 8
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:10:27,749857033-05:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:10:27,752840589-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '1a0b4120-405a-11ec-aba1-c5f3c07a3dc7', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_single_start', '8'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 1a0b4120-405a-11ec-aba1-c5f3c07a3dc7 -- ceph config set osd osd_recovery_max_single_start 8
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:260 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_op_priority 63
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:10:31,805566368-05:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:10:31,808532511-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '1a0b4120-405a-11ec-aba1-c5f3c07a3dc7', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_op_priority', '63'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 1a0b4120-405a-11ec-aba1-c5f3c07a3dc7 -- ceph config set osd osd_recovery_op_priority 63
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:262 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config show-with-defaults osd.0
# ./benchmarks/bench-rados:263 - configure_ceph_pool() > grep 'osd_max_backfills\|osd_recovery'
# ./benchmarks/bench-rados:263 - configure_ceph_pool() > column -t -s ' '
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
osd_max_backfills                        32         mon
osd_recovery_cost                        20971520   default
osd_recovery_delay_start                 0.000000   default
osd_recovery_max_active                  32         mon
osd_recovery_max_active_hdd              3          default
osd_recovery_max_active_ssd              10         default
osd_recovery_max_chunk                   8388608    default
osd_recovery_max_omap_entries_per_chunk  8096       default
osd_recovery_max_single_start            8          mon
osd_recovery_op_priority                 63         mon
osd_recovery_op_warn_multiple            16         default
osd_recovery_priority                    5          default
osd_recovery_retry_interval              30.000000  default
osd_recovery_sleep                       0.000000   default
osd_recovery_sleep_hdd                   0.100000   default
osd_recovery_sleep_hybrid                0.025000   default
osd_recovery_sleep_ssd                   0.000000   default
# ./benchmarks/bench-rados:265 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool create bench_rados 128 128 replicated --size 1 --pg-num-min 128
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:10:40,065094302-05:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:10:40,067866489-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '1a0b4120-405a-11ec-aba1-c5f3c07a3dc7', '--', 'ceph', 'osd', 'pool', 'create', 'bench_rados', '128', '128', 'replicated', '--size', '1', '--pg-num-min', '128'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 1a0b4120-405a-11ec-aba1-c5f3c07a3dc7 -- ceph osd pool create bench_rados 128 128 replicated --size 1 --pg-num-min 128
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
pool 'bench_rados' created
# ./benchmarks/bench-rados:267 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados min_size 1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:10:44,543147346-05:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:10:44,545800699-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '1a0b4120-405a-11ec-aba1-c5f3c07a3dc7', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'min_size', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 1a0b4120-405a-11ec-aba1-c5f3c07a3dc7 -- ceph osd pool set bench_rados min_size 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 min_size to 1
# ./benchmarks/bench-rados:268 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados pg_autoscale_mode off
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:10:48,925000520-05:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:10:48,927697635-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '1a0b4120-405a-11ec-aba1-c5f3c07a3dc7', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'pg_autoscale_mode', 'off'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 1a0b4120-405a-11ec-aba1-c5f3c07a3dc7 -- ceph osd pool set bench_rados pg_autoscale_mode off
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 pg_autoscale_mode to off
# ./benchmarks/bench-rados:269 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool application enable bench_rados benchmark
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:10:53,376229477-05:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:10:53,379140496-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '1a0b4120-405a-11ec-aba1-c5f3c07a3dc7', '--', 'ceph', 'osd', 'pool', 'application', 'enable', 'bench_rados', 'benchmark'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 1a0b4120-405a-11ec-aba1-c5f3c07a3dc7 -- ceph osd pool application enable bench_rados benchmark
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
enabled application 'benchmark' on pool 'bench_rados'
# ./benchmarks/bench-rados:270 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados noscrub 1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:10:57,931926215-05:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:10:57,934564680-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '1a0b4120-405a-11ec-aba1-c5f3c07a3dc7', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'noscrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 1a0b4120-405a-11ec-aba1-c5f3c07a3dc7 -- ceph osd pool set bench_rados noscrub 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 noscrub to 1
# ./benchmarks/bench-rados:271 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados nodeep-scrub 1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:11:03,028688319-05:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:11:03,031500832-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '1a0b4120-405a-11ec-aba1-c5f3c07a3dc7', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'nodeep-scrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 1a0b4120-405a-11ec-aba1-c5f3c07a3dc7 -- ceph osd pool set bench_rados nodeep-scrub 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 nodeep-scrub to 1
# ./benchmarks/bench-rados:272 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool ls detail
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:11:07,324286026-05:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:11:07,327231840-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '1a0b4120-405a-11ec-aba1-c5f3c07a3dc7', '--', 'ceph', 'osd', 'pool', 'ls', 'detail'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 1a0b4120-405a-11ec-aba1-c5f3c07a3dc7 -- ceph osd pool ls detail
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
pool 1 'device_health_metrics' replicated size 1 min_size 1 crush_rule 0 object_hash rjenkins pg_num 64 pgp_num 64 autoscale_mode on last_change 17 lfor 0/0/15 flags hashpspool stripe_width 0 pg_num_min 1 application mgr_devicehealth
pool 2 'bench_rados' replicated size 1 min_size 1 crush_rule 0 object_hash rjenkins pg_num 128 pgp_num 128 autoscale_mode off last_change 19 flags hashpspool,noscrub,nodeep-scrub stripe_width 0 pg_num_min 128 application benchmark

# ./benchmarks/bench-rados:273 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd df tree
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:11:11,437449512-05:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:11:11,440213554-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '1a0b4120-405a-11ec-aba1-c5f3c07a3dc7', '--', 'ceph', 'osd', 'df', 'tree'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 1a0b4120-405a-11ec-aba1-c5f3c07a3dc7 -- ceph osd df tree
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
ID  CLASS  WEIGHT   REWEIGHT  SIZE     RAW USE  DATA     OMAP  META     AVAIL    %USE  VAR   PGS  STATUS  TYPE NAME      
-1         0.09769         -  100 GiB  5.7 MiB  160 KiB   0 B  5.6 MiB  100 GiB  0.01  1.00    -          root default   
-3         0.09769         -  100 GiB  5.7 MiB  160 KiB   0 B  5.6 MiB  100 GiB  0.01  1.00    -              host node-1
 0    ssd  0.09769   1.00000  100 GiB  5.7 MiB  160 KiB   0 B  5.6 MiB  100 GiB  0.01  1.00  192      up          osd.0  
                       TOTAL  100 GiB  5.7 MiB  160 KiB   0 B  5.6 MiB  100 GiB  0.01                                    
MIN/MAX VAR: 1.00/1.00  STDDEV: 0


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:11:15,586991422-05:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:11:39,633561246-05:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:11:48,843752616-05:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:11:57,894910974-05:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:11:57,902061757-05:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:12:06,948324568-05:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:12:06,955447609-05:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:12:15,918269831-05:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:12:15,925004870-05:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:12:25,021148593-05:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:12:25,028840056-05:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:12:34,208165227-05:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:12:34,215173461-05:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:12:34,220528048-05:00] INFO: > The cluster is idle now.[0m


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:12:34,224062272-05:00] STAGE: Start benchmarking of mode write ...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:12:34,230132127-05:00] INFO: > Start system activity collection services[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:12:34,235138567-05:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:341 - rados_bench() > rm --force /mnt/sda8/ceph-research/scripts/output/sys_activity_256KB_write.dat.rados_bench_host.1
# ./benchmarks/bench-rados:342 - rados_bench() > sar_pid_rados_bench_host=192806
# ./benchmarks/bench-rados:342 - rados_bench() > S_TIME_FORMAT=ISO
# ./benchmarks/bench-rados:342 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/output/sys_activity_256KB_write.dat.rados_bench_host.1 2

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:12:34,241407047-05:00] INFO: >> for OSD host[0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:12:34,249794081-05:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.2 bash
10.10.2.2: b'41774\n'
[1] 01:12:34 [SUCCESS] ljishen@10.10.2.2
41774

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:12:34,436525276-05:00] INFO: > Run rados bench[0m
# ./benchmarks/bench-rados:370 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/output/rados_bench_256KB_write.log.1
## ./benchmarks/bench-rados:361 - rados_bench() > count_cpus 32-63,96-127
# ./benchmarks/bench-rados:361 - rados_bench() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 60 write --pool bench_rados -b 262144 -O 262144 --concurrent-ios 128 --show-time --write-object --write-omap --write-xattr --no-cleanup
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:12:34,455327899-05:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:12:34,457952037-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '1a0b4120-405a-11ec-aba1-c5f3c07a3dc7', '--', 'rados', 'bench', '60', 'write', '--pool', 'bench_rados', '-b', '262144', '-O', '262144', '--concurrent-ios', '128', '--show-time', '--write-object', '--write-omap', '--write-xattr', '--no-cleanup'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 1a0b4120-405a-11ec-aba1-c5f3c07a3dc7 -- rados bench 60 write --pool bench_rados -b 262144 -O 262144 --concurrent-ios 128 --show-time --write-object --write-omap --write-xattr --no-cleanup
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
hints = 1
2021-11-08T06:12:37.646285+0000 Maintaining 128 concurrent writes of 262144 bytes to objects of size 262144 for up to 60 seconds or 0 objects
2021-11-08T06:12:37.646297+0000 Object prefix: benchmark_data_node-0.bluefield2.ucsc-cmps10_7
2021-11-08T06:12:37.655466+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-11-08T06:12:37.655466+0000     0       0         0         0         0         0           -           0
2021-11-08T06:12:38.655568+0000     1     128      1292      1164   290.986       291    0.082663    0.104475
2021-11-08T06:12:39.655649+0000     2     128      2278      2150   268.732     246.5    0.206104    0.112574
2021-11-08T06:12:40.655729+0000     3     128      2882      2754   229.484       151     0.23741     0.13307
2021-11-08T06:12:41.655804+0000     4     128      3674      3546   221.609       198    0.132234    0.142717
2021-11-08T06:12:42.655887+0000     5     128      4421      4293   214.634    186.75    0.182867    0.141956
2021-11-08T06:12:43.655971+0000     6     128      4790      4662   194.235     92.25    0.111297    0.157502
2021-11-08T06:12:44.656051+0000     7     128      5256      5128   183.129     116.5     0.26721     0.17187
2021-11-08T06:12:45.656132+0000     8     128      5734      5606   175.174     119.5    0.218336    0.179575
2021-11-08T06:12:46.656223+0000     9     128      6342      6214   172.598       152     0.26175    0.183396
2021-11-08T06:12:47.656312+0000    10     128      6718      6590   164.737        94    0.239532    0.191395
2021-11-08T06:12:48.656400+0000    11     128      7288      7160   162.714     142.5    0.169268    0.194502
2021-11-08T06:12:49.656482+0000    12     128      8174      8046   167.612     221.5   0.0988042    0.189902
2021-11-08T06:12:50.656566+0000    13     128      8665      8537    164.16    122.75    0.270459    0.190896
2021-11-08T06:12:51.656650+0000    14     128      9113      8985   160.433       112    0.249252    0.197875
2021-11-08T06:12:52.656730+0000    15     128      9450      9322   155.354     84.25    0.321067    0.202044
2021-11-08T06:12:53.656812+0000    16     128     10150     10022   156.581       175    0.200795    0.203687
2021-11-08T06:12:54.656897+0000    17     128     10584     10456   153.752     108.5    0.300535    0.205848
2021-11-08T06:12:55.656976+0000    18     128     11001     10873   151.002    104.25    0.311523    0.210525
2021-11-08T06:12:56.657061+0000    19     128     11350     11222   147.646     87.25    0.300007    0.213942
2021-11-08T06:12:57.657136+0000 min lat: 0.00774451 max lat: 0.586966 avg lat: 0.215951
2021-11-08T06:12:57.657136+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-11-08T06:12:57.657136+0000    20     128     11924     11796   147.438     143.5    0.156602    0.215951
2021-11-08T06:12:58.657235+0000    21     128     12492     12364   147.178       142    0.295315    0.216569
2021-11-08T06:12:59.657312+0000    22     128     12840     12712   144.443        87    0.264052    0.219839
2021-11-08T06:13:00.657392+0000    23     128     13266     13138   142.793     106.5    0.264093     0.22342
2021-11-08T06:13:01.657476+0000    24     128     13695     13567   141.311    107.25    0.187716    0.225735
2021-11-08T06:13:02.657565+0000    25     128     14302     14174   141.728    151.75    0.158349    0.224906
2021-11-08T06:13:03.657640+0000    26     128     14681     14553   139.921     94.75     0.28422    0.226361
2021-11-08T06:13:04.657720+0000    27     128     15096     14968   138.581    103.75    0.247403    0.229347
2021-11-08T06:13:05.657801+0000    28     128     15430     15302   136.614      83.5    0.464427    0.231881
2021-11-08T06:13:06.657884+0000    29     128     15935     15807   136.256    126.25    0.223907    0.233543
2021-11-08T06:13:07.657969+0000    30     128     16522     16394   136.606    146.75    0.228446    0.233049
2021-11-08T06:13:08.658053+0000    31     128     16891     16763   135.174     92.25    0.268012    0.235128
2021-11-08T06:13:09.658127+0000    32     128     17318     17190   134.286    106.75    0.264882     0.23729
2021-11-08T06:13:10.658204+0000    33     128     17663     17535    132.83     86.25    0.242448    0.239289
2021-11-08T06:13:11.658282+0000    34     128     18195     18067   132.835       133    0.220296    0.240133
2021-11-08T06:13:12.658371+0000    35     128     18708     18580   132.703    128.25   0.0654676    0.240273
2021-11-08T06:13:13.658466+0000    36     128     19054     18926    131.42      86.5   0.0761832    0.241908
2021-11-08T06:13:14.658555+0000    37     128     19487     19359   130.793    108.25    0.267583     0.24354
2021-11-08T06:13:15.658640+0000    38     128     19931     19803   130.272       111    0.363513    0.244504
2021-11-08T06:13:16.658720+0000    39     128     20582     20454   131.105    162.75    0.197953    0.243134
2021-11-08T06:13:17.658801+0000 min lat: 0.00774451 max lat: 0.614976 avg lat: 0.244797
2021-11-08T06:13:17.658801+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-11-08T06:13:17.658801+0000    40     128     20972     20844   130.264      97.5    0.321886    0.244797
2021-11-08T06:13:18.658888+0000    41     128     21307     21179    129.13     83.75    0.342724    0.246479
2021-11-08T06:13:19.658965+0000    42     128     21713     21585   128.472     101.5    0.326511    0.248017
2021-11-08T06:13:20.659042+0000    43     128     22350     22222   129.187    159.25    0.183772     0.24722
2021-11-08T06:13:21.659116+0000    44     128     22787     22659   128.734    109.25   0.0832769    0.247555
2021-11-08T06:13:22.659200+0000    45     127     23148     23021   127.884      90.5    0.303945    0.248926
2021-11-08T06:13:23.659275+0000    46     128     23589     23461   127.495       110   0.0761864    0.250163
2021-11-08T06:13:24.659352+0000    47     128     24162     24034    127.83    143.25    0.142733     0.24973
2021-11-08T06:13:25.659426+0000    48     128     24678     24550   127.854       129    0.215699    0.248919
2021-11-08T06:13:26.659508+0000    49     128     25123     24995   127.515    111.25    0.317381    0.250435
2021-11-08T06:13:27.659585+0000    50     128     25465     25337   126.675      85.5   0.0662695    0.251572
2021-11-08T06:13:28.659664+0000    51     128     25897     25769   126.308       108    0.167916    0.252424
2021-11-08T06:13:29.659743+0000    52     128     26562     26434   127.076    166.25    0.158916    0.251295
2021-11-08T06:13:30.659829+0000    53     128     27004     26876   126.763     110.5    0.310311    0.251992
2021-11-08T06:13:31.659918+0000    54     128     27371     27243   126.115     91.75    0.326131    0.253099
2021-11-08T06:13:32.660002+0000    55     128     27754     27626   125.562     95.75    0.285037    0.254303
2021-11-08T06:13:33.660084+0000    56     128     28356     28228   126.008     150.5    0.166405    0.253406
2021-11-08T06:13:34.660165+0000    57     128     28859     28731   126.003    125.75     0.29267    0.253143
2021-11-08T06:13:35.660249+0000    58     128     29209     29081   125.339      87.5    0.284548    0.254059
2021-11-08T06:13:36.660330+0000    59     128     29584     29456   124.803     93.75    0.281982    0.255263
2021-11-08T06:13:37.660404+0000 min lat: 0.00774451 max lat: 0.627199 avg lat: 0.25591
2021-11-08T06:13:37.660404+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-11-08T06:13:37.660404+0000    60     128     30107     29979   124.902    130.75    0.145507     0.25591
2021-11-08T06:13:38.660541+0000 Total time run:         60.2299
Total writes made:      30107
Write size:             262144
Object size:            262144
Bandwidth (MB/sec):     124.967
Stddev Bandwidth:       41.0937
Max bandwidth (MB/sec): 291
Min bandwidth (MB/sec): 83.5
Average IOPS:           499
Stddev IOPS:            164.375
Max IOPS:               1164
Min IOPS:               334
Average Latency(s):     0.255928
Stddev Latency(s):      0.103154
Max latency(s):         0.627199
Min latency(s):         0.00774451

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:13:39,330802956-05:00] INFO: > Stop system activity collection services[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:13:39,336129309-05:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:391 - rados_bench() > kill -INT 192806

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:13:39,341699492-05:00] INFO: >> for OSD host[0m
# ./benchmarks/bench-rados:396 - rados_bench() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.2 kill -INT 41774
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:13:39,348969640-05:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.2 kill -INT 41774
[1] 01:13:39 [SUCCESS] ljishen@10.10.2.2

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:13:39,527992240-05:00] INFO: >>> Copy system activity file from OSD host[0m
## ./benchmarks/bench-rados:403 - rados_bench() > basename -- /tmp/bench-rados/sys_activity_256KB_write.dat.osd_host.1
# ./benchmarks/bench-rados:403 - rados_bench() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.2:/tmp/bench-rados/sys_activity_256KB_write.dat.osd_host.1 /mnt/sda8/ceph-research/scripts/output/sys_activity_256KB_write.dat.osd_host.1


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:13:39,698809298-05:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:14:03,789721603-05:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 30.11k objects, 7.4 GiB
    usage:   22 GiB used, 78 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:14:03,797171250-05:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:14:12,926799628-05:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 30.11k objects, 7.4 GiB
    usage:   22 GiB used, 78 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:14:12,934064005-05:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:14:21,921872946-05:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 30.11k objects, 7.4 GiB
    usage:   22 GiB used, 78 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:14:21,929261638-05:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:14:30,909353463-05:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 30.11k objects, 7.4 GiB
    usage:   22 GiB used, 78 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:14:30,916163544-05:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:14:39,891257137-05:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 30.11k objects, 7.4 GiB
    usage:   22 GiB used, 78 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:14:39,898580747-05:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:14:39,904223046-05:00] INFO: > The cluster is idle now.[0m


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:14:39,907742291-05:00] STAGE: Start benchmarking of mode seq ...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:14:39,913984622-05:00] INFO: > Start system activity collection services[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:14:39,919031177-05:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:341 - rados_bench() > rm --force /mnt/sda8/ceph-research/scripts/output/sys_activity_256KB_seq.dat.rados_bench_host.1
# ./benchmarks/bench-rados:342 - rados_bench() > sar_pid_rados_bench_host=194195
# ./benchmarks/bench-rados:342 - rados_bench() > S_TIME_FORMAT=ISO
# ./benchmarks/bench-rados:342 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/output/sys_activity_256KB_seq.dat.rados_bench_host.1 2

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:14:39,925311008-05:00] INFO: >> for OSD host[0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:14:39,933792852-05:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.2 bash
10.10.2.2: b'41869\n'
[1] 01:14:40 [SUCCESS] ljishen@10.10.2.2
41869

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:14:40,120983864-05:00] INFO: > Run rados bench[0m
# ./benchmarks/bench-rados:379 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/output/rados_bench_256KB_seq.log.1
## ./benchmarks/bench-rados:376 - rados_bench() > count_cpus 32-63,96-127
# ./benchmarks/bench-rados:376 - rados_bench() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 128 --show-time
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:14:40,140229804-05:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:14:40,143083605-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '1a0b4120-405a-11ec-aba1-c5f3c07a3dc7', '--', 'rados', 'bench', '99999999', 'seq', '--pool', 'bench_rados', '--concurrent-ios', '128', '--show-time'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 1a0b4120-405a-11ec-aba1-c5f3c07a3dc7 -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 128 --show-time
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
hints = 1
2021-11-08T06:14:43.179220+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-11-08T06:14:43.179220+0000     0       0         0         0         0         0           -           0
2021-11-08T06:14:44.179335+0000     1     128      7085      6957   1738.93   1739.25   0.0194695   0.0181879
2021-11-08T06:14:45.179434+0000     2     127     14036     13909   1738.38      1738   0.0183564   0.0182881
2021-11-08T06:14:46.179529+0000     3     127     21027     20900   1741.45   1747.75   0.0188063   0.0182865
2021-11-08T06:14:47.179594+0000     4     128     27992     27864   1741.31      1741   0.0222354   0.0183049
2021-11-08T06:14:48.179695+0000 Total time run:       4.3282
Total reads made:     30107
Read size:            262144
Object size:          262144
Bandwidth (MB/sec):   1739
Average IOPS:         6956
Stddev IOPS:          17.3781
Max IOPS:             6991
Min IOPS:             6952
Average Latency(s):   0.0183359
Max latency(s):       0.0286636
Min latency(s):       0.00263652

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:14:48,898694190-05:00] INFO: > Stop system activity collection services[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:14:48,904001518-05:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:391 - rados_bench() > kill -INT 194195

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:14:48,909478104-05:00] INFO: >> for OSD host[0m
# ./benchmarks/bench-rados:396 - rados_bench() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.2 kill -INT 41869
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:14:48,917013022-05:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.2 kill -INT 41869
[1] 01:14:49 [SUCCESS] ljishen@10.10.2.2

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:14:49,096570832-05:00] INFO: >>> Copy system activity file from OSD host[0m
## ./benchmarks/bench-rados:403 - rados_bench() > basename -- /tmp/bench-rados/sys_activity_256KB_seq.dat.osd_host.1
# ./benchmarks/bench-rados:403 - rados_bench() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.2:/tmp/bench-rados/sys_activity_256KB_seq.dat.osd_host.1 /mnt/sda8/ceph-research/scripts/output/sys_activity_256KB_seq.dat.osd_host.1


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:14:49,245422062-05:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:15:13,224174943-05:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 30.11k objects, 7.4 GiB
    usage:   22 GiB used, 78 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:15:13,231250564-05:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:15:22,316035797-05:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 30.11k objects, 7.4 GiB
    usage:   22 GiB used, 78 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:15:22,323126046-05:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:15:31,416673616-05:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 30.11k objects, 7.4 GiB
    usage:   22 GiB used, 78 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:15:31,423514034-05:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:15:40,486409681-05:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 30.11k objects, 7.4 GiB
    usage:   22 GiB used, 78 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:15:40,493594578-05:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:15:49,558715875-05:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 30.11k objects, 7.4 GiB
    usage:   22 GiB used, 78 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:15:49,566204586-05:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:15:49,571859659-05:00] INFO: > The cluster is idle now.[0m


[1;7;39;49m[2021-11-08T01:15:49,574115643-05:00][RUNNING][ROUND 2/4/21] object_size=256KB[0m


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:15:49,577363126-05:00] STAGE: Launch a Ceph cluster (CEPH_HOST: 10.10.2.3, OSD_HOST: 10.10.2.2) ...[0m
# ./benchmarks/bench-rados:183 - launch_ceph_cluster() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.3 bash -euo pipefail
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:15:49,586128073-05:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.3 bash -euo pipefail
10.10.2.3: b'\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T01:15:49,986059121-05:00] INFO: > Remove existing clusters\x1b[0m\n'
10.10.2.3: b'## bash:17 -  > basename -- /var/lib/ceph/1a0b4120-405a-11ec-aba1-c5f3c07a3dc7\n'
10.10.2.3: b'# bash:17 -  > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm --host 10.10.2.3 --host 10.10.2.2 rm-cluster --force --fsid 1a0b4120-405a-11ec-aba1-c5f3c07a3dc7\n'
10.10.2.3: b"\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T01:15:49,996494309-05:00] DEBUG: HOST_PARAMS: ['--host', '10.10.2.3', '--host', '10.10.2.2']\x1b[0m\n"
10.10.2.3: b"\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T01:15:50,000316411-05:00] DEBUG: CEPHADM_PARAMS: ['rm-cluster', '--force', '--fsid', '1a0b4120-405a-11ec-aba1-c5f3c07a3dc7']\x1b[0m\n"
10.10.2.3: b"# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:63 -  > /tmp/bench-rados/ceph-research/scripts/parallel-ssh --host 10.10.2.3 --host 10.10.2.2 'sudo python3 -u - rm-cluster --force --fsid 1a0b4120-405a-11ec-aba1-c5f3c07a3dc7'\n"
10.10.2.3: b'\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T01:15:50,008510351-05:00] DEBUG: command from stdin\x1b[0m\n'
10.10.2.3: b"# /tmp/bench-rados/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host 10.10.2.3 --host 10.10.2.2 'sudo python3 -u - rm-cluster --force --fsid 1a0b4120-405a-11ec-aba1-c5f3c07a3dc7'\n"
10.10.2.3: b'[1] 01:15:55 [SUCCESS] 10.10.2.3\n[2] 01:16:01 [SUCCESS] 10.10.2.2\n'
10.10.2.3: b'\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T01:16:01,943991132-05:00] INFO: > Deploy a new cluster\x1b[0m\n'
10.10.2.3: b'# bash:24 -  > /tmp/bench-rados/ceph-research/scripts/ceph-deploy -m 10.10.2.3 -o node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/10.10.2.2:/dev/nvme0n1\n'
10.10.2.3: b'\n\n\x1b[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T01:16:01,955411310-05:00] STAGE: Verify passwordless SSH to hosts, passwordless sudo via ljishen, and the existence of devices on hosts...\x1b[0m\n'
10.10.2.3: b'\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T01:16:01,960231464-05:00] INFO: > Check the host for the monitor\x1b[0m\n'
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:181 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@10.10.2.3 sudo --non-interactive --validate\n'
10.10.2.3: b'\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T01:16:02,110443717-05:00] INFO: > Check hosts for OSDs\x1b[0m\n'
10.10.2.3: b"\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T01:16:02,115274752-05:00] INFO: >> Check host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'\x1b[0m\n"
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:200 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us sudo --non-interactive --validate\n'
10.10.2.3: b"\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T01:16:02,262113371-05:00] INFO: >>> Check device '/dev/nvme0n1'\x1b[0m\n"
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:215 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us test -b /dev/nvme0n1\n'
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:226 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us lsblk --nodeps --noheadings --output TYPE /dev/nvme0n1\n'
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:227 - verify_parameters() > grep --word-regexp disk\n'
10.10.2.3: b'\n\n\x1b[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T01:16:02,538587219-05:00] STAGE: Reset devices on hosts...\x1b[0m\n'
10.10.2.3: b"\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T01:16:02,543706838-05:00] INFO: > Resetting device '/dev/nvme0n1' on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'\x1b[0m\n"
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:288 - reset_host_devices() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us sudo sh\n'
10.10.2.3: b'  Removing ceph--8dd60051--84e0--4b4a--afb0--ab0c2dfb2e30-osd--block--5bf3ba9d--0a99--48ed--8627--3ad8e82a1cc2 (253:0)\n'
10.10.2.3: b'  Archiving volume group "ceph-8dd60051-84e0-4b4a-afb0-ab0c2dfb2e30" metadata (seqno 5).\n  Releasing logical volume "osd-block-5bf3ba9d-0a99-48ed-8627-3ad8e82a1cc2"\n'
10.10.2.3: b'  Creating volume group backup "/etc/lvm/backup/ceph-8dd60051-84e0-4b4a-afb0-ab0c2dfb2e30" (seqno 6).\n'
10.10.2.3: b'  Logical volume "osd-block-5bf3ba9d-0a99-48ed-8627-3ad8e82a1cc2" successfully removed\n'
10.10.2.3: b'  Removing physical volume "/dev/nvme0n1" from volume group "ceph-8dd60051-84e0-4b4a-afb0-ab0c2dfb2e30"\n'
10.10.2.3: b'  Volume group "ceph-8dd60051-84e0-4b4a-afb0-ab0c2dfb2e30" successfully removed\n'
10.10.2.3: b'\n\n\x1b[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T01:16:02,887436802-05:00] STAGE: Bootstrap a new cluster...\x1b[0m\n'
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:317 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.3 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults\n'
10.10.2.3: b'\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T01:16:02,897168043-05:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.3: b"\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T01:16:02,900344325-05:00] DEBUG: CEPHADM_PARAMS: ['bootstrap', '--config', '/tmp/bench-rados/ceph-research/scripts/ceph.conf', '--mon-ip', '10.10.2.3', '--output-dir', '/tmp/bench-rados/deployment_data_root/etc/ceph', '--ssh-user', 'ljishen', '--skip-dashboard', '--no-minimize-config', '--allow-overwrite', '--allow-fqdn-hostname', '--orphan-initial-daemons', '--skip-monitoring-stack', '--single-host-defaults']\x1b[0m\n"
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.3 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults\n'
10.10.2.3: b'Verifying podman|docker is present...\nVerifying lvm2 is present...\n'
10.10.2.3: b'Verifying time synchronization is in place...\n'
10.10.2.3: b'Unit ntp.service is enabled and running\n'
10.10.2.3: b'Repeating the final host check...\ndocker (/usr/bin/docker) is present\n'
10.10.2.3: b'systemctl is present\n'
10.10.2.3: b'lvcreate is present\n'
10.10.2.3: b'Unit ntp.service is enabled and running\n'
10.10.2.3: b'Host looks OK\n'
10.10.2.3: b'Cluster fsid: 599a25e4-405b-11ec-aba1-c5f3c07a3dc7\n'
10.10.2.3: b'Verifying IP 10.10.2.3 port 3300 ...\n'
10.10.2.3: b'Verifying IP 10.10.2.3 port 6789 ...\n'
10.10.2.3: b'Mon IP `10.10.2.3` is in CIDR network `10.10.2.0/24`\n'
10.10.2.3: b'- internal network (--cluster-network) has not been provided, OSD replication will default to the public_network\n'
10.10.2.3: b'Adjusting default settings to suit single-host cluster...\nPulling container image quay.io/ceph/ceph:v16...\n'
10.10.2.3: b'Ceph version: ceph version 16.2.6 (ee28fb57e47e9f88813e24bbf4c14496ca299d31) pacific (stable)\n'
10.10.2.3: b'Extracting ceph user uid/gid from container image...\n'
10.10.2.3: b'Creating initial keys...\n'
10.10.2.3: b'Creating initial monmap...\n'
10.10.2.3: b'Creating mon...\n'
10.10.2.3: b'Waiting for mon to start...\n'
10.10.2.3: b'Waiting for mon...\n'
10.10.2.3: b'mon is available\nSetting mon public_network to 10.10.2.0/24\n'
10.10.2.3: b'Wrote config to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf\n'
10.10.2.3: b'Wrote keyring to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring\n'
10.10.2.3: b'Creating mgr...\n'
10.10.2.3: b'Verifying port 9283 ...\n'
10.10.2.3: b'Waiting for mgr to start...\nWaiting for mgr...\n'
10.10.2.3: b'mgr not available, waiting (1/15)...\n'
10.10.2.3: b'mgr not available, waiting (2/15)...\n'
10.10.2.3: b'mgr is available\n'
10.10.2.3: b'Enabling cephadm module...\n'
10.10.2.3: b'Waiting for the mgr to restart...\nWaiting for mgr epoch 5...\n'
10.10.2.3: b'mgr epoch 5 is available\nSetting orchestrator backend to cephadm...\n'
10.10.2.3: b'Generating ssh key...\n'
10.10.2.3: b'Wrote public SSH key to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub\n'
10.10.2.3: b'Adding key to ljishen@localhost authorized_keys...\n'
10.10.2.3: b'Adding host node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us...\n'
10.10.2.3: b'Deploying unmanaged mon service...\n'
10.10.2.3: b'Deploying unmanaged mgr service...\n'
10.10.2.3: b'You can access the Ceph CLI with:\n\n\tsudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --fsid 599a25e4-405b-11ec-aba1-c5f3c07a3dc7 -c /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf -k /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring\n\n'
10.10.2.3: b'Please consider enabling telemetry to help improve Ceph:\n\n\tceph telemetry on\n\nFor more information see:\n\n\thttps://docs.ceph.com/docs/pacific/mgr/telemetry/\n\nBootstrap complete.\n'
10.10.2.3: b'\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T01:17:10,030234587-05:00] INFO: Sleep for 20 seconds\x1b[0m\n'
10.10.2.3: b'\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T01:17:30,037834738-05:00] INFO: > Set the default number of monitor daemons to 1\x1b[0m\n'
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:337 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1\n'
10.10.2.3: b'\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T01:17:30,048329770-05:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.3: b"\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T01:17:30,052444313-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mon', '1']\x1b[0m\n"
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1\n'
10.10.2.3: b'Inferring fsid 599a25e4-405b-11ec-aba1-c5f3c07a3dc7\n'
10.10.2.3: b'Inferring config /var/lib/ceph/599a25e4-405b-11ec-aba1-c5f3c07a3dc7/mon.node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.3: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.3: b'Scheduled mon update...\n'
10.10.2.3: b'\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T01:17:39,585185817-05:00] INFO: > Set the default number of manager daemons to 1\x1b[0m\n'
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:342 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1\n'
10.10.2.3: b'\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T01:17:39,595429856-05:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.3: b"\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T01:17:39,599005412-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mgr', '1']\x1b[0m\n"
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1\n'
10.10.2.3: b'Inferring fsid 599a25e4-405b-11ec-aba1-c5f3c07a3dc7\n'
10.10.2.3: b'Inferring config /var/lib/ceph/599a25e4-405b-11ec-aba1-c5f3c07a3dc7/mon.node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.3: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.3: b'Scheduled mgr update...\n'
10.10.2.3: b'\n\n\x1b[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T01:17:49,817374930-05:00] STAGE: Add hosts to the cluster...\x1b[0m\n'
10.10.2.3: b"\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T01:17:49,823530595-05:00] INFO: > Adding host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'\x1b[0m\n"
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:361 - add_hosts() > ssh-copy-id -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -f -i /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\n'
10.10.2.3: b'/usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: "/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub"\n'
10.10.2.3: b'\nNumber of key(s) added: 1\n\nNow try logging into the machine, with:   "ssh -o \'GlobalKnownHostsFile=/dev/null\' -o \'LogLevel=ERROR\' -o \'PasswordAuthentication=no\' -o \'StrictHostKeyChecking=no\' -o \'UserKnownHostsFile=/dev/null\' \'ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\'"\nand check to make sure that only the key(s) you wanted were added.\n\n'
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:367 - add_hosts() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us 10.10.2.2\n'
10.10.2.3: b'\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T01:17:50,028327601-05:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.3: b"\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T01:17:50,031594284-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'host', 'add', 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us', '10.10.2.2']\x1b[0m\n"
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us 10.10.2.2\n'
10.10.2.3: b'Inferring fsid 599a25e4-405b-11ec-aba1-c5f3c07a3dc7\n'
10.10.2.3: b'Inferring config /var/lib/ceph/599a25e4-405b-11ec-aba1-c5f3c07a3dc7/mon.node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.3: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.3: b"Added host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us' with addr '10.10.2.2'\n"
10.10.2.3: b'\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T01:17:59,997102061-05:00] INFO: Sleep for 20 seconds\x1b[0m\n'
10.10.2.3: b'\n\n\x1b[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T01:18:20,002340820-05:00] STAGE: Add OSDs to the cluster...\x1b[0m\n'
10.10.2.3: b"\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T01:18:20,009290512-05:00] INFO: > Adding devices ['/dev/nvme0n1'] on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us' as OSDs\x1b[0m\n"
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:388 - add_host_devices_as_osds() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1\n'
10.10.2.3: b'\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T01:18:20,019132332-05:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.3: b"\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T01:18:20,022685356-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'daemon', 'add', 'osd', 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1']\x1b[0m\n"
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1\n'
10.10.2.3: b'Inferring fsid 599a25e4-405b-11ec-aba1-c5f3c07a3dc7\n'
10.10.2.3: b'Inferring config /var/lib/ceph/599a25e4-405b-11ec-aba1-c5f3c07a3dc7/mon.node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.3: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.3: b"Created osd(s) 0 on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'\n"
10.10.2.3: b'\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T01:18:45,783650433-05:00] INFO: > Sleep for 20 seconds\x1b[0m\n'
10.10.2.3: b'\n\n\x1b[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T01:19:05,789439092-05:00] STAGE: Show Cluster Status\x1b[0m\n'
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:408 - show_status() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status\n'
10.10.2.3: b'\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T01:19:05,799553937-05:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.3: b"\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T01:19:05,803183956-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', '--status']\x1b[0m\n"
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status\n'
10.10.2.3: b'Inferring fsid 599a25e4-405b-11ec-aba1-c5f3c07a3dc7\n'
10.10.2.3: b'Inferring config /var/lib/ceph/599a25e4-405b-11ec-aba1-c5f3c07a3dc7/mon.node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.3: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.3: b'  cluster:\n    id:     599a25e4-405b-11ec-aba1-c5f3c07a3dc7\n    health: HEALTH_WARN\n            1 pool(s) have no replicas configured\n \n  services:\n    mon: 1 daemons, quorum node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us (age 2m)\n    mgr: node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us.kfkvzc(active, since 2m)\n    osd: 1 osds: 1 up (since 17s), 1 in (since 41s)\n \n  data:\n    pools:   1 pools, 1 pgs\n    objects: 0 objects, 0 B\n    usage:   4.7 MiB used, 100 GiB / 100 GiB avail\n    pgs:     1 active+clean\n \n'
[1] 01:19:15 [SUCCESS] ljishen@10.10.2.3

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:15:49,986059121-05:00] INFO: > Remove existing clusters[0m
## bash:17 -  > basename -- /var/lib/ceph/1a0b4120-405a-11ec-aba1-c5f3c07a3dc7
# bash:17 -  > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm --host 10.10.2.3 --host 10.10.2.2 rm-cluster --force --fsid 1a0b4120-405a-11ec-aba1-c5f3c07a3dc7
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:15:49,996494309-05:00] DEBUG: HOST_PARAMS: ['--host', '10.10.2.3', '--host', '10.10.2.2'][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:15:50,000316411-05:00] DEBUG: CEPHADM_PARAMS: ['rm-cluster', '--force', '--fsid', '1a0b4120-405a-11ec-aba1-c5f3c07a3dc7'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:63 -  > /tmp/bench-rados/ceph-research/scripts/parallel-ssh --host 10.10.2.3 --host 10.10.2.2 'sudo python3 -u - rm-cluster --force --fsid 1a0b4120-405a-11ec-aba1-c5f3c07a3dc7'
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:15:50,008510351-05:00] DEBUG: command from stdin[0m
# /tmp/bench-rados/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host 10.10.2.3 --host 10.10.2.2 'sudo python3 -u - rm-cluster --force --fsid 1a0b4120-405a-11ec-aba1-c5f3c07a3dc7'
[1] 01:15:55 [SUCCESS] 10.10.2.3
[2] 01:16:01 [SUCCESS] 10.10.2.2

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:16:01,943991132-05:00] INFO: > Deploy a new cluster[0m
# bash:24 -  > /tmp/bench-rados/ceph-research/scripts/ceph-deploy -m 10.10.2.3 -o node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/10.10.2.2:/dev/nvme0n1


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:16:01,955411310-05:00] STAGE: Verify passwordless SSH to hosts, passwordless sudo via ljishen, and the existence of devices on hosts...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:16:01,960231464-05:00] INFO: > Check the host for the monitor[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:181 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@10.10.2.3 sudo --non-interactive --validate

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:16:02,110443717-05:00] INFO: > Check hosts for OSDs[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:16:02,115274752-05:00] INFO: >> Check host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:200 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us sudo --non-interactive --validate

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:16:02,262113371-05:00] INFO: >>> Check device '/dev/nvme0n1'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:215 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us test -b /dev/nvme0n1
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:226 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us lsblk --nodeps --noheadings --output TYPE /dev/nvme0n1
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:227 - verify_parameters() > grep --word-regexp disk


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:16:02,538587219-05:00] STAGE: Reset devices on hosts...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:16:02,543706838-05:00] INFO: > Resetting device '/dev/nvme0n1' on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:288 - reset_host_devices() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us sudo sh
  Removing ceph--8dd60051--84e0--4b4a--afb0--ab0c2dfb2e30-osd--block--5bf3ba9d--0a99--48ed--8627--3ad8e82a1cc2 (253:0)
  Archiving volume group "ceph-8dd60051-84e0-4b4a-afb0-ab0c2dfb2e30" metadata (seqno 5).
  Releasing logical volume "osd-block-5bf3ba9d-0a99-48ed-8627-3ad8e82a1cc2"
  Creating volume group backup "/etc/lvm/backup/ceph-8dd60051-84e0-4b4a-afb0-ab0c2dfb2e30" (seqno 6).
  Logical volume "osd-block-5bf3ba9d-0a99-48ed-8627-3ad8e82a1cc2" successfully removed
  Removing physical volume "/dev/nvme0n1" from volume group "ceph-8dd60051-84e0-4b4a-afb0-ab0c2dfb2e30"
  Volume group "ceph-8dd60051-84e0-4b4a-afb0-ab0c2dfb2e30" successfully removed


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:16:02,887436802-05:00] STAGE: Bootstrap a new cluster...[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:317 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.3 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:16:02,897168043-05:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:16:02,900344325-05:00] DEBUG: CEPHADM_PARAMS: ['bootstrap', '--config', '/tmp/bench-rados/ceph-research/scripts/ceph.conf', '--mon-ip', '10.10.2.3', '--output-dir', '/tmp/bench-rados/deployment_data_root/etc/ceph', '--ssh-user', 'ljishen', '--skip-dashboard', '--no-minimize-config', '--allow-overwrite', '--allow-fqdn-hostname', '--orphan-initial-daemons', '--skip-monitoring-stack', '--single-host-defaults'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.3 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults
Verifying podman|docker is present...
Verifying lvm2 is present...
Verifying time synchronization is in place...
Unit ntp.service is enabled and running
Repeating the final host check...
docker (/usr/bin/docker) is present
systemctl is present
lvcreate is present
Unit ntp.service is enabled and running
Host looks OK
Cluster fsid: 599a25e4-405b-11ec-aba1-c5f3c07a3dc7
Verifying IP 10.10.2.3 port 3300 ...
Verifying IP 10.10.2.3 port 6789 ...
Mon IP `10.10.2.3` is in CIDR network `10.10.2.0/24`
- internal network (--cluster-network) has not been provided, OSD replication will default to the public_network
Adjusting default settings to suit single-host cluster...
Pulling container image quay.io/ceph/ceph:v16...
Ceph version: ceph version 16.2.6 (ee28fb57e47e9f88813e24bbf4c14496ca299d31) pacific (stable)
Extracting ceph user uid/gid from container image...
Creating initial keys...
Creating initial monmap...
Creating mon...
Waiting for mon to start...
Waiting for mon...
mon is available
Setting mon public_network to 10.10.2.0/24
Wrote config to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf
Wrote keyring to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring
Creating mgr...
Verifying port 9283 ...
Waiting for mgr to start...
Waiting for mgr...
mgr not available, waiting (1/15)...
mgr not available, waiting (2/15)...
mgr is available
Enabling cephadm module...
Waiting for the mgr to restart...
Waiting for mgr epoch 5...
mgr epoch 5 is available
Setting orchestrator backend to cephadm...
Generating ssh key...
Wrote public SSH key to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub
Adding key to ljishen@localhost authorized_keys...
Adding host node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us...
Deploying unmanaged mon service...
Deploying unmanaged mgr service...
You can access the Ceph CLI with:

	sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --fsid 599a25e4-405b-11ec-aba1-c5f3c07a3dc7 -c /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf -k /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring

Please consider enabling telemetry to help improve Ceph:

	ceph telemetry on

For more information see:

	https://docs.ceph.com/docs/pacific/mgr/telemetry/

Bootstrap complete.

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:17:10,030234587-05:00] INFO: Sleep for 20 seconds[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:17:30,037834738-05:00] INFO: > Set the default number of monitor daemons to 1[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:337 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:17:30,048329770-05:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:17:30,052444313-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mon', '1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1
Inferring fsid 599a25e4-405b-11ec-aba1-c5f3c07a3dc7
Inferring config /var/lib/ceph/599a25e4-405b-11ec-aba1-c5f3c07a3dc7/mon.node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Scheduled mon update...

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:17:39,585185817-05:00] INFO: > Set the default number of manager daemons to 1[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:342 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:17:39,595429856-05:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:17:39,599005412-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mgr', '1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1
Inferring fsid 599a25e4-405b-11ec-aba1-c5f3c07a3dc7
Inferring config /var/lib/ceph/599a25e4-405b-11ec-aba1-c5f3c07a3dc7/mon.node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Scheduled mgr update...


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:17:49,817374930-05:00] STAGE: Add hosts to the cluster...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:17:49,823530595-05:00] INFO: > Adding host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:361 - add_hosts() > ssh-copy-id -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -f -i /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us
/usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: "/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub"

Number of key(s) added: 1

Now try logging into the machine, with:   "ssh -o 'GlobalKnownHostsFile=/dev/null' -o 'LogLevel=ERROR' -o 'PasswordAuthentication=no' -o 'StrictHostKeyChecking=no' -o 'UserKnownHostsFile=/dev/null' 'ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'"
and check to make sure that only the key(s) you wanted were added.

# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:367 - add_hosts() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us 10.10.2.2
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:17:50,028327601-05:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:17:50,031594284-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'host', 'add', 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us', '10.10.2.2'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us 10.10.2.2
Inferring fsid 599a25e4-405b-11ec-aba1-c5f3c07a3dc7
Inferring config /var/lib/ceph/599a25e4-405b-11ec-aba1-c5f3c07a3dc7/mon.node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Added host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us' with addr '10.10.2.2'

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:17:59,997102061-05:00] INFO: Sleep for 20 seconds[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:18:20,002340820-05:00] STAGE: Add OSDs to the cluster...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:18:20,009290512-05:00] INFO: > Adding devices ['/dev/nvme0n1'] on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us' as OSDs[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:388 - add_host_devices_as_osds() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:18:20,019132332-05:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:18:20,022685356-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'daemon', 'add', 'osd', 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1
Inferring fsid 599a25e4-405b-11ec-aba1-c5f3c07a3dc7
Inferring config /var/lib/ceph/599a25e4-405b-11ec-aba1-c5f3c07a3dc7/mon.node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Created osd(s) 0 on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:18:45,783650433-05:00] INFO: > Sleep for 20 seconds[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:19:05,789439092-05:00] STAGE: Show Cluster Status[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:408 - show_status() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:19:05,799553937-05:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:19:05,803183956-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', '--status'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status
Inferring fsid 599a25e4-405b-11ec-aba1-c5f3c07a3dc7
Inferring config /var/lib/ceph/599a25e4-405b-11ec-aba1-c5f3c07a3dc7/mon.node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
  cluster:
    id:     599a25e4-405b-11ec-aba1-c5f3c07a3dc7
    health: HEALTH_WARN
            1 pool(s) have no replicas configured
 
  services:
    mon: 1 daemons, quorum node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us (age 2m)
    mgr: node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us.kfkvzc(active, since 2m)
    osd: 1 osds: 1 up (since 17s), 1 in (since 41s)
 
  data:
    pools:   1 pools, 1 pgs
    objects: 0 objects, 0 B
    usage:   4.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     1 active+clean
 

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:19:15,318664202-05:00] INFO: > Copy configuration files to local[0m
# ./benchmarks/bench-rados:220 - launch_ceph_cluster() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.3 sudo chown --changes --recursive ljishen /tmp/bench-rados/deployment_data_root/etc/ceph
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:19:15,325971478-05:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.3 sudo chown --changes --recursive ljishen /tmp/bench-rados/deployment_data_root/etc/ceph
[1] 01:19:15 [SUCCESS] ljishen@10.10.2.3
# ./benchmarks/bench-rados:223 - launch_ceph_cluster() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.3:/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf
# ./benchmarks/bench-rados:226 - launch_ceph_cluster() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.3:/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:19:15,800268325-05:00] STAGE: Update local ceph.conf as a client...[0m


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:19:15,803626957-05:00] STAGE: Configure Ceph pool...[0m
# ./benchmarks/bench-rados:257 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_max_backfills 32
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:19:15,824628428-05:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:19:15,827209284-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '599a25e4-405b-11ec-aba1-c5f3c07a3dc7', '--', 'ceph', 'config', 'set', 'osd', 'osd_max_backfills', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 599a25e4-405b-11ec-aba1-c5f3c07a3dc7 -- ceph config set osd osd_max_backfills 32
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:258 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_active 32
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:19:19,883200392-05:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:19:19,886109717-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '599a25e4-405b-11ec-aba1-c5f3c07a3dc7', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_active', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 599a25e4-405b-11ec-aba1-c5f3c07a3dc7 -- ceph config set osd osd_recovery_max_active 32
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:259 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_single_start 8
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:19:24,129118057-05:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:19:24,131813157-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '599a25e4-405b-11ec-aba1-c5f3c07a3dc7', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_single_start', '8'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 599a25e4-405b-11ec-aba1-c5f3c07a3dc7 -- ceph config set osd osd_recovery_max_single_start 8
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:260 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_op_priority 63
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:19:28,413599864-05:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:19:28,416627983-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '599a25e4-405b-11ec-aba1-c5f3c07a3dc7', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_op_priority', '63'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 599a25e4-405b-11ec-aba1-c5f3c07a3dc7 -- ceph config set osd osd_recovery_op_priority 63
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:262 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config show-with-defaults osd.0
# ./benchmarks/bench-rados:263 - configure_ceph_pool() > grep 'osd_max_backfills\|osd_recovery'
# ./benchmarks/bench-rados:263 - configure_ceph_pool() > column -t -s ' '
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
osd_max_backfills                        32         mon
osd_recovery_cost                        20971520   default
osd_recovery_delay_start                 0.000000   default
osd_recovery_max_active                  32         mon
osd_recovery_max_active_hdd              3          default
osd_recovery_max_active_ssd              10         default
osd_recovery_max_chunk                   8388608    default
osd_recovery_max_omap_entries_per_chunk  8096       default
osd_recovery_max_single_start            8          mon
osd_recovery_op_priority                 63         mon
osd_recovery_op_warn_multiple            16         default
osd_recovery_priority                    5          default
osd_recovery_retry_interval              30.000000  default
osd_recovery_sleep                       0.000000   default
osd_recovery_sleep_hdd                   0.100000   default
osd_recovery_sleep_hybrid                0.025000   default
osd_recovery_sleep_ssd                   0.000000   default
# ./benchmarks/bench-rados:265 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool create bench_rados 128 128 replicated --size 1 --pg-num-min 128
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:19:36,626289437-05:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:19:36,629105336-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '599a25e4-405b-11ec-aba1-c5f3c07a3dc7', '--', 'ceph', 'osd', 'pool', 'create', 'bench_rados', '128', '128', 'replicated', '--size', '1', '--pg-num-min', '128'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 599a25e4-405b-11ec-aba1-c5f3c07a3dc7 -- ceph osd pool create bench_rados 128 128 replicated --size 1 --pg-num-min 128
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
pool 'bench_rados' created
# ./benchmarks/bench-rados:267 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados min_size 1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:19:40,804045057-05:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:19:40,806774162-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '599a25e4-405b-11ec-aba1-c5f3c07a3dc7', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'min_size', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 599a25e4-405b-11ec-aba1-c5f3c07a3dc7 -- ceph osd pool set bench_rados min_size 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 min_size to 1
# ./benchmarks/bench-rados:268 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados pg_autoscale_mode off
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:19:45,171748072-05:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:19:45,174594057-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '599a25e4-405b-11ec-aba1-c5f3c07a3dc7', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'pg_autoscale_mode', 'off'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 599a25e4-405b-11ec-aba1-c5f3c07a3dc7 -- ceph osd pool set bench_rados pg_autoscale_mode off
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 pg_autoscale_mode to off
# ./benchmarks/bench-rados:269 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool application enable bench_rados benchmark
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:19:49,616988594-05:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:19:49,619625435-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '599a25e4-405b-11ec-aba1-c5f3c07a3dc7', '--', 'ceph', 'osd', 'pool', 'application', 'enable', 'bench_rados', 'benchmark'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 599a25e4-405b-11ec-aba1-c5f3c07a3dc7 -- ceph osd pool application enable bench_rados benchmark
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
enabled application 'benchmark' on pool 'bench_rados'
# ./benchmarks/bench-rados:270 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados noscrub 1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:19:54,818823602-05:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:19:54,821600758-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '599a25e4-405b-11ec-aba1-c5f3c07a3dc7', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'noscrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 599a25e4-405b-11ec-aba1-c5f3c07a3dc7 -- ceph osd pool set bench_rados noscrub 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 noscrub to 1
# ./benchmarks/bench-rados:271 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados nodeep-scrub 1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:19:59,705743415-05:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:19:59,708508538-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '599a25e4-405b-11ec-aba1-c5f3c07a3dc7', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'nodeep-scrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 599a25e4-405b-11ec-aba1-c5f3c07a3dc7 -- ceph osd pool set bench_rados nodeep-scrub 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 nodeep-scrub to 1
# ./benchmarks/bench-rados:272 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool ls detail
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:20:04,757724779-05:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:20:04,760870680-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '599a25e4-405b-11ec-aba1-c5f3c07a3dc7', '--', 'ceph', 'osd', 'pool', 'ls', 'detail'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 599a25e4-405b-11ec-aba1-c5f3c07a3dc7 -- ceph osd pool ls detail
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
pool 1 'device_health_metrics' replicated size 1 min_size 1 crush_rule 0 object_hash rjenkins pg_num 64 pgp_num 64 autoscale_mode on last_change 18 lfor 0/0/15 flags hashpspool stripe_width 0 pg_num_min 1 application mgr_devicehealth
pool 2 'bench_rados' replicated size 1 min_size 1 crush_rule 0 object_hash rjenkins pg_num 128 pgp_num 128 autoscale_mode off last_change 20 flags hashpspool,noscrub,nodeep-scrub stripe_width 0 pg_num_min 128 application benchmark

# ./benchmarks/bench-rados:273 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd df tree
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:20:08,925537574-05:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:20:08,928429135-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '599a25e4-405b-11ec-aba1-c5f3c07a3dc7', '--', 'ceph', 'osd', 'df', 'tree'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 599a25e4-405b-11ec-aba1-c5f3c07a3dc7 -- ceph osd df tree
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
ID  CLASS  WEIGHT   REWEIGHT  SIZE     RAW USE  DATA     OMAP  META     AVAIL    %USE  VAR   PGS  STATUS  TYPE NAME      
-1         0.09769         -  100 GiB  5.7 MiB  168 KiB   0 B  5.6 MiB  100 GiB  0.01  1.00    -          root default   
-3         0.09769         -  100 GiB  5.7 MiB  168 KiB   0 B  5.6 MiB  100 GiB  0.01  1.00    -              host node-1
 0    ssd  0.09769   1.00000  100 GiB  5.7 MiB  168 KiB   0 B  5.6 MiB  100 GiB  0.01  1.00  192      up          osd.0  
                       TOTAL  100 GiB  5.7 MiB  168 KiB   0 B  5.6 MiB  100 GiB  0.01                                    
MIN/MAX VAR: 1.00/1.00  STDDEV: 0


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:20:13,075897387-05:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:20:37,190051908-05:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:20:46,291584455-05:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:20:55,559801119-05:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:20:55,566987517-05:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:21:04,668894374-05:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:21:04,675709122-05:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:21:13,760777522-05:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:21:13,768047839-05:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:21:22,861661275-05:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:21:22,869179348-05:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:21:31,854991771-05:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:21:31,862211131-05:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:21:31,867644175-05:00] INFO: > The cluster is idle now.[0m


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:21:31,870951099-05:00] STAGE: Start benchmarking of mode write ...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:21:31,877296371-05:00] INFO: > Start system activity collection services[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:21:31,882376088-05:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:341 - rados_bench() > rm --force /mnt/sda8/ceph-research/scripts/output/sys_activity_256KB_write.dat.rados_bench_host.2
# ./benchmarks/bench-rados:342 - rados_bench() > sar_pid_rados_bench_host=199820
# ./benchmarks/bench-rados:342 - rados_bench() > S_TIME_FORMAT=ISO
# ./benchmarks/bench-rados:342 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/output/sys_activity_256KB_write.dat.rados_bench_host.2 2

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:21:31,888938008-05:00] INFO: >> for OSD host[0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:21:31,897392637-05:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.2 bash
10.10.2.2: b'45467\n'
[1] 01:21:32 [SUCCESS] ljishen@10.10.2.2
45467

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:21:32,085487521-05:00] INFO: > Run rados bench[0m
# ./benchmarks/bench-rados:370 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/output/rados_bench_256KB_write.log.2
## ./benchmarks/bench-rados:361 - rados_bench() > count_cpus 32-63,96-127
# ./benchmarks/bench-rados:361 - rados_bench() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 60 write --pool bench_rados -b 262144 -O 262144 --concurrent-ios 128 --show-time --write-object --write-omap --write-xattr --no-cleanup
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:21:32,104503550-05:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:21:32,107288741-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '599a25e4-405b-11ec-aba1-c5f3c07a3dc7', '--', 'rados', 'bench', '60', 'write', '--pool', 'bench_rados', '-b', '262144', '-O', '262144', '--concurrent-ios', '128', '--show-time', '--write-object', '--write-omap', '--write-xattr', '--no-cleanup'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 599a25e4-405b-11ec-aba1-c5f3c07a3dc7 -- rados bench 60 write --pool bench_rados -b 262144 -O 262144 --concurrent-ios 128 --show-time --write-object --write-omap --write-xattr --no-cleanup
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
hints = 1
2021-11-08T06:21:35.406709+0000 Maintaining 128 concurrent writes of 262144 bytes to objects of size 262144 for up to 60 seconds or 0 objects
2021-11-08T06:21:35.406720+0000 Object prefix: benchmark_data_node-0.bluefield2.ucsc-cmps10_7
2021-11-08T06:21:35.416455+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-11-08T06:21:35.416455+0000     0       0         0         0         0         0           -           0
2021-11-08T06:21:36.416559+0000     1     128      1280      1152   287.985       288   0.0980614    0.104617
2021-11-08T06:21:37.416633+0000     2     128      2339      2211   276.358    264.75    0.174921    0.110614
2021-11-08T06:21:38.416713+0000     3     128      2963      2835   236.234       156    0.160869    0.130226
2021-11-08T06:21:39.416788+0000     4     128      3777      3649   228.046     203.5    0.122802    0.137444
2021-11-08T06:21:40.416863+0000     5     128      4530      4402   220.084    188.25   0.0858941    0.141457
2021-11-08T06:21:41.416935+0000     6     128      4911      4783   199.277     95.25    0.312694    0.155702
2021-11-08T06:21:42.417010+0000     7     128      5310      5182   185.058     99.75    0.288211    0.168472
2021-11-08T06:21:43.417084+0000     8     128      5965      5837   182.393    163.75    0.170878    0.173638
2021-11-08T06:21:44.417160+0000     9     128      6415      6287   174.626     112.5    0.303981    0.178345
2021-11-08T06:21:45.417235+0000    10     128      6863      6735   168.363       112    0.236486    0.188455
2021-11-08T06:21:46.417310+0000    11     128      7441      7313   166.192     144.5    0.199136    0.191009
2021-11-08T06:21:47.417384+0000    12     128      8322      8194   170.696    220.25    0.153461    0.184932
2021-11-08T06:21:48.417457+0000    13     128      8791      8663   166.584    117.25    0.258472    0.189424
2021-11-08T06:21:49.417527+0000    14     128      9169      9041   161.435      94.5    0.340779    0.195963
2021-11-08T06:21:50.417605+0000    15     128      9582      9454   157.555    103.25     0.23409    0.201822
2021-11-08T06:21:51.417683+0000    16     128     10146     10018    156.52       141    0.174567     0.20285
2021-11-08T06:21:52.417759+0000    17     128     10546     10418   153.195       100    0.261292    0.204865
2021-11-08T06:21:53.417830+0000    18     128     10994     10866   150.906       112    0.291703     0.21078
2021-11-08T06:21:54.417904+0000    19     128     11336     11208   147.463      85.5       0.306    0.214415
2021-11-08T06:21:55.417982+0000 min lat: 0.0084911 max lat: 0.646498 avg lat: 0.216076
2021-11-08T06:21:55.417982+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-11-08T06:21:55.417982+0000    20     128     11938     11810   147.614     150.5    0.190256    0.216076
2021-11-08T06:21:56.418065+0000    21     128     12428     12300   146.418     122.5    0.230074    0.215873
2021-11-08T06:21:57.418132+0000    22     128     12860     12732   144.671       108    0.329317    0.219475
2021-11-08T06:21:58.418206+0000    23     128     13240     13112   142.511        95    0.291275    0.222613
2021-11-08T06:21:59.418279+0000    24     128     13740     13612   141.781       125    0.161894    0.225069
2021-11-08T06:22:00.418355+0000    25     128     14317     14189    141.88    144.25    0.169699    0.224814
2021-11-08T06:22:01.418431+0000    26     128     14746     14618   140.547    107.25    0.291647    0.226955
2021-11-08T06:22:02.418504+0000    27     128     15083     14955   138.462     84.25    0.358526    0.229353
2021-11-08T06:22:03.418577+0000    28     128     15510     15382   137.329    106.75    0.303375    0.232401
2021-11-08T06:22:04.418652+0000    29     128     15998     15870     136.8       122    0.180054    0.232462
2021-11-08T06:22:05.418727+0000    30     128     16595     16467   137.215    149.25    0.306292    0.232261
2021-11-08T06:22:06.418810+0000    31     128     16951     16823   135.659        89    0.372025    0.234445
2021-11-08T06:22:07.418882+0000    32     128     17363     17235   134.638       103    0.264648     0.23634
2021-11-08T06:22:08.418957+0000    33     128     17823     17695   134.043       115    0.152788    0.238224
2021-11-08T06:22:09.419025+0000    34     128     18414     18286   134.446    147.75    0.164085    0.237179
2021-11-08T06:22:10.419095+0000    35     128     18856     18728   133.762     110.5    0.289871    0.238459
2021-11-08T06:22:11.419175+0000    36     128     19190     19062   132.365      83.5   0.0852904    0.239954
2021-11-08T06:22:12.419257+0000    37     128     19627     19499    131.74    109.25    0.251899    0.241707
2021-11-08T06:22:13.419323+0000    38     128     20195     20067    132.01       142    0.196734    0.241458
2021-11-08T06:22:14.419395+0000    39     128     20720     20592    131.99    131.25    0.332607    0.241312
2021-11-08T06:22:15.419468+0000 min lat: 0.0084911 max lat: 0.646498 avg lat: 0.242892
2021-11-08T06:22:15.419468+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-11-08T06:22:15.419468+0000    40     128     21089     20961   130.997     92.25    0.325027    0.242892
2021-11-08T06:22:16.419552+0000    41     128     21463     21335   130.082      93.5    0.349381    0.244631
2021-11-08T06:22:17.419624+0000    42     128     22056     21928   130.514    148.25    0.144929    0.244878
2021-11-08T06:22:18.419694+0000    43     128     22613     22485   130.717    139.25     0.15118    0.243717
2021-11-08T06:22:19.419766+0000    44     128     23049     22921   130.223       109    0.313292    0.245167
2021-11-08T06:22:20.419840+0000    45     128     23416     23288   129.368     91.75    0.311105    0.246357
2021-11-08T06:22:21.419910+0000    46     127     23892     23765   129.148    119.25    0.198657     0.24722
2021-11-08T06:22:22.419987+0000    47     128     24563     24435   129.964     167.5    0.147256    0.245964
2021-11-08T06:22:23.420057+0000    48     128     24899     24771   129.006        84   0.0663512    0.246787
2021-11-08T06:22:24.420128+0000    49     128     25338     25210   128.613    109.75   0.0960509    0.248157
2021-11-08T06:22:25.420194+0000    50     128     25675     25547   127.726     84.25   0.0736808    0.249232
2021-11-08T06:22:26.420272+0000    51     128     26281     26153   128.192     151.5    0.152774    0.248854
2021-11-08T06:22:27.420347+0000    52     128     26821     26693   128.322       135    0.277067    0.248543
2021-11-08T06:22:28.420419+0000    53     128     27243     27115   127.892     105.5    0.261034    0.249807
2021-11-08T06:22:29.420485+0000    54     128     27571     27443   127.042        82    0.276791      0.2506
2021-11-08T06:22:30.420556+0000    55     128     28082     27954   127.054    127.75       0.195    0.251379
2021-11-08T06:22:31.420630+0000    56     128     28711     28583   127.593    157.25    0.175476    0.250423
2021-11-08T06:22:32.420706+0000    57     128     29056     28928   126.868     86.25   0.0705852    0.251293
2021-11-08T06:22:33.420771+0000    58     128     29494     29366   126.568     109.5     0.26958    0.252339
2021-11-08T06:22:34.420845+0000    59     128     29820     29692   125.804      81.5    0.308332    0.253231
2021-11-08T06:22:35.420921+0000 min lat: 0.0084911 max lat: 0.646498 avg lat: 0.253121
2021-11-08T06:22:35.420921+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-11-08T06:22:35.420921+0000    60     128     30413     30285   126.178    148.25    0.207443    0.253121
2021-11-08T06:22:36.421048+0000 Total time run:         60.1382
Total writes made:      30413
Write size:             262144
Object size:            262144
Bandwidth (MB/sec):     126.43
Stddev Bandwidth:       41.5544
Max bandwidth (MB/sec): 288
Min bandwidth (MB/sec): 81.5
Average IOPS:           505
Stddev IOPS:            166.218
Max IOPS:               1152
Min IOPS:               326
Average Latency(s):     0.252978
Stddev Latency(s):      0.101271
Max latency(s):         0.646498
Min latency(s):         0.0084911

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:22:37,106749692-05:00] INFO: > Stop system activity collection services[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:22:37,112221548-05:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:391 - rados_bench() > kill -INT 199820

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:22:37,117711207-05:00] INFO: >> for OSD host[0m
# ./benchmarks/bench-rados:396 - rados_bench() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.2 kill -INT 45467
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:22:37,125035124-05:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.2 kill -INT 45467
[1] 01:22:37 [SUCCESS] ljishen@10.10.2.2

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:22:37,303935234-05:00] INFO: >>> Copy system activity file from OSD host[0m
## ./benchmarks/bench-rados:403 - rados_bench() > basename -- /tmp/bench-rados/sys_activity_256KB_write.dat.osd_host.2
# ./benchmarks/bench-rados:403 - rados_bench() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.2:/tmp/bench-rados/sys_activity_256KB_write.dat.osd_host.2 /mnt/sda8/ceph-research/scripts/output/sys_activity_256KB_write.dat.osd_host.2


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:22:37,474850531-05:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:23:01,490664639-05:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 30.41k objects, 7.4 GiB
    usage:   23 GiB used, 77 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:23:01,498004497-05:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:23:10,606259499-05:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 30.41k objects, 7.4 GiB
    usage:   23 GiB used, 77 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:23:10,613609986-05:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:23:19,699104286-05:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 30.41k objects, 7.4 GiB
    usage:   23 GiB used, 77 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:23:19,706435486-05:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:23:28,276223098-05:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 30.41k objects, 7.4 GiB
    usage:   23 GiB used, 77 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:23:28,283131181-05:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:23:37,369025798-05:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 30.41k objects, 7.4 GiB
    usage:   23 GiB used, 77 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:23:37,376077953-05:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:23:37,381446825-05:00] INFO: > The cluster is idle now.[0m


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:23:37,384606311-05:00] STAGE: Start benchmarking of mode seq ...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:23:37,391338954-05:00] INFO: > Start system activity collection services[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:23:37,396548064-05:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:341 - rados_bench() > rm --force /mnt/sda8/ceph-research/scripts/output/sys_activity_256KB_seq.dat.rados_bench_host.2
# ./benchmarks/bench-rados:342 - rados_bench() > sar_pid_rados_bench_host=201172
# ./benchmarks/bench-rados:342 - rados_bench() > S_TIME_FORMAT=ISO
# ./benchmarks/bench-rados:342 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/output/sys_activity_256KB_seq.dat.rados_bench_host.2 2

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:23:37,403255219-05:00] INFO: >> for OSD host[0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:23:37,411734534-05:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.2 bash
10.10.2.2: b'45567\n'
[1] 01:23:37 [SUCCESS] ljishen@10.10.2.2
45567

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:23:37,597254576-05:00] INFO: > Run rados bench[0m
# ./benchmarks/bench-rados:379 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/output/rados_bench_256KB_seq.log.2
## ./benchmarks/bench-rados:376 - rados_bench() > count_cpus 32-63,96-127
# ./benchmarks/bench-rados:376 - rados_bench() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 128 --show-time
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:23:37,616957110-05:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:23:37,619843061-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '599a25e4-405b-11ec-aba1-c5f3c07a3dc7', '--', 'rados', 'bench', '99999999', 'seq', '--pool', 'bench_rados', '--concurrent-ios', '128', '--show-time'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 599a25e4-405b-11ec-aba1-c5f3c07a3dc7 -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 128 --show-time
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
hints = 1
2021-11-08T06:23:40.763522+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-11-08T06:23:40.763522+0000     0       0         0         0         0         0           -           0
2021-11-08T06:23:41.763641+0000     1     128      7124      6996   1748.67      1749   0.0152518   0.0180996
2021-11-08T06:23:42.763756+0000     2     127     14233     14106   1762.98    1777.5   0.0180972   0.0180344
2021-11-08T06:23:43.763837+0000     3     128     21332     21204   1766.77    1774.5   0.0191874   0.0180272
2021-11-08T06:23:44.763901+0000     4     128     28447     28319   1769.74   1778.75   0.0184807   0.0180088
2021-11-08T06:23:45.764003+0000 Total time run:       4.29932
Total reads made:     30413
Read size:            262144
Object size:          262144
Bandwidth (MB/sec):   1768.48
Average IOPS:         7073
Stddev IOPS:          56.2872
Max IOPS:             7115
Min IOPS:             6996
Average Latency(s):   0.018031
Max latency(s):       0.0304457
Min latency(s):       0.00259115

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:23:46,496882210-05:00] INFO: > Stop system activity collection services[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:23:46,502242084-05:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:391 - rados_bench() > kill -INT 201172

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:23:46,507699473-05:00] INFO: >> for OSD host[0m
# ./benchmarks/bench-rados:396 - rados_bench() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.2 kill -INT 45567
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:23:46,515303739-05:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.2 kill -INT 45567
[1] 01:23:46 [SUCCESS] ljishen@10.10.2.2

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:23:46,696812418-05:00] INFO: >>> Copy system activity file from OSD host[0m
## ./benchmarks/bench-rados:403 - rados_bench() > basename -- /tmp/bench-rados/sys_activity_256KB_seq.dat.osd_host.2
# ./benchmarks/bench-rados:403 - rados_bench() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.2:/tmp/bench-rados/sys_activity_256KB_seq.dat.osd_host.2 /mnt/sda8/ceph-research/scripts/output/sys_activity_256KB_seq.dat.osd_host.2


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:23:46,844010072-05:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:24:10,902258420-05:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 30.41k objects, 7.4 GiB
    usage:   23 GiB used, 77 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:24:10,909293042-05:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:24:19,976945753-05:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 30.41k objects, 7.4 GiB
    usage:   23 GiB used, 77 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:24:19,983996696-05:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:24:29,018910223-05:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 30.41k objects, 7.4 GiB
    usage:   23 GiB used, 77 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:24:29,025761409-05:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:24:38,046611315-05:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 30.41k objects, 7.4 GiB
    usage:   23 GiB used, 77 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:24:38,053676765-05:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:24:47,156330609-05:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 30.41k objects, 7.4 GiB
    usage:   23 GiB used, 77 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:24:47,163043174-05:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:24:47,168398741-05:00] INFO: > The cluster is idle now.[0m


[1;7;39;49m[2021-11-08T01:24:47,170820126-05:00][RUNNING][ROUND 3/4/21] object_size=256KB[0m


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:24:47,174070824-05:00] STAGE: Launch a Ceph cluster (CEPH_HOST: 10.10.2.3, OSD_HOST: 10.10.2.2) ...[0m
# ./benchmarks/bench-rados:183 - launch_ceph_cluster() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.3 bash -euo pipefail
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:24:47,182880663-05:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.3 bash -euo pipefail
10.10.2.3: b'\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T01:24:47,576488194-05:00] INFO: > Remove existing clusters\x1b[0m\n'
10.10.2.3: b'## bash:17 -  > basename -- /var/lib/ceph/599a25e4-405b-11ec-aba1-c5f3c07a3dc7\n'
10.10.2.3: b'# bash:17 -  > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm --host 10.10.2.3 --host 10.10.2.2 rm-cluster --force --fsid 599a25e4-405b-11ec-aba1-c5f3c07a3dc7\n'
10.10.2.3: b"\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T01:24:47,587016557-05:00] DEBUG: HOST_PARAMS: ['--host', '10.10.2.3', '--host', '10.10.2.2']\x1b[0m\n"
10.10.2.3: b"\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T01:24:47,590355005-05:00] DEBUG: CEPHADM_PARAMS: ['rm-cluster', '--force', '--fsid', '599a25e4-405b-11ec-aba1-c5f3c07a3dc7']\x1b[0m\n"
10.10.2.3: b"# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:63 -  > /tmp/bench-rados/ceph-research/scripts/parallel-ssh --host 10.10.2.3 --host 10.10.2.2 'sudo python3 -u - rm-cluster --force --fsid 599a25e4-405b-11ec-aba1-c5f3c07a3dc7'\n"
10.10.2.3: b'\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T01:24:47,599035071-05:00] DEBUG: command from stdin\x1b[0m\n'
10.10.2.3: b"# /tmp/bench-rados/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host 10.10.2.3 --host 10.10.2.2 'sudo python3 -u - rm-cluster --force --fsid 599a25e4-405b-11ec-aba1-c5f3c07a3dc7'\n"
10.10.2.3: b'[1] 01:24:53 [SUCCESS] 10.10.2.3\n[2] 01:24:59 [SUCCESS] 10.10.2.2\n'
10.10.2.3: b'\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T01:24:59,317052721-05:00] INFO: > Deploy a new cluster\x1b[0m\n'
10.10.2.3: b'# bash:24 -  > /tmp/bench-rados/ceph-research/scripts/ceph-deploy -m 10.10.2.3 -o node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/10.10.2.2:/dev/nvme0n1\n'
10.10.2.3: b'\n\n\x1b[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T01:24:59,328346208-05:00] STAGE: Verify passwordless SSH to hosts, passwordless sudo via ljishen, and the existence of devices on hosts...\x1b[0m\n'
10.10.2.3: b'\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T01:24:59,332878498-05:00] INFO: > Check the host for the monitor\x1b[0m\n'
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:181 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@10.10.2.3 sudo --non-interactive --validate\n'
10.10.2.3: b'\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T01:24:59,482211992-05:00] INFO: > Check hosts for OSDs\x1b[0m\n'
10.10.2.3: b"\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T01:24:59,486711080-05:00] INFO: >> Check host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'\x1b[0m\n"
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:200 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us sudo --non-interactive --validate\n'
10.10.2.3: b"\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T01:24:59,634105466-05:00] INFO: >>> Check device '/dev/nvme0n1'\x1b[0m\n"
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:215 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us test -b /dev/nvme0n1\n'
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:226 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us lsblk --nodeps --noheadings --output TYPE /dev/nvme0n1\n'
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:227 - verify_parameters() > grep --word-regexp disk\n'
10.10.2.3: b'\n\n\x1b[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T01:24:59,910448798-05:00] STAGE: Reset devices on hosts...\x1b[0m\n'
10.10.2.3: b"\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T01:24:59,915610206-05:00] INFO: > Resetting device '/dev/nvme0n1' on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'\x1b[0m\n"
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:288 - reset_host_devices() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us sudo sh\n'
10.10.2.3: b'  Removing ceph--91797d8a--25c5--461a--88c9--43a8390c4110-osd--block--648dc3b9--0202--4eca--ac0b--6213df76b425 (253:0)\n'
10.10.2.3: b'  Archiving volume group "ceph-91797d8a-25c5-461a-88c9-43a8390c4110" metadata (seqno 5).\n  Releasing logical volume "osd-block-648dc3b9-0202-4eca-ac0b-6213df76b425"\n'
10.10.2.3: b'  Creating volume group backup "/etc/lvm/backup/ceph-91797d8a-25c5-461a-88c9-43a8390c4110" (seqno 6).\n'
10.10.2.3: b'  Logical volume "osd-block-648dc3b9-0202-4eca-ac0b-6213df76b425" successfully removed\n'
10.10.2.3: b'  Removing physical volume "/dev/nvme0n1" from volume group "ceph-91797d8a-25c5-461a-88c9-43a8390c4110"\n'
10.10.2.3: b'  Volume group "ceph-91797d8a-25c5-461a-88c9-43a8390c4110" successfully removed\n'
10.10.2.3: b'\n\n\x1b[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T01:25:00,355462166-05:00] STAGE: Bootstrap a new cluster...\x1b[0m\n'
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:317 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.3 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults\n'
10.10.2.3: b'\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T01:25:00,364944686-05:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.3: b"\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T01:25:00,368315525-05:00] DEBUG: CEPHADM_PARAMS: ['bootstrap', '--config', '/tmp/bench-rados/ceph-research/scripts/ceph.conf', '--mon-ip', '10.10.2.3', '--output-dir', '/tmp/bench-rados/deployment_data_root/etc/ceph', '--ssh-user', 'ljishen', '--skip-dashboard', '--no-minimize-config', '--allow-overwrite', '--allow-fqdn-hostname', '--orphan-initial-daemons', '--skip-monitoring-stack', '--single-host-defaults']\x1b[0m\n"
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.3 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults\n'
10.10.2.3: b'Verifying podman|docker is present...\nVerifying lvm2 is present...\n'
10.10.2.3: b'Verifying time synchronization is in place...\n'
10.10.2.3: b'Unit ntp.service is enabled and running\n'
10.10.2.3: b'Repeating the final host check...\ndocker (/usr/bin/docker) is present\n'
10.10.2.3: b'systemctl is present\nlvcreate is present\n'
10.10.2.3: b'Unit ntp.service is enabled and running\n'
10.10.2.3: b'Host looks OK\n'
10.10.2.3: b'Cluster fsid: 99f46a22-405c-11ec-aba1-c5f3c07a3dc7\n'
10.10.2.3: b'Verifying IP 10.10.2.3 port 3300 ...\n'
10.10.2.3: b'Verifying IP 10.10.2.3 port 6789 ...\n'
10.10.2.3: b'Mon IP `10.10.2.3` is in CIDR network `10.10.2.0/24`\n- internal network (--cluster-network) has not been provided, OSD replication will default to the public_network\n'
10.10.2.3: b'Adjusting default settings to suit single-host cluster...\n'
10.10.2.3: b'Pulling container image quay.io/ceph/ceph:v16...\n'
10.10.2.3: b'Ceph version: ceph version 16.2.6 (ee28fb57e47e9f88813e24bbf4c14496ca299d31) pacific (stable)\n'
10.10.2.3: b'Extracting ceph user uid/gid from container image...\n'
10.10.2.3: b'Creating initial keys...\n'
10.10.2.3: b'Creating initial monmap...\n'
10.10.2.3: b'Creating mon...\n'
10.10.2.3: b'Waiting for mon to start...\n'
10.10.2.3: b'Waiting for mon...\n'
10.10.2.3: b'mon is available\nSetting mon public_network to 10.10.2.0/24\n'
10.10.2.3: b'Wrote config to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf\n'
10.10.2.3: b'Wrote keyring to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring\n'
10.10.2.3: b'Creating mgr...\n'
10.10.2.3: b'Verifying port 9283 ...\n'
10.10.2.3: b'Waiting for mgr to start...\n'
10.10.2.3: b'Waiting for mgr...\n'
10.10.2.3: b'mgr not available, waiting (1/15)...\n'
10.10.2.3: b'mgr not available, waiting (2/15)...\n'
10.10.2.3: b'mgr is available\n'
10.10.2.3: b'Enabling cephadm module...\n'
10.10.2.3: b'Waiting for the mgr to restart...\n'
10.10.2.3: b'Waiting for mgr epoch 5...\n'
10.10.2.3: b'mgr epoch 5 is available\nSetting orchestrator backend to cephadm...\n'
10.10.2.3: b'Generating ssh key...\n'
10.10.2.3: b'Wrote public SSH key to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub\n'
10.10.2.3: b'Adding key to ljishen@localhost authorized_keys...\n'
10.10.2.3: b'Adding host node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us...\n'
10.10.2.3: b'Deploying unmanaged mon service...\n'
10.10.2.3: b'Deploying unmanaged mgr service...\n'
10.10.2.3: b'You can access the Ceph CLI with:\n\n\tsudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --fsid 99f46a22-405c-11ec-aba1-c5f3c07a3dc7 -c /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf -k /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring\n\n'
10.10.2.3: b'Please consider enabling telemetry to help improve Ceph:\n\n\tceph telemetry on\n\nFor more information see:\n\n\thttps://docs.ceph.com/docs/pacific/mgr/telemetry/\n\n'
10.10.2.3: b'Bootstrap complete.\n'
10.10.2.3: b'\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T01:26:09,969367340-05:00] INFO: Sleep for 20 seconds\x1b[0m\n'
10.10.2.3: b'\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T01:26:29,977388965-05:00] INFO: > Set the default number of monitor daemons to 1\x1b[0m\n'
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:337 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1\n'
10.10.2.3: b'\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T01:26:29,987259828-05:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.3: b"\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T01:26:29,990883104-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mon', '1']\x1b[0m\n"
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1\n'
10.10.2.3: b'Inferring fsid 99f46a22-405c-11ec-aba1-c5f3c07a3dc7\n'
10.10.2.3: b'Inferring config /var/lib/ceph/99f46a22-405c-11ec-aba1-c5f3c07a3dc7/mon.node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.3: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.3: b'Scheduled mon update...\n'
10.10.2.3: b'\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T01:26:39,745836649-05:00] INFO: > Set the default number of manager daemons to 1\x1b[0m\n'
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:342 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1\n'
10.10.2.3: b'\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T01:26:39,755753138-05:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.3: b"\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T01:26:39,759507091-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mgr', '1']\x1b[0m\n"
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1\n'
10.10.2.3: b'Inferring fsid 99f46a22-405c-11ec-aba1-c5f3c07a3dc7\n'
10.10.2.3: b'Inferring config /var/lib/ceph/99f46a22-405c-11ec-aba1-c5f3c07a3dc7/mon.node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.3: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.3: b'Scheduled mgr update...\n'
10.10.2.3: b'\n\n\x1b[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T01:26:50,552568654-05:00] STAGE: Add hosts to the cluster...\x1b[0m\n'
10.10.2.3: b"\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T01:26:50,558468034-05:00] INFO: > Adding host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'\x1b[0m\n"
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:361 - add_hosts() > ssh-copy-id -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -f -i /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\n'
10.10.2.3: b'/usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: "/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub"\n'
10.10.2.3: b'\nNumber of key(s) added: 1\n\nNow try logging into the machine, with:   "ssh -o \'GlobalKnownHostsFile=/dev/null\' -o \'LogLevel=ERROR\' -o \'PasswordAuthentication=no\' -o \'StrictHostKeyChecking=no\' -o \'UserKnownHostsFile=/dev/null\' \'ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\'"\nand check to make sure that only the key(s) you wanted were added.\n\n'
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:367 - add_hosts() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us 10.10.2.2\n'
10.10.2.3: b'\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T01:26:50,768054241-05:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.3: b"\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T01:26:50,771404592-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'host', 'add', 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us', '10.10.2.2']\x1b[0m\n"
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us 10.10.2.2\n'
10.10.2.3: b'Inferring fsid 99f46a22-405c-11ec-aba1-c5f3c07a3dc7\n'
10.10.2.3: b'Inferring config /var/lib/ceph/99f46a22-405c-11ec-aba1-c5f3c07a3dc7/mon.node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.3: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.3: b"Added host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us' with addr '10.10.2.2'\n"
10.10.2.3: b'\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T01:27:00,857691165-05:00] INFO: Sleep for 20 seconds\x1b[0m\n'
10.10.2.3: b'\n\n\x1b[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T01:27:20,862969165-05:00] STAGE: Add OSDs to the cluster...\x1b[0m\n'
10.10.2.3: b"\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T01:27:20,870113143-05:00] INFO: > Adding devices ['/dev/nvme0n1'] on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us' as OSDs\x1b[0m\n"
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:388 - add_host_devices_as_osds() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1\n'
10.10.2.3: b'\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T01:27:20,880062474-05:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.3: b"\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T01:27:20,883527732-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'daemon', 'add', 'osd', 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1']\x1b[0m\n"
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1\n'
10.10.2.3: b'Inferring fsid 99f46a22-405c-11ec-aba1-c5f3c07a3dc7\n'
10.10.2.3: b'Inferring config /var/lib/ceph/99f46a22-405c-11ec-aba1-c5f3c07a3dc7/mon.node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.3: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.3: b"Created osd(s) 0 on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'\n"
10.10.2.3: b'\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T01:27:47,430055139-05:00] INFO: > Sleep for 20 seconds\x1b[0m\n'
10.10.2.3: b'\n\n\x1b[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T01:28:07,435898477-05:00] STAGE: Show Cluster Status\x1b[0m\n'
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:408 - show_status() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status\n'
10.10.2.3: b'\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T01:28:07,446931062-05:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.3: b"\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T01:28:07,450659477-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', '--status']\x1b[0m\n"
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status\n'
10.10.2.3: b'Inferring fsid 99f46a22-405c-11ec-aba1-c5f3c07a3dc7\n'
10.10.2.3: b'Inferring config /var/lib/ceph/99f46a22-405c-11ec-aba1-c5f3c07a3dc7/mon.node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.3: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.3: b'  cluster:\n    id:     99f46a22-405c-11ec-aba1-c5f3c07a3dc7\n    health: HEALTH_WARN\n            1 pool(s) have no replicas configured\n \n  services:\n    mon: 1 daemons, quorum node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us (age 2m)\n    mgr: node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us.gvwmpl(active, since 2m)\n    osd: 1 osds: 1 up (since 18s), 1 in (since 41s)\n \n  data:\n    pools:   1 pools, 1 pgs\n    objects: 0 objects, 0 B\n    usage:   4.7 MiB used, 100 GiB / 100 GiB avail\n    pgs:     1 active+clean\n \n'
[1] 01:28:16 [SUCCESS] ljishen@10.10.2.3

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:24:47,576488194-05:00] INFO: > Remove existing clusters[0m
## bash:17 -  > basename -- /var/lib/ceph/599a25e4-405b-11ec-aba1-c5f3c07a3dc7
# bash:17 -  > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm --host 10.10.2.3 --host 10.10.2.2 rm-cluster --force --fsid 599a25e4-405b-11ec-aba1-c5f3c07a3dc7
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:24:47,587016557-05:00] DEBUG: HOST_PARAMS: ['--host', '10.10.2.3', '--host', '10.10.2.2'][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:24:47,590355005-05:00] DEBUG: CEPHADM_PARAMS: ['rm-cluster', '--force', '--fsid', '599a25e4-405b-11ec-aba1-c5f3c07a3dc7'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:63 -  > /tmp/bench-rados/ceph-research/scripts/parallel-ssh --host 10.10.2.3 --host 10.10.2.2 'sudo python3 -u - rm-cluster --force --fsid 599a25e4-405b-11ec-aba1-c5f3c07a3dc7'
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:24:47,599035071-05:00] DEBUG: command from stdin[0m
# /tmp/bench-rados/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host 10.10.2.3 --host 10.10.2.2 'sudo python3 -u - rm-cluster --force --fsid 599a25e4-405b-11ec-aba1-c5f3c07a3dc7'
[1] 01:24:53 [SUCCESS] 10.10.2.3
[2] 01:24:59 [SUCCESS] 10.10.2.2

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:24:59,317052721-05:00] INFO: > Deploy a new cluster[0m
# bash:24 -  > /tmp/bench-rados/ceph-research/scripts/ceph-deploy -m 10.10.2.3 -o node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/10.10.2.2:/dev/nvme0n1


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:24:59,328346208-05:00] STAGE: Verify passwordless SSH to hosts, passwordless sudo via ljishen, and the existence of devices on hosts...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:24:59,332878498-05:00] INFO: > Check the host for the monitor[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:181 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@10.10.2.3 sudo --non-interactive --validate

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:24:59,482211992-05:00] INFO: > Check hosts for OSDs[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:24:59,486711080-05:00] INFO: >> Check host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:200 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us sudo --non-interactive --validate

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:24:59,634105466-05:00] INFO: >>> Check device '/dev/nvme0n1'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:215 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us test -b /dev/nvme0n1
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:226 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us lsblk --nodeps --noheadings --output TYPE /dev/nvme0n1
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:227 - verify_parameters() > grep --word-regexp disk


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:24:59,910448798-05:00] STAGE: Reset devices on hosts...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:24:59,915610206-05:00] INFO: > Resetting device '/dev/nvme0n1' on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:288 - reset_host_devices() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us sudo sh
  Removing ceph--91797d8a--25c5--461a--88c9--43a8390c4110-osd--block--648dc3b9--0202--4eca--ac0b--6213df76b425 (253:0)
  Archiving volume group "ceph-91797d8a-25c5-461a-88c9-43a8390c4110" metadata (seqno 5).
  Releasing logical volume "osd-block-648dc3b9-0202-4eca-ac0b-6213df76b425"
  Creating volume group backup "/etc/lvm/backup/ceph-91797d8a-25c5-461a-88c9-43a8390c4110" (seqno 6).
  Logical volume "osd-block-648dc3b9-0202-4eca-ac0b-6213df76b425" successfully removed
  Removing physical volume "/dev/nvme0n1" from volume group "ceph-91797d8a-25c5-461a-88c9-43a8390c4110"
  Volume group "ceph-91797d8a-25c5-461a-88c9-43a8390c4110" successfully removed


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:25:00,355462166-05:00] STAGE: Bootstrap a new cluster...[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:317 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.3 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:25:00,364944686-05:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:25:00,368315525-05:00] DEBUG: CEPHADM_PARAMS: ['bootstrap', '--config', '/tmp/bench-rados/ceph-research/scripts/ceph.conf', '--mon-ip', '10.10.2.3', '--output-dir', '/tmp/bench-rados/deployment_data_root/etc/ceph', '--ssh-user', 'ljishen', '--skip-dashboard', '--no-minimize-config', '--allow-overwrite', '--allow-fqdn-hostname', '--orphan-initial-daemons', '--skip-monitoring-stack', '--single-host-defaults'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.3 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults
Verifying podman|docker is present...
Verifying lvm2 is present...
Verifying time synchronization is in place...
Unit ntp.service is enabled and running
Repeating the final host check...
docker (/usr/bin/docker) is present
systemctl is present
lvcreate is present
Unit ntp.service is enabled and running
Host looks OK
Cluster fsid: 99f46a22-405c-11ec-aba1-c5f3c07a3dc7
Verifying IP 10.10.2.3 port 3300 ...
Verifying IP 10.10.2.3 port 6789 ...
Mon IP `10.10.2.3` is in CIDR network `10.10.2.0/24`
- internal network (--cluster-network) has not been provided, OSD replication will default to the public_network
Adjusting default settings to suit single-host cluster...
Pulling container image quay.io/ceph/ceph:v16...
Ceph version: ceph version 16.2.6 (ee28fb57e47e9f88813e24bbf4c14496ca299d31) pacific (stable)
Extracting ceph user uid/gid from container image...
Creating initial keys...
Creating initial monmap...
Creating mon...
Waiting for mon to start...
Waiting for mon...
mon is available
Setting mon public_network to 10.10.2.0/24
Wrote config to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf
Wrote keyring to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring
Creating mgr...
Verifying port 9283 ...
Waiting for mgr to start...
Waiting for mgr...
mgr not available, waiting (1/15)...
mgr not available, waiting (2/15)...
mgr is available
Enabling cephadm module...
Waiting for the mgr to restart...
Waiting for mgr epoch 5...
mgr epoch 5 is available
Setting orchestrator backend to cephadm...
Generating ssh key...
Wrote public SSH key to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub
Adding key to ljishen@localhost authorized_keys...
Adding host node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us...
Deploying unmanaged mon service...
Deploying unmanaged mgr service...
You can access the Ceph CLI with:

	sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --fsid 99f46a22-405c-11ec-aba1-c5f3c07a3dc7 -c /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf -k /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring

Please consider enabling telemetry to help improve Ceph:

	ceph telemetry on

For more information see:

	https://docs.ceph.com/docs/pacific/mgr/telemetry/

Bootstrap complete.

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:26:09,969367340-05:00] INFO: Sleep for 20 seconds[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:26:29,977388965-05:00] INFO: > Set the default number of monitor daemons to 1[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:337 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:26:29,987259828-05:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:26:29,990883104-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mon', '1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1
Inferring fsid 99f46a22-405c-11ec-aba1-c5f3c07a3dc7
Inferring config /var/lib/ceph/99f46a22-405c-11ec-aba1-c5f3c07a3dc7/mon.node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Scheduled mon update...

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:26:39,745836649-05:00] INFO: > Set the default number of manager daemons to 1[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:342 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:26:39,755753138-05:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:26:39,759507091-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mgr', '1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1
Inferring fsid 99f46a22-405c-11ec-aba1-c5f3c07a3dc7
Inferring config /var/lib/ceph/99f46a22-405c-11ec-aba1-c5f3c07a3dc7/mon.node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Scheduled mgr update...


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:26:50,552568654-05:00] STAGE: Add hosts to the cluster...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:26:50,558468034-05:00] INFO: > Adding host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:361 - add_hosts() > ssh-copy-id -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -f -i /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us
/usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: "/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub"

Number of key(s) added: 1

Now try logging into the machine, with:   "ssh -o 'GlobalKnownHostsFile=/dev/null' -o 'LogLevel=ERROR' -o 'PasswordAuthentication=no' -o 'StrictHostKeyChecking=no' -o 'UserKnownHostsFile=/dev/null' 'ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'"
and check to make sure that only the key(s) you wanted were added.

# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:367 - add_hosts() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us 10.10.2.2
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:26:50,768054241-05:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:26:50,771404592-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'host', 'add', 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us', '10.10.2.2'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us 10.10.2.2
Inferring fsid 99f46a22-405c-11ec-aba1-c5f3c07a3dc7
Inferring config /var/lib/ceph/99f46a22-405c-11ec-aba1-c5f3c07a3dc7/mon.node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Added host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us' with addr '10.10.2.2'

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:27:00,857691165-05:00] INFO: Sleep for 20 seconds[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:27:20,862969165-05:00] STAGE: Add OSDs to the cluster...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:27:20,870113143-05:00] INFO: > Adding devices ['/dev/nvme0n1'] on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us' as OSDs[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:388 - add_host_devices_as_osds() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:27:20,880062474-05:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:27:20,883527732-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'daemon', 'add', 'osd', 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1
Inferring fsid 99f46a22-405c-11ec-aba1-c5f3c07a3dc7
Inferring config /var/lib/ceph/99f46a22-405c-11ec-aba1-c5f3c07a3dc7/mon.node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Created osd(s) 0 on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:27:47,430055139-05:00] INFO: > Sleep for 20 seconds[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:28:07,435898477-05:00] STAGE: Show Cluster Status[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:408 - show_status() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:28:07,446931062-05:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:28:07,450659477-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', '--status'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status
Inferring fsid 99f46a22-405c-11ec-aba1-c5f3c07a3dc7
Inferring config /var/lib/ceph/99f46a22-405c-11ec-aba1-c5f3c07a3dc7/mon.node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
  cluster:
    id:     99f46a22-405c-11ec-aba1-c5f3c07a3dc7
    health: HEALTH_WARN
            1 pool(s) have no replicas configured
 
  services:
    mon: 1 daemons, quorum node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us (age 2m)
    mgr: node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us.gvwmpl(active, since 2m)
    osd: 1 osds: 1 up (since 18s), 1 in (since 41s)
 
  data:
    pools:   1 pools, 1 pgs
    objects: 0 objects, 0 B
    usage:   4.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     1 active+clean
 

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:28:16,529639503-05:00] INFO: > Copy configuration files to local[0m
# ./benchmarks/bench-rados:220 - launch_ceph_cluster() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.3 sudo chown --changes --recursive ljishen /tmp/bench-rados/deployment_data_root/etc/ceph
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:28:16,537059140-05:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.3 sudo chown --changes --recursive ljishen /tmp/bench-rados/deployment_data_root/etc/ceph
[1] 01:28:16 [SUCCESS] ljishen@10.10.2.3
# ./benchmarks/bench-rados:223 - launch_ceph_cluster() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.3:/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf
# ./benchmarks/bench-rados:226 - launch_ceph_cluster() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.3:/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:28:17,008024737-05:00] STAGE: Update local ceph.conf as a client...[0m


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:28:17,011389220-05:00] STAGE: Configure Ceph pool...[0m
# ./benchmarks/bench-rados:257 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_max_backfills 32
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:28:17,032206887-05:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:28:17,034781291-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '99f46a22-405c-11ec-aba1-c5f3c07a3dc7', '--', 'ceph', 'config', 'set', 'osd', 'osd_max_backfills', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 99f46a22-405c-11ec-aba1-c5f3c07a3dc7 -- ceph config set osd osd_max_backfills 32
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:258 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_active 32
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:28:21,069271363-05:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:28:21,072090628-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '99f46a22-405c-11ec-aba1-c5f3c07a3dc7', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_active', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 99f46a22-405c-11ec-aba1-c5f3c07a3dc7 -- ceph config set osd osd_recovery_max_active 32
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:259 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_single_start 8
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:28:25,198153600-05:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:28:25,200774301-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '99f46a22-405c-11ec-aba1-c5f3c07a3dc7', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_single_start', '8'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 99f46a22-405c-11ec-aba1-c5f3c07a3dc7 -- ceph config set osd osd_recovery_max_single_start 8
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:260 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_op_priority 63
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:28:29,310783110-05:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:28:29,313614939-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '99f46a22-405c-11ec-aba1-c5f3c07a3dc7', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_op_priority', '63'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 99f46a22-405c-11ec-aba1-c5f3c07a3dc7 -- ceph config set osd osd_recovery_op_priority 63
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:262 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config show-with-defaults osd.0
# ./benchmarks/bench-rados:263 - configure_ceph_pool() > grep 'osd_max_backfills\|osd_recovery'
# ./benchmarks/bench-rados:263 - configure_ceph_pool() > column -t -s ' '
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
osd_max_backfills                        32         mon
osd_recovery_cost                        20971520   default
osd_recovery_delay_start                 0.000000   default
osd_recovery_max_active                  32         mon
osd_recovery_max_active_hdd              3          default
osd_recovery_max_active_ssd              10         default
osd_recovery_max_chunk                   8388608    default
osd_recovery_max_omap_entries_per_chunk  8096       default
osd_recovery_max_single_start            8          mon
osd_recovery_op_priority                 63         mon
osd_recovery_op_warn_multiple            16         default
osd_recovery_priority                    5          default
osd_recovery_retry_interval              30.000000  default
osd_recovery_sleep                       0.000000   default
osd_recovery_sleep_hdd                   0.100000   default
osd_recovery_sleep_hybrid                0.025000   default
osd_recovery_sleep_ssd                   0.000000   default
# ./benchmarks/bench-rados:265 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool create bench_rados 128 128 replicated --size 1 --pg-num-min 128
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:28:37,513223603-05:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:28:37,515792826-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '99f46a22-405c-11ec-aba1-c5f3c07a3dc7', '--', 'ceph', 'osd', 'pool', 'create', 'bench_rados', '128', '128', 'replicated', '--size', '1', '--pg-num-min', '128'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 99f46a22-405c-11ec-aba1-c5f3c07a3dc7 -- ceph osd pool create bench_rados 128 128 replicated --size 1 --pg-num-min 128
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
pool 'bench_rados' created
# ./benchmarks/bench-rados:267 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados min_size 1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:28:42,461041281-05:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:28:42,463743085-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '99f46a22-405c-11ec-aba1-c5f3c07a3dc7', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'min_size', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 99f46a22-405c-11ec-aba1-c5f3c07a3dc7 -- ceph osd pool set bench_rados min_size 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 min_size to 1
# ./benchmarks/bench-rados:268 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados pg_autoscale_mode off
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:28:46,902796367-05:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:28:46,905610172-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '99f46a22-405c-11ec-aba1-c5f3c07a3dc7', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'pg_autoscale_mode', 'off'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 99f46a22-405c-11ec-aba1-c5f3c07a3dc7 -- ceph osd pool set bench_rados pg_autoscale_mode off
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 pg_autoscale_mode to off
# ./benchmarks/bench-rados:269 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool application enable bench_rados benchmark
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:28:51,492832774-05:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:28:51,495760313-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '99f46a22-405c-11ec-aba1-c5f3c07a3dc7', '--', 'ceph', 'osd', 'pool', 'application', 'enable', 'bench_rados', 'benchmark'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 99f46a22-405c-11ec-aba1-c5f3c07a3dc7 -- ceph osd pool application enable bench_rados benchmark
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
enabled application 'benchmark' on pool 'bench_rados'
# ./benchmarks/bench-rados:270 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados noscrub 1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:28:56,471812018-05:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:28:56,474648575-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '99f46a22-405c-11ec-aba1-c5f3c07a3dc7', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'noscrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 99f46a22-405c-11ec-aba1-c5f3c07a3dc7 -- ceph osd pool set bench_rados noscrub 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 noscrub to 1
# ./benchmarks/bench-rados:271 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados nodeep-scrub 1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:29:01,916925862-05:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:29:01,919593452-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '99f46a22-405c-11ec-aba1-c5f3c07a3dc7', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'nodeep-scrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 99f46a22-405c-11ec-aba1-c5f3c07a3dc7 -- ceph osd pool set bench_rados nodeep-scrub 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 nodeep-scrub to 1
# ./benchmarks/bench-rados:272 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool ls detail
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:29:07,146420679-05:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:29:07,149104609-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '99f46a22-405c-11ec-aba1-c5f3c07a3dc7', '--', 'ceph', 'osd', 'pool', 'ls', 'detail'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 99f46a22-405c-11ec-aba1-c5f3c07a3dc7 -- ceph osd pool ls detail
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
pool 1 'device_health_metrics' replicated size 1 min_size 1 crush_rule 0 object_hash rjenkins pg_num 64 pgp_num 64 autoscale_mode on last_change 17 lfor 0/0/14 flags hashpspool stripe_width 0 pg_num_min 1 application mgr_devicehealth
pool 2 'bench_rados' replicated size 1 min_size 1 crush_rule 0 object_hash rjenkins pg_num 128 pgp_num 128 autoscale_mode off last_change 20 flags hashpspool,noscrub,nodeep-scrub stripe_width 0 pg_num_min 128 application benchmark

# ./benchmarks/bench-rados:273 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd df tree
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:29:11,385339991-05:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:29:11,388130011-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '99f46a22-405c-11ec-aba1-c5f3c07a3dc7', '--', 'ceph', 'osd', 'df', 'tree'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 99f46a22-405c-11ec-aba1-c5f3c07a3dc7 -- ceph osd df tree
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
ID  CLASS  WEIGHT   REWEIGHT  SIZE     RAW USE  DATA     OMAP  META     AVAIL    %USE  VAR   PGS  STATUS  TYPE NAME      
-1         0.09769         -  100 GiB  5.7 MiB  168 KiB   0 B  5.6 MiB  100 GiB  0.01  1.00    -          root default   
-3         0.09769         -  100 GiB  5.7 MiB  168 KiB   0 B  5.6 MiB  100 GiB  0.01  1.00    -              host node-1
 0    ssd  0.09769   1.00000  100 GiB  5.7 MiB  168 KiB   0 B  5.6 MiB  100 GiB  0.01  1.00  192      up          osd.0  
                       TOTAL  100 GiB  5.7 MiB  168 KiB   0 B  5.6 MiB  100 GiB  0.01                                    
MIN/MAX VAR: 1.00/1.00  STDDEV: 0


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:29:15,390433730-05:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:29:39,495680918-05:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:29:48,614521892-05:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:29:48,621511499-05:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:29:57,742717197-05:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:29:57,749998515-05:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:30:06,901485259-05:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:30:06,908666608-05:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:30:15,979541270-05:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:30:15,986675420-05:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:30:24,985150946-05:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:30:24,992495182-05:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:30:24,998102243-05:00] INFO: > The cluster is idle now.[0m


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:30:25,001542068-05:00] STAGE: Start benchmarking of mode write ...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:30:25,007934599-05:00] INFO: > Start system activity collection services[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:30:25,013187242-05:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:341 - rados_bench() > rm --force /mnt/sda8/ceph-research/scripts/output/sys_activity_256KB_write.dat.rados_bench_host.3
# ./benchmarks/bench-rados:342 - rados_bench() > sar_pid_rados_bench_host=206591
# ./benchmarks/bench-rados:342 - rados_bench() > S_TIME_FORMAT=ISO
# ./benchmarks/bench-rados:342 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/output/sys_activity_256KB_write.dat.rados_bench_host.3 2

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:30:25,020244818-05:00] INFO: >> for OSD host[0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:30:25,028621781-05:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.2 bash
10.10.2.2: b'49181\n'
[1] 01:30:25 [SUCCESS] ljishen@10.10.2.2
49181

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:30:25,213176608-05:00] INFO: > Run rados bench[0m
# ./benchmarks/bench-rados:370 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/output/rados_bench_256KB_write.log.3
## ./benchmarks/bench-rados:361 - rados_bench() > count_cpus 32-63,96-127
# ./benchmarks/bench-rados:361 - rados_bench() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 60 write --pool bench_rados -b 262144 -O 262144 --concurrent-ios 128 --show-time --write-object --write-omap --write-xattr --no-cleanup
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:30:25,232899311-05:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:30:25,235621623-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '99f46a22-405c-11ec-aba1-c5f3c07a3dc7', '--', 'rados', 'bench', '60', 'write', '--pool', 'bench_rados', '-b', '262144', '-O', '262144', '--concurrent-ios', '128', '--show-time', '--write-object', '--write-omap', '--write-xattr', '--no-cleanup'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 99f46a22-405c-11ec-aba1-c5f3c07a3dc7 -- rados bench 60 write --pool bench_rados -b 262144 -O 262144 --concurrent-ios 128 --show-time --write-object --write-omap --write-xattr --no-cleanup
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
hints = 1
2021-11-08T06:30:28.455669+0000 Maintaining 128 concurrent writes of 262144 bytes to objects of size 262144 for up to 60 seconds or 0 objects
2021-11-08T06:30:28.455681+0000 Object prefix: benchmark_data_node-0.bluefield2.ucsc-cmps10_7
2021-11-08T06:30:28.464215+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-11-08T06:30:28.464215+0000     0       0         0         0         0         0           -           0
2021-11-08T06:30:29.464309+0000     1     128      1293      1165   291.234    291.25   0.0889359     0.10221
2021-11-08T06:30:30.464383+0000     2     128      2355      2227   278.357     265.5    0.161022    0.109564
2021-11-08T06:30:31.464461+0000     3     128      2996      2868   238.984    160.25    0.154111     0.12968
2021-11-08T06:30:32.464534+0000     4     128      3942      3814   238.358     236.5   0.0929912    0.132856
2021-11-08T06:30:33.464605+0000     5     128      4574      4446   222.284       158    0.314554    0.138906
2021-11-08T06:30:34.464676+0000     6     128      5002      4874   203.069       107    0.310852    0.155266
2021-11-08T06:30:35.464745+0000     7     128      5366      5238   187.058        91    0.302904    0.166771
2021-11-08T06:30:36.464813+0000     8     128      6024      5896   184.237     164.5    0.199511    0.171651
2021-11-08T06:30:37.464883+0000     9     128      6500      6372   176.988       119    0.314728    0.177554
2021-11-08T06:30:38.464954+0000    10     128      6921      6793   169.813    105.25    0.277472    0.186061
2021-11-08T06:30:39.465027+0000    11     128      7678      7550   171.579    189.25   0.0870281    0.185833
2021-11-08T06:30:40.465094+0000    12     128      8560      8432   175.654     220.5    0.258021    0.180136
2021-11-08T06:30:41.465166+0000    13     128      8925      8797   169.161     91.25    0.338244    0.186421
2021-11-08T06:30:42.465235+0000    14     128      9332      9204   164.346    101.75    0.228355    0.192333
2021-11-08T06:30:43.465303+0000    15     128      9921      9793   163.205    147.25    0.184673    0.195214
2021-11-08T06:30:44.465370+0000    16     128     10427     10299   160.911     126.5    0.283464    0.195644
2021-11-08T06:30:45.465439+0000    17     128     10869     10741   157.945     110.5    0.339042    0.201081
2021-11-08T06:30:46.465504+0000    18     128     11251     11123   154.475      95.5    0.314275    0.205435
2021-11-08T06:30:47.465572+0000    19     128     11733     11605   152.687     120.5    0.146614    0.207992
2021-11-08T06:30:48.465640+0000 min lat: 0.00706005 max lat: 0.516198 avg lat: 0.207733
2021-11-08T06:30:48.465640+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-11-08T06:30:48.465640+0000    20     128     12415     12287   153.577     170.5    0.164464    0.207733
2021-11-08T06:30:49.465713+0000    21     128     12797     12669   150.811      95.5     0.29263    0.210786
2021-11-08T06:30:50.465778+0000    22     128     13171     13043   148.206      93.5    0.296621    0.213878
2021-11-08T06:30:51.465848+0000    23     128     13624     13496   146.685    113.25    0.203259    0.217359
2021-11-08T06:30:52.465916+0000    24     128     14302     14174   147.636     169.5    0.185701    0.216248
2021-11-08T06:30:53.465984+0000    25     128     14740     14612    146.11     109.5    0.137928    0.218002
2021-11-08T06:30:54.466054+0000    26     128     15140     15012   144.336       100     0.27903    0.221032
2021-11-08T06:30:55.466126+0000    27     128     15492     15364   142.249        88   0.0809801    0.223282
2021-11-08T06:30:56.466196+0000    28     128     16145     16017   142.999    163.25    0.135539    0.223327
2021-11-08T06:30:57.466272+0000    29     128     16665     16537    142.55       130    0.286727    0.223961
2021-11-08T06:30:58.466339+0000    30     128     16991     16863   140.515      81.5    0.228266    0.225842
2021-11-08T06:30:59.466406+0000    31     128     17432     17304   139.539    110.25   0.0466392    0.228695
2021-11-08T06:31:00.466472+0000    32     128     17884     17756   138.709       113    0.135903    0.229532
2021-11-08T06:31:01.466539+0000    33     128     18552     18424   139.566       167    0.188395    0.228233
2021-11-08T06:31:02.466607+0000    34     128     18920     18792   138.167        92    0.308784    0.230178
2021-11-08T06:31:03.466679+0000    35     128     19280     19152   136.791        90    0.308257    0.232015
2021-11-08T06:31:04.466749+0000    36     127     19785     19658   136.504     126.5    0.133603    0.233853
2021-11-08T06:31:05.466822+0000    37     128     20445     20317   137.268    164.75     0.15747    0.232582
2021-11-08T06:31:06.466892+0000    38     128     20871     20743   136.458     106.5    0.329824    0.233547
2021-11-08T06:31:07.466965+0000    39     128     21247     21119   135.369        94    0.295654    0.235144
2021-11-08T06:31:08.467041+0000 min lat: 0.00706005 max lat: 0.595639 avg lat: 0.236858
2021-11-08T06:31:08.467041+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-11-08T06:31:08.467041+0000    40     127     21661     21534   134.578    103.75    0.299469    0.236858
2021-11-08T06:31:09.467136+0000    41     128     22267     22139   134.984    151.25    0.147371    0.236348
2021-11-08T06:31:10.467207+0000    42     128     22826     22698   135.098    139.75    0.279401    0.236269
2021-11-08T06:31:11.467276+0000    43     128     23181     23053    134.02     88.75     0.30621    0.237811
2021-11-08T06:31:12.467346+0000    44     128     23554     23426   133.093     93.25     0.47814    0.239243
2021-11-08T06:31:13.467412+0000    45     128     24139     24011   133.385    146.25    0.165379     0.23925
2021-11-08T06:31:14.467482+0000    46     128     24704     24576   133.556    141.25    0.270346    0.238351
2021-11-08T06:31:15.467557+0000    47     128     25137     25009   133.017    108.25    0.281316    0.239969
2021-11-08T06:31:16.467626+0000    48     128     25489     25361   132.079        88   0.0781541    0.241138
2021-11-08T06:31:17.467702+0000    49     128     26000     25872   131.991    127.75    0.195692    0.242106
2021-11-08T06:31:18.467772+0000    50     128     26649     26521   132.596    162.25    0.148138    0.240974
2021-11-08T06:31:19.467840+0000    51     128     26988     26860   131.657     84.75   0.0704618    0.241949
2021-11-08T06:31:20.467907+0000    52     128     27436     27308   131.279       112    0.300285    0.243233
2021-11-08T06:31:21.467980+0000    53     128     27787     27659   130.458     87.75   0.0843523    0.244187
2021-11-08T06:31:22.468056+0000    54     128     28492     28364   131.306    176.25    0.240482    0.243224
2021-11-08T06:31:23.468128+0000    55     128     28991     28863   131.186    124.75    0.336668     0.24352
2021-11-08T06:31:24.468200+0000    56     128     29375     29247   130.558        96    0.320866    0.244582
2021-11-08T06:31:25.468271+0000    57     128     29740     29612   129.868     91.25    0.348508    0.245638
2021-11-08T06:31:26.468347+0000    58     128     30335     30207   130.193    148.75    0.128618    0.245269
2021-11-08T06:31:27.468417+0000    59     128     30873     30745   130.266     134.5    0.167574    0.244839
2021-11-08T06:31:28.468484+0000 min lat: 0.00706005 max lat: 0.618205 avg lat: 0.245966
2021-11-08T06:31:28.468484+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-11-08T06:31:28.468484+0000    60     128     31293     31165   129.845       105   0.0729236    0.245966
2021-11-08T06:31:29.468584+0000 Total time run:         60.2369
Total writes made:      31293
Write size:             262144
Object size:            262144
Bandwidth (MB/sec):     129.875
Stddev Bandwidth:       44.339
Max bandwidth (MB/sec): 291.25
Min bandwidth (MB/sec): 81.5
Average IOPS:           519
Stddev IOPS:            177.356
Max IOPS:               1165
Min IOPS:               326
Average Latency(s):     0.246191
Stddev Latency(s):      0.0989972
Max latency(s):         0.618205
Min latency(s):         0.00706005

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:31:30,155108898-05:00] INFO: > Stop system activity collection services[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:31:30,160488230-05:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:391 - rados_bench() > kill -INT 206591

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:31:30,165859536-05:00] INFO: >> for OSD host[0m
# ./benchmarks/bench-rados:396 - rados_bench() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.2 kill -INT 49181
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:31:30,173077073-05:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.2 kill -INT 49181
[1] 01:31:30 [SUCCESS] ljishen@10.10.2.2

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:31:30,356328825-05:00] INFO: >>> Copy system activity file from OSD host[0m
## ./benchmarks/bench-rados:403 - rados_bench() > basename -- /tmp/bench-rados/sys_activity_256KB_write.dat.osd_host.3
# ./benchmarks/bench-rados:403 - rados_bench() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.2:/tmp/bench-rados/sys_activity_256KB_write.dat.osd_host.3 /mnt/sda8/ceph-research/scripts/output/sys_activity_256KB_write.dat.osd_host.3


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:31:30,531533285-05:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:31:54,491616618-05:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 31.29k objects, 7.6 GiB
    usage:   23 GiB used, 77 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:31:54,498948932-05:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:32:03,518273459-05:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 31.29k objects, 7.6 GiB
    usage:   23 GiB used, 77 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:32:03,525589351-05:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:32:12,599429763-05:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 31.29k objects, 7.6 GiB
    usage:   23 GiB used, 77 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:32:12,606886951-05:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:32:21,760953228-05:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 31.29k objects, 7.6 GiB
    usage:   23 GiB used, 77 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:32:21,768192125-05:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:32:30,814195478-05:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 31.29k objects, 7.6 GiB
    usage:   23 GiB used, 77 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:32:30,821532501-05:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:32:30,827314942-05:00] INFO: > The cluster is idle now.[0m


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:32:30,831001833-05:00] STAGE: Start benchmarking of mode seq ...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:32:30,837733194-05:00] INFO: > Start system activity collection services[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:32:30,842932296-05:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:341 - rados_bench() > rm --force /mnt/sda8/ceph-research/scripts/output/sys_activity_256KB_seq.dat.rados_bench_host.3
# ./benchmarks/bench-rados:342 - rados_bench() > sar_pid_rados_bench_host=207960
# ./benchmarks/bench-rados:342 - rados_bench() > S_TIME_FORMAT=ISO
# ./benchmarks/bench-rados:342 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/output/sys_activity_256KB_seq.dat.rados_bench_host.3 2

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:32:30,849615585-05:00] INFO: >> for OSD host[0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:32:30,858033967-05:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.2 bash
10.10.2.2: b'49283\n'
[1] 01:32:31 [SUCCESS] ljishen@10.10.2.2
49283

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:32:31,045702275-05:00] INFO: > Run rados bench[0m
# ./benchmarks/bench-rados:379 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/output/rados_bench_256KB_seq.log.3
## ./benchmarks/bench-rados:376 - rados_bench() > count_cpus 32-63,96-127
# ./benchmarks/bench-rados:376 - rados_bench() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 128 --show-time
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:32:31,064623457-05:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:32:31,067293571-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '99f46a22-405c-11ec-aba1-c5f3c07a3dc7', '--', 'rados', 'bench', '99999999', 'seq', '--pool', 'bench_rados', '--concurrent-ios', '128', '--show-time'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 99f46a22-405c-11ec-aba1-c5f3c07a3dc7 -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 128 --show-time
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
hints = 1
2021-11-08T06:32:34.230452+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-11-08T06:32:34.230452+0000     0       0         0         0         0         0           -           0
2021-11-08T06:32:35.230568+0000     1     128      6989      6861   1714.93   1715.25   0.0160899   0.0184499
2021-11-08T06:32:36.230651+0000     2     128     13909     13781   1722.39      1730   0.0208128   0.0184576
2021-11-08T06:32:37.230775+0000     3     127     20838     20711   1725.69    1732.5   0.0119306   0.0184492
2021-11-08T06:32:38.230855+0000     4     128     27761     27633   1726.86    1730.5   0.0180353   0.0184571
2021-11-08T06:32:39.230959+0000 Total time run:       4.53181
Total reads made:     31293
Read size:            262144
Object size:          262144
Bandwidth (MB/sec):   1726.3
Average IOPS:         6905
Stddev IOPS:          31.7949
Max IOPS:             6930
Min IOPS:             6861
Average Latency(s):   0.0184731
Max latency(s):       0.0305381
Min latency(s):       0.00215339

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:32:39,868683202-05:00] INFO: > Stop system activity collection services[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:32:39,874157032-05:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:391 - rados_bench() > kill -INT 207960

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:32:39,879772969-05:00] INFO: >> for OSD host[0m
# ./benchmarks/bench-rados:396 - rados_bench() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.2 kill -INT 49283
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:32:39,887529944-05:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.2 kill -INT 49283
[1] 01:32:40 [SUCCESS] ljishen@10.10.2.2

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:32:40,068333140-05:00] INFO: >>> Copy system activity file from OSD host[0m
## ./benchmarks/bench-rados:403 - rados_bench() > basename -- /tmp/bench-rados/sys_activity_256KB_seq.dat.osd_host.3
# ./benchmarks/bench-rados:403 - rados_bench() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.2:/tmp/bench-rados/sys_activity_256KB_seq.dat.osd_host.3 /mnt/sda8/ceph-research/scripts/output/sys_activity_256KB_seq.dat.osd_host.3


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:32:40,216671200-05:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:33:04,368646326-05:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 31.29k objects, 7.6 GiB
    usage:   23 GiB used, 77 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:33:04,376041368-05:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:33:13,353282233-05:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 31.29k objects, 7.6 GiB
    usage:   23 GiB used, 77 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:33:13,360684018-05:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:33:22,476038579-05:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 31.29k objects, 7.6 GiB
    usage:   23 GiB used, 77 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:33:22,483507420-05:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:33:31,536695257-05:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 31.29k objects, 7.6 GiB
    usage:   23 GiB used, 77 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:33:31,543853833-05:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:33:40,621829257-05:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 31.29k objects, 7.6 GiB
    usage:   23 GiB used, 77 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:33:40,629042566-05:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:33:40,634753944-05:00] INFO: > The cluster is idle now.[0m


[1;7;39;49m[2021-11-08T01:33:40,638169082-05:00][RUNNING][ROUND 1/5/21] object_size=1MB[0m


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:33:40,641579752-05:00] STAGE: Launch a Ceph cluster (CEPH_HOST: 10.10.2.3, OSD_HOST: 10.10.2.2) ...[0m
# ./benchmarks/bench-rados:183 - launch_ceph_cluster() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.3 bash -euo pipefail
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:33:40,650360447-05:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.3 bash -euo pipefail
10.10.2.3: b'\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T01:33:41,076831359-05:00] INFO: > Remove existing clusters\x1b[0m\n'
10.10.2.3: b'## bash:17 -  > basename -- /var/lib/ceph/99f46a22-405c-11ec-aba1-c5f3c07a3dc7\n'
10.10.2.3: b'# bash:17 -  > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm --host 10.10.2.3 --host 10.10.2.2 rm-cluster --force --fsid 99f46a22-405c-11ec-aba1-c5f3c07a3dc7\n'
10.10.2.3: b"\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T01:33:41,086935513-05:00] DEBUG: HOST_PARAMS: ['--host', '10.10.2.3', '--host', '10.10.2.2']\x1b[0m\n"
10.10.2.3: b"\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T01:33:41,090020624-05:00] DEBUG: CEPHADM_PARAMS: ['rm-cluster', '--force', '--fsid', '99f46a22-405c-11ec-aba1-c5f3c07a3dc7']\x1b[0m\n"
10.10.2.3: b"# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:63 -  > /tmp/bench-rados/ceph-research/scripts/parallel-ssh --host 10.10.2.3 --host 10.10.2.2 'sudo python3 -u - rm-cluster --force --fsid 99f46a22-405c-11ec-aba1-c5f3c07a3dc7'\n"
10.10.2.3: b'\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T01:33:41,098584182-05:00] DEBUG: command from stdin\x1b[0m\n'
10.10.2.3: b"# /tmp/bench-rados/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host 10.10.2.3 --host 10.10.2.2 'sudo python3 -u - rm-cluster --force --fsid 99f46a22-405c-11ec-aba1-c5f3c07a3dc7'\n"
10.10.2.3: b'[1] 01:33:46 [SUCCESS] 10.10.2.3\n[2] 01:33:52 [SUCCESS] 10.10.2.2\n'
10.10.2.3: b'\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T01:33:52,881229588-05:00] INFO: > Deploy a new cluster\x1b[0m\n'
10.10.2.3: b'# bash:24 -  > /tmp/bench-rados/ceph-research/scripts/ceph-deploy -m 10.10.2.3 -o node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/10.10.2.2:/dev/nvme0n1\n'
10.10.2.3: b'\n\n\x1b[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T01:33:52,892414171-05:00] STAGE: Verify passwordless SSH to hosts, passwordless sudo via ljishen, and the existence of devices on hosts...\x1b[0m\n'
10.10.2.3: b'\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T01:33:52,896920614-05:00] INFO: > Check the host for the monitor\x1b[0m\n'
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:181 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@10.10.2.3 sudo --non-interactive --validate\n'
10.10.2.3: b'\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T01:33:53,045605893-05:00] INFO: > Check hosts for OSDs\x1b[0m\n'
10.10.2.3: b"\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T01:33:53,050407663-05:00] INFO: >> Check host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'\x1b[0m\n"
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:200 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us sudo --non-interactive --validate\n'
10.10.2.3: b"\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T01:33:53,197928694-05:00] INFO: >>> Check device '/dev/nvme0n1'\x1b[0m\n"
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:215 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us test -b /dev/nvme0n1\n'
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:226 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us lsblk --nodeps --noheadings --output TYPE /dev/nvme0n1\n'
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:227 - verify_parameters() > grep --word-regexp disk\n'
10.10.2.3: b'\n\n\x1b[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T01:33:53,478769464-05:00] STAGE: Reset devices on hosts...\x1b[0m\n'
10.10.2.3: b"\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T01:33:53,483267020-05:00] INFO: > Resetting device '/dev/nvme0n1' on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'\x1b[0m\n"
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:288 - reset_host_devices() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us sudo sh\n'
10.10.2.3: b'  Removing ceph--5f2f63e4--449d--415b--a68e--4c76b328b261-osd--block--7bc28a3d--e6cc--41f2--a8b8--d41002f16c25 (253:0)\n'
10.10.2.3: b'  Archiving volume group "ceph-5f2f63e4-449d-415b-a68e-4c76b328b261" metadata (seqno 5).\n  Releasing logical volume "osd-block-7bc28a3d-e6cc-41f2-a8b8-d41002f16c25"\n'
10.10.2.3: b'  Creating volume group backup "/etc/lvm/backup/ceph-5f2f63e4-449d-415b-a68e-4c76b328b261" (seqno 6).\n'
10.10.2.3: b'  Logical volume "osd-block-7bc28a3d-e6cc-41f2-a8b8-d41002f16c25" successfully removed\n'
10.10.2.3: b'  Removing physical volume "/dev/nvme0n1" from volume group "ceph-5f2f63e4-449d-415b-a68e-4c76b328b261"\n'
10.10.2.3: b'  Volume group "ceph-5f2f63e4-449d-415b-a68e-4c76b328b261" successfully removed\n'
10.10.2.3: b'\n\n\x1b[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T01:33:53,863742860-05:00] STAGE: Bootstrap a new cluster...\x1b[0m\n'
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:317 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.3 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults\n'
10.10.2.3: b'\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T01:33:53,873318056-05:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.3: b"\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T01:33:53,876938027-05:00] DEBUG: CEPHADM_PARAMS: ['bootstrap', '--config', '/tmp/bench-rados/ceph-research/scripts/ceph.conf', '--mon-ip', '10.10.2.3', '--output-dir', '/tmp/bench-rados/deployment_data_root/etc/ceph', '--ssh-user', 'ljishen', '--skip-dashboard', '--no-minimize-config', '--allow-overwrite', '--allow-fqdn-hostname', '--orphan-initial-daemons', '--skip-monitoring-stack', '--single-host-defaults']\x1b[0m\n"
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.3 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults\n'
10.10.2.3: b'Verifying podman|docker is present...\nVerifying lvm2 is present...\n'
10.10.2.3: b'Verifying time synchronization is in place...\n'
10.10.2.3: b'Unit ntp.service is enabled and running\n'
10.10.2.3: b'Repeating the final host check...\n'
10.10.2.3: b'docker (/usr/bin/docker) is present\n'
10.10.2.3: b'systemctl is present\n'
10.10.2.3: b'lvcreate is present\n'
10.10.2.3: b'Unit ntp.service is enabled and running\n'
10.10.2.3: b'Host looks OK\n'
10.10.2.3: b'Cluster fsid: d7f3f620-405d-11ec-aba1-c5f3c07a3dc7\n'
10.10.2.3: b'Verifying IP 10.10.2.3 port 3300 ...\n'
10.10.2.3: b'Verifying IP 10.10.2.3 port 6789 ...\n'
10.10.2.3: b'Mon IP `10.10.2.3` is in CIDR network `10.10.2.0/24`\n'
10.10.2.3: b'- internal network (--cluster-network) has not been provided, OSD replication will default to the public_network\n'
10.10.2.3: b'Adjusting default settings to suit single-host cluster...\n'
10.10.2.3: b'Pulling container image quay.io/ceph/ceph:v16...\n'
10.10.2.3: b'Ceph version: ceph version 16.2.6 (ee28fb57e47e9f88813e24bbf4c14496ca299d31) pacific (stable)\n'
10.10.2.3: b'Extracting ceph user uid/gid from container image...\n'
10.10.2.3: b'Creating initial keys...\n'
10.10.2.3: b'Creating initial monmap...\n'
10.10.2.3: b'Creating mon...\n'
10.10.2.3: b'Waiting for mon to start...\n'
10.10.2.3: b'Waiting for mon...\n'
10.10.2.3: b'mon is available\nSetting mon public_network to 10.10.2.0/24\n'
10.10.2.3: b'Wrote config to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf\n'
10.10.2.3: b'Wrote keyring to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring\n'
10.10.2.3: b'Creating mgr...\n'
10.10.2.3: b'Verifying port 9283 ...\n'
10.10.2.3: b'Waiting for mgr to start...\nWaiting for mgr...\n'
10.10.2.3: b'mgr not available, waiting (1/15)...\n'
10.10.2.3: b'mgr not available, waiting (2/15)...\n'
10.10.2.3: b'mgr is available\n'
10.10.2.3: b'Enabling cephadm module...\n'
10.10.2.3: b'Waiting for the mgr to restart...\n'
10.10.2.3: b'Waiting for mgr epoch 5...\n'
10.10.2.3: b'mgr epoch 5 is available\nSetting orchestrator backend to cephadm...\n'
10.10.2.3: b'Generating ssh key...\n'
10.10.2.3: b'Wrote public SSH key to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub\n'
10.10.2.3: b'Adding key to ljishen@localhost authorized_keys...\n'
10.10.2.3: b'Adding host node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us...\n'
10.10.2.3: b'Deploying unmanaged mon service...\n'
10.10.2.3: b'Deploying unmanaged mgr service...\n'
10.10.2.3: b'You can access the Ceph CLI with:\n\n\tsudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --fsid d7f3f620-405d-11ec-aba1-c5f3c07a3dc7 -c /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf -k /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring\n\n'
10.10.2.3: b'Please consider enabling telemetry to help improve Ceph:\n\n\tceph telemetry on\n\nFor more information see:\n\n\thttps://docs.ceph.com/docs/pacific/mgr/telemetry/\n\n'
10.10.2.3: b'Bootstrap complete.\n'
10.10.2.3: b'\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T01:35:01,388254171-05:00] INFO: Sleep for 20 seconds\x1b[0m\n'
10.10.2.3: b'\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T01:35:21,395410830-05:00] INFO: > Set the default number of monitor daemons to 1\x1b[0m\n'
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:337 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1\n'
10.10.2.3: b'\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T01:35:21,405071007-05:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.3: b"\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T01:35:21,408703731-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mon', '1']\x1b[0m\n"
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1\n'
10.10.2.3: b'Inferring fsid d7f3f620-405d-11ec-aba1-c5f3c07a3dc7\n'
10.10.2.3: b'Inferring config /var/lib/ceph/d7f3f620-405d-11ec-aba1-c5f3c07a3dc7/mon.node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.3: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.3: b'Scheduled mon update...\n'
10.10.2.3: b'\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T01:35:31,654405250-05:00] INFO: > Set the default number of manager daemons to 1\x1b[0m\n'
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:342 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1\n'
10.10.2.3: b'\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T01:35:31,663794074-05:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.3: b"\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T01:35:31,667846691-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mgr', '1']\x1b[0m\n"
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1\n'
10.10.2.3: b'Inferring fsid d7f3f620-405d-11ec-aba1-c5f3c07a3dc7\n'
10.10.2.3: b'Inferring config /var/lib/ceph/d7f3f620-405d-11ec-aba1-c5f3c07a3dc7/mon.node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.3: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.3: b'Scheduled mgr update...\n'
10.10.2.3: b'\n\n\x1b[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T01:35:40,620903857-05:00] STAGE: Add hosts to the cluster...\x1b[0m\n'
10.10.2.3: b"\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T01:35:40,626976244-05:00] INFO: > Adding host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'\x1b[0m\n"
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:361 - add_hosts() > ssh-copy-id -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -f -i /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\n'
10.10.2.3: b'/usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: "/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub"\n'
10.10.2.3: b'\nNumber of key(s) added: 1\n\nNow try logging into the machine, with:   "ssh -o \'GlobalKnownHostsFile=/dev/null\' -o \'LogLevel=ERROR\' -o \'PasswordAuthentication=no\' -o \'StrictHostKeyChecking=no\' -o \'UserKnownHostsFile=/dev/null\' \'ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\'"\nand check to make sure that only the key(s) you wanted were added.\n\n'
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:367 - add_hosts() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us 10.10.2.2\n'
10.10.2.3: b'\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T01:35:40,837385756-05:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.3: b"\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T01:35:40,841193330-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'host', 'add', 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us', '10.10.2.2']\x1b[0m\n"
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us 10.10.2.2\n'
10.10.2.3: b'Inferring fsid d7f3f620-405d-11ec-aba1-c5f3c07a3dc7\n'
10.10.2.3: b'Inferring config /var/lib/ceph/d7f3f620-405d-11ec-aba1-c5f3c07a3dc7/mon.node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.3: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.3: b"Added host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us' with addr '10.10.2.2'\n"
10.10.2.3: b'\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T01:35:51,141913164-05:00] INFO: Sleep for 20 seconds\x1b[0m\n'
10.10.2.3: b'\n\n\x1b[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T01:36:11,147171877-05:00] STAGE: Add OSDs to the cluster...\x1b[0m\n'
10.10.2.3: b"\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T01:36:11,153656192-05:00] INFO: > Adding devices ['/dev/nvme0n1'] on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us' as OSDs\x1b[0m\n"
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:388 - add_host_devices_as_osds() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1\n'
10.10.2.3: b'\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T01:36:11,163457585-05:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.3: b"\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T01:36:11,166787487-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'daemon', 'add', 'osd', 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1']\x1b[0m\n"
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1\n'
10.10.2.3: b'Inferring fsid d7f3f620-405d-11ec-aba1-c5f3c07a3dc7\n'
10.10.2.3: b'Inferring config /var/lib/ceph/d7f3f620-405d-11ec-aba1-c5f3c07a3dc7/mon.node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.3: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.3: b"Created osd(s) 0 on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'\n"
10.10.2.3: b'\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T01:36:37,712481641-05:00] INFO: > Sleep for 20 seconds\x1b[0m\n'
10.10.2.3: b'\n\n\x1b[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T01:36:57,718442620-05:00] STAGE: Show Cluster Status\x1b[0m\n'
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:408 - show_status() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status\n'
10.10.2.3: b'\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T01:36:57,728465651-05:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.3: b"\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T01:36:57,732079039-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', '--status']\x1b[0m\n"
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status\n'
10.10.2.3: b'Inferring fsid d7f3f620-405d-11ec-aba1-c5f3c07a3dc7\n'
10.10.2.3: b'Inferring config /var/lib/ceph/d7f3f620-405d-11ec-aba1-c5f3c07a3dc7/mon.node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.3: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.3: b'  cluster:\n    id:     d7f3f620-405d-11ec-aba1-c5f3c07a3dc7\n    health: HEALTH_WARN\n            1 pool(s) have no replicas configured\n \n  services:\n    mon: 1 daemons, quorum node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us (age 2m)\n    mgr: node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us.axwhnz(active, since 2m)\n    osd: 1 osds: 1 up (since 18s), 1 in (since 42s)\n \n  data:\n    pools:   1 pools, 1 pgs\n    objects: 0 objects, 0 B\n    usage:   4.7 MiB used, 100 GiB / 100 GiB avail\n    pgs:     1 active+clean\n \n'
[1] 01:37:07 [SUCCESS] ljishen@10.10.2.3

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:33:41,076831359-05:00] INFO: > Remove existing clusters[0m
## bash:17 -  > basename -- /var/lib/ceph/99f46a22-405c-11ec-aba1-c5f3c07a3dc7
# bash:17 -  > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm --host 10.10.2.3 --host 10.10.2.2 rm-cluster --force --fsid 99f46a22-405c-11ec-aba1-c5f3c07a3dc7
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:33:41,086935513-05:00] DEBUG: HOST_PARAMS: ['--host', '10.10.2.3', '--host', '10.10.2.2'][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:33:41,090020624-05:00] DEBUG: CEPHADM_PARAMS: ['rm-cluster', '--force', '--fsid', '99f46a22-405c-11ec-aba1-c5f3c07a3dc7'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:63 -  > /tmp/bench-rados/ceph-research/scripts/parallel-ssh --host 10.10.2.3 --host 10.10.2.2 'sudo python3 -u - rm-cluster --force --fsid 99f46a22-405c-11ec-aba1-c5f3c07a3dc7'
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:33:41,098584182-05:00] DEBUG: command from stdin[0m
# /tmp/bench-rados/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host 10.10.2.3 --host 10.10.2.2 'sudo python3 -u - rm-cluster --force --fsid 99f46a22-405c-11ec-aba1-c5f3c07a3dc7'
[1] 01:33:46 [SUCCESS] 10.10.2.3
[2] 01:33:52 [SUCCESS] 10.10.2.2

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:33:52,881229588-05:00] INFO: > Deploy a new cluster[0m
# bash:24 -  > /tmp/bench-rados/ceph-research/scripts/ceph-deploy -m 10.10.2.3 -o node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/10.10.2.2:/dev/nvme0n1


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:33:52,892414171-05:00] STAGE: Verify passwordless SSH to hosts, passwordless sudo via ljishen, and the existence of devices on hosts...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:33:52,896920614-05:00] INFO: > Check the host for the monitor[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:181 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@10.10.2.3 sudo --non-interactive --validate

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:33:53,045605893-05:00] INFO: > Check hosts for OSDs[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:33:53,050407663-05:00] INFO: >> Check host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:200 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us sudo --non-interactive --validate

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:33:53,197928694-05:00] INFO: >>> Check device '/dev/nvme0n1'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:215 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us test -b /dev/nvme0n1
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:226 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us lsblk --nodeps --noheadings --output TYPE /dev/nvme0n1
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:227 - verify_parameters() > grep --word-regexp disk


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:33:53,478769464-05:00] STAGE: Reset devices on hosts...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:33:53,483267020-05:00] INFO: > Resetting device '/dev/nvme0n1' on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:288 - reset_host_devices() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us sudo sh
  Removing ceph--5f2f63e4--449d--415b--a68e--4c76b328b261-osd--block--7bc28a3d--e6cc--41f2--a8b8--d41002f16c25 (253:0)
  Archiving volume group "ceph-5f2f63e4-449d-415b-a68e-4c76b328b261" metadata (seqno 5).
  Releasing logical volume "osd-block-7bc28a3d-e6cc-41f2-a8b8-d41002f16c25"
  Creating volume group backup "/etc/lvm/backup/ceph-5f2f63e4-449d-415b-a68e-4c76b328b261" (seqno 6).
  Logical volume "osd-block-7bc28a3d-e6cc-41f2-a8b8-d41002f16c25" successfully removed
  Removing physical volume "/dev/nvme0n1" from volume group "ceph-5f2f63e4-449d-415b-a68e-4c76b328b261"
  Volume group "ceph-5f2f63e4-449d-415b-a68e-4c76b328b261" successfully removed


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:33:53,863742860-05:00] STAGE: Bootstrap a new cluster...[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:317 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.3 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:33:53,873318056-05:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:33:53,876938027-05:00] DEBUG: CEPHADM_PARAMS: ['bootstrap', '--config', '/tmp/bench-rados/ceph-research/scripts/ceph.conf', '--mon-ip', '10.10.2.3', '--output-dir', '/tmp/bench-rados/deployment_data_root/etc/ceph', '--ssh-user', 'ljishen', '--skip-dashboard', '--no-minimize-config', '--allow-overwrite', '--allow-fqdn-hostname', '--orphan-initial-daemons', '--skip-monitoring-stack', '--single-host-defaults'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.3 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults
Verifying podman|docker is present...
Verifying lvm2 is present...
Verifying time synchronization is in place...
Unit ntp.service is enabled and running
Repeating the final host check...
docker (/usr/bin/docker) is present
systemctl is present
lvcreate is present
Unit ntp.service is enabled and running
Host looks OK
Cluster fsid: d7f3f620-405d-11ec-aba1-c5f3c07a3dc7
Verifying IP 10.10.2.3 port 3300 ...
Verifying IP 10.10.2.3 port 6789 ...
Mon IP `10.10.2.3` is in CIDR network `10.10.2.0/24`
- internal network (--cluster-network) has not been provided, OSD replication will default to the public_network
Adjusting default settings to suit single-host cluster...
Pulling container image quay.io/ceph/ceph:v16...
Ceph version: ceph version 16.2.6 (ee28fb57e47e9f88813e24bbf4c14496ca299d31) pacific (stable)
Extracting ceph user uid/gid from container image...
Creating initial keys...
Creating initial monmap...
Creating mon...
Waiting for mon to start...
Waiting for mon...
mon is available
Setting mon public_network to 10.10.2.0/24
Wrote config to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf
Wrote keyring to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring
Creating mgr...
Verifying port 9283 ...
Waiting for mgr to start...
Waiting for mgr...
mgr not available, waiting (1/15)...
mgr not available, waiting (2/15)...
mgr is available
Enabling cephadm module...
Waiting for the mgr to restart...
Waiting for mgr epoch 5...
mgr epoch 5 is available
Setting orchestrator backend to cephadm...
Generating ssh key...
Wrote public SSH key to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub
Adding key to ljishen@localhost authorized_keys...
Adding host node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us...
Deploying unmanaged mon service...
Deploying unmanaged mgr service...
You can access the Ceph CLI with:

	sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --fsid d7f3f620-405d-11ec-aba1-c5f3c07a3dc7 -c /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf -k /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring

Please consider enabling telemetry to help improve Ceph:

	ceph telemetry on

For more information see:

	https://docs.ceph.com/docs/pacific/mgr/telemetry/

Bootstrap complete.

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:35:01,388254171-05:00] INFO: Sleep for 20 seconds[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:35:21,395410830-05:00] INFO: > Set the default number of monitor daemons to 1[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:337 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:35:21,405071007-05:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:35:21,408703731-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mon', '1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1
Inferring fsid d7f3f620-405d-11ec-aba1-c5f3c07a3dc7
Inferring config /var/lib/ceph/d7f3f620-405d-11ec-aba1-c5f3c07a3dc7/mon.node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Scheduled mon update...

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:35:31,654405250-05:00] INFO: > Set the default number of manager daemons to 1[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:342 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:35:31,663794074-05:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:35:31,667846691-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mgr', '1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1
Inferring fsid d7f3f620-405d-11ec-aba1-c5f3c07a3dc7
Inferring config /var/lib/ceph/d7f3f620-405d-11ec-aba1-c5f3c07a3dc7/mon.node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Scheduled mgr update...


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:35:40,620903857-05:00] STAGE: Add hosts to the cluster...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:35:40,626976244-05:00] INFO: > Adding host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:361 - add_hosts() > ssh-copy-id -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -f -i /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us
/usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: "/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub"

Number of key(s) added: 1

Now try logging into the machine, with:   "ssh -o 'GlobalKnownHostsFile=/dev/null' -o 'LogLevel=ERROR' -o 'PasswordAuthentication=no' -o 'StrictHostKeyChecking=no' -o 'UserKnownHostsFile=/dev/null' 'ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'"
and check to make sure that only the key(s) you wanted were added.

# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:367 - add_hosts() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us 10.10.2.2
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:35:40,837385756-05:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:35:40,841193330-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'host', 'add', 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us', '10.10.2.2'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us 10.10.2.2
Inferring fsid d7f3f620-405d-11ec-aba1-c5f3c07a3dc7
Inferring config /var/lib/ceph/d7f3f620-405d-11ec-aba1-c5f3c07a3dc7/mon.node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Added host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us' with addr '10.10.2.2'

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:35:51,141913164-05:00] INFO: Sleep for 20 seconds[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:36:11,147171877-05:00] STAGE: Add OSDs to the cluster...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:36:11,153656192-05:00] INFO: > Adding devices ['/dev/nvme0n1'] on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us' as OSDs[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:388 - add_host_devices_as_osds() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:36:11,163457585-05:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:36:11,166787487-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'daemon', 'add', 'osd', 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1
Inferring fsid d7f3f620-405d-11ec-aba1-c5f3c07a3dc7
Inferring config /var/lib/ceph/d7f3f620-405d-11ec-aba1-c5f3c07a3dc7/mon.node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Created osd(s) 0 on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:36:37,712481641-05:00] INFO: > Sleep for 20 seconds[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:36:57,718442620-05:00] STAGE: Show Cluster Status[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:408 - show_status() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:36:57,728465651-05:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:36:57,732079039-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', '--status'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status
Inferring fsid d7f3f620-405d-11ec-aba1-c5f3c07a3dc7
Inferring config /var/lib/ceph/d7f3f620-405d-11ec-aba1-c5f3c07a3dc7/mon.node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
  cluster:
    id:     d7f3f620-405d-11ec-aba1-c5f3c07a3dc7
    health: HEALTH_WARN
            1 pool(s) have no replicas configured
 
  services:
    mon: 1 daemons, quorum node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us (age 2m)
    mgr: node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us.axwhnz(active, since 2m)
    osd: 1 osds: 1 up (since 18s), 1 in (since 42s)
 
  data:
    pools:   1 pools, 1 pgs
    objects: 0 objects, 0 B
    usage:   4.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     1 active+clean
 

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:37:07,257505393-05:00] INFO: > Copy configuration files to local[0m
# ./benchmarks/bench-rados:220 - launch_ceph_cluster() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.3 sudo chown --changes --recursive ljishen /tmp/bench-rados/deployment_data_root/etc/ceph
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:37:07,265059234-05:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.3 sudo chown --changes --recursive ljishen /tmp/bench-rados/deployment_data_root/etc/ceph
[1] 01:37:07 [SUCCESS] ljishen@10.10.2.3
# ./benchmarks/bench-rados:223 - launch_ceph_cluster() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.3:/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf
# ./benchmarks/bench-rados:226 - launch_ceph_cluster() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.3:/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:37:07,740347008-05:00] STAGE: Update local ceph.conf as a client...[0m


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:37:07,743886861-05:00] STAGE: Configure Ceph pool...[0m
# ./benchmarks/bench-rados:257 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_max_backfills 32
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:37:07,764777366-05:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:37:07,767318627-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'd7f3f620-405d-11ec-aba1-c5f3c07a3dc7', '--', 'ceph', 'config', 'set', 'osd', 'osd_max_backfills', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid d7f3f620-405d-11ec-aba1-c5f3c07a3dc7 -- ceph config set osd osd_max_backfills 32
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:258 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_active 32
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:37:12,048222176-05:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:37:12,050961952-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'd7f3f620-405d-11ec-aba1-c5f3c07a3dc7', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_active', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid d7f3f620-405d-11ec-aba1-c5f3c07a3dc7 -- ceph config set osd osd_recovery_max_active 32
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:259 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_single_start 8
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:37:16,160069257-05:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:37:16,162856441-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'd7f3f620-405d-11ec-aba1-c5f3c07a3dc7', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_single_start', '8'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid d7f3f620-405d-11ec-aba1-c5f3c07a3dc7 -- ceph config set osd osd_recovery_max_single_start 8
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:260 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_op_priority 63
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:37:20,273631920-05:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:37:20,276549561-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'd7f3f620-405d-11ec-aba1-c5f3c07a3dc7', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_op_priority', '63'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid d7f3f620-405d-11ec-aba1-c5f3c07a3dc7 -- ceph config set osd osd_recovery_op_priority 63
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:262 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config show-with-defaults osd.0
# ./benchmarks/bench-rados:263 - configure_ceph_pool() > grep 'osd_max_backfills\|osd_recovery'
# ./benchmarks/bench-rados:263 - configure_ceph_pool() > column -t -s ' '
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
osd_max_backfills                        32         mon
osd_recovery_cost                        20971520   default
osd_recovery_delay_start                 0.000000   default
osd_recovery_max_active                  32         mon
osd_recovery_max_active_hdd              3          default
osd_recovery_max_active_ssd              10         default
osd_recovery_max_chunk                   8388608    default
osd_recovery_max_omap_entries_per_chunk  8096       default
osd_recovery_max_single_start            8          mon
osd_recovery_op_priority                 63         mon
osd_recovery_op_warn_multiple            16         default
osd_recovery_priority                    5          default
osd_recovery_retry_interval              30.000000  default
osd_recovery_sleep                       0.000000   default
osd_recovery_sleep_hdd                   0.100000   default
osd_recovery_sleep_hybrid                0.025000   default
osd_recovery_sleep_ssd                   0.000000   default
# ./benchmarks/bench-rados:265 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool create bench_rados 128 128 replicated --size 1 --pg-num-min 128
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:37:28,349735116-05:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:37:28,352828258-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'd7f3f620-405d-11ec-aba1-c5f3c07a3dc7', '--', 'ceph', 'osd', 'pool', 'create', 'bench_rados', '128', '128', 'replicated', '--size', '1', '--pg-num-min', '128'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid d7f3f620-405d-11ec-aba1-c5f3c07a3dc7 -- ceph osd pool create bench_rados 128 128 replicated --size 1 --pg-num-min 128
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
pool 'bench_rados' created
# ./benchmarks/bench-rados:267 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados min_size 1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:37:32,978605121-05:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:37:32,981401864-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'd7f3f620-405d-11ec-aba1-c5f3c07a3dc7', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'min_size', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid d7f3f620-405d-11ec-aba1-c5f3c07a3dc7 -- ceph osd pool set bench_rados min_size 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 min_size to 1
# ./benchmarks/bench-rados:268 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados pg_autoscale_mode off
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:37:38,399991907-05:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:37:38,402820430-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'd7f3f620-405d-11ec-aba1-c5f3c07a3dc7', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'pg_autoscale_mode', 'off'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid d7f3f620-405d-11ec-aba1-c5f3c07a3dc7 -- ceph osd pool set bench_rados pg_autoscale_mode off
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 pg_autoscale_mode to off
# ./benchmarks/bench-rados:269 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool application enable bench_rados benchmark
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:37:42,789210400-05:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:37:42,791915660-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'd7f3f620-405d-11ec-aba1-c5f3c07a3dc7', '--', 'ceph', 'osd', 'pool', 'application', 'enable', 'bench_rados', 'benchmark'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid d7f3f620-405d-11ec-aba1-c5f3c07a3dc7 -- ceph osd pool application enable bench_rados benchmark
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
enabled application 'benchmark' on pool 'bench_rados'
# ./benchmarks/bench-rados:270 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados noscrub 1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:37:47,985587794-05:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:37:47,988553816-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'd7f3f620-405d-11ec-aba1-c5f3c07a3dc7', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'noscrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid d7f3f620-405d-11ec-aba1-c5f3c07a3dc7 -- ceph osd pool set bench_rados noscrub 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 noscrub to 1
# ./benchmarks/bench-rados:271 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados nodeep-scrub 1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:37:52,390917375-05:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:37:52,393606785-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'd7f3f620-405d-11ec-aba1-c5f3c07a3dc7', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'nodeep-scrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid d7f3f620-405d-11ec-aba1-c5f3c07a3dc7 -- ceph osd pool set bench_rados nodeep-scrub 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 nodeep-scrub to 1
# ./benchmarks/bench-rados:272 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool ls detail
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:37:56,932653753-05:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:37:56,935654751-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'd7f3f620-405d-11ec-aba1-c5f3c07a3dc7', '--', 'ceph', 'osd', 'pool', 'ls', 'detail'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid d7f3f620-405d-11ec-aba1-c5f3c07a3dc7 -- ceph osd pool ls detail
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
pool 1 'device_health_metrics' replicated size 1 min_size 1 crush_rule 0 object_hash rjenkins pg_num 64 pgp_num 64 autoscale_mode on last_change 17 lfor 0/0/14 flags hashpspool stripe_width 0 pg_num_min 1 application mgr_devicehealth
pool 2 'bench_rados' replicated size 1 min_size 1 crush_rule 0 object_hash rjenkins pg_num 128 pgp_num 128 autoscale_mode off last_change 20 flags hashpspool,noscrub,nodeep-scrub stripe_width 0 pg_num_min 128 application benchmark

# ./benchmarks/bench-rados:273 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd df tree
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:38:01,199152229-05:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:38:01,202062485-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'd7f3f620-405d-11ec-aba1-c5f3c07a3dc7', '--', 'ceph', 'osd', 'df', 'tree'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid d7f3f620-405d-11ec-aba1-c5f3c07a3dc7 -- ceph osd df tree
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
ID  CLASS  WEIGHT   REWEIGHT  SIZE     RAW USE  DATA     OMAP  META     AVAIL    %USE  VAR   PGS  STATUS  TYPE NAME      
-1         0.09769         -  100 GiB  5.7 MiB  168 KiB   0 B  5.6 MiB  100 GiB  0.01  1.00    -          root default   
-3         0.09769         -  100 GiB  5.7 MiB  168 KiB   0 B  5.6 MiB  100 GiB  0.01  1.00    -              host node-1
 0    ssd  0.09769   1.00000  100 GiB  5.7 MiB  168 KiB   0 B  5.6 MiB  100 GiB  0.01  1.00  192      up          osd.0  
                       TOTAL  100 GiB  5.7 MiB  168 KiB   0 B  5.6 MiB  100 GiB  0.01                                    
MIN/MAX VAR: 1.00/1.00  STDDEV: 0


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:38:05,372441690-05:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:38:29,280410941-05:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:38:38,366575566-05:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:38:47,640133009-05:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:38:47,647801926-05:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:38:56,775029930-05:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:38:56,782578822-05:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:39:05,895531739-05:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:39:05,902748675-05:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:39:15,087628733-05:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:39:15,095548103-05:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:39:24,194907214-05:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:39:24,202540926-05:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:39:24,208257533-05:00] INFO: > The cluster is idle now.[0m


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:39:24,211875895-05:00] STAGE: Start benchmarking of mode write ...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:39:24,218452153-05:00] INFO: > Start system activity collection services[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:39:24,223446769-05:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:341 - rados_bench() > rm --force /mnt/sda8/ceph-research/scripts/output/sys_activity_1MB_write.dat.rados_bench_host.1
# ./benchmarks/bench-rados:342 - rados_bench() > sar_pid_rados_bench_host=213525
# ./benchmarks/bench-rados:342 - rados_bench() > S_TIME_FORMAT=ISO
# ./benchmarks/bench-rados:342 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/output/sys_activity_1MB_write.dat.rados_bench_host.1 2

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:39:24,230227854-05:00] INFO: >> for OSD host[0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:39:24,238737417-05:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.2 bash
10.10.2.2: b'52892\n'
[1] 01:39:24 [SUCCESS] ljishen@10.10.2.2
52892

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:39:24,425275556-05:00] INFO: > Run rados bench[0m
# ./benchmarks/bench-rados:370 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/output/rados_bench_1MB_write.log.1
## ./benchmarks/bench-rados:361 - rados_bench() > count_cpus 32-63,96-127
# ./benchmarks/bench-rados:361 - rados_bench() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 60 write --pool bench_rados -b 1048576 -O 1048576 --concurrent-ios 128 --show-time --write-object --write-omap --write-xattr --no-cleanup
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:39:24,444709946-05:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:39:24,447354181-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'd7f3f620-405d-11ec-aba1-c5f3c07a3dc7', '--', 'rados', 'bench', '60', 'write', '--pool', 'bench_rados', '-b', '1048576', '-O', '1048576', '--concurrent-ios', '128', '--show-time', '--write-object', '--write-omap', '--write-xattr', '--no-cleanup'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid d7f3f620-405d-11ec-aba1-c5f3c07a3dc7 -- rados bench 60 write --pool bench_rados -b 1048576 -O 1048576 --concurrent-ios 128 --show-time --write-object --write-omap --write-xattr --no-cleanup
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
hints = 1
2021-11-08T06:39:27.637637+0000 Maintaining 128 concurrent writes of 1048576 bytes to objects of size 1048576 for up to 60 seconds or 0 objects
2021-11-08T06:39:27.637650+0000 Object prefix: benchmark_data_node-0.bluefield2.ucsc-cmps10_7
2021-11-08T06:39:27.669352+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-11-08T06:39:27.669352+0000     0       0         0         0         0         0           -           0
2021-11-08T06:39:28.669457+0000     1     127       335       208   207.988       208    0.379127    0.412408
2021-11-08T06:39:29.669547+0000     2     127       587       460   229.983       252    0.580063    0.419966
2021-11-08T06:39:30.669619+0000     3     127       722       595   198.319       135    0.882706    0.508306
2021-11-08T06:39:31.669690+0000     4     127      1006       879   219.734       284     0.38331    0.531345
2021-11-08T06:39:32.669760+0000     5     127      1129      1002   200.386       123    0.848544    0.533518
2021-11-08T06:39:33.669841+0000     6     127      1228      1101   183.487        99     1.23552    0.600808
2021-11-08T06:39:34.669911+0000     7     127      1339      1212    173.13       111     1.17791    0.672183
2021-11-08T06:39:35.669989+0000     8     127      1493      1366   170.737       154    0.837601    0.698422
2021-11-08T06:39:36.670057+0000     9     127      1641      1514    168.21       148    0.893125    0.714627
2021-11-08T06:39:37.670127+0000    10     127      1774      1647   164.688       133    0.790032    0.729752
2021-11-08T06:39:38.670201+0000    11     127      2084      1957   177.896       310    0.420703    0.693963
2021-11-08T06:39:39.670270+0000    12     127      2181      2054   171.154        97     1.10468    0.698007
2021-11-08T06:39:40.670335+0000    13     127      2267      2140   164.604        86     1.40917    0.727705
2021-11-08T06:39:41.670401+0000    14     127      2383      2256   161.131       116     1.02069    0.760219
2021-11-08T06:39:42.670466+0000    15     127      2527      2400   159.989       144    0.855988    0.770403
2021-11-08T06:39:43.670533+0000    16     127      2630      2503   156.426       103    0.996638    0.776477
2021-11-08T06:39:44.670609+0000    17     127      2728      2601   152.989        98     1.28153    0.796337
2021-11-08T06:39:45.670675+0000    18     127      2812      2685   149.156        84     1.32798    0.817177
2021-11-08T06:39:46.670741+0000    19     127      3073      2946   155.042       261    0.390512    0.811278
2021-11-08T06:39:47.670806+0000 min lat: 0.334067 max lat: 1.61972 avg lat: 0.803286
2021-11-08T06:39:47.670806+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-11-08T06:39:47.670806+0000    20     127      3188      3061   153.039       115    0.884944    0.803286
2021-11-08T06:39:48.670877+0000    21     127      3281      3154    150.18        93     1.43346    0.817536
2021-11-08T06:39:49.670947+0000    22     127      3398      3271   148.671       117     1.04397    0.835949
2021-11-08T06:39:50.671022+0000    23     127      3543      3416   148.511       145    0.863106    0.840347
2021-11-08T06:39:51.671093+0000    24     127      3663      3536   147.323       120     1.02729    0.842867
2021-11-08T06:39:52.671161+0000    25     127      3745      3618    144.71        82     1.25087    0.853979
2021-11-08T06:39:53.671230+0000    26     127      3836      3709   142.644        91     1.42241    0.869563
2021-11-08T06:39:54.671301+0000    27     127      3958      3831   141.879       122    0.912671    0.880111
2021-11-08T06:39:55.671366+0000    28     127      4101      3974   141.919       143    0.869644    0.880868
2021-11-08T06:39:56.671441+0000    29     127      4194      4067   140.232        93      1.1554    0.883882
2021-11-08T06:39:57.671511+0000    30     127      4297      4170    138.99       103     1.35559    0.896357
2021-11-08T06:39:58.671581+0000    31     127      4384      4257   137.313        87      1.2612    0.907179
2021-11-08T06:39:59.671654+0000    32     127      4542      4415   137.959       158    0.867455    0.911893
2021-11-08T06:40:00.671724+0000    33     127      4650      4523   137.051       108    0.934429    0.911295
2021-11-08T06:40:01.671793+0000    34     127      4741      4614   135.696        91     1.33329    0.916913
2021-11-08T06:40:02.671870+0000    35     127      4836      4709   134.533        95     1.41923    0.928377
2021-11-08T06:40:03.671944+0000    36     127      4958      4831   134.185       122     1.00642    0.937157
2021-11-08T06:40:04.672013+0000    37     127      5113      4986   134.747       155    0.772466    0.935505
2021-11-08T06:40:05.672087+0000    38     127      5215      5088   133.885       102    0.996954    0.936734
2021-11-08T06:40:06.672156+0000    39     127      5305      5178    132.76        90     1.37915    0.943913
2021-11-08T06:40:07.672226+0000 min lat: 0.334067 max lat: 1.68905 avg lat: 0.952334
2021-11-08T06:40:07.672226+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-11-08T06:40:07.672226+0000    40     127      5388      5261   131.516        83     1.31698    0.952334
2021-11-08T06:40:08.672303+0000    41     127      5554      5427   132.356       166    0.793138    0.954728
2021-11-08T06:40:09.672380+0000    42     127      5668      5541   131.919       114    0.873448    0.952709
2021-11-08T06:40:10.672450+0000    43     127      5757      5630   130.921        89     1.30314    0.955814
2021-11-08T06:40:11.672517+0000    44     127      5862      5735   130.332       105     1.28127    0.964792
2021-11-08T06:40:12.672583+0000    45     127      5986      5859   130.191       124    0.981308    0.970558
2021-11-08T06:40:13.672653+0000    46     127      6133      6006   130.556       147    0.866395     0.96825
2021-11-08T06:40:14.672740+0000    47     127      6226      6099   129.757        93     1.05511    0.968303
2021-11-08T06:40:15.672814+0000    48     127      6323      6196   129.074        97     1.26568    0.974296
2021-11-08T06:40:16.672880+0000    49     127      6415      6288   128.317        92     1.42896    0.981144
2021-11-08T06:40:17.672947+0000    50     127      6552      6425   128.491       137    0.796481    0.983528
2021-11-08T06:40:18.673016+0000    51     127      6693      6566   128.736       141    0.822441     0.98109
2021-11-08T06:40:19.673088+0000    52     127      6790      6663   128.126        97     1.28928    0.983263
2021-11-08T06:40:20.673160+0000    53     127      6873      6746   127.274        83     1.32429    0.989118
2021-11-08T06:40:21.673231+0000    54     127      6993      6866   127.139       120     1.02462    0.994853
2021-11-08T06:40:22.673298+0000    55     127      7136      7009   127.427       143    0.877623    0.993234
2021-11-08T06:40:23.673365+0000    56     127      7252      7125   127.223       116      1.0564    0.992384
2021-11-08T06:40:24.673435+0000    57     127      7328      7201   126.324        76     1.43511    0.995835
2021-11-08T06:40:25.673504+0000    58     127      7425      7298   125.819        97     1.27444     1.00211
2021-11-08T06:40:26.673574+0000    59     127      7564      7437   126.042       139    0.819935      1.0049
2021-11-08T06:40:27.673641+0000 min lat: 0.334067 max lat: 1.68998 avg lat: 1.00215
2021-11-08T06:40:27.673641+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-11-08T06:40:27.673641+0000    60     127      7713      7586   126.424       149     0.86643     1.00215
2021-11-08T06:40:28.673757+0000 Total time run:         60.3091
Total writes made:      7714
Write size:             1048576
Object size:            1048576
Bandwidth (MB/sec):     127.908
Stddev Bandwidth:       48.5252
Max bandwidth (MB/sec): 310
Min bandwidth (MB/sec): 76
Average IOPS:           127
Stddev IOPS:            48.5252
Max IOPS:               310
Min IOPS:               76
Average Latency(s):     0.995244
Stddev Latency(s):      0.344921
Max latency(s):         1.68998
Min latency(s):         0.271101

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:40:29,298498310-05:00] INFO: > Stop system activity collection services[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:40:29,303921484-05:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:391 - rados_bench() > kill -INT 213525

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:40:29,309426172-05:00] INFO: >> for OSD host[0m
# ./benchmarks/bench-rados:396 - rados_bench() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.2 kill -INT 52892
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:40:29,316777592-05:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.2 kill -INT 52892
[1] 01:40:29 [SUCCESS] ljishen@10.10.2.2

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:40:29,495869464-05:00] INFO: >>> Copy system activity file from OSD host[0m
## ./benchmarks/bench-rados:403 - rados_bench() > basename -- /tmp/bench-rados/sys_activity_1MB_write.dat.osd_host.1
# ./benchmarks/bench-rados:403 - rados_bench() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.2:/tmp/bench-rados/sys_activity_1MB_write.dat.osd_host.1 /mnt/sda8/ceph-research/scripts/output/sys_activity_1MB_write.dat.osd_host.1


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:40:29,670476761-05:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:40:53,716234281-05:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 7.71k objects, 7.5 GiB
    usage:   23 GiB used, 77 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:40:53,723569340-05:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:41:02,724182997-05:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 7.71k objects, 7.5 GiB
    usage:   23 GiB used, 77 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:41:02,731753190-05:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:41:11,799403065-05:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 7.71k objects, 7.5 GiB
    usage:   23 GiB used, 77 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:41:11,806854263-05:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:41:20,926152477-05:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 7.71k objects, 7.5 GiB
    usage:   23 GiB used, 77 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:41:20,933648931-05:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:41:30,085568040-05:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 7.71k objects, 7.5 GiB
    usage:   23 GiB used, 77 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:41:30,092738138-05:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:41:30,098488549-05:00] INFO: > The cluster is idle now.[0m


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:41:30,101935207-05:00] STAGE: Start benchmarking of mode seq ...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:41:30,109106246-05:00] INFO: > Start system activity collection services[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:41:30,114495557-05:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:341 - rados_bench() > rm --force /mnt/sda8/ceph-research/scripts/output/sys_activity_1MB_seq.dat.rados_bench_host.1
# ./benchmarks/bench-rados:342 - rados_bench() > sar_pid_rados_bench_host=214896
# ./benchmarks/bench-rados:342 - rados_bench() > S_TIME_FORMAT=ISO
# ./benchmarks/bench-rados:342 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/output/sys_activity_1MB_seq.dat.rados_bench_host.1 2

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:41:30,121814595-05:00] INFO: >> for OSD host[0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:41:30,130353263-05:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.2 bash
10.10.2.2: b'53009\n'
[1] 01:41:30 [SUCCESS] ljishen@10.10.2.2
53009

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:41:30,316971466-05:00] INFO: > Run rados bench[0m
# ./benchmarks/bench-rados:379 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/output/rados_bench_1MB_seq.log.1
## ./benchmarks/bench-rados:376 - rados_bench() > count_cpus 32-63,96-127
# ./benchmarks/bench-rados:376 - rados_bench() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 128 --show-time
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:41:30,335844728-05:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:41:30,338769752-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'd7f3f620-405d-11ec-aba1-c5f3c07a3dc7', '--', 'rados', 'bench', '99999999', 'seq', '--pool', 'bench_rados', '--concurrent-ios', '128', '--show-time'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid d7f3f620-405d-11ec-aba1-c5f3c07a3dc7 -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 128 --show-time
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
hints = 1
2021-11-08T06:41:33.665038+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-11-08T06:41:33.665038+0000     0       0         0         0         0         0           -           0
2021-11-08T06:41:34.665150+0000     1     127      1872      1745   1744.69      1745   0.0713548   0.0704328
2021-11-08T06:41:35.665304+0000     2     127      3613      3486   1742.71      1741   0.0736317   0.0718851
2021-11-08T06:41:36.665419+0000     3     127      5372      5245   1748.07      1759   0.0738475   0.0721776
2021-11-08T06:41:37.665528+0000     4     127      7133      7006   1751.26      1761   0.0750063   0.0722717
2021-11-08T06:41:38.665684+0000 Total time run:       4.38712
Total reads made:     7714
Read size:            1048576
Object size:          1048576
Bandwidth (MB/sec):   1758.33
Average IOPS:         1758
Stddev IOPS:          9.98332
Max IOPS:             1761
Min IOPS:             1741
Average Latency(s):   0.0721176
Max latency(s):       0.123096
Min latency(s):       0.0194683

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:41:39,327328092-05:00] INFO: > Stop system activity collection services[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:41:39,333064397-05:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:391 - rados_bench() > kill -INT 214896

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:41:39,338873649-05:00] INFO: >> for OSD host[0m
# ./benchmarks/bench-rados:396 - rados_bench() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.2 kill -INT 53009
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:41:39,346518792-05:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.2 kill -INT 53009
[1] 01:41:39 [SUCCESS] ljishen@10.10.2.2

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:41:39,528248479-05:00] INFO: >>> Copy system activity file from OSD host[0m
## ./benchmarks/bench-rados:403 - rados_bench() > basename -- /tmp/bench-rados/sys_activity_1MB_seq.dat.osd_host.1
# ./benchmarks/bench-rados:403 - rados_bench() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.2:/tmp/bench-rados/sys_activity_1MB_seq.dat.osd_host.1 /mnt/sda8/ceph-research/scripts/output/sys_activity_1MB_seq.dat.osd_host.1


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:41:39,676097749-05:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:42:03,775925924-05:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 7.71k objects, 7.5 GiB
    usage:   23 GiB used, 77 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:42:03,782954495-05:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:42:12,901248637-05:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 7.71k objects, 7.5 GiB
    usage:   23 GiB used, 77 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:42:12,908394880-05:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:42:22,094034283-05:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 7.71k objects, 7.5 GiB
    usage:   23 GiB used, 77 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:42:22,101638259-05:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:42:31,229661744-05:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 7.71k objects, 7.5 GiB
    usage:   23 GiB used, 77 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:42:31,237025968-05:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:42:40,395841061-05:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 7.71k objects, 7.5 GiB
    usage:   23 GiB used, 77 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:42:40,403282721-05:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:42:40,408867651-05:00] INFO: > The cluster is idle now.[0m


[1;7;39;49m[2021-11-08T01:42:40,411225195-05:00][RUNNING][ROUND 2/5/21] object_size=1MB[0m


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:42:40,414661825-05:00] STAGE: Launch a Ceph cluster (CEPH_HOST: 10.10.2.3, OSD_HOST: 10.10.2.2) ...[0m
# ./benchmarks/bench-rados:183 - launch_ceph_cluster() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.3 bash -euo pipefail
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:42:40,423759867-05:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.3 bash -euo pipefail
10.10.2.3: b'\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T01:42:40,872247249-05:00] INFO: > Remove existing clusters\x1b[0m\n'
10.10.2.3: b'## bash:17 -  > basename -- /var/lib/ceph/d7f3f620-405d-11ec-aba1-c5f3c07a3dc7\n'
10.10.2.3: b'# bash:17 -  > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm --host 10.10.2.3 --host 10.10.2.2 rm-cluster --force --fsid d7f3f620-405d-11ec-aba1-c5f3c07a3dc7\n'
10.10.2.3: b"\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T01:42:40,883320633-05:00] DEBUG: HOST_PARAMS: ['--host', '10.10.2.3', '--host', '10.10.2.2']\x1b[0m\n"
10.10.2.3: b"\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T01:42:40,886820126-05:00] DEBUG: CEPHADM_PARAMS: ['rm-cluster', '--force', '--fsid', 'd7f3f620-405d-11ec-aba1-c5f3c07a3dc7']\x1b[0m\n"
10.10.2.3: b"# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:63 -  > /tmp/bench-rados/ceph-research/scripts/parallel-ssh --host 10.10.2.3 --host 10.10.2.2 'sudo python3 -u - rm-cluster --force --fsid d7f3f620-405d-11ec-aba1-c5f3c07a3dc7'\n"
10.10.2.3: b'\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T01:42:40,894496449-05:00] DEBUG: command from stdin\x1b[0m\n'
10.10.2.3: b"# /tmp/bench-rados/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host 10.10.2.3 --host 10.10.2.2 'sudo python3 -u - rm-cluster --force --fsid d7f3f620-405d-11ec-aba1-c5f3c07a3dc7'\n"
10.10.2.3: b'[1] 01:42:46 [SUCCESS] 10.10.2.3\n[2] 01:42:52 [SUCCESS] 10.10.2.2\n'
10.10.2.3: b'\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T01:42:52,980484670-05:00] INFO: > Deploy a new cluster\x1b[0m\n'
10.10.2.3: b'# bash:24 -  > /tmp/bench-rados/ceph-research/scripts/ceph-deploy -m 10.10.2.3 -o node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/10.10.2.2:/dev/nvme0n1\n'
10.10.2.3: b'\n\n\x1b[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T01:42:52,991887114-05:00] STAGE: Verify passwordless SSH to hosts, passwordless sudo via ljishen, and the existence of devices on hosts...\x1b[0m\n'
10.10.2.3: b'\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T01:42:52,996425077-05:00] INFO: > Check the host for the monitor\x1b[0m\n'
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:181 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@10.10.2.3 sudo --non-interactive --validate\n'
10.10.2.3: b'\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T01:42:53,142011672-05:00] INFO: > Check hosts for OSDs\x1b[0m\n'
10.10.2.3: b"\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T01:42:53,147249334-05:00] INFO: >> Check host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'\x1b[0m\n"
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:200 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us sudo --non-interactive --validate\n'
10.10.2.3: b"\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T01:42:53,293969047-05:00] INFO: >>> Check device '/dev/nvme0n1'\x1b[0m\n"
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:215 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us test -b /dev/nvme0n1\n'
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:226 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us lsblk --nodeps --noheadings --output TYPE /dev/nvme0n1\n'
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:227 - verify_parameters() > grep --word-regexp disk\n'
10.10.2.3: b'\n\n\x1b[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T01:42:53,574299208-05:00] STAGE: Reset devices on hosts...\x1b[0m\n'
10.10.2.3: b"\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T01:42:53,579358936-05:00] INFO: > Resetting device '/dev/nvme0n1' on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'\x1b[0m\n"
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:288 - reset_host_devices() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us sudo sh\n'
10.10.2.3: b'  Removing ceph--62246f83--fd9f--4aba--ba13--60509b697adf-osd--block--1951785f--6ffd--45c7--9dc0--6ff28264161f (253:0)\n'
10.10.2.3: b'  Archiving volume group "ceph-62246f83-fd9f-4aba-ba13-60509b697adf" metadata (seqno 5).\n  Releasing logical volume "osd-block-1951785f-6ffd-45c7-9dc0-6ff28264161f"\n'
10.10.2.3: b'  Creating volume group backup "/etc/lvm/backup/ceph-62246f83-fd9f-4aba-ba13-60509b697adf" (seqno 6).\n'
10.10.2.3: b'  Logical volume "osd-block-1951785f-6ffd-45c7-9dc0-6ff28264161f" successfully removed\n'
10.10.2.3: b'  Removing physical volume "/dev/nvme0n1" from volume group "ceph-62246f83-fd9f-4aba-ba13-60509b697adf"\n'
10.10.2.3: b'  Volume group "ceph-62246f83-fd9f-4aba-ba13-60509b697adf" successfully removed\n'
10.10.2.3: b'\n\n\x1b[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T01:42:53,963526318-05:00] STAGE: Bootstrap a new cluster...\x1b[0m\n'
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:317 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.3 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults\n'
10.10.2.3: b'\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T01:42:53,973376554-05:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.3: b"\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T01:42:53,976913608-05:00] DEBUG: CEPHADM_PARAMS: ['bootstrap', '--config', '/tmp/bench-rados/ceph-research/scripts/ceph.conf', '--mon-ip', '10.10.2.3', '--output-dir', '/tmp/bench-rados/deployment_data_root/etc/ceph', '--ssh-user', 'ljishen', '--skip-dashboard', '--no-minimize-config', '--allow-overwrite', '--allow-fqdn-hostname', '--orphan-initial-daemons', '--skip-monitoring-stack', '--single-host-defaults']\x1b[0m\n"
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.3 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults\n'
10.10.2.3: b'Verifying podman|docker is present...\nVerifying lvm2 is present...\n'
10.10.2.3: b'Verifying time synchronization is in place...\n'
10.10.2.3: b'Unit ntp.service is enabled and running\n'
10.10.2.3: b'Repeating the final host check...\n'
10.10.2.3: b'docker (/usr/bin/docker) is present\n'
10.10.2.3: b'systemctl is present\n'
10.10.2.3: b'lvcreate is present\n'
10.10.2.3: b'Unit ntp.service is enabled and running\n'
10.10.2.3: b'Host looks OK\n'
10.10.2.3: b'Cluster fsid: 19e09dda-405f-11ec-aba1-c5f3c07a3dc7\n'
10.10.2.3: b'Verifying IP 10.10.2.3 port 3300 ...\n'
10.10.2.3: b'Verifying IP 10.10.2.3 port 6789 ...\n'
10.10.2.3: b'Mon IP `10.10.2.3` is in CIDR network `10.10.2.0/24`\n- internal network (--cluster-network) has not been provided, OSD replication will default to the public_network\n'
10.10.2.3: b'Adjusting default settings to suit single-host cluster...\n'
10.10.2.3: b'Pulling container image quay.io/ceph/ceph:v16...\n'
10.10.2.3: b'Ceph version: ceph version 16.2.6 (ee28fb57e47e9f88813e24bbf4c14496ca299d31) pacific (stable)\n'
10.10.2.3: b'Extracting ceph user uid/gid from container image...\n'
10.10.2.3: b'Creating initial keys...\n'
10.10.2.3: b'Creating initial monmap...\n'
10.10.2.3: b'Creating mon...\n'
10.10.2.3: b'Waiting for mon to start...\n'
10.10.2.3: b'Waiting for mon...\n'
10.10.2.3: b'mon is available\n'
10.10.2.3: b'Setting mon public_network to 10.10.2.0/24\n'
10.10.2.3: b'Wrote config to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf\n'
10.10.2.3: b'Wrote keyring to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring\n'
10.10.2.3: b'Creating mgr...\n'
10.10.2.3: b'Verifying port 9283 ...\n'
10.10.2.3: b'Waiting for mgr to start...\n'
10.10.2.3: b'Waiting for mgr...\n'
10.10.2.3: b'mgr not available, waiting (1/15)...\n'
10.10.2.3: b'mgr not available, waiting (2/15)...\n'
10.10.2.3: b'mgr is available\n'
10.10.2.3: b'Enabling cephadm module...\n'
10.10.2.3: b'Waiting for the mgr to restart...\n'
10.10.2.3: b'Waiting for mgr epoch 5...\n'
10.10.2.3: b'mgr epoch 5 is available\nSetting orchestrator backend to cephadm...\n'
10.10.2.3: b'Generating ssh key...\n'
10.10.2.3: b'Wrote public SSH key to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub\n'
10.10.2.3: b'Adding key to ljishen@localhost authorized_keys...\n'
10.10.2.3: b'Adding host node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us...\n'
10.10.2.3: b'Deploying unmanaged mon service...\n'
10.10.2.3: b'Deploying unmanaged mgr service...\n'
10.10.2.3: b'You can access the Ceph CLI with:\n\n\tsudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --fsid 19e09dda-405f-11ec-aba1-c5f3c07a3dc7 -c /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf -k /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring\n\n'
10.10.2.3: b'Please consider enabling telemetry to help improve Ceph:\n\n\tceph telemetry on\n\nFor more information see:\n\n\thttps://docs.ceph.com/docs/pacific/mgr/telemetry/\n\n'
10.10.2.3: b'Bootstrap complete.\n'
10.10.2.3: b'\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T01:44:02,577357487-05:00] INFO: Sleep for 20 seconds\x1b[0m\n'
10.10.2.3: b'\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T01:44:22,584853850-05:00] INFO: > Set the default number of monitor daemons to 1\x1b[0m\n# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:337 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1\n'
10.10.2.3: b'\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T01:44:22,594983002-05:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.3: b"\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T01:44:22,598725163-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mon', '1']\x1b[0m\n"
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1\n'
10.10.2.3: b'Inferring fsid 19e09dda-405f-11ec-aba1-c5f3c07a3dc7\n'
10.10.2.3: b'Inferring config /var/lib/ceph/19e09dda-405f-11ec-aba1-c5f3c07a3dc7/mon.node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.3: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.3: b'Scheduled mon update...\n'
10.10.2.3: b'\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T01:44:32,332184387-05:00] INFO: > Set the default number of manager daemons to 1\x1b[0m\n'
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:342 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1\n'
10.10.2.3: b'\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T01:44:32,342039812-05:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.3: b"\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T01:44:32,345022901-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mgr', '1']\x1b[0m\n"
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1\n'
10.10.2.3: b'Inferring fsid 19e09dda-405f-11ec-aba1-c5f3c07a3dc7\n'
10.10.2.3: b'Inferring config /var/lib/ceph/19e09dda-405f-11ec-aba1-c5f3c07a3dc7/mon.node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.3: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.3: b'Scheduled mgr update...\n'
10.10.2.3: b'\n\n\x1b[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T01:44:42,745550984-05:00] STAGE: Add hosts to the cluster...\x1b[0m\n'
10.10.2.3: b"\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T01:44:42,751272460-05:00] INFO: > Adding host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'\x1b[0m\n"
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:361 - add_hosts() > ssh-copy-id -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -f -i /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\n'
10.10.2.3: b'/usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: "/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub"\n'
10.10.2.3: b'\nNumber of key(s) added: 1\n\nNow try logging into the machine, with:   "ssh -o \'GlobalKnownHostsFile=/dev/null\' -o \'LogLevel=ERROR\' -o \'PasswordAuthentication=no\' -o \'StrictHostKeyChecking=no\' -o \'UserKnownHostsFile=/dev/null\' \'ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\'"\nand check to make sure that only the key(s) you wanted were added.\n\n'
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:367 - add_hosts() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us 10.10.2.2\n'
10.10.2.3: b'\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T01:44:42,956152661-05:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.3: b"\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T01:44:42,959531346-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'host', 'add', 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us', '10.10.2.2']\x1b[0m\n"
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us 10.10.2.2\n'
10.10.2.3: b'Inferring fsid 19e09dda-405f-11ec-aba1-c5f3c07a3dc7\n'
10.10.2.3: b'Inferring config /var/lib/ceph/19e09dda-405f-11ec-aba1-c5f3c07a3dc7/mon.node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.3: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.3: b"Added host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us' with addr '10.10.2.2'\n"
10.10.2.3: b'\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T01:44:52,685261581-05:00] INFO: Sleep for 20 seconds\x1b[0m\n'
10.10.2.3: b'\n\n\x1b[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T01:45:12,690416668-05:00] STAGE: Add OSDs to the cluster...\x1b[0m\n'
10.10.2.3: b"\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T01:45:12,697397659-05:00] INFO: > Adding devices ['/dev/nvme0n1'] on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us' as OSDs\x1b[0m\n"
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:388 - add_host_devices_as_osds() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1\n'
10.10.2.3: b'\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T01:45:12,707514869-05:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.3: b"\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T01:45:12,710818162-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'daemon', 'add', 'osd', 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1']\x1b[0m\n"
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1\n'
10.10.2.3: b'Inferring fsid 19e09dda-405f-11ec-aba1-c5f3c07a3dc7\n'
10.10.2.3: b'Inferring config /var/lib/ceph/19e09dda-405f-11ec-aba1-c5f3c07a3dc7/mon.node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.3: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.3: b"Created osd(s) 0 on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'\n"
10.10.2.3: b'\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T01:45:37,503627764-05:00] INFO: > Sleep for 20 seconds\x1b[0m\n'
10.10.2.3: b'\n\n\x1b[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T01:45:57,509276030-05:00] STAGE: Show Cluster Status\x1b[0m\n'
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:408 - show_status() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status\n'
10.10.2.3: b'\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T01:45:57,520323204-05:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.3: b"\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T01:45:57,523925421-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', '--status']\x1b[0m\n"
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status\n'
10.10.2.3: b'Inferring fsid 19e09dda-405f-11ec-aba1-c5f3c07a3dc7\n'
10.10.2.3: b'Inferring config /var/lib/ceph/19e09dda-405f-11ec-aba1-c5f3c07a3dc7/mon.node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.3: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.3: b'  cluster:\n    id:     19e09dda-405f-11ec-aba1-c5f3c07a3dc7\n    health: HEALTH_WARN\n            1 pool(s) have no replicas configured\n \n  services:\n    mon: 1 daemons, quorum node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us (age 2m)\n    mgr: node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us.qnqqsk(active, since 2m)\n    osd: 1 osds: 1 up (since 18s), 1 in (since 40s)\n \n  data:\n    pools:   1 pools, 1 pgs\n    objects: 0 objects, 0 B\n    usage:   4.7 MiB used, 100 GiB / 100 GiB avail\n    pgs:     1 active+clean\n \n'
[1] 01:46:07 [SUCCESS] ljishen@10.10.2.3

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:42:40,872247249-05:00] INFO: > Remove existing clusters[0m
## bash:17 -  > basename -- /var/lib/ceph/d7f3f620-405d-11ec-aba1-c5f3c07a3dc7
# bash:17 -  > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm --host 10.10.2.3 --host 10.10.2.2 rm-cluster --force --fsid d7f3f620-405d-11ec-aba1-c5f3c07a3dc7
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:42:40,883320633-05:00] DEBUG: HOST_PARAMS: ['--host', '10.10.2.3', '--host', '10.10.2.2'][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:42:40,886820126-05:00] DEBUG: CEPHADM_PARAMS: ['rm-cluster', '--force', '--fsid', 'd7f3f620-405d-11ec-aba1-c5f3c07a3dc7'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:63 -  > /tmp/bench-rados/ceph-research/scripts/parallel-ssh --host 10.10.2.3 --host 10.10.2.2 'sudo python3 -u - rm-cluster --force --fsid d7f3f620-405d-11ec-aba1-c5f3c07a3dc7'
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:42:40,894496449-05:00] DEBUG: command from stdin[0m
# /tmp/bench-rados/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host 10.10.2.3 --host 10.10.2.2 'sudo python3 -u - rm-cluster --force --fsid d7f3f620-405d-11ec-aba1-c5f3c07a3dc7'
[1] 01:42:46 [SUCCESS] 10.10.2.3
[2] 01:42:52 [SUCCESS] 10.10.2.2

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:42:52,980484670-05:00] INFO: > Deploy a new cluster[0m
# bash:24 -  > /tmp/bench-rados/ceph-research/scripts/ceph-deploy -m 10.10.2.3 -o node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/10.10.2.2:/dev/nvme0n1


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:42:52,991887114-05:00] STAGE: Verify passwordless SSH to hosts, passwordless sudo via ljishen, and the existence of devices on hosts...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:42:52,996425077-05:00] INFO: > Check the host for the monitor[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:181 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@10.10.2.3 sudo --non-interactive --validate

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:42:53,142011672-05:00] INFO: > Check hosts for OSDs[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:42:53,147249334-05:00] INFO: >> Check host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:200 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us sudo --non-interactive --validate

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:42:53,293969047-05:00] INFO: >>> Check device '/dev/nvme0n1'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:215 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us test -b /dev/nvme0n1
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:226 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us lsblk --nodeps --noheadings --output TYPE /dev/nvme0n1
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:227 - verify_parameters() > grep --word-regexp disk


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:42:53,574299208-05:00] STAGE: Reset devices on hosts...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:42:53,579358936-05:00] INFO: > Resetting device '/dev/nvme0n1' on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:288 - reset_host_devices() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us sudo sh
  Removing ceph--62246f83--fd9f--4aba--ba13--60509b697adf-osd--block--1951785f--6ffd--45c7--9dc0--6ff28264161f (253:0)
  Archiving volume group "ceph-62246f83-fd9f-4aba-ba13-60509b697adf" metadata (seqno 5).
  Releasing logical volume "osd-block-1951785f-6ffd-45c7-9dc0-6ff28264161f"
  Creating volume group backup "/etc/lvm/backup/ceph-62246f83-fd9f-4aba-ba13-60509b697adf" (seqno 6).
  Logical volume "osd-block-1951785f-6ffd-45c7-9dc0-6ff28264161f" successfully removed
  Removing physical volume "/dev/nvme0n1" from volume group "ceph-62246f83-fd9f-4aba-ba13-60509b697adf"
  Volume group "ceph-62246f83-fd9f-4aba-ba13-60509b697adf" successfully removed


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:42:53,963526318-05:00] STAGE: Bootstrap a new cluster...[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:317 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.3 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:42:53,973376554-05:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:42:53,976913608-05:00] DEBUG: CEPHADM_PARAMS: ['bootstrap', '--config', '/tmp/bench-rados/ceph-research/scripts/ceph.conf', '--mon-ip', '10.10.2.3', '--output-dir', '/tmp/bench-rados/deployment_data_root/etc/ceph', '--ssh-user', 'ljishen', '--skip-dashboard', '--no-minimize-config', '--allow-overwrite', '--allow-fqdn-hostname', '--orphan-initial-daemons', '--skip-monitoring-stack', '--single-host-defaults'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.3 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults
Verifying podman|docker is present...
Verifying lvm2 is present...
Verifying time synchronization is in place...
Unit ntp.service is enabled and running
Repeating the final host check...
docker (/usr/bin/docker) is present
systemctl is present
lvcreate is present
Unit ntp.service is enabled and running
Host looks OK
Cluster fsid: 19e09dda-405f-11ec-aba1-c5f3c07a3dc7
Verifying IP 10.10.2.3 port 3300 ...
Verifying IP 10.10.2.3 port 6789 ...
Mon IP `10.10.2.3` is in CIDR network `10.10.2.0/24`
- internal network (--cluster-network) has not been provided, OSD replication will default to the public_network
Adjusting default settings to suit single-host cluster...
Pulling container image quay.io/ceph/ceph:v16...
Ceph version: ceph version 16.2.6 (ee28fb57e47e9f88813e24bbf4c14496ca299d31) pacific (stable)
Extracting ceph user uid/gid from container image...
Creating initial keys...
Creating initial monmap...
Creating mon...
Waiting for mon to start...
Waiting for mon...
mon is available
Setting mon public_network to 10.10.2.0/24
Wrote config to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf
Wrote keyring to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring
Creating mgr...
Verifying port 9283 ...
Waiting for mgr to start...
Waiting for mgr...
mgr not available, waiting (1/15)...
mgr not available, waiting (2/15)...
mgr is available
Enabling cephadm module...
Waiting for the mgr to restart...
Waiting for mgr epoch 5...
mgr epoch 5 is available
Setting orchestrator backend to cephadm...
Generating ssh key...
Wrote public SSH key to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub
Adding key to ljishen@localhost authorized_keys...
Adding host node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us...
Deploying unmanaged mon service...
Deploying unmanaged mgr service...
You can access the Ceph CLI with:

	sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --fsid 19e09dda-405f-11ec-aba1-c5f3c07a3dc7 -c /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf -k /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring

Please consider enabling telemetry to help improve Ceph:

	ceph telemetry on

For more information see:

	https://docs.ceph.com/docs/pacific/mgr/telemetry/

Bootstrap complete.

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:44:02,577357487-05:00] INFO: Sleep for 20 seconds[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:44:22,584853850-05:00] INFO: > Set the default number of monitor daemons to 1[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:337 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:44:22,594983002-05:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:44:22,598725163-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mon', '1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1
Inferring fsid 19e09dda-405f-11ec-aba1-c5f3c07a3dc7
Inferring config /var/lib/ceph/19e09dda-405f-11ec-aba1-c5f3c07a3dc7/mon.node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Scheduled mon update...

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:44:32,332184387-05:00] INFO: > Set the default number of manager daemons to 1[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:342 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:44:32,342039812-05:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:44:32,345022901-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mgr', '1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1
Inferring fsid 19e09dda-405f-11ec-aba1-c5f3c07a3dc7
Inferring config /var/lib/ceph/19e09dda-405f-11ec-aba1-c5f3c07a3dc7/mon.node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Scheduled mgr update...


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:44:42,745550984-05:00] STAGE: Add hosts to the cluster...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:44:42,751272460-05:00] INFO: > Adding host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:361 - add_hosts() > ssh-copy-id -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -f -i /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us
/usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: "/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub"

Number of key(s) added: 1

Now try logging into the machine, with:   "ssh -o 'GlobalKnownHostsFile=/dev/null' -o 'LogLevel=ERROR' -o 'PasswordAuthentication=no' -o 'StrictHostKeyChecking=no' -o 'UserKnownHostsFile=/dev/null' 'ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'"
and check to make sure that only the key(s) you wanted were added.

# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:367 - add_hosts() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us 10.10.2.2
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:44:42,956152661-05:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:44:42,959531346-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'host', 'add', 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us', '10.10.2.2'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us 10.10.2.2
Inferring fsid 19e09dda-405f-11ec-aba1-c5f3c07a3dc7
Inferring config /var/lib/ceph/19e09dda-405f-11ec-aba1-c5f3c07a3dc7/mon.node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Added host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us' with addr '10.10.2.2'

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:44:52,685261581-05:00] INFO: Sleep for 20 seconds[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:45:12,690416668-05:00] STAGE: Add OSDs to the cluster...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:45:12,697397659-05:00] INFO: > Adding devices ['/dev/nvme0n1'] on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us' as OSDs[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:388 - add_host_devices_as_osds() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:45:12,707514869-05:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:45:12,710818162-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'daemon', 'add', 'osd', 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1
Inferring fsid 19e09dda-405f-11ec-aba1-c5f3c07a3dc7
Inferring config /var/lib/ceph/19e09dda-405f-11ec-aba1-c5f3c07a3dc7/mon.node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Created osd(s) 0 on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:45:37,503627764-05:00] INFO: > Sleep for 20 seconds[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:45:57,509276030-05:00] STAGE: Show Cluster Status[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:408 - show_status() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:45:57,520323204-05:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:45:57,523925421-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', '--status'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status
Inferring fsid 19e09dda-405f-11ec-aba1-c5f3c07a3dc7
Inferring config /var/lib/ceph/19e09dda-405f-11ec-aba1-c5f3c07a3dc7/mon.node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
  cluster:
    id:     19e09dda-405f-11ec-aba1-c5f3c07a3dc7
    health: HEALTH_WARN
            1 pool(s) have no replicas configured
 
  services:
    mon: 1 daemons, quorum node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us (age 2m)
    mgr: node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us.qnqqsk(active, since 2m)
    osd: 1 osds: 1 up (since 18s), 1 in (since 40s)
 
  data:
    pools:   1 pools, 1 pgs
    objects: 0 objects, 0 B
    usage:   4.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     1 active+clean
 

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:46:07,056416399-05:00] INFO: > Copy configuration files to local[0m
# ./benchmarks/bench-rados:220 - launch_ceph_cluster() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.3 sudo chown --changes --recursive ljishen /tmp/bench-rados/deployment_data_root/etc/ceph
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:46:07,063810649-05:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.3 sudo chown --changes --recursive ljishen /tmp/bench-rados/deployment_data_root/etc/ceph
[1] 01:46:07 [SUCCESS] ljishen@10.10.2.3
# ./benchmarks/bench-rados:223 - launch_ceph_cluster() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.3:/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf
# ./benchmarks/bench-rados:226 - launch_ceph_cluster() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.3:/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:46:07,536271139-05:00] STAGE: Update local ceph.conf as a client...[0m


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:46:07,539546404-05:00] STAGE: Configure Ceph pool...[0m
# ./benchmarks/bench-rados:257 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_max_backfills 32
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:46:07,560798802-05:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:46:07,563338170-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '19e09dda-405f-11ec-aba1-c5f3c07a3dc7', '--', 'ceph', 'config', 'set', 'osd', 'osd_max_backfills', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 19e09dda-405f-11ec-aba1-c5f3c07a3dc7 -- ceph config set osd osd_max_backfills 32
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:258 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_active 32
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:46:11,693552785-05:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:46:11,696219302-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '19e09dda-405f-11ec-aba1-c5f3c07a3dc7', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_active', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 19e09dda-405f-11ec-aba1-c5f3c07a3dc7 -- ceph config set osd osd_recovery_max_active 32
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:259 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_single_start 8
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:46:15,906543711-05:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:46:15,909147500-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '19e09dda-405f-11ec-aba1-c5f3c07a3dc7', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_single_start', '8'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 19e09dda-405f-11ec-aba1-c5f3c07a3dc7 -- ceph config set osd osd_recovery_max_single_start 8
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:260 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_op_priority 63
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:46:20,229931483-05:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:46:20,232618178-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '19e09dda-405f-11ec-aba1-c5f3c07a3dc7', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_op_priority', '63'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 19e09dda-405f-11ec-aba1-c5f3c07a3dc7 -- ceph config set osd osd_recovery_op_priority 63
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:262 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config show-with-defaults osd.0
# ./benchmarks/bench-rados:263 - configure_ceph_pool() > grep 'osd_max_backfills\|osd_recovery'
# ./benchmarks/bench-rados:263 - configure_ceph_pool() > column -t -s ' '
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
osd_max_backfills                        32         mon
osd_recovery_cost                        20971520   default
osd_recovery_delay_start                 0.000000   default
osd_recovery_max_active                  32         mon
osd_recovery_max_active_hdd              3          default
osd_recovery_max_active_ssd              10         default
osd_recovery_max_chunk                   8388608    default
osd_recovery_max_omap_entries_per_chunk  8096       default
osd_recovery_max_single_start            8          mon
osd_recovery_op_priority                 3          default
osd_recovery_op_warn_multiple            16         default
osd_recovery_priority                    5          default
osd_recovery_retry_interval              30.000000  default
osd_recovery_sleep                       0.000000   default
osd_recovery_sleep_hdd                   0.100000   default
osd_recovery_sleep_hybrid                0.025000   default
osd_recovery_sleep_ssd                   0.000000   default
# ./benchmarks/bench-rados:265 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool create bench_rados 128 128 replicated --size 1 --pg-num-min 128
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:46:28,825469661-05:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:46:28,828356604-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '19e09dda-405f-11ec-aba1-c5f3c07a3dc7', '--', 'ceph', 'osd', 'pool', 'create', 'bench_rados', '128', '128', 'replicated', '--size', '1', '--pg-num-min', '128'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 19e09dda-405f-11ec-aba1-c5f3c07a3dc7 -- ceph osd pool create bench_rados 128 128 replicated --size 1 --pg-num-min 128
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
pool 'bench_rados' created
# ./benchmarks/bench-rados:267 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados min_size 1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:46:34,149052268-05:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:46:34,151637893-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '19e09dda-405f-11ec-aba1-c5f3c07a3dc7', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'min_size', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 19e09dda-405f-11ec-aba1-c5f3c07a3dc7 -- ceph osd pool set bench_rados min_size 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 min_size to 1
# ./benchmarks/bench-rados:268 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados pg_autoscale_mode off
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:46:39,461000562-05:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:46:39,463651520-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '19e09dda-405f-11ec-aba1-c5f3c07a3dc7', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'pg_autoscale_mode', 'off'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 19e09dda-405f-11ec-aba1-c5f3c07a3dc7 -- ceph osd pool set bench_rados pg_autoscale_mode off
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 pg_autoscale_mode to off
# ./benchmarks/bench-rados:269 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool application enable bench_rados benchmark
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:46:44,703817993-05:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:46:44,706518985-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '19e09dda-405f-11ec-aba1-c5f3c07a3dc7', '--', 'ceph', 'osd', 'pool', 'application', 'enable', 'bench_rados', 'benchmark'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 19e09dda-405f-11ec-aba1-c5f3c07a3dc7 -- ceph osd pool application enable bench_rados benchmark
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
enabled application 'benchmark' on pool 'bench_rados'
# ./benchmarks/bench-rados:270 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados noscrub 1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:46:49,540097631-05:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:46:49,542831715-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '19e09dda-405f-11ec-aba1-c5f3c07a3dc7', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'noscrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 19e09dda-405f-11ec-aba1-c5f3c07a3dc7 -- ceph osd pool set bench_rados noscrub 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 noscrub to 1
# ./benchmarks/bench-rados:271 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados nodeep-scrub 1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:46:54,391788574-05:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:46:54,394465771-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '19e09dda-405f-11ec-aba1-c5f3c07a3dc7', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'nodeep-scrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 19e09dda-405f-11ec-aba1-c5f3c07a3dc7 -- ceph osd pool set bench_rados nodeep-scrub 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 nodeep-scrub to 1
# ./benchmarks/bench-rados:272 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool ls detail
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:46:59,710160966-05:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:46:59,713009496-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '19e09dda-405f-11ec-aba1-c5f3c07a3dc7', '--', 'ceph', 'osd', 'pool', 'ls', 'detail'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 19e09dda-405f-11ec-aba1-c5f3c07a3dc7 -- ceph osd pool ls detail
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
pool 1 'device_health_metrics' replicated size 1 min_size 1 crush_rule 0 object_hash rjenkins pg_num 64 pgp_num 64 autoscale_mode on last_change 16 lfor 0/0/14 flags hashpspool stripe_width 0 pg_num_min 1 application mgr_devicehealth
pool 2 'bench_rados' replicated size 1 min_size 1 crush_rule 0 object_hash rjenkins pg_num 128 pgp_num 128 autoscale_mode off last_change 19 flags hashpspool,noscrub,nodeep-scrub stripe_width 0 pg_num_min 128 application benchmark

# ./benchmarks/bench-rados:273 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd df tree
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:47:03,786524638-05:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:47:03,789202617-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '19e09dda-405f-11ec-aba1-c5f3c07a3dc7', '--', 'ceph', 'osd', 'df', 'tree'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 19e09dda-405f-11ec-aba1-c5f3c07a3dc7 -- ceph osd df tree
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
ID  CLASS  WEIGHT   REWEIGHT  SIZE     RAW USE  DATA     OMAP  META     AVAIL    %USE  VAR   PGS  STATUS  TYPE NAME      
-1         0.09769         -  100 GiB  5.7 MiB  160 KiB   0 B  5.6 MiB  100 GiB  0.01  1.00    -          root default   
-3         0.09769         -  100 GiB  5.7 MiB  160 KiB   0 B  5.6 MiB  100 GiB  0.01  1.00    -              host node-1
 0    ssd  0.09769   1.00000  100 GiB  5.7 MiB  160 KiB   0 B  5.6 MiB  100 GiB  0.01  1.00  192      up          osd.0  
                       TOTAL  100 GiB  5.7 MiB  160 KiB   0 B  5.6 MiB  100 GiB  0.01                                    
MIN/MAX VAR: 1.00/1.00  STDDEV: 0


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:47:08,050867344-05:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:47:32,042080038-05:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:47:41,128151391-05:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:47:41,135715972-05:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:47:50,236397002-05:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:47:50,243794258-05:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:47:59,337360711-05:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:47:59,344354396-05:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:48:08,538377678-05:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:48:08,545576931-05:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:48:17,672970278-05:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:48:17,680077387-05:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:48:17,685797332-05:00] INFO: > The cluster is idle now.[0m


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:48:17,689230234-05:00] STAGE: Start benchmarking of mode write ...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:48:17,696142485-05:00] INFO: > Start system activity collection services[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:48:17,701703289-05:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:341 - rados_bench() > rm --force /mnt/sda8/ceph-research/scripts/output/sys_activity_1MB_write.dat.rados_bench_host.2
# ./benchmarks/bench-rados:342 - rados_bench() > sar_pid_rados_bench_host=220282
# ./benchmarks/bench-rados:342 - rados_bench() > S_TIME_FORMAT=ISO
# ./benchmarks/bench-rados:342 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/output/sys_activity_1MB_write.dat.rados_bench_host.2 2

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:48:17,708324793-05:00] INFO: >> for OSD host[0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:48:17,716734228-05:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.2 bash
10.10.2.2: b'56605\n'
[1] 01:48:17 [SUCCESS] ljishen@10.10.2.2
56605

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:48:17,905276172-05:00] INFO: > Run rados bench[0m
# ./benchmarks/bench-rados:370 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/output/rados_bench_1MB_write.log.2
## ./benchmarks/bench-rados:361 - rados_bench() > count_cpus 32-63,96-127
# ./benchmarks/bench-rados:361 - rados_bench() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 60 write --pool bench_rados -b 1048576 -O 1048576 --concurrent-ios 128 --show-time --write-object --write-omap --write-xattr --no-cleanup
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:48:17,924404144-05:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:48:17,927042549-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '19e09dda-405f-11ec-aba1-c5f3c07a3dc7', '--', 'rados', 'bench', '60', 'write', '--pool', 'bench_rados', '-b', '1048576', '-O', '1048576', '--concurrent-ios', '128', '--show-time', '--write-object', '--write-omap', '--write-xattr', '--no-cleanup'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 19e09dda-405f-11ec-aba1-c5f3c07a3dc7 -- rados bench 60 write --pool bench_rados -b 1048576 -O 1048576 --concurrent-ios 128 --show-time --write-object --write-omap --write-xattr --no-cleanup
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
hints = 1
2021-11-08T06:48:21.022418+0000 Maintaining 128 concurrent writes of 1048576 bytes to objects of size 1048576 for up to 60 seconds or 0 objects
2021-11-08T06:48:21.022429+0000 Object prefix: benchmark_data_node-0.bluefield2.ucsc-cmps10_7
2021-11-08T06:48:21.055300+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-11-08T06:48:21.055300+0000     0       0         0         0         0         0           -           0
2021-11-08T06:48:22.055408+0000     1     127       325       198   197.989       198    0.403231    0.409436
2021-11-08T06:48:23.055481+0000     2     127       577       450   224.985       252    0.533242    0.415434
2021-11-08T06:48:24.055551+0000     3     127       716       589    196.32       139    0.907957    0.514368
2021-11-08T06:48:25.055617+0000     4     127      1003       876   218.986       287    0.378947    0.536683
2021-11-08T06:48:26.055686+0000     5     127      1136      1009   201.787       133    0.829254    0.537436
2021-11-08T06:48:27.055755+0000     6     127      1234      1107   184.488        98     1.27129    0.603656
2021-11-08T06:48:28.055824+0000     7     127      1339      1212   173.131       105     1.12681    0.671721
2021-11-08T06:48:29.055896+0000     8     127      1483      1356   169.488       144    0.814758    0.702608
2021-11-08T06:48:30.055962+0000     9     127      1614      1487   165.211       131    0.807455    0.718943
2021-11-08T06:48:31.056035+0000    10     127      1769      1642   164.189       155    0.864777    0.733381
2021-11-08T06:48:32.056113+0000    11     127      2079      1952   177.442       310    0.383937    0.698816
2021-11-08T06:48:33.056184+0000    12     127      2172      2045   170.405        93    0.979492    0.699389
2021-11-08T06:48:34.056255+0000    13     127      2257      2130   163.835        85     1.37234    0.725004
2021-11-08T06:48:35.056326+0000    14     127      2368      2241    160.06       111     1.17965    0.760307
2021-11-08T06:48:36.056393+0000    15     127      2510      2383   158.856       142    0.880658    0.774391
2021-11-08T06:48:37.056460+0000    16     127      2628      2501   156.302       118     1.01189    0.782987
2021-11-08T06:48:38.056530+0000    17     127      2705      2578   151.637        77     1.42495    0.797627
2021-11-08T06:48:39.056599+0000    18     127      2791      2664    147.99        86     1.41981    0.820417
2021-11-08T06:48:40.056673+0000    19     127      2975      2848   149.884       184    0.534865    0.837268
2021-11-08T06:48:41.056745+0000 min lat: 0.347869 max lat: 1.65085 avg lat: 0.814573
2021-11-08T06:48:41.056745+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-11-08T06:48:41.056745+0000    20     127      3164      3037   151.839       189    0.729226    0.814573
2021-11-08T06:48:42.056819+0000    21     127      3261      3134   149.228        97     1.25378    0.826134
2021-11-08T06:48:43.056889+0000    22     127      3349      3222   146.444        88     1.29433    0.842687
2021-11-08T06:48:44.056967+0000    23     127      3493      3366   146.338       144    0.898715    0.850035
2021-11-08T06:48:45.057041+0000    24     127      3637      3510    146.24       144    0.909906    0.850215
2021-11-08T06:48:46.057109+0000    25     127      3734      3607    144.27        97     1.27257    0.859726
2021-11-08T06:48:47.057180+0000    26     127      3814      3687   141.798        80     1.29839    0.872202
2021-11-08T06:48:48.057257+0000    27     127      3945      3818   141.397       131    0.873408    0.886157
2021-11-08T06:48:49.057328+0000    28     127      4071      3944   140.847       126    0.908306    0.886867
2021-11-08T06:48:50.057396+0000    29     127      4187      4060    139.99       116     1.15823    0.890563
2021-11-08T06:48:51.057464+0000    30     127      4271      4144   138.124        84     1.47943    0.900629
2021-11-08T06:48:52.057533+0000    31     127      4349      4222   136.184        78     1.30863    0.911613
2021-11-08T06:48:53.057601+0000    32     127      4491      4364   136.365       142    0.948171     0.91913
2021-11-08T06:48:54.057673+0000    33     127      4633      4506   136.536       142     0.85037    0.918081
2021-11-08T06:48:55.057740+0000    34     127      4732      4605   135.432        99      1.1347    0.921763
2021-11-08T06:48:56.057807+0000    35     127      4828      4701   134.305        96     1.39493     0.93118
2021-11-08T06:48:57.057875+0000    36     127      4926      4799   133.296        98     1.11164    0.940062
2021-11-08T06:48:58.057951+0000    37     127      5081      4954   133.882       155    0.843459    0.941757
2021-11-08T06:48:59.058024+0000    38     127      5194      5067   133.333       113    0.964284    0.942245
2021-11-08T06:49:00.058094+0000    39     127      5281      5154   132.145        87     1.31347    0.947519
2021-11-08T06:49:01.058162+0000 min lat: 0.347869 max lat: 1.65913 avg lat: 0.954503
2021-11-08T06:49:01.058162+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-11-08T06:49:01.058162+0000    40     127      5362      5235   130.866        81     1.23245    0.954503
2021-11-08T06:49:02.058396+0000    41     127      5522      5395   131.576       160    0.747838    0.960309
2021-11-08T06:49:03.058478+0000    42     127      5659      5532   131.704       137    0.774892    0.958387
2021-11-08T06:49:04.058550+0000    43     127      5751      5624   130.781        92     1.18576    0.961195
2021-11-08T06:49:05.058621+0000    44     127      5816      5689   129.286        65     1.48962    0.967278
2021-11-08T06:49:06.058688+0000    45     127      5935      5808   129.057       119     1.21627    0.976697
2021-11-08T06:49:07.058755+0000    46     127      6071      5944   129.208       136    0.815365    0.977321
2021-11-08T06:49:08.058825+0000    47     127      6203      6076   129.267       132    0.929559    0.976031
2021-11-08T06:49:09.058892+0000    48     127      6282      6155    128.22        79     1.25169     0.97897
2021-11-08T06:49:10.058961+0000    49     127      6369      6242   127.378        87     1.37293    0.985469
2021-11-08T06:49:11.059028+0000    50     127      6514      6387   127.731       145    0.893946    0.991104
2021-11-08T06:49:12.059095+0000    51     127      6660      6533   128.089       146    0.821353    0.988284
2021-11-08T06:49:13.059170+0000    52     127      6749      6622   127.337        89      1.0722    0.988239
2021-11-08T06:49:14.059241+0000    53     127      6850      6723    126.84       101     1.29649    0.993983
2021-11-08T06:49:15.059308+0000    54     127      6948      6821   126.306        98     1.19019    0.999786
2021-11-08T06:49:16.059393+0000    55     127      7087      6960   126.536       139    0.821982     1.00046
2021-11-08T06:49:17.059466+0000    56     127      7205      7078   126.384       118    0.937373    0.998897
2021-11-08T06:49:18.059541+0000    57     127      7298      7171   125.798        93     1.34184     1.00125
2021-11-08T06:49:19.059612+0000    58     127      7383      7256   125.094        85     1.42774     1.00653
2021-11-08T06:49:20.059682+0000    59     127      7516      7389   125.228       133    0.883865     1.01154
2021-11-08T06:49:21.059750+0000 min lat: 0.347869 max lat: 1.74871 avg lat: 1.00891
2021-11-08T06:49:21.059750+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-11-08T06:49:21.059750+0000    60     127      7669      7542   125.691       153     0.86185     1.00891
2021-11-08T06:49:22.059866+0000 Total time run:         60.2145
Total writes made:      7670
Write size:             1048576
Object size:            1048576
Bandwidth (MB/sec):     127.378
Stddev Bandwidth:       47.4907
Max bandwidth (MB/sec): 310
Min bandwidth (MB/sec): 65
Average IOPS:           127
Stddev IOPS:            47.4907
Max IOPS:               310
Min IOPS:               65
Average Latency(s):     0.999868
Stddev Latency(s):      0.34568
Max latency(s):         1.74871
Min latency(s):         0.194922

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:49:22,757841669-05:00] INFO: > Stop system activity collection services[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:49:22,763684945-05:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:391 - rados_bench() > kill -INT 220282

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:49:22,769434665-05:00] INFO: >> for OSD host[0m
# ./benchmarks/bench-rados:396 - rados_bench() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.2 kill -INT 56605
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:49:22,777340230-05:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.2 kill -INT 56605
[1] 01:49:22 [SUCCESS] ljishen@10.10.2.2

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:49:22,956427940-05:00] INFO: >>> Copy system activity file from OSD host[0m
## ./benchmarks/bench-rados:403 - rados_bench() > basename -- /tmp/bench-rados/sys_activity_1MB_write.dat.osd_host.2
# ./benchmarks/bench-rados:403 - rados_bench() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.2:/tmp/bench-rados/sys_activity_1MB_write.dat.osd_host.2 /mnt/sda8/ceph-research/scripts/output/sys_activity_1MB_write.dat.osd_host.2


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:49:23,126524038-05:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:49:47,283745869-05:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 7.67k objects, 7.5 GiB
    usage:   23 GiB used, 77 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:49:47,291326751-05:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:49:56,450180978-05:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 7.67k objects, 7.5 GiB
    usage:   23 GiB used, 77 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:49:56,457775376-05:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:50:05,761029750-05:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 7.67k objects, 7.5 GiB
    usage:   23 GiB used, 77 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:50:05,768209165-05:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:50:14,738582211-05:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 7.67k objects, 7.5 GiB
    usage:   23 GiB used, 77 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:50:14,745696504-05:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:50:23,735085816-05:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 7.67k objects, 7.5 GiB
    usage:   23 GiB used, 77 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:50:23,742516225-05:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:50:23,748251057-05:00] INFO: > The cluster is idle now.[0m


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:50:23,751895909-05:00] STAGE: Start benchmarking of mode seq ...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:50:23,758552178-05:00] INFO: > Start system activity collection services[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:50:23,763768773-05:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:341 - rados_bench() > rm --force /mnt/sda8/ceph-research/scripts/output/sys_activity_1MB_seq.dat.rados_bench_host.2
# ./benchmarks/bench-rados:342 - rados_bench() > sar_pid_rados_bench_host=221631
# ./benchmarks/bench-rados:342 - rados_bench() > S_TIME_FORMAT=ISO
# ./benchmarks/bench-rados:342 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/output/sys_activity_1MB_seq.dat.rados_bench_host.2 2

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:50:23,770330635-05:00] INFO: >> for OSD host[0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:50:23,778821333-05:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.2 bash
10.10.2.2: b'56709\n'
[1] 01:50:23 [SUCCESS] ljishen@10.10.2.2
56709

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:50:23,965204378-05:00] INFO: > Run rados bench[0m
# ./benchmarks/bench-rados:379 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/output/rados_bench_1MB_seq.log.2
## ./benchmarks/bench-rados:376 - rados_bench() > count_cpus 32-63,96-127
# ./benchmarks/bench-rados:376 - rados_bench() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 128 --show-time
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:50:23,983756485-05:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:50:23,986363670-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '19e09dda-405f-11ec-aba1-c5f3c07a3dc7', '--', 'rados', 'bench', '99999999', 'seq', '--pool', 'bench_rados', '--concurrent-ios', '128', '--show-time'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 19e09dda-405f-11ec-aba1-c5f3c07a3dc7 -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 128 --show-time
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
hints = 1
2021-11-08T06:50:27.243690+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-11-08T06:50:27.243690+0000     0       0         0         0         0         0           -           0
2021-11-08T06:50:28.243812+0000     1     127      1465      1338   1337.74      1338   0.0930681   0.0908103
2021-11-08T06:50:29.243897+0000     2     127      2801      2674   1336.82      1336   0.0940389   0.0932182
2021-11-08T06:50:30.243980+0000     3     127      4149      4022   1340.51      1348   0.0889816    0.093713
2021-11-08T06:50:31.244121+0000     4     127      5504      5377   1344.08      1355    0.093832   0.0939644
2021-11-08T06:50:32.244190+0000     5     127      6850      6723   1344.45      1346   0.0942266   0.0941554
2021-11-08T06:50:33.244288+0000 Total time run:       5.68266
Total reads made:     7670
Read size:            1048576
Object size:          1048576
Bandwidth (MB/sec):   1349.72
Average IOPS:         1349
Stddev IOPS:          7.73305
Max IOPS:             1355
Min IOPS:             1336
Average Latency(s):   0.0939798
Max latency(s):       0.153845
Min latency(s):       0.0244704

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:50:33,968935452-05:00] INFO: > Stop system activity collection services[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:50:33,974582248-05:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:391 - rados_bench() > kill -INT 221631

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:50:33,980033155-05:00] INFO: >> for OSD host[0m
# ./benchmarks/bench-rados:396 - rados_bench() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.2 kill -INT 56709
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:50:33,987648592-05:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.2 kill -INT 56709
[1] 01:50:34 [SUCCESS] ljishen@10.10.2.2

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:50:34,167976532-05:00] INFO: >>> Copy system activity file from OSD host[0m
## ./benchmarks/bench-rados:403 - rados_bench() > basename -- /tmp/bench-rados/sys_activity_1MB_seq.dat.osd_host.2
# ./benchmarks/bench-rados:403 - rados_bench() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.2:/tmp/bench-rados/sys_activity_1MB_seq.dat.osd_host.2 /mnt/sda8/ceph-research/scripts/output/sys_activity_1MB_seq.dat.osd_host.2


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:50:34,317196734-05:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:50:58,429722143-05:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 7.67k objects, 7.5 GiB
    usage:   23 GiB used, 77 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:50:58,437427921-05:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:51:07,590886733-05:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 7.67k objects, 7.5 GiB
    usage:   23 GiB used, 77 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:51:07,598671851-05:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:51:16,901803963-05:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 7.67k objects, 7.5 GiB
    usage:   23 GiB used, 77 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:51:16,909561869-05:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:51:26,024774143-05:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 7.67k objects, 7.5 GiB
    usage:   23 GiB used, 77 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:51:26,031988765-05:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:51:35,119522971-05:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 7.67k objects, 7.5 GiB
    usage:   23 GiB used, 77 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:51:35,127207428-05:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:51:35,133054322-05:00] INFO: > The cluster is idle now.[0m


[1;7;39;49m[2021-11-08T01:51:35,135286460-05:00][RUNNING][ROUND 3/5/21] object_size=1MB[0m


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:51:35,138732848-05:00] STAGE: Launch a Ceph cluster (CEPH_HOST: 10.10.2.3, OSD_HOST: 10.10.2.2) ...[0m
# ./benchmarks/bench-rados:183 - launch_ceph_cluster() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.3 bash -euo pipefail
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:51:35,147657354-05:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.3 bash -euo pipefail
10.10.2.3: b'\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T01:51:35,594036333-05:00] INFO: > Remove existing clusters\x1b[0m\n'
10.10.2.3: b'## bash:17 -  > basename -- /var/lib/ceph/19e09dda-405f-11ec-aba1-c5f3c07a3dc7\n'
10.10.2.3: b'# bash:17 -  > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm --host 10.10.2.3 --host 10.10.2.2 rm-cluster --force --fsid 19e09dda-405f-11ec-aba1-c5f3c07a3dc7\n'
10.10.2.3: b"\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T01:51:35,604339122-05:00] DEBUG: HOST_PARAMS: ['--host', '10.10.2.3', '--host', '10.10.2.2']\x1b[0m\n"
10.10.2.3: b"\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T01:51:35,608128252-05:00] DEBUG: CEPHADM_PARAMS: ['rm-cluster', '--force', '--fsid', '19e09dda-405f-11ec-aba1-c5f3c07a3dc7']\x1b[0m\n"
10.10.2.3: b"# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:63 -  > /tmp/bench-rados/ceph-research/scripts/parallel-ssh --host 10.10.2.3 --host 10.10.2.2 'sudo python3 -u - rm-cluster --force --fsid 19e09dda-405f-11ec-aba1-c5f3c07a3dc7'\n"
10.10.2.3: b'\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T01:51:35,616385211-05:00] DEBUG: command from stdin\x1b[0m\n'
10.10.2.3: b"# /tmp/bench-rados/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host 10.10.2.3 --host 10.10.2.2 'sudo python3 -u - rm-cluster --force --fsid 19e09dda-405f-11ec-aba1-c5f3c07a3dc7'\n"
10.10.2.3: b'[1] 01:51:41 [SUCCESS] 10.10.2.3\n[2] 01:51:47 [SUCCESS] 10.10.2.2\n'
10.10.2.3: b'\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T01:51:47,224509913-05:00] INFO: > Deploy a new cluster\x1b[0m\n'
10.10.2.3: b'# bash:24 -  > /tmp/bench-rados/ceph-research/scripts/ceph-deploy -m 10.10.2.3 -o node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/10.10.2.2:/dev/nvme0n1\n'
10.10.2.3: b'\n\n\x1b[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T01:51:47,235336031-05:00] STAGE: Verify passwordless SSH to hosts, passwordless sudo via ljishen, and the existence of devices on hosts...\x1b[0m\n'
10.10.2.3: b'\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T01:51:47,240451242-05:00] INFO: > Check the host for the monitor\x1b[0m\n'
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:181 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@10.10.2.3 sudo --non-interactive --validate\n'
10.10.2.3: b'\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T01:51:47,390362388-05:00] INFO: > Check hosts for OSDs\x1b[0m\n'
10.10.2.3: b"\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T01:51:47,395085820-05:00] INFO: >> Check host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'\x1b[0m\n"
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:200 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us sudo --non-interactive --validate\n'
10.10.2.3: b"\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T01:51:47,542289588-05:00] INFO: >>> Check device '/dev/nvme0n1'\x1b[0m\n"
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:215 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us test -b /dev/nvme0n1\n'
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:226 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us lsblk --nodeps --noheadings --output TYPE /dev/nvme0n1\n'
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:227 - verify_parameters() > grep --word-regexp disk\n'
10.10.2.3: b'\n\n\x1b[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T01:51:47,822339694-05:00] STAGE: Reset devices on hosts...\x1b[0m\n'
10.10.2.3: b"\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T01:51:47,827450266-05:00] INFO: > Resetting device '/dev/nvme0n1' on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'\x1b[0m\n"
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:288 - reset_host_devices() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us sudo sh\n'
10.10.2.3: b'  Removing ceph--7df9de08--e164--431d--a66b--ebdb3143c9f3-osd--block--35dc3c90--4062--407a--b5e1--b3b52734c5d1 (253:0)\n'
10.10.2.3: b'  Archiving volume group "ceph-7df9de08-e164-431d-a66b-ebdb3143c9f3" metadata (seqno 5).\n  Releasing logical volume "osd-block-35dc3c90-4062-407a-b5e1-b3b52734c5d1"\n'
10.10.2.3: b'  Creating volume group backup "/etc/lvm/backup/ceph-7df9de08-e164-431d-a66b-ebdb3143c9f3" (seqno 6).\n'
10.10.2.3: b'  Logical volume "osd-block-35dc3c90-4062-407a-b5e1-b3b52734c5d1" successfully removed\n'
10.10.2.3: b'  Removing physical volume "/dev/nvme0n1" from volume group "ceph-7df9de08-e164-431d-a66b-ebdb3143c9f3"\n'
10.10.2.3: b'  Volume group "ceph-7df9de08-e164-431d-a66b-ebdb3143c9f3" successfully removed\n'
10.10.2.3: b'\n\n\x1b[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T01:51:48,219616075-05:00] STAGE: Bootstrap a new cluster...\x1b[0m\n'
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:317 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.3 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults\n'
10.10.2.3: b'\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T01:51:48,229618618-05:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.3: b"\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T01:51:48,233113632-05:00] DEBUG: CEPHADM_PARAMS: ['bootstrap', '--config', '/tmp/bench-rados/ceph-research/scripts/ceph.conf', '--mon-ip', '10.10.2.3', '--output-dir', '/tmp/bench-rados/deployment_data_root/etc/ceph', '--ssh-user', 'ljishen', '--skip-dashboard', '--no-minimize-config', '--allow-overwrite', '--allow-fqdn-hostname', '--orphan-initial-daemons', '--skip-monitoring-stack', '--single-host-defaults']\x1b[0m\n"
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.3 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults\n'
10.10.2.3: b'Verifying podman|docker is present...\nVerifying lvm2 is present...\n'
10.10.2.3: b'Verifying time synchronization is in place...\n'
10.10.2.3: b'Unit ntp.service is enabled and running\n'
10.10.2.3: b'Repeating the final host check...\ndocker (/usr/bin/docker) is present\n'
10.10.2.3: b'systemctl is present\n'
10.10.2.3: b'lvcreate is present\n'
10.10.2.3: b'Unit ntp.service is enabled and running\n'
10.10.2.3: b'Host looks OK\n'
10.10.2.3: b'Cluster fsid: 5851b134-4060-11ec-aba1-c5f3c07a3dc7\n'
10.10.2.3: b'Verifying IP 10.10.2.3 port 3300 ...\n'
10.10.2.3: b'Verifying IP 10.10.2.3 port 6789 ...\n'
10.10.2.3: b'Mon IP `10.10.2.3` is in CIDR network `10.10.2.0/24`\n- internal network (--cluster-network) has not been provided, OSD replication will default to the public_network\n'
10.10.2.3: b'Adjusting default settings to suit single-host cluster...\n'
10.10.2.3: b'Pulling container image quay.io/ceph/ceph:v16...\n'
10.10.2.3: b'Ceph version: ceph version 16.2.6 (ee28fb57e47e9f88813e24bbf4c14496ca299d31) pacific (stable)\n'
10.10.2.3: b'Extracting ceph user uid/gid from container image...\n'
10.10.2.3: b'Creating initial keys...\n'
10.10.2.3: b'Creating initial monmap...\n'
10.10.2.3: b'Creating mon...\n'
10.10.2.3: b'Waiting for mon to start...\n'
10.10.2.3: b'Waiting for mon...\n'
10.10.2.3: b'mon is available\n'
10.10.2.3: b'Setting mon public_network to 10.10.2.0/24\n'
10.10.2.3: b'Wrote config to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf\n'
10.10.2.3: b'Wrote keyring to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring\n'
10.10.2.3: b'Creating mgr...\n'
10.10.2.3: b'Verifying port 9283 ...\n'
10.10.2.3: b'Waiting for mgr to start...\n'
10.10.2.3: b'Waiting for mgr...\n'
10.10.2.3: b'mgr not available, waiting (1/15)...\n'
10.10.2.3: b'mgr not available, waiting (2/15)...\n'
10.10.2.3: b'mgr is available\n'
10.10.2.3: b'Enabling cephadm module...\n'
10.10.2.3: b'Waiting for the mgr to restart...\n'
10.10.2.3: b'Waiting for mgr epoch 5...\n'
10.10.2.3: b'mgr epoch 5 is available\n'
10.10.2.3: b'Setting orchestrator backend to cephadm...\n'
10.10.2.3: b'Generating ssh key...\n'
10.10.2.3: b'Wrote public SSH key to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub\n'
10.10.2.3: b'Adding key to ljishen@localhost authorized_keys...\n'
10.10.2.3: b'Adding host node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us...\n'
10.10.2.3: b'Deploying unmanaged mon service...\n'
10.10.2.3: b'Deploying unmanaged mgr service...\n'
10.10.2.3: b'You can access the Ceph CLI with:\n\n\tsudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --fsid 5851b134-4060-11ec-aba1-c5f3c07a3dc7 -c /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf -k /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring\n\n'
10.10.2.3: b'Please consider enabling telemetry to help improve Ceph:\n\n\tceph telemetry on\n\nFor more information see:\n\n\thttps://docs.ceph.com/docs/pacific/mgr/telemetry/\n\nBootstrap complete.\n'
10.10.2.3: b'\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T01:52:57,192955437-05:00] INFO: Sleep for 20 seconds\x1b[0m\n'
10.10.2.3: b'\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T01:53:17,200246992-05:00] INFO: > Set the default number of monitor daemons to 1\x1b[0m\n'
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:337 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1\n'
10.10.2.3: b'\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T01:53:17,210490920-05:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.3: b"\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T01:53:17,214341786-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mon', '1']\x1b[0m\n"
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1\n'
10.10.2.3: b'Inferring fsid 5851b134-4060-11ec-aba1-c5f3c07a3dc7\n'
10.10.2.3: b'Inferring config /var/lib/ceph/5851b134-4060-11ec-aba1-c5f3c07a3dc7/mon.node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.3: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.3: b'Scheduled mon update...\n'
10.10.2.3: b'\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T01:53:27,719433883-05:00] INFO: > Set the default number of manager daemons to 1\x1b[0m\n'
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:342 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1\n'
10.10.2.3: b'\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T01:53:27,728533081-05:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.3: b"\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T01:53:27,732129467-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mgr', '1']\x1b[0m\n"
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1\n'
10.10.2.3: b'Inferring fsid 5851b134-4060-11ec-aba1-c5f3c07a3dc7\n'
10.10.2.3: b'Inferring config /var/lib/ceph/5851b134-4060-11ec-aba1-c5f3c07a3dc7/mon.node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.3: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.3: b'Scheduled mgr update...\n'
10.10.2.3: b'\n\n\x1b[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T01:53:38,165262310-05:00] STAGE: Add hosts to the cluster...\x1b[0m\n'
10.10.2.3: b"\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T01:53:38,171164517-05:00] INFO: > Adding host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'\x1b[0m\n"
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:361 - add_hosts() > ssh-copy-id -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -f -i /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\n'
10.10.2.3: b'/usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: "/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub"\n'
10.10.2.3: b'\nNumber of key(s) added: 1\n\nNow try logging into the machine, with:   "ssh -o \'GlobalKnownHostsFile=/dev/null\' -o \'LogLevel=ERROR\' -o \'PasswordAuthentication=no\' -o \'StrictHostKeyChecking=no\' -o \'UserKnownHostsFile=/dev/null\' \'ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\'"\nand check to make sure that only the key(s) you wanted were added.\n\n'
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:367 - add_hosts() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us 10.10.2.2\n'
10.10.2.3: b'\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T01:53:38,375913212-05:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.3: b"\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T01:53:38,379445226-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'host', 'add', 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us', '10.10.2.2']\x1b[0m\n"
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us 10.10.2.2\n'
10.10.2.3: b'Inferring fsid 5851b134-4060-11ec-aba1-c5f3c07a3dc7\n'
10.10.2.3: b'Inferring config /var/lib/ceph/5851b134-4060-11ec-aba1-c5f3c07a3dc7/mon.node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.3: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.3: b"Added host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us' with addr '10.10.2.2'\n"
10.10.2.3: b'\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T01:53:48,324001461-05:00] INFO: Sleep for 20 seconds\x1b[0m\n'
10.10.2.3: b'\n\n\x1b[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T01:54:08,328699454-05:00] STAGE: Add OSDs to the cluster...\x1b[0m\n'
10.10.2.3: b"\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T01:54:08,335583814-05:00] INFO: > Adding devices ['/dev/nvme0n1'] on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us' as OSDs\x1b[0m\n"
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:388 - add_host_devices_as_osds() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1\n'
10.10.2.3: b'\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T01:54:08,345700583-05:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.3: b"\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T01:54:08,349240201-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'daemon', 'add', 'osd', 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1']\x1b[0m\n"
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1\n'
10.10.2.3: b'Inferring fsid 5851b134-4060-11ec-aba1-c5f3c07a3dc7\n'
10.10.2.3: b'Inferring config /var/lib/ceph/5851b134-4060-11ec-aba1-c5f3c07a3dc7/mon.node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.3: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.3: b"Created osd(s) 0 on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'\n"
10.10.2.3: b'\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T01:54:34,103929731-05:00] INFO: > Sleep for 20 seconds\x1b[0m\n'
10.10.2.3: b'\n\n\x1b[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T01:54:54,110020383-05:00] STAGE: Show Cluster Status\x1b[0m\n'
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:408 - show_status() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status\n'
10.10.2.3: b'\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T01:54:54,120088409-05:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.3: b"\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T01:54:54,123597931-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', '--status']\x1b[0m\n"
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status\n'
10.10.2.3: b'Inferring fsid 5851b134-4060-11ec-aba1-c5f3c07a3dc7\n'
10.10.2.3: b'Inferring config /var/lib/ceph/5851b134-4060-11ec-aba1-c5f3c07a3dc7/mon.node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.3: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.3: b'  cluster:\n    id:     5851b134-4060-11ec-aba1-c5f3c07a3dc7\n    health: HEALTH_WARN\n            1 pool(s) have no replicas configured\n \n  services:\n    mon: 1 daemons, quorum node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us (age 2m)\n    mgr: node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us.mpsrua(active, since 2m)\n    osd: 1 osds: 1 up (since 19s), 1 in (since 42s)\n \n  data:\n    pools:   1 pools, 1 pgs\n    objects: 0 objects, 0 B\n    usage:   4.7 MiB used, 100 GiB / 100 GiB avail\n    pgs:     1 active+clean\n \n'
[1] 01:55:03 [SUCCESS] ljishen@10.10.2.3

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:51:35,594036333-05:00] INFO: > Remove existing clusters[0m
## bash:17 -  > basename -- /var/lib/ceph/19e09dda-405f-11ec-aba1-c5f3c07a3dc7
# bash:17 -  > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm --host 10.10.2.3 --host 10.10.2.2 rm-cluster --force --fsid 19e09dda-405f-11ec-aba1-c5f3c07a3dc7
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:51:35,604339122-05:00] DEBUG: HOST_PARAMS: ['--host', '10.10.2.3', '--host', '10.10.2.2'][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:51:35,608128252-05:00] DEBUG: CEPHADM_PARAMS: ['rm-cluster', '--force', '--fsid', '19e09dda-405f-11ec-aba1-c5f3c07a3dc7'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:63 -  > /tmp/bench-rados/ceph-research/scripts/parallel-ssh --host 10.10.2.3 --host 10.10.2.2 'sudo python3 -u - rm-cluster --force --fsid 19e09dda-405f-11ec-aba1-c5f3c07a3dc7'
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:51:35,616385211-05:00] DEBUG: command from stdin[0m
# /tmp/bench-rados/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host 10.10.2.3 --host 10.10.2.2 'sudo python3 -u - rm-cluster --force --fsid 19e09dda-405f-11ec-aba1-c5f3c07a3dc7'
[1] 01:51:41 [SUCCESS] 10.10.2.3
[2] 01:51:47 [SUCCESS] 10.10.2.2

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:51:47,224509913-05:00] INFO: > Deploy a new cluster[0m
# bash:24 -  > /tmp/bench-rados/ceph-research/scripts/ceph-deploy -m 10.10.2.3 -o node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/10.10.2.2:/dev/nvme0n1


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:51:47,235336031-05:00] STAGE: Verify passwordless SSH to hosts, passwordless sudo via ljishen, and the existence of devices on hosts...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:51:47,240451242-05:00] INFO: > Check the host for the monitor[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:181 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@10.10.2.3 sudo --non-interactive --validate

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:51:47,390362388-05:00] INFO: > Check hosts for OSDs[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:51:47,395085820-05:00] INFO: >> Check host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:200 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us sudo --non-interactive --validate

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:51:47,542289588-05:00] INFO: >>> Check device '/dev/nvme0n1'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:215 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us test -b /dev/nvme0n1
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:226 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us lsblk --nodeps --noheadings --output TYPE /dev/nvme0n1
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:227 - verify_parameters() > grep --word-regexp disk


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:51:47,822339694-05:00] STAGE: Reset devices on hosts...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:51:47,827450266-05:00] INFO: > Resetting device '/dev/nvme0n1' on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:288 - reset_host_devices() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us sudo sh
  Removing ceph--7df9de08--e164--431d--a66b--ebdb3143c9f3-osd--block--35dc3c90--4062--407a--b5e1--b3b52734c5d1 (253:0)
  Archiving volume group "ceph-7df9de08-e164-431d-a66b-ebdb3143c9f3" metadata (seqno 5).
  Releasing logical volume "osd-block-35dc3c90-4062-407a-b5e1-b3b52734c5d1"
  Creating volume group backup "/etc/lvm/backup/ceph-7df9de08-e164-431d-a66b-ebdb3143c9f3" (seqno 6).
  Logical volume "osd-block-35dc3c90-4062-407a-b5e1-b3b52734c5d1" successfully removed
  Removing physical volume "/dev/nvme0n1" from volume group "ceph-7df9de08-e164-431d-a66b-ebdb3143c9f3"
  Volume group "ceph-7df9de08-e164-431d-a66b-ebdb3143c9f3" successfully removed


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:51:48,219616075-05:00] STAGE: Bootstrap a new cluster...[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:317 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.3 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:51:48,229618618-05:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:51:48,233113632-05:00] DEBUG: CEPHADM_PARAMS: ['bootstrap', '--config', '/tmp/bench-rados/ceph-research/scripts/ceph.conf', '--mon-ip', '10.10.2.3', '--output-dir', '/tmp/bench-rados/deployment_data_root/etc/ceph', '--ssh-user', 'ljishen', '--skip-dashboard', '--no-minimize-config', '--allow-overwrite', '--allow-fqdn-hostname', '--orphan-initial-daemons', '--skip-monitoring-stack', '--single-host-defaults'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.3 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults
Verifying podman|docker is present...
Verifying lvm2 is present...
Verifying time synchronization is in place...
Unit ntp.service is enabled and running
Repeating the final host check...
docker (/usr/bin/docker) is present
systemctl is present
lvcreate is present
Unit ntp.service is enabled and running
Host looks OK
Cluster fsid: 5851b134-4060-11ec-aba1-c5f3c07a3dc7
Verifying IP 10.10.2.3 port 3300 ...
Verifying IP 10.10.2.3 port 6789 ...
Mon IP `10.10.2.3` is in CIDR network `10.10.2.0/24`
- internal network (--cluster-network) has not been provided, OSD replication will default to the public_network
Adjusting default settings to suit single-host cluster...
Pulling container image quay.io/ceph/ceph:v16...
Ceph version: ceph version 16.2.6 (ee28fb57e47e9f88813e24bbf4c14496ca299d31) pacific (stable)
Extracting ceph user uid/gid from container image...
Creating initial keys...
Creating initial monmap...
Creating mon...
Waiting for mon to start...
Waiting for mon...
mon is available
Setting mon public_network to 10.10.2.0/24
Wrote config to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf
Wrote keyring to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring
Creating mgr...
Verifying port 9283 ...
Waiting for mgr to start...
Waiting for mgr...
mgr not available, waiting (1/15)...
mgr not available, waiting (2/15)...
mgr is available
Enabling cephadm module...
Waiting for the mgr to restart...
Waiting for mgr epoch 5...
mgr epoch 5 is available
Setting orchestrator backend to cephadm...
Generating ssh key...
Wrote public SSH key to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub
Adding key to ljishen@localhost authorized_keys...
Adding host node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us...
Deploying unmanaged mon service...
Deploying unmanaged mgr service...
You can access the Ceph CLI with:

	sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --fsid 5851b134-4060-11ec-aba1-c5f3c07a3dc7 -c /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf -k /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring

Please consider enabling telemetry to help improve Ceph:

	ceph telemetry on

For more information see:

	https://docs.ceph.com/docs/pacific/mgr/telemetry/

Bootstrap complete.

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:52:57,192955437-05:00] INFO: Sleep for 20 seconds[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:53:17,200246992-05:00] INFO: > Set the default number of monitor daemons to 1[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:337 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:53:17,210490920-05:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:53:17,214341786-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mon', '1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1
Inferring fsid 5851b134-4060-11ec-aba1-c5f3c07a3dc7
Inferring config /var/lib/ceph/5851b134-4060-11ec-aba1-c5f3c07a3dc7/mon.node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Scheduled mon update...

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:53:27,719433883-05:00] INFO: > Set the default number of manager daemons to 1[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:342 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:53:27,728533081-05:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:53:27,732129467-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mgr', '1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1
Inferring fsid 5851b134-4060-11ec-aba1-c5f3c07a3dc7
Inferring config /var/lib/ceph/5851b134-4060-11ec-aba1-c5f3c07a3dc7/mon.node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Scheduled mgr update...


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:53:38,165262310-05:00] STAGE: Add hosts to the cluster...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:53:38,171164517-05:00] INFO: > Adding host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:361 - add_hosts() > ssh-copy-id -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -f -i /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us
/usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: "/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub"

Number of key(s) added: 1

Now try logging into the machine, with:   "ssh -o 'GlobalKnownHostsFile=/dev/null' -o 'LogLevel=ERROR' -o 'PasswordAuthentication=no' -o 'StrictHostKeyChecking=no' -o 'UserKnownHostsFile=/dev/null' 'ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'"
and check to make sure that only the key(s) you wanted were added.

# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:367 - add_hosts() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us 10.10.2.2
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:53:38,375913212-05:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:53:38,379445226-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'host', 'add', 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us', '10.10.2.2'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us 10.10.2.2
Inferring fsid 5851b134-4060-11ec-aba1-c5f3c07a3dc7
Inferring config /var/lib/ceph/5851b134-4060-11ec-aba1-c5f3c07a3dc7/mon.node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Added host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us' with addr '10.10.2.2'

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:53:48,324001461-05:00] INFO: Sleep for 20 seconds[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:54:08,328699454-05:00] STAGE: Add OSDs to the cluster...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:54:08,335583814-05:00] INFO: > Adding devices ['/dev/nvme0n1'] on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us' as OSDs[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:388 - add_host_devices_as_osds() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:54:08,345700583-05:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:54:08,349240201-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'daemon', 'add', 'osd', 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1
Inferring fsid 5851b134-4060-11ec-aba1-c5f3c07a3dc7
Inferring config /var/lib/ceph/5851b134-4060-11ec-aba1-c5f3c07a3dc7/mon.node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Created osd(s) 0 on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:54:34,103929731-05:00] INFO: > Sleep for 20 seconds[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:54:54,110020383-05:00] STAGE: Show Cluster Status[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:408 - show_status() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:54:54,120088409-05:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:54:54,123597931-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', '--status'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status
Inferring fsid 5851b134-4060-11ec-aba1-c5f3c07a3dc7
Inferring config /var/lib/ceph/5851b134-4060-11ec-aba1-c5f3c07a3dc7/mon.node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
  cluster:
    id:     5851b134-4060-11ec-aba1-c5f3c07a3dc7
    health: HEALTH_WARN
            1 pool(s) have no replicas configured
 
  services:
    mon: 1 daemons, quorum node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us (age 2m)
    mgr: node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us.mpsrua(active, since 2m)
    osd: 1 osds: 1 up (since 19s), 1 in (since 42s)
 
  data:
    pools:   1 pools, 1 pgs
    objects: 0 objects, 0 B
    usage:   4.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     1 active+clean
 

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:55:03,902716471-05:00] INFO: > Copy configuration files to local[0m
# ./benchmarks/bench-rados:220 - launch_ceph_cluster() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.3 sudo chown --changes --recursive ljishen /tmp/bench-rados/deployment_data_root/etc/ceph
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:55:03,910215439-05:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.3 sudo chown --changes --recursive ljishen /tmp/bench-rados/deployment_data_root/etc/ceph
[1] 01:55:04 [SUCCESS] ljishen@10.10.2.3
# ./benchmarks/bench-rados:223 - launch_ceph_cluster() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.3:/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf
# ./benchmarks/bench-rados:226 - launch_ceph_cluster() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.3:/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:55:04,380672817-05:00] STAGE: Update local ceph.conf as a client...[0m


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:55:04,384066375-05:00] STAGE: Configure Ceph pool...[0m
# ./benchmarks/bench-rados:257 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_max_backfills 32
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:55:04,405060527-05:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:55:04,407698650-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '5851b134-4060-11ec-aba1-c5f3c07a3dc7', '--', 'ceph', 'config', 'set', 'osd', 'osd_max_backfills', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 5851b134-4060-11ec-aba1-c5f3c07a3dc7 -- ceph config set osd osd_max_backfills 32
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:258 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_active 32
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:55:08,601995885-05:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:55:08,604689533-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '5851b134-4060-11ec-aba1-c5f3c07a3dc7', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_active', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 5851b134-4060-11ec-aba1-c5f3c07a3dc7 -- ceph config set osd osd_recovery_max_active 32
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:259 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_single_start 8
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:55:12,780112964-05:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:55:12,782979889-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '5851b134-4060-11ec-aba1-c5f3c07a3dc7', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_single_start', '8'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 5851b134-4060-11ec-aba1-c5f3c07a3dc7 -- ceph config set osd osd_recovery_max_single_start 8
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:260 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_op_priority 63
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:55:16,968151859-05:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:55:16,970813327-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '5851b134-4060-11ec-aba1-c5f3c07a3dc7', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_op_priority', '63'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 5851b134-4060-11ec-aba1-c5f3c07a3dc7 -- ceph config set osd osd_recovery_op_priority 63
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:262 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config show-with-defaults osd.0
# ./benchmarks/bench-rados:263 - configure_ceph_pool() > grep 'osd_max_backfills\|osd_recovery'
# ./benchmarks/bench-rados:263 - configure_ceph_pool() > column -t -s ' '
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
osd_max_backfills                        32         mon
osd_recovery_cost                        20971520   default
osd_recovery_delay_start                 0.000000   default
osd_recovery_max_active                  32         mon
osd_recovery_max_active_hdd              3          default
osd_recovery_max_active_ssd              10         default
osd_recovery_max_chunk                   8388608    default
osd_recovery_max_omap_entries_per_chunk  8096       default
osd_recovery_max_single_start            8          mon
osd_recovery_op_priority                 63         mon
osd_recovery_op_warn_multiple            16         default
osd_recovery_priority                    5          default
osd_recovery_retry_interval              30.000000  default
osd_recovery_sleep                       0.000000   default
osd_recovery_sleep_hdd                   0.100000   default
osd_recovery_sleep_hybrid                0.025000   default
osd_recovery_sleep_ssd                   0.000000   default
# ./benchmarks/bench-rados:265 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool create bench_rados 128 128 replicated --size 1 --pg-num-min 128
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:55:25,135109770-05:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:55:25,137812626-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '5851b134-4060-11ec-aba1-c5f3c07a3dc7', '--', 'ceph', 'osd', 'pool', 'create', 'bench_rados', '128', '128', 'replicated', '--size', '1', '--pg-num-min', '128'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 5851b134-4060-11ec-aba1-c5f3c07a3dc7 -- ceph osd pool create bench_rados 128 128 replicated --size 1 --pg-num-min 128
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
pool 'bench_rados' created
# ./benchmarks/bench-rados:267 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados min_size 1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:55:29,824264060-05:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:55:29,826848443-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '5851b134-4060-11ec-aba1-c5f3c07a3dc7', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'min_size', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 5851b134-4060-11ec-aba1-c5f3c07a3dc7 -- ceph osd pool set bench_rados min_size 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 min_size to 1
# ./benchmarks/bench-rados:268 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados pg_autoscale_mode off
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:55:34,428760942-05:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:55:34,431684374-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '5851b134-4060-11ec-aba1-c5f3c07a3dc7', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'pg_autoscale_mode', 'off'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 5851b134-4060-11ec-aba1-c5f3c07a3dc7 -- ceph osd pool set bench_rados pg_autoscale_mode off
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 pg_autoscale_mode to off
# ./benchmarks/bench-rados:269 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool application enable bench_rados benchmark
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:55:39,498117482-05:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:55:39,501053727-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '5851b134-4060-11ec-aba1-c5f3c07a3dc7', '--', 'ceph', 'osd', 'pool', 'application', 'enable', 'bench_rados', 'benchmark'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 5851b134-4060-11ec-aba1-c5f3c07a3dc7 -- ceph osd pool application enable bench_rados benchmark
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
enabled application 'benchmark' on pool 'bench_rados'
# ./benchmarks/bench-rados:270 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados noscrub 1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:55:44,552688496-05:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:55:44,555549570-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '5851b134-4060-11ec-aba1-c5f3c07a3dc7', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'noscrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 5851b134-4060-11ec-aba1-c5f3c07a3dc7 -- ceph osd pool set bench_rados noscrub 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 noscrub to 1
# ./benchmarks/bench-rados:271 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados nodeep-scrub 1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:55:49,465224114-05:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:55:49,468291296-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '5851b134-4060-11ec-aba1-c5f3c07a3dc7', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'nodeep-scrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 5851b134-4060-11ec-aba1-c5f3c07a3dc7 -- ceph osd pool set bench_rados nodeep-scrub 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 nodeep-scrub to 1
# ./benchmarks/bench-rados:272 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool ls detail
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:55:54,478484555-05:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:55:54,481165259-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '5851b134-4060-11ec-aba1-c5f3c07a3dc7', '--', 'ceph', 'osd', 'pool', 'ls', 'detail'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 5851b134-4060-11ec-aba1-c5f3c07a3dc7 -- ceph osd pool ls detail
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
pool 1 'device_health_metrics' replicated size 1 min_size 1 crush_rule 0 object_hash rjenkins pg_num 64 pgp_num 64 autoscale_mode on last_change 17 lfor 0/0/14 flags hashpspool stripe_width 0 pg_num_min 1 application mgr_devicehealth
pool 2 'bench_rados' replicated size 1 min_size 1 crush_rule 0 object_hash rjenkins pg_num 128 pgp_num 128 autoscale_mode off last_change 20 flags hashpspool,noscrub,nodeep-scrub stripe_width 0 pg_num_min 128 application benchmark

# ./benchmarks/bench-rados:273 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd df tree
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:55:58,541880484-05:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:55:58,544808725-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '5851b134-4060-11ec-aba1-c5f3c07a3dc7', '--', 'ceph', 'osd', 'df', 'tree'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 5851b134-4060-11ec-aba1-c5f3c07a3dc7 -- ceph osd df tree
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
ID  CLASS  WEIGHT   REWEIGHT  SIZE     RAW USE  DATA     OMAP  META     AVAIL    %USE  VAR   PGS  STATUS  TYPE NAME      
-1         0.09769         -  100 GiB  5.7 MiB  168 KiB   0 B  5.6 MiB  100 GiB  0.01  1.00    -          root default   
-3         0.09769         -  100 GiB  5.7 MiB  168 KiB   0 B  5.6 MiB  100 GiB  0.01  1.00    -              host node-1
 0    ssd  0.09769   1.00000  100 GiB  5.7 MiB  168 KiB   0 B  5.6 MiB  100 GiB  0.01  1.00  192      up          osd.0  
                       TOTAL  100 GiB  5.7 MiB  168 KiB   0 B  5.6 MiB  100 GiB  0.01                                    
MIN/MAX VAR: 1.00/1.00  STDDEV: 0


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:56:02,648460914-05:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:56:26,703435890-05:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:56:35,596703345-05:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:56:44,877012196-05:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:56:44,884496337-05:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:56:53,929303067-05:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:56:53,936964732-05:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:57:03,075982249-05:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:57:03,083702734-05:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:57:11,953855611-05:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:57:11,961808625-05:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:57:21,116194462-05:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:57:21,123577882-05:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:57:21,129345456-05:00] INFO: > The cluster is idle now.[0m


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:57:21,132806912-05:00] STAGE: Start benchmarking of mode write ...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:57:21,139671104-05:00] INFO: > Start system activity collection services[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:57:21,145122081-05:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:341 - rados_bench() > rm --force /mnt/sda8/ceph-research/scripts/output/sys_activity_1MB_write.dat.rados_bench_host.3
# ./benchmarks/bench-rados:342 - rados_bench() > sar_pid_rados_bench_host=227233
# ./benchmarks/bench-rados:342 - rados_bench() > S_TIME_FORMAT=ISO
# ./benchmarks/bench-rados:342 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/output/sys_activity_1MB_write.dat.rados_bench_host.3 2

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:57:21,151935597-05:00] INFO: >> for OSD host[0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:57:21,160583651-05:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.2 bash
10.10.2.2: b'60319\n'
[1] 01:57:21 [SUCCESS] ljishen@10.10.2.2
60319

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:57:21,345219547-05:00] INFO: > Run rados bench[0m
# ./benchmarks/bench-rados:370 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/output/rados_bench_1MB_write.log.3
## ./benchmarks/bench-rados:361 - rados_bench() > count_cpus 32-63,96-127
# ./benchmarks/bench-rados:361 - rados_bench() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 60 write --pool bench_rados -b 1048576 -O 1048576 --concurrent-ios 128 --show-time --write-object --write-omap --write-xattr --no-cleanup
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:57:21,364706968-05:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:57:21,367284147-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '5851b134-4060-11ec-aba1-c5f3c07a3dc7', '--', 'rados', 'bench', '60', 'write', '--pool', 'bench_rados', '-b', '1048576', '-O', '1048576', '--concurrent-ios', '128', '--show-time', '--write-object', '--write-omap', '--write-xattr', '--no-cleanup'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 5851b134-4060-11ec-aba1-c5f3c07a3dc7 -- rados bench 60 write --pool bench_rados -b 1048576 -O 1048576 --concurrent-ios 128 --show-time --write-object --write-omap --write-xattr --no-cleanup
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
hints = 1
2021-11-08T06:57:24.412676+0000 Maintaining 128 concurrent writes of 1048576 bytes to objects of size 1048576 for up to 60 seconds or 0 objects
2021-11-08T06:57:24.412689+0000 Object prefix: benchmark_data_node-0.bluefield2.ucsc-cmps10_7
2021-11-08T06:57:24.444784+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-11-08T06:57:24.444784+0000     0       0         0         0         0         0           -           0
2021-11-08T06:57:25.444920+0000     1     127       336       209   208.989       209    0.371473    0.407668
2021-11-08T06:57:26.444995+0000     2     127       575       448   223.986       239    0.543537    0.413152
2021-11-08T06:57:27.445071+0000     3     127       721       594   197.987       146    0.947439    0.518892
2021-11-08T06:57:28.445138+0000     4     127       991       864   215.985       270    0.378239    0.540589
2021-11-08T06:57:29.445207+0000     5     127      1134      1007   201.386       143    0.898086    0.541338
2021-11-08T06:57:30.445281+0000     6     127      1213      1086   180.988        79      1.3242    0.595578
2021-11-08T06:57:31.445357+0000     7     127      1320      1193   170.417       107     1.20554    0.671611
2021-11-08T06:57:32.445434+0000     8     127      1484      1357   169.613       164    0.805129    0.704266
2021-11-08T06:57:33.445509+0000     9     127      1618      1491   165.655       134    0.844171    0.719095
2021-11-08T06:57:34.445581+0000    10     127      1764      1637   163.688       146    0.826842    0.735385
2021-11-08T06:57:35.445656+0000    11     127      2072      1945   176.806       308    0.391101    0.702692
2021-11-08T06:57:36.445732+0000    12     127      2163      2036   169.654        91    0.889997    0.699655
2021-11-08T06:57:37.445804+0000    13     127      2256      2129   163.757        93     1.49823    0.727476
2021-11-08T06:57:38.445873+0000    14     127      2360      2233   159.489       104     1.13491    0.761635
2021-11-08T06:57:39.445949+0000    15     127      2504      2377   158.455       144      0.9206    0.776349
2021-11-08T06:57:40.446024+0000    16     127      2622      2495   155.926       118    0.998849    0.785502
2021-11-08T06:57:41.446103+0000    17     127      2699      2572   151.283        77     1.36698    0.799852
2021-11-08T06:57:42.446173+0000    18     127      2798      2671   148.378        99     1.28556      0.8253
2021-11-08T06:57:43.446241+0000    19     127      2985      2858    150.41       187    0.482042    0.835914
2021-11-08T06:57:44.446321+0000 min lat: 0.352812 max lat: 1.72716 avg lat: 0.814037
2021-11-08T06:57:44.446321+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-11-08T06:57:44.446321+0000    20     127      3166      3039   151.939       181     0.75576    0.814037
2021-11-08T06:57:45.446399+0000    21     127      3251      3124   148.751        85     1.33699    0.822896
2021-11-08T06:57:46.446468+0000    22     127      3349      3222   146.444        98     1.28177    0.841311
2021-11-08T06:57:47.446538+0000    23     127      3498      3371   146.555       149    0.843246     0.84881
2021-11-08T06:57:48.446604+0000    24     127      3641      3514   146.406       143    0.940241    0.850337
2021-11-08T06:57:49.446680+0000    25     127      3716      3589    143.55        75     1.24966    0.857273
2021-11-08T06:57:50.446771+0000    26     127      3809      3682   141.605        93     1.36895    0.872347
2021-11-08T06:57:51.446844+0000    27     127      3947      3820   141.471       138    0.897442    0.886174
2021-11-08T06:57:52.446922+0000    28     127      4087      3960   141.418       140    0.866652    0.885996
2021-11-08T06:57:53.446993+0000    29     127      4193      4066   140.197       106      1.1795    0.889115
2021-11-08T06:57:54.447065+0000    30     127      4271      4144   138.123        78     1.28167     0.89842
2021-11-08T06:57:55.447141+0000    31     127      4370      4243   136.861        99     1.25741    0.911123
2021-11-08T06:57:56.447216+0000    32     127      4507      4380   136.865       137    0.792007    0.916992
2021-11-08T06:57:57.447292+0000    33     127      4647      4520    136.96       140    0.891635    0.916243
2021-11-08T06:57:58.447359+0000    34     127      4733      4606   135.461        86     1.23143    0.921189
2021-11-08T06:57:59.447434+0000    35     127      4812      4685   133.847        79     1.27029    0.930282
2021-11-08T06:58:00.447512+0000    36     127      4930      4803   133.407       118     1.09474    0.941041
2021-11-08T06:58:01.447587+0000    37     127      5077      4950   133.774       147    0.843808    0.941626
2021-11-08T06:58:02.447663+0000    38     127      5181      5054    132.99       104    0.973406    0.941886
2021-11-08T06:58:03.447736+0000    39     127      5278      5151   132.067        97     1.27395    0.948844
2021-11-08T06:58:04.447808+0000 min lat: 0.352812 max lat: 1.72716 avg lat: 0.957956
2021-11-08T06:58:04.447808+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-11-08T06:58:04.447808+0000    40     127      5364      5237   130.915        86     1.39263    0.957956
2021-11-08T06:58:05.447885+0000    41     127      5500      5373   131.039       136    0.860595    0.963987
2021-11-08T06:58:06.447956+0000    42     127      5650      5523    131.49       150    0.845544    0.960898
2021-11-08T06:58:07.448027+0000    43     127      5734      5607   130.386        84      1.0525    0.961972
2021-11-08T06:58:08.448097+0000    44     127      5818      5691   129.331        84     1.41779    0.968191
2021-11-08T06:58:09.448168+0000    45     127      5928      5801   128.902       110     1.14147    0.977502
2021-11-08T06:58:10.448242+0000    46     127      6075      5948   129.295       147    0.836226    0.977868
2021-11-08T06:58:11.448323+0000    47     127      6205      6078    129.31       130    0.935513     0.97643
2021-11-08T06:58:12.448398+0000    48     127      6286      6159   128.303        81     1.29705    0.979529
2021-11-08T06:58:13.448470+0000    49     127      6374      6247    127.48        88     1.33227    0.985645
2021-11-08T06:58:14.448542+0000    50     127      6511      6384   127.671       137    0.809178    0.991248
2021-11-08T06:58:15.448615+0000    51     127      6648      6521   127.853       137    0.816746    0.988935
2021-11-08T06:58:16.448693+0000    52     127      6756      6629   127.471       108     1.11512    0.989101
2021-11-08T06:58:17.448769+0000    53     127      6835      6708   126.557        79     1.31033    0.994078
2021-11-08T06:58:18.448836+0000    54     127      6941      6814   126.176       106     1.24829     1.00083
2021-11-08T06:58:19.448907+0000    55     127      7064      6937   126.118       123    0.939032     1.00249
2021-11-08T06:58:20.448980+0000    56     127      7205      7078   126.384       141    0.866113     1.00034
2021-11-08T06:58:21.449057+0000    57     127      7303      7176   125.885        98     1.31325     1.00279
2021-11-08T06:58:22.449128+0000    58     127      7387      7260   125.163        84     1.35827     1.00826
2021-11-08T06:58:23.449202+0000    59     127      7504      7377   125.025       117    0.992466     1.01345
2021-11-08T06:58:24.449273+0000 min lat: 0.352812 max lat: 1.72716 avg lat: 1.01117
2021-11-08T06:58:24.449273+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-11-08T06:58:24.449273+0000    60     127      7657      7530   125.491       153    0.729628     1.01117
2021-11-08T06:58:25.449393+0000 Total time run:         60.2296
Total writes made:      7658
Write size:             1048576
Object size:            1048576
Bandwidth (MB/sec):     127.147
Stddev Bandwidth:       46.0631
Max bandwidth (MB/sec): 308
Min bandwidth (MB/sec): 75
Average IOPS:           127
Stddev IOPS:            46.0631
Max IOPS:               308
Min IOPS:               75
Average Latency(s):     1.00166
Stddev Latency(s):      0.35132
Max latency(s):         1.72716
Min latency(s):         0.143226

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:58:26,144953961-05:00] INFO: > Stop system activity collection services[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:58:26,150769335-05:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:391 - rados_bench() > kill -INT 227233

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:58:26,156753718-05:00] INFO: >> for OSD host[0m
# ./benchmarks/bench-rados:396 - rados_bench() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.2 kill -INT 60319
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:58:26,164337576-05:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.2 kill -INT 60319
[1] 01:58:26 [SUCCESS] ljishen@10.10.2.2

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:58:26,344140761-05:00] INFO: >>> Copy system activity file from OSD host[0m
## ./benchmarks/bench-rados:403 - rados_bench() > basename -- /tmp/bench-rados/sys_activity_1MB_write.dat.osd_host.3
# ./benchmarks/bench-rados:403 - rados_bench() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.2:/tmp/bench-rados/sys_activity_1MB_write.dat.osd_host.3 /mnt/sda8/ceph-research/scripts/output/sys_activity_1MB_write.dat.osd_host.3


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:58:26,514533072-05:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:58:50,388378536-05:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 7.66k objects, 7.5 GiB
    usage:   23 GiB used, 77 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:58:50,396315660-05:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:58:59,485245699-05:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 7.66k objects, 7.5 GiB
    usage:   23 GiB used, 77 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:58:59,493196129-05:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:59:08,565992364-05:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 7.66k objects, 7.5 GiB
    usage:   23 GiB used, 77 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:59:08,573168743-05:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:59:17,675368539-05:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 7.66k objects, 7.5 GiB
    usage:   23 GiB used, 77 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:59:17,683164868-05:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:59:26,761729867-05:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 7.66k objects, 7.5 GiB
    usage:   23 GiB used, 77 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:59:26,769218837-05:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:59:26,774872236-05:00] INFO: > The cluster is idle now.[0m


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:59:26,778405146-05:00] STAGE: Start benchmarking of mode seq ...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:59:26,785307089-05:00] INFO: > Start system activity collection services[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:59:26,790694276-05:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:341 - rados_bench() > rm --force /mnt/sda8/ceph-research/scripts/output/sys_activity_1MB_seq.dat.rados_bench_host.3
# ./benchmarks/bench-rados:342 - rados_bench() > sar_pid_rados_bench_host=228586
# ./benchmarks/bench-rados:342 - rados_bench() > S_TIME_FORMAT=ISO
# ./benchmarks/bench-rados:342 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/output/sys_activity_1MB_seq.dat.rados_bench_host.3 2

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:59:26,797583104-05:00] INFO: >> for OSD host[0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:59:26,806299247-05:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.2 bash
10.10.2.2: b'60420\n'
[1] 01:59:26 [SUCCESS] ljishen@10.10.2.2
60420

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:59:26,993762334-05:00] INFO: > Run rados bench[0m
# ./benchmarks/bench-rados:379 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/output/rados_bench_1MB_seq.log.3
## ./benchmarks/bench-rados:376 - rados_bench() > count_cpus 32-63,96-127
# ./benchmarks/bench-rados:376 - rados_bench() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 128 --show-time
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:59:27,012847056-05:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:59:27,015542959-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '5851b134-4060-11ec-aba1-c5f3c07a3dc7', '--', 'rados', 'bench', '99999999', 'seq', '--pool', 'bench_rados', '--concurrent-ios', '128', '--show-time'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 5851b134-4060-11ec-aba1-c5f3c07a3dc7 -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 128 --show-time
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
hints = 1
2021-11-08T06:59:30.014904+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-11-08T06:59:30.014904+0000     0       0         0         0         0         0           -           0
2021-11-08T06:59:31.015000+0000     1     127      1440      1313   1312.79      1313   0.0927629   0.0923017
2021-11-08T06:59:32.015084+0000     2     127      2759      2632   1315.84      1319    0.147719    0.094664
2021-11-08T06:59:33.015172+0000     3     127      4098      3971   1323.52      1339    0.092654   0.0949598
2021-11-08T06:59:34.015249+0000     4     127      5432      5305   1326.11      1334   0.0946535   0.0951477
2021-11-08T06:59:35.015327+0000     5     127      6762      6635   1326.87      1330      0.0961   0.0953909
2021-11-08T06:59:36.015428+0000 Total time run:       5.75369
Total reads made:     7658
Read size:            1048576
Object size:          1048576
Bandwidth (MB/sec):   1330.97
Average IOPS:         1330
Stddev IOPS:          10.7471
Max IOPS:             1339
Min IOPS:             1313
Average Latency(s):   0.0952765
Max latency(s):       0.172962
Min latency(s):       0.0250594

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:59:36,681122170-05:00] INFO: > Stop system activity collection services[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:59:36,687041520-05:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:391 - rados_bench() > kill -INT 228586

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:59:36,692954378-05:00] INFO: >> for OSD host[0m
# ./benchmarks/bench-rados:396 - rados_bench() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.2 kill -INT 60420
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:59:36,700458627-05:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.2 kill -INT 60420
[1] 01:59:36 [SUCCESS] ljishen@10.10.2.2

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:59:36,880512294-05:00] INFO: >>> Copy system activity file from OSD host[0m
## ./benchmarks/bench-rados:403 - rados_bench() > basename -- /tmp/bench-rados/sys_activity_1MB_seq.dat.osd_host.3
# ./benchmarks/bench-rados:403 - rados_bench() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.2:/tmp/bench-rados/sys_activity_1MB_seq.dat.osd_host.3 /mnt/sda8/ceph-research/scripts/output/sys_activity_1MB_seq.dat.osd_host.3


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T01:59:37,028882737-05:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:00:01,048513153-05:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 7.66k objects, 7.5 GiB
    usage:   23 GiB used, 77 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:00:01,055616306-05:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:00:10,110656141-05:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 7.66k objects, 7.5 GiB
    usage:   23 GiB used, 77 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:00:10,118176630-05:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:00:19,087686786-05:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 7.66k objects, 7.5 GiB
    usage:   23 GiB used, 77 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:00:19,096075001-05:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:00:28,133019409-05:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 7.66k objects, 7.5 GiB
    usage:   23 GiB used, 77 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:00:28,140463064-05:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:00:37,153742658-05:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 7.66k objects, 7.5 GiB
    usage:   23 GiB used, 77 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:00:37,160992877-05:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:00:37,167235897-05:00] INFO: > The cluster is idle now.[0m


[1;7;39;49m[2021-11-08T02:00:37,170889025-05:00][RUNNING][ROUND 1/6/21] object_size=4MB[0m


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:00:37,174140876-05:00] STAGE: Launch a Ceph cluster (CEPH_HOST: 10.10.2.3, OSD_HOST: 10.10.2.2) ...[0m
# ./benchmarks/bench-rados:183 - launch_ceph_cluster() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.3 bash -euo pipefail
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:00:37,183313350-05:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.3 bash -euo pipefail
10.10.2.3: b'\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T02:00:37,610160084-05:00] INFO: > Remove existing clusters\x1b[0m\n'
10.10.2.3: b'## bash:17 -  > basename -- /var/lib/ceph/5851b134-4060-11ec-aba1-c5f3c07a3dc7\n'
10.10.2.3: b'# bash:17 -  > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm --host 10.10.2.3 --host 10.10.2.2 rm-cluster --force --fsid 5851b134-4060-11ec-aba1-c5f3c07a3dc7\n'
10.10.2.3: b"\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T02:00:37,620318631-05:00] DEBUG: HOST_PARAMS: ['--host', '10.10.2.3', '--host', '10.10.2.2']\x1b[0m\n"
10.10.2.3: b"\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T02:00:37,624091590-05:00] DEBUG: CEPHADM_PARAMS: ['rm-cluster', '--force', '--fsid', '5851b134-4060-11ec-aba1-c5f3c07a3dc7']\x1b[0m\n"
10.10.2.3: b"# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:63 -  > /tmp/bench-rados/ceph-research/scripts/parallel-ssh --host 10.10.2.3 --host 10.10.2.2 'sudo python3 -u - rm-cluster --force --fsid 5851b134-4060-11ec-aba1-c5f3c07a3dc7'\n"
10.10.2.3: b'\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T02:00:37,632377915-05:00] DEBUG: command from stdin\x1b[0m\n'
10.10.2.3: b"# /tmp/bench-rados/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host 10.10.2.3 --host 10.10.2.2 'sudo python3 -u - rm-cluster --force --fsid 5851b134-4060-11ec-aba1-c5f3c07a3dc7'\n"
10.10.2.3: b'[1] 02:00:43 [SUCCESS] 10.10.2.3\n[2] 02:00:49 [SUCCESS] 10.10.2.2\n'
10.10.2.3: b'\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T02:00:49,601624470-05:00] INFO: > Deploy a new cluster\x1b[0m\n'
10.10.2.3: b'# bash:24 -  > /tmp/bench-rados/ceph-research/scripts/ceph-deploy -m 10.10.2.3 -o node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/10.10.2.2:/dev/nvme0n1\n'
10.10.2.3: b'\n\n\x1b[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T02:00:49,613385852-05:00] STAGE: Verify passwordless SSH to hosts, passwordless sudo via ljishen, and the existence of devices on hosts...\x1b[0m\n'
10.10.2.3: b'\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T02:00:49,617989208-05:00] INFO: > Check the host for the monitor\x1b[0m\n'
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:181 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@10.10.2.3 sudo --non-interactive --validate\n'
10.10.2.3: b'\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T02:00:49,766117220-05:00] INFO: > Check hosts for OSDs\x1b[0m\n'
10.10.2.3: b"\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T02:00:49,771950577-05:00] INFO: >> Check host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'\x1b[0m\n"
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:200 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us sudo --non-interactive --validate\n'
10.10.2.3: b"\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T02:00:49,922016466-05:00] INFO: >>> Check device '/dev/nvme0n1'\x1b[0m\n"
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:215 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us test -b /dev/nvme0n1\n'
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:226 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us lsblk --nodeps --noheadings --output TYPE /dev/nvme0n1\n# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:227 - verify_parameters() > grep --word-regexp disk\n'
10.10.2.3: b'\n\n\x1b[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T02:00:50,198535110-05:00] STAGE: Reset devices on hosts...\x1b[0m\n'
10.10.2.3: b"\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T02:00:50,203587944-05:00] INFO: > Resetting device '/dev/nvme0n1' on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'\x1b[0m\n"
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:288 - reset_host_devices() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us sudo sh\n'
10.10.2.3: b'  Removing ceph--a4b44c15--8b69--4592--b2f8--7a69ec8046c0-osd--block--e90fc274--01fe--4330--8771--89a40f92d146 (253:0)\n'
10.10.2.3: b'  Archiving volume group "ceph-a4b44c15-8b69-4592-b2f8-7a69ec8046c0" metadata (seqno 5).\n  Releasing logical volume "osd-block-e90fc274-01fe-4330-8771-89a40f92d146"\n'
10.10.2.3: b'  Creating volume group backup "/etc/lvm/backup/ceph-a4b44c15-8b69-4592-b2f8-7a69ec8046c0" (seqno 6).\n'
10.10.2.3: b'  Logical volume "osd-block-e90fc274-01fe-4330-8771-89a40f92d146" successfully removed\n'
10.10.2.3: b'  Removing physical volume "/dev/nvme0n1" from volume group "ceph-a4b44c15-8b69-4592-b2f8-7a69ec8046c0"\n'
10.10.2.3: b'  Volume group "ceph-a4b44c15-8b69-4592-b2f8-7a69ec8046c0" successfully removed\n'
10.10.2.3: b'\n\n\x1b[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T02:00:50,567408211-05:00] STAGE: Bootstrap a new cluster...\x1b[0m\n'
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:317 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.3 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults\n'
10.10.2.3: b'\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T02:00:50,577116078-05:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.3: b"\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T02:00:50,580746137-05:00] DEBUG: CEPHADM_PARAMS: ['bootstrap', '--config', '/tmp/bench-rados/ceph-research/scripts/ceph.conf', '--mon-ip', '10.10.2.3', '--output-dir', '/tmp/bench-rados/deployment_data_root/etc/ceph', '--ssh-user', 'ljishen', '--skip-dashboard', '--no-minimize-config', '--allow-overwrite', '--allow-fqdn-hostname', '--orphan-initial-daemons', '--skip-monitoring-stack', '--single-host-defaults']\x1b[0m\n"
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.3 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults\n'
10.10.2.3: b'Verifying podman|docker is present...\nVerifying lvm2 is present...\n'
10.10.2.3: b'Verifying time synchronization is in place...\n'
10.10.2.3: b'Unit ntp.service is enabled and running\n'
10.10.2.3: b'Repeating the final host check...\n'
10.10.2.3: b'docker (/usr/bin/docker) is present\n'
10.10.2.3: b'systemctl is present\n'
10.10.2.3: b'lvcreate is present\n'
10.10.2.3: b'Unit ntp.service is enabled and running\n'
10.10.2.3: b'Host looks OK\n'
10.10.2.3: b'Cluster fsid: 9b956ac0-4061-11ec-aba1-c5f3c07a3dc7\n'
10.10.2.3: b'Verifying IP 10.10.2.3 port 3300 ...\n'
10.10.2.3: b'Verifying IP 10.10.2.3 port 6789 ...\n'
10.10.2.3: b'Mon IP `10.10.2.3` is in CIDR network `10.10.2.0/24`\n'
10.10.2.3: b'- internal network (--cluster-network) has not been provided, OSD replication will default to the public_network\n'
10.10.2.3: b'Adjusting default settings to suit single-host cluster...\n'
10.10.2.3: b'Pulling container image quay.io/ceph/ceph:v16...\n'
10.10.2.3: b'Ceph version: ceph version 16.2.6 (ee28fb57e47e9f88813e24bbf4c14496ca299d31) pacific (stable)\n'
10.10.2.3: b'Extracting ceph user uid/gid from container image...\n'
10.10.2.3: b'Creating initial keys...\n'
10.10.2.3: b'Creating initial monmap...\n'
10.10.2.3: b'Creating mon...\n'
10.10.2.3: b'Waiting for mon to start...\n'
10.10.2.3: b'Waiting for mon...\n'
10.10.2.3: b'mon is available\nSetting mon public_network to 10.10.2.0/24\n'
10.10.2.3: b'Wrote config to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf\n'
10.10.2.3: b'Wrote keyring to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring\n'
10.10.2.3: b'Creating mgr...\n'
10.10.2.3: b'Verifying port 9283 ...\n'
10.10.2.3: b'Waiting for mgr to start...\nWaiting for mgr...\n'
10.10.2.3: b'mgr not available, waiting (1/15)...\n'
10.10.2.3: b'mgr not available, waiting (2/15)...\n'
10.10.2.3: b'mgr is available\n'
10.10.2.3: b'Enabling cephadm module...\n'
10.10.2.3: b'Waiting for the mgr to restart...\n'
10.10.2.3: b'Waiting for mgr epoch 5...\n'
10.10.2.3: b'mgr epoch 5 is available\nSetting orchestrator backend to cephadm...\n'
10.10.2.3: b'Generating ssh key...\n'
10.10.2.3: b'Wrote public SSH key to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub\n'
10.10.2.3: b'Adding key to ljishen@localhost authorized_keys...\n'
10.10.2.3: b'Adding host node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us...\n'
10.10.2.3: b'Deploying unmanaged mon service...\n'
10.10.2.3: b'Deploying unmanaged mgr service...\n'
10.10.2.3: b'You can access the Ceph CLI with:\n\n\tsudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --fsid 9b956ac0-4061-11ec-aba1-c5f3c07a3dc7 -c /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf -k /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring\n\n'
10.10.2.3: b'Please consider enabling telemetry to help improve Ceph:\n\n\tceph telemetry on\n\nFor more information see:\n\n\thttps://docs.ceph.com/docs/pacific/mgr/telemetry/\n\nBootstrap complete.\n'
10.10.2.3: b'\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T02:02:00,897134455-05:00] INFO: Sleep for 20 seconds\x1b[0m\n'
10.10.2.3: b'\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T02:02:20,904420326-05:00] INFO: > Set the default number of monitor daemons to 1\x1b[0m\n'
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:337 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1\n'
10.10.2.3: b'\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T02:02:20,914870724-05:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.3: b"\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T02:02:20,918797864-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mon', '1']\x1b[0m\n"
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1\n'
10.10.2.3: b'Inferring fsid 9b956ac0-4061-11ec-aba1-c5f3c07a3dc7\n'
10.10.2.3: b'Inferring config /var/lib/ceph/9b956ac0-4061-11ec-aba1-c5f3c07a3dc7/mon.node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.3: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.3: b'Scheduled mon update...\n'
10.10.2.3: b'\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T02:02:30,807067324-05:00] INFO: > Set the default number of manager daemons to 1\x1b[0m\n'
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:342 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1\n'
10.10.2.3: b'\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T02:02:30,816635849-05:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.3: b"\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T02:02:30,820125693-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mgr', '1']\x1b[0m\n"
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1\n'
10.10.2.3: b'Inferring fsid 9b956ac0-4061-11ec-aba1-c5f3c07a3dc7\n'
10.10.2.3: b'Inferring config /var/lib/ceph/9b956ac0-4061-11ec-aba1-c5f3c07a3dc7/mon.node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.3: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.3: b'Scheduled mgr update...\n'
10.10.2.3: b'\n\n\x1b[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T02:02:40,987990745-05:00] STAGE: Add hosts to the cluster...\x1b[0m\n'
10.10.2.3: b"\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T02:02:40,993872944-05:00] INFO: > Adding host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'\x1b[0m\n"
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:361 - add_hosts() > ssh-copy-id -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -f -i /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\n'
10.10.2.3: b'/usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: "/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub"\n'
10.10.2.3: b'\nNumber of key(s) added: 1\n\nNow try logging into the machine, with:   "ssh -o \'GlobalKnownHostsFile=/dev/null\' -o \'LogLevel=ERROR\' -o \'PasswordAuthentication=no\' -o \'StrictHostKeyChecking=no\' -o \'UserKnownHostsFile=/dev/null\' \'ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\'"\nand check to make sure that only the key(s) you wanted were added.\n\n'
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:367 - add_hosts() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us 10.10.2.2\n'
10.10.2.3: b'\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T02:02:41,201208251-05:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.3: b"\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T02:02:41,204553893-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'host', 'add', 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us', '10.10.2.2']\x1b[0m\n"
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us 10.10.2.2\n'
10.10.2.3: b'Inferring fsid 9b956ac0-4061-11ec-aba1-c5f3c07a3dc7\n'
10.10.2.3: b'Inferring config /var/lib/ceph/9b956ac0-4061-11ec-aba1-c5f3c07a3dc7/mon.node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.3: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.3: b"Added host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us' with addr '10.10.2.2'\n"
10.10.2.3: b'\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T02:02:50,876940771-05:00] INFO: Sleep for 20 seconds\x1b[0m\n'
10.10.2.3: b'\n\n\x1b[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T02:03:10,882027139-05:00] STAGE: Add OSDs to the cluster...\x1b[0m\n'
10.10.2.3: b"\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T02:03:10,888094196-05:00] INFO: > Adding devices ['/dev/nvme0n1'] on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us' as OSDs\x1b[0m\n"
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:388 - add_host_devices_as_osds() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1\n'
10.10.2.3: b'\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T02:03:10,897515563-05:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.3: b"\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T02:03:10,901027169-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'daemon', 'add', 'osd', 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1']\x1b[0m\n"
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1\n'
10.10.2.3: b'Inferring fsid 9b956ac0-4061-11ec-aba1-c5f3c07a3dc7\n'
10.10.2.3: b'Inferring config /var/lib/ceph/9b956ac0-4061-11ec-aba1-c5f3c07a3dc7/mon.node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.3: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.3: b"Created osd(s) 0 on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'\n"
10.10.2.3: b'\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T02:03:37,263791671-05:00] INFO: > Sleep for 20 seconds\x1b[0m\n'
10.10.2.3: b'\n\n\x1b[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T02:03:57,269411562-05:00] STAGE: Show Cluster Status\x1b[0m\n'
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:408 - show_status() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status\n'
10.10.2.3: b'\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T02:03:57,279728829-05:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.3: b"\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T02:03:57,283388765-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', '--status']\x1b[0m\n"
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status\n'
10.10.2.3: b'Inferring fsid 9b956ac0-4061-11ec-aba1-c5f3c07a3dc7\n'
10.10.2.3: b'Inferring config /var/lib/ceph/9b956ac0-4061-11ec-aba1-c5f3c07a3dc7/mon.node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.3: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.3: b'  cluster:\n    id:     9b956ac0-4061-11ec-aba1-c5f3c07a3dc7\n    health: HEALTH_WARN\n            1 pool(s) have no replicas configured\n \n  services:\n    mon: 1 daemons, quorum node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us (age 2m)\n    mgr: node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us.iiiqam(active, since 2m)\n    osd: 1 osds: 1 up (since 18s), 1 in (since 42s)\n \n  data:\n    pools:   1 pools, 1 pgs\n    objects: 0 objects, 0 B\n    usage:   4.7 MiB used, 100 GiB / 100 GiB avail\n    pgs:     1 active+clean\n \n'
[1] 02:04:06 [SUCCESS] ljishen@10.10.2.3

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:00:37,610160084-05:00] INFO: > Remove existing clusters[0m
## bash:17 -  > basename -- /var/lib/ceph/5851b134-4060-11ec-aba1-c5f3c07a3dc7
# bash:17 -  > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm --host 10.10.2.3 --host 10.10.2.2 rm-cluster --force --fsid 5851b134-4060-11ec-aba1-c5f3c07a3dc7
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:00:37,620318631-05:00] DEBUG: HOST_PARAMS: ['--host', '10.10.2.3', '--host', '10.10.2.2'][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:00:37,624091590-05:00] DEBUG: CEPHADM_PARAMS: ['rm-cluster', '--force', '--fsid', '5851b134-4060-11ec-aba1-c5f3c07a3dc7'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:63 -  > /tmp/bench-rados/ceph-research/scripts/parallel-ssh --host 10.10.2.3 --host 10.10.2.2 'sudo python3 -u - rm-cluster --force --fsid 5851b134-4060-11ec-aba1-c5f3c07a3dc7'
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:00:37,632377915-05:00] DEBUG: command from stdin[0m
# /tmp/bench-rados/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host 10.10.2.3 --host 10.10.2.2 'sudo python3 -u - rm-cluster --force --fsid 5851b134-4060-11ec-aba1-c5f3c07a3dc7'
[1] 02:00:43 [SUCCESS] 10.10.2.3
[2] 02:00:49 [SUCCESS] 10.10.2.2

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:00:49,601624470-05:00] INFO: > Deploy a new cluster[0m
# bash:24 -  > /tmp/bench-rados/ceph-research/scripts/ceph-deploy -m 10.10.2.3 -o node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/10.10.2.2:/dev/nvme0n1


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:00:49,613385852-05:00] STAGE: Verify passwordless SSH to hosts, passwordless sudo via ljishen, and the existence of devices on hosts...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:00:49,617989208-05:00] INFO: > Check the host for the monitor[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:181 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@10.10.2.3 sudo --non-interactive --validate

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:00:49,766117220-05:00] INFO: > Check hosts for OSDs[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:00:49,771950577-05:00] INFO: >> Check host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:200 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us sudo --non-interactive --validate

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:00:49,922016466-05:00] INFO: >>> Check device '/dev/nvme0n1'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:215 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us test -b /dev/nvme0n1
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:226 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us lsblk --nodeps --noheadings --output TYPE /dev/nvme0n1
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:227 - verify_parameters() > grep --word-regexp disk


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:00:50,198535110-05:00] STAGE: Reset devices on hosts...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:00:50,203587944-05:00] INFO: > Resetting device '/dev/nvme0n1' on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:288 - reset_host_devices() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us sudo sh
  Removing ceph--a4b44c15--8b69--4592--b2f8--7a69ec8046c0-osd--block--e90fc274--01fe--4330--8771--89a40f92d146 (253:0)
  Archiving volume group "ceph-a4b44c15-8b69-4592-b2f8-7a69ec8046c0" metadata (seqno 5).
  Releasing logical volume "osd-block-e90fc274-01fe-4330-8771-89a40f92d146"
  Creating volume group backup "/etc/lvm/backup/ceph-a4b44c15-8b69-4592-b2f8-7a69ec8046c0" (seqno 6).
  Logical volume "osd-block-e90fc274-01fe-4330-8771-89a40f92d146" successfully removed
  Removing physical volume "/dev/nvme0n1" from volume group "ceph-a4b44c15-8b69-4592-b2f8-7a69ec8046c0"
  Volume group "ceph-a4b44c15-8b69-4592-b2f8-7a69ec8046c0" successfully removed


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:00:50,567408211-05:00] STAGE: Bootstrap a new cluster...[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:317 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.3 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:00:50,577116078-05:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:00:50,580746137-05:00] DEBUG: CEPHADM_PARAMS: ['bootstrap', '--config', '/tmp/bench-rados/ceph-research/scripts/ceph.conf', '--mon-ip', '10.10.2.3', '--output-dir', '/tmp/bench-rados/deployment_data_root/etc/ceph', '--ssh-user', 'ljishen', '--skip-dashboard', '--no-minimize-config', '--allow-overwrite', '--allow-fqdn-hostname', '--orphan-initial-daemons', '--skip-monitoring-stack', '--single-host-defaults'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.3 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults
Verifying podman|docker is present...
Verifying lvm2 is present...
Verifying time synchronization is in place...
Unit ntp.service is enabled and running
Repeating the final host check...
docker (/usr/bin/docker) is present
systemctl is present
lvcreate is present
Unit ntp.service is enabled and running
Host looks OK
Cluster fsid: 9b956ac0-4061-11ec-aba1-c5f3c07a3dc7
Verifying IP 10.10.2.3 port 3300 ...
Verifying IP 10.10.2.3 port 6789 ...
Mon IP `10.10.2.3` is in CIDR network `10.10.2.0/24`
- internal network (--cluster-network) has not been provided, OSD replication will default to the public_network
Adjusting default settings to suit single-host cluster...
Pulling container image quay.io/ceph/ceph:v16...
Ceph version: ceph version 16.2.6 (ee28fb57e47e9f88813e24bbf4c14496ca299d31) pacific (stable)
Extracting ceph user uid/gid from container image...
Creating initial keys...
Creating initial monmap...
Creating mon...
Waiting for mon to start...
Waiting for mon...
mon is available
Setting mon public_network to 10.10.2.0/24
Wrote config to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf
Wrote keyring to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring
Creating mgr...
Verifying port 9283 ...
Waiting for mgr to start...
Waiting for mgr...
mgr not available, waiting (1/15)...
mgr not available, waiting (2/15)...
mgr is available
Enabling cephadm module...
Waiting for the mgr to restart...
Waiting for mgr epoch 5...
mgr epoch 5 is available
Setting orchestrator backend to cephadm...
Generating ssh key...
Wrote public SSH key to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub
Adding key to ljishen@localhost authorized_keys...
Adding host node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us...
Deploying unmanaged mon service...
Deploying unmanaged mgr service...
You can access the Ceph CLI with:

	sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --fsid 9b956ac0-4061-11ec-aba1-c5f3c07a3dc7 -c /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf -k /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring

Please consider enabling telemetry to help improve Ceph:

	ceph telemetry on

For more information see:

	https://docs.ceph.com/docs/pacific/mgr/telemetry/

Bootstrap complete.

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:02:00,897134455-05:00] INFO: Sleep for 20 seconds[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:02:20,904420326-05:00] INFO: > Set the default number of monitor daemons to 1[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:337 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:02:20,914870724-05:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:02:20,918797864-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mon', '1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1
Inferring fsid 9b956ac0-4061-11ec-aba1-c5f3c07a3dc7
Inferring config /var/lib/ceph/9b956ac0-4061-11ec-aba1-c5f3c07a3dc7/mon.node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Scheduled mon update...

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:02:30,807067324-05:00] INFO: > Set the default number of manager daemons to 1[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:342 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:02:30,816635849-05:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:02:30,820125693-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mgr', '1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1
Inferring fsid 9b956ac0-4061-11ec-aba1-c5f3c07a3dc7
Inferring config /var/lib/ceph/9b956ac0-4061-11ec-aba1-c5f3c07a3dc7/mon.node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Scheduled mgr update...


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:02:40,987990745-05:00] STAGE: Add hosts to the cluster...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:02:40,993872944-05:00] INFO: > Adding host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:361 - add_hosts() > ssh-copy-id -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -f -i /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us
/usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: "/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub"

Number of key(s) added: 1

Now try logging into the machine, with:   "ssh -o 'GlobalKnownHostsFile=/dev/null' -o 'LogLevel=ERROR' -o 'PasswordAuthentication=no' -o 'StrictHostKeyChecking=no' -o 'UserKnownHostsFile=/dev/null' 'ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'"
and check to make sure that only the key(s) you wanted were added.

# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:367 - add_hosts() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us 10.10.2.2
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:02:41,201208251-05:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:02:41,204553893-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'host', 'add', 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us', '10.10.2.2'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us 10.10.2.2
Inferring fsid 9b956ac0-4061-11ec-aba1-c5f3c07a3dc7
Inferring config /var/lib/ceph/9b956ac0-4061-11ec-aba1-c5f3c07a3dc7/mon.node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Added host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us' with addr '10.10.2.2'

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:02:50,876940771-05:00] INFO: Sleep for 20 seconds[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:03:10,882027139-05:00] STAGE: Add OSDs to the cluster...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:03:10,888094196-05:00] INFO: > Adding devices ['/dev/nvme0n1'] on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us' as OSDs[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:388 - add_host_devices_as_osds() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:03:10,897515563-05:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:03:10,901027169-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'daemon', 'add', 'osd', 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1
Inferring fsid 9b956ac0-4061-11ec-aba1-c5f3c07a3dc7
Inferring config /var/lib/ceph/9b956ac0-4061-11ec-aba1-c5f3c07a3dc7/mon.node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Created osd(s) 0 on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:03:37,263791671-05:00] INFO: > Sleep for 20 seconds[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:03:57,269411562-05:00] STAGE: Show Cluster Status[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:408 - show_status() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:03:57,279728829-05:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:03:57,283388765-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', '--status'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status
Inferring fsid 9b956ac0-4061-11ec-aba1-c5f3c07a3dc7
Inferring config /var/lib/ceph/9b956ac0-4061-11ec-aba1-c5f3c07a3dc7/mon.node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
  cluster:
    id:     9b956ac0-4061-11ec-aba1-c5f3c07a3dc7
    health: HEALTH_WARN
            1 pool(s) have no replicas configured
 
  services:
    mon: 1 daemons, quorum node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us (age 2m)
    mgr: node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us.iiiqam(active, since 2m)
    osd: 1 osds: 1 up (since 18s), 1 in (since 42s)
 
  data:
    pools:   1 pools, 1 pgs
    objects: 0 objects, 0 B
    usage:   4.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     1 active+clean
 

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:04:06,971352778-05:00] INFO: > Copy configuration files to local[0m
# ./benchmarks/bench-rados:220 - launch_ceph_cluster() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.3 sudo chown --changes --recursive ljishen /tmp/bench-rados/deployment_data_root/etc/ceph
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:04:06,978845315-05:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.3 sudo chown --changes --recursive ljishen /tmp/bench-rados/deployment_data_root/etc/ceph
[1] 02:04:07 [SUCCESS] ljishen@10.10.2.3
# ./benchmarks/bench-rados:223 - launch_ceph_cluster() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.3:/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf
# ./benchmarks/bench-rados:226 - launch_ceph_cluster() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.3:/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:04:07,448532144-05:00] STAGE: Update local ceph.conf as a client...[0m


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:04:07,452016023-05:00] STAGE: Configure Ceph pool...[0m
# ./benchmarks/bench-rados:257 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_max_backfills 32
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:04:07,472827981-05:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:04:07,475573728-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '9b956ac0-4061-11ec-aba1-c5f3c07a3dc7', '--', 'ceph', 'config', 'set', 'osd', 'osd_max_backfills', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 9b956ac0-4061-11ec-aba1-c5f3c07a3dc7 -- ceph config set osd osd_max_backfills 32
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:258 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_active 32
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:04:11,710806848-05:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:04:11,713439542-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '9b956ac0-4061-11ec-aba1-c5f3c07a3dc7', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_active', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 9b956ac0-4061-11ec-aba1-c5f3c07a3dc7 -- ceph config set osd osd_recovery_max_active 32
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:259 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_single_start 8
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:04:15,879907044-05:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:04:15,882643473-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '9b956ac0-4061-11ec-aba1-c5f3c07a3dc7', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_single_start', '8'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 9b956ac0-4061-11ec-aba1-c5f3c07a3dc7 -- ceph config set osd osd_recovery_max_single_start 8
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:260 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_op_priority 63
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:04:20,107916406-05:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:04:20,110673624-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '9b956ac0-4061-11ec-aba1-c5f3c07a3dc7', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_op_priority', '63'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 9b956ac0-4061-11ec-aba1-c5f3c07a3dc7 -- ceph config set osd osd_recovery_op_priority 63
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:262 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config show-with-defaults osd.0
# ./benchmarks/bench-rados:263 - configure_ceph_pool() > grep 'osd_max_backfills\|osd_recovery'
# ./benchmarks/bench-rados:263 - configure_ceph_pool() > column -t -s ' '
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
osd_max_backfills                        32         mon
osd_recovery_cost                        20971520   default
osd_recovery_delay_start                 0.000000   default
osd_recovery_max_active                  32         mon
osd_recovery_max_active_hdd              3          default
osd_recovery_max_active_ssd              10         default
osd_recovery_max_chunk                   8388608    default
osd_recovery_max_omap_entries_per_chunk  8096       default
osd_recovery_max_single_start            8          mon
osd_recovery_op_priority                 3          default
osd_recovery_op_warn_multiple            16         default
osd_recovery_priority                    5          default
osd_recovery_retry_interval              30.000000  default
osd_recovery_sleep                       0.000000   default
osd_recovery_sleep_hdd                   0.100000   default
osd_recovery_sleep_hybrid                0.025000   default
osd_recovery_sleep_ssd                   0.000000   default
# ./benchmarks/bench-rados:265 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool create bench_rados 128 128 replicated --size 1 --pg-num-min 128
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:04:28,638461304-05:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:04:28,641146988-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '9b956ac0-4061-11ec-aba1-c5f3c07a3dc7', '--', 'ceph', 'osd', 'pool', 'create', 'bench_rados', '128', '128', 'replicated', '--size', '1', '--pg-num-min', '128'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 9b956ac0-4061-11ec-aba1-c5f3c07a3dc7 -- ceph osd pool create bench_rados 128 128 replicated --size 1 --pg-num-min 128
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
pool 'bench_rados' created
# ./benchmarks/bench-rados:267 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados min_size 1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:04:33,358181072-05:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:04:33,361233998-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '9b956ac0-4061-11ec-aba1-c5f3c07a3dc7', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'min_size', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 9b956ac0-4061-11ec-aba1-c5f3c07a3dc7 -- ceph osd pool set bench_rados min_size 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 min_size to 1
# ./benchmarks/bench-rados:268 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados pg_autoscale_mode off
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:04:37,853596952-05:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:04:37,856433219-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '9b956ac0-4061-11ec-aba1-c5f3c07a3dc7', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'pg_autoscale_mode', 'off'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 9b956ac0-4061-11ec-aba1-c5f3c07a3dc7 -- ceph osd pool set bench_rados pg_autoscale_mode off
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 pg_autoscale_mode to off
# ./benchmarks/bench-rados:269 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool application enable bench_rados benchmark
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:04:42,294974730-05:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:04:42,297608416-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '9b956ac0-4061-11ec-aba1-c5f3c07a3dc7', '--', 'ceph', 'osd', 'pool', 'application', 'enable', 'bench_rados', 'benchmark'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 9b956ac0-4061-11ec-aba1-c5f3c07a3dc7 -- ceph osd pool application enable bench_rados benchmark
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
enabled application 'benchmark' on pool 'bench_rados'
# ./benchmarks/bench-rados:270 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados noscrub 1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:04:47,645796640-05:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:04:47,648628149-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '9b956ac0-4061-11ec-aba1-c5f3c07a3dc7', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'noscrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 9b956ac0-4061-11ec-aba1-c5f3c07a3dc7 -- ceph osd pool set bench_rados noscrub 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 noscrub to 1
# ./benchmarks/bench-rados:271 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados nodeep-scrub 1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:04:52,576359893-05:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:04:52,579445400-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '9b956ac0-4061-11ec-aba1-c5f3c07a3dc7', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'nodeep-scrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 9b956ac0-4061-11ec-aba1-c5f3c07a3dc7 -- ceph osd pool set bench_rados nodeep-scrub 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 nodeep-scrub to 1
# ./benchmarks/bench-rados:272 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool ls detail
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:04:57,460733942-05:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:04:57,463363680-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '9b956ac0-4061-11ec-aba1-c5f3c07a3dc7', '--', 'ceph', 'osd', 'pool', 'ls', 'detail'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 9b956ac0-4061-11ec-aba1-c5f3c07a3dc7 -- ceph osd pool ls detail
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
pool 1 'device_health_metrics' replicated size 1 min_size 1 crush_rule 0 object_hash rjenkins pg_num 64 pgp_num 64 autoscale_mode on last_change 18 lfor 0/0/15 flags hashpspool stripe_width 0 pg_num_min 1 application mgr_devicehealth
pool 2 'bench_rados' replicated size 1 min_size 1 crush_rule 0 object_hash rjenkins pg_num 128 pgp_num 128 autoscale_mode off last_change 21 flags hashpspool,noscrub,nodeep-scrub stripe_width 0 pg_num_min 128 application benchmark

# ./benchmarks/bench-rados:273 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd df tree
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:05:01,646686851-05:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:05:01,649435814-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '9b956ac0-4061-11ec-aba1-c5f3c07a3dc7', '--', 'ceph', 'osd', 'df', 'tree'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 9b956ac0-4061-11ec-aba1-c5f3c07a3dc7 -- ceph osd df tree
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
ID  CLASS  WEIGHT   REWEIGHT  SIZE     RAW USE  DATA     OMAP  META     AVAIL    %USE  VAR   PGS  STATUS  TYPE NAME      
-1         0.09769         -  100 GiB  5.7 MiB  176 KiB   0 B  5.6 MiB  100 GiB  0.01  1.00    -          root default   
-3         0.09769         -  100 GiB  5.7 MiB  176 KiB   0 B  5.6 MiB  100 GiB  0.01  1.00    -              host node-1
 0    ssd  0.09769   1.00000  100 GiB  5.7 MiB  176 KiB   0 B  5.6 MiB  100 GiB  0.01  1.00  192      up          osd.0  
                       TOTAL  100 GiB  5.7 MiB  176 KiB   0 B  5.6 MiB  100 GiB  0.01                                    
MIN/MAX VAR: 1.00/1.00  STDDEV: 0


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:05:05,958208685-05:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:05:29,924287255-05:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:05:39,151179324-05:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:05:39,158817525-05:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:05:48,360368059-05:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:05:48,367712757-05:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:05:57,530825225-05:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:05:57,538448358-05:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:06:06,680672387-05:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:06:06,688097807-05:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:06:15,782976636-05:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:06:15,790665363-05:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:06:15,796611293-05:00] INFO: > The cluster is idle now.[0m


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:06:15,800433510-05:00] STAGE: Start benchmarking of mode write ...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:06:15,808117868-05:00] INFO: > Start system activity collection services[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:06:15,813749586-05:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:341 - rados_bench() > rm --force /mnt/sda8/ceph-research/scripts/output/sys_activity_4MB_write.dat.rados_bench_host.1
# ./benchmarks/bench-rados:342 - rados_bench() > sar_pid_rados_bench_host=233997
# ./benchmarks/bench-rados:342 - rados_bench() > S_TIME_FORMAT=ISO
# ./benchmarks/bench-rados:342 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/output/sys_activity_4MB_write.dat.rados_bench_host.1 2

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:06:15,820553894-05:00] INFO: >> for OSD host[0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:06:15,829123902-05:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.2 bash
10.10.2.2: b'64037\n'
[1] 02:06:16 [SUCCESS] ljishen@10.10.2.2
64037

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:06:16,018459773-05:00] INFO: > Run rados bench[0m
# ./benchmarks/bench-rados:370 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/output/rados_bench_4MB_write.log.1
## ./benchmarks/bench-rados:361 - rados_bench() > count_cpus 32-63,96-127
# ./benchmarks/bench-rados:361 - rados_bench() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 60 write --pool bench_rados -b 4194304 -O 4194304 --concurrent-ios 128 --show-time --write-object --write-omap --write-xattr --no-cleanup
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:06:16,038124539-05:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:06:16,040831223-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '9b956ac0-4061-11ec-aba1-c5f3c07a3dc7', '--', 'rados', 'bench', '60', 'write', '--pool', 'bench_rados', '-b', '4194304', '-O', '4194304', '--concurrent-ios', '128', '--show-time', '--write-object', '--write-omap', '--write-xattr', '--no-cleanup'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 9b956ac0-4061-11ec-aba1-c5f3c07a3dc7 -- rados bench 60 write --pool bench_rados -b 4194304 -O 4194304 --concurrent-ios 128 --show-time --write-object --write-omap --write-xattr --no-cleanup
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
hints = 1
2021-11-08T07:06:19.333395+0000 Maintaining 128 concurrent writes of 4194304 bytes to objects of size 4194304 for up to 60 seconds or 0 objects
2021-11-08T07:06:19.333408+0000 Object prefix: benchmark_data_node-0.bluefield2.ucsc-cmps10_7
2021-11-08T07:06:19.456382+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-11-08T07:06:19.456382+0000     0       0         0         0         0         0           -           0
2021-11-08T07:06:20.456471+0000     1      84        84         0         0         0           -           0
2021-11-08T07:06:21.456542+0000     2     127       143        16   31.9983        32      1.7747     1.66475
2021-11-08T07:06:22.456614+0000     3     127       186        59    78.662       172     2.30294     2.04363
2021-11-08T07:06:23.456693+0000     4     127       263       136   135.991       308     2.30604     2.19843
2021-11-08T07:06:24.456760+0000     5     127       282       155   123.992        76     2.49822     2.22447
2021-11-08T07:06:25.456828+0000     6     127       308       181   120.659       104     3.07151     2.33075
2021-11-08T07:06:26.456896+0000     7     127       344       217   123.992       144      3.6112     2.50648
2021-11-08T07:06:27.456970+0000     8     127       399       272   135.991       220     3.57641     2.75865
2021-11-08T07:06:28.457039+0000     9     127       438       311   138.213       156     2.92334      2.8151
2021-11-08T07:06:29.457120+0000    10     127       519       392   156.789       324     2.28594      2.7379
2021-11-08T07:06:30.457192+0000    11     127       537       410   149.081        72     2.45671     2.72389
2021-11-08T07:06:31.457261+0000    12     127       561       434   144.657        96     3.03516     2.73212
2021-11-08T07:06:32.457331+0000    13     127       599       472   145.221       152      3.5692     2.78156
2021-11-08T07:06:33.457399+0000    14     127       632       505   144.276       132      4.0773     2.85232
2021-11-08T07:06:34.457466+0000    15     127       656       529   141.057        96     4.55084     2.92016
2021-11-08T07:06:35.457533+0000    16     127       687       560    139.99       124     4.03683     3.00052
2021-11-08T07:06:36.457604+0000    17     127       768       641   150.813       324     2.87721      3.0567
2021-11-08T07:06:37.457672+0000    18     127       791       664   147.545        92     2.59411     3.04278
2021-11-08T07:06:38.457740+0000    19     127       808       681   143.359        68      3.0219     3.03974
2021-11-08T07:06:39.457807+0000 min lat: 1.55143 max lat: 4.81988 avg lat: 3.05523
2021-11-08T07:06:39.457807+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-11-08T07:06:39.457807+0000    20     127       845       718    143.59       148     3.56032     3.05523
2021-11-08T07:06:40.457879+0000    21     127       885       758   144.371       160     4.13543     3.10002
2021-11-08T07:06:41.457945+0000    22     127       911       784   142.536       104     4.56864     3.14233
2021-11-08T07:06:42.458013+0000    23     127       922       795   138.251        44     4.43896       3.161
2021-11-08T07:06:43.458083+0000    24     127       956       829   138.157       136     4.44309     3.21447
2021-11-08T07:06:44.458151+0000    25     127       991       864   138.231       140     4.40966     3.26626
2021-11-08T07:06:45.458221+0000    26     127      1031       904   139.067       160     4.45047     3.32089
2021-11-08T07:06:46.458289+0000    27     127      1042       915   135.546        44     4.52792     3.33558
2021-11-08T07:06:47.458357+0000    28     127      1066       939   134.134        96     4.49439     3.36714
2021-11-08T07:06:48.458422+0000    29     127      1100       973   134.198       136     4.48817     3.40877
2021-11-08T07:06:49.458491+0000    30     127      1139      1012   134.924       156     4.50255     3.45354
2021-11-08T07:06:50.458562+0000    31     127      1163      1036   133.668        96      4.2142     3.47934
2021-11-08T07:06:51.458637+0000    32     127      1179      1052   131.491        64     4.31383     3.49629
2021-11-08T07:06:52.458708+0000    33     127      1210      1083   131.264       124     4.50965     3.52623
2021-11-08T07:06:53.458775+0000    34     127      1244      1117   131.403       136     4.40379     3.55612
2021-11-08T07:06:54.458845+0000    35     127      1283      1156   132.105       156     4.49149     3.58843
2021-11-08T07:06:55.458917+0000    36     127      1299      1172   130.213        64     4.45052      3.6006
2021-11-08T07:06:56.458986+0000    37     127      1319      1192   128.856        80     4.38514     3.61544
2021-11-08T07:06:57.459056+0000    38     127      1353      1226   129.044       136     4.48282     3.64052
2021-11-08T07:06:58.459128+0000    39     127      1394      1267    129.94       164      4.4415     3.66855
2021-11-08T07:06:59.459200+0000 min lat: 1.55143 max lat: 4.81988 avg lat: 3.68481
2021-11-08T07:06:59.459200+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-11-08T07:06:59.459200+0000    40     127      1419      1292   129.191       100     4.50899     3.68481
2021-11-08T07:07:00.459273+0000    41     127      1433      1306   127.406        56     4.28432     3.69311
2021-11-08T07:07:01.459341+0000    42     127      1464      1337   127.325       124     4.39167     3.71176
2021-11-08T07:07:02.459411+0000    43     127      1502      1375   127.898       152     4.39234     3.73279
2021-11-08T07:07:03.459484+0000    44     127      1541      1414   128.537       156     4.40792      3.7538
2021-11-08T07:07:04.459556+0000    45     127      1554      1427   126.836        52     4.33272     3.76082
2021-11-08T07:07:05.459628+0000    46     127      1573      1446    125.73        76     4.50607     3.77132
2021-11-08T07:07:06.459701+0000    47     127      1610      1483   126.204       148     4.54787     3.79097
2021-11-08T07:07:07.459774+0000    48     127      1648      1521   126.741       152     4.56087     3.81016
2021-11-08T07:07:08.459843+0000    49     127      1676      1549    126.44       112     4.24525     3.82382
2021-11-08T07:07:09.459911+0000    50     127      1686      1559   124.711        40      4.5677     3.82885
2021-11-08T07:07:10.459981+0000    51     127      1716      1589   124.619       120     4.48507     3.84295
2021-11-08T07:07:11.460052+0000    52     127      1754      1627   125.145       152     4.54248     3.85933
2021-11-08T07:07:12.460121+0000    53     127      1790      1663   125.501       144     4.55667     3.87422
2021-11-08T07:07:13.460196+0000    54     127      1810      1683   124.658        80     4.45184     3.88183
2021-11-08T07:07:14.460271+0000    55     127      1826      1699   123.555        64     4.41111     3.88692
2021-11-08T07:07:15.460339+0000    56     127      1865      1738   124.134       156      4.4687     3.90057
2021-11-08T07:07:16.460410+0000    57     127      1898      1771   124.272       132     4.50779     3.91182
2021-11-08T07:07:17.460478+0000    58     127      1932      1805   124.474       136     4.33337     3.92275
2021-11-08T07:07:18.460546+0000    59     127      1948      1821   123.449        64     4.24639     3.92722
2021-11-08T07:07:19.460618+0000 min lat: 1.55143 max lat: 4.99027 avg lat: 3.93513
2021-11-08T07:07:19.460618+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-11-08T07:07:19.460618+0000    60     127      1976      1849   123.258       112     4.46908     3.93513
2021-11-08T07:07:20.460728+0000 Total time run:         60.233
Total writes made:      1977
Write size:             4194304
Object size:            4194304
Bandwidth (MB/sec):     131.29
Stddev Bandwidth:       62.2798
Max bandwidth (MB/sec): 324
Min bandwidth (MB/sec): 0
Average IOPS:           32
Stddev IOPS:            15.57
Max IOPS:               81
Min IOPS:               0
Average Latency(s):     3.84526
Stddev Latency(s):      0.938839
Max latency(s):         4.99027
Min latency(s):         0.199872

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:07:21,156846906-05:00] INFO: > Stop system activity collection services[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:07:21,162680755-05:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:391 - rados_bench() > kill -INT 233997

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:07:21,168702478-05:00] INFO: >> for OSD host[0m
# ./benchmarks/bench-rados:396 - rados_bench() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.2 kill -INT 64037
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:07:21,176205344-05:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.2 kill -INT 64037
[1] 02:07:21 [SUCCESS] ljishen@10.10.2.2

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:07:21,360669371-05:00] INFO: >>> Copy system activity file from OSD host[0m
## ./benchmarks/bench-rados:403 - rados_bench() > basename -- /tmp/bench-rados/sys_activity_4MB_write.dat.osd_host.1
# ./benchmarks/bench-rados:403 - rados_bench() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.2:/tmp/bench-rados/sys_activity_4MB_write.dat.osd_host.1 /mnt/sda8/ceph-research/scripts/output/sys_activity_4MB_write.dat.osd_host.1


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:07:21,530660410-05:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:07:45,590584427-05:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 1.98k objects, 7.7 GiB
    usage:   23 GiB used, 77 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:07:45,598180529-05:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:07:54,867086329-05:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 1.98k objects, 7.7 GiB
    usage:   23 GiB used, 77 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:07:54,874652724-05:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:08:03,974257629-05:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 1.98k objects, 7.7 GiB
    usage:   23 GiB used, 77 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:08:03,982024192-05:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:08:13,059298779-05:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 1.98k objects, 7.7 GiB
    usage:   23 GiB used, 77 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:08:13,066978208-05:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:08:22,235834256-05:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 1.98k objects, 7.7 GiB
    usage:   23 GiB used, 77 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:08:22,243462168-05:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:08:22,249635707-05:00] INFO: > The cluster is idle now.[0m


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:08:22,253251454-05:00] STAGE: Start benchmarking of mode seq ...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:08:22,259637474-05:00] INFO: > Start system activity collection services[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:08:22,265265966-05:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:341 - rados_bench() > rm --force /mnt/sda8/ceph-research/scripts/output/sys_activity_4MB_seq.dat.rados_bench_host.1
# ./benchmarks/bench-rados:342 - rados_bench() > sar_pid_rados_bench_host=235369
# ./benchmarks/bench-rados:342 - rados_bench() > S_TIME_FORMAT=ISO
# ./benchmarks/bench-rados:342 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/output/sys_activity_4MB_seq.dat.rados_bench_host.1 2

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:08:22,271993450-05:00] INFO: >> for OSD host[0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:08:22,280852353-05:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.2 bash
10.10.2.2: b'64134\n'
[1] 02:08:22 [SUCCESS] ljishen@10.10.2.2
64134

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:08:22,465858973-05:00] INFO: > Run rados bench[0m
# ./benchmarks/bench-rados:379 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/output/rados_bench_4MB_seq.log.1
## ./benchmarks/bench-rados:376 - rados_bench() > count_cpus 32-63,96-127
# ./benchmarks/bench-rados:376 - rados_bench() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 128 --show-time
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:08:22,484859607-05:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:08:22,487484536-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '9b956ac0-4061-11ec-aba1-c5f3c07a3dc7', '--', 'rados', 'bench', '99999999', 'seq', '--pool', 'bench_rados', '--concurrent-ios', '128', '--show-time'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 9b956ac0-4061-11ec-aba1-c5f3c07a3dc7 -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 128 --show-time
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
hints = 1
2021-11-08T07:08:25.756554+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-11-08T07:08:25.756554+0000     0       0         0         0         0         0           -           0
2021-11-08T07:08:26.756647+0000     1     127       368       241    963.84       964    0.387246    0.366268
2021-11-08T07:08:27.756725+0000     2     127       692       565   1129.86      1296    0.389424    0.382289
2021-11-08T07:08:28.756804+0000     3     127      1018       891   1187.87      1304    0.387947      0.3858
2021-11-08T07:08:29.756877+0000     4     127      1350      1223   1222.88      1328    0.387172    0.385765
2021-11-08T07:08:30.756963+0000     5     127      1677      1550   1239.88      1308    0.391754    0.386665
2021-11-08T07:08:31.757072+0000 Total time run:       5.99465
Total reads made:     1977
Read size:            4194304
Object size:          4194304
Bandwidth (MB/sec):   1319.18
Average IOPS:         329
Stddev IOPS:          38.6846
Max IOPS:             332
Min IOPS:             241
Average Latency(s):   0.376644
Max latency(s):       0.404094
Min latency(s):       0.0772841

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:08:32,376693056-05:00] INFO: > Stop system activity collection services[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:08:32,382486709-05:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:391 - rados_bench() > kill -INT 235369

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:08:32,388262489-05:00] INFO: >> for OSD host[0m
# ./benchmarks/bench-rados:396 - rados_bench() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.2 kill -INT 64134
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:08:32,395930206-05:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.2 kill -INT 64134
[1] 02:08:32 [SUCCESS] ljishen@10.10.2.2

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:08:32,576043259-05:00] INFO: >>> Copy system activity file from OSD host[0m
## ./benchmarks/bench-rados:403 - rados_bench() > basename -- /tmp/bench-rados/sys_activity_4MB_seq.dat.osd_host.1
# ./benchmarks/bench-rados:403 - rados_bench() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.2:/tmp/bench-rados/sys_activity_4MB_seq.dat.osd_host.1 /mnt/sda8/ceph-research/scripts/output/sys_activity_4MB_seq.dat.osd_host.1


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:08:32,724975135-05:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:08:56,748546751-05:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 1.98k objects, 7.7 GiB
    usage:   23 GiB used, 77 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:08:56,755899764-05:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:09:05,907604293-05:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 1.98k objects, 7.7 GiB
    usage:   23 GiB used, 77 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:09:05,915193712-05:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:09:15,060575994-05:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 1.98k objects, 7.7 GiB
    usage:   23 GiB used, 77 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:09:15,068403653-05:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:09:24,043735967-05:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 1.98k objects, 7.7 GiB
    usage:   23 GiB used, 77 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:09:24,051521095-05:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:09:33,327700188-05:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 1.98k objects, 7.7 GiB
    usage:   23 GiB used, 77 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:09:33,335528808-05:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:09:33,341912995-05:00] INFO: > The cluster is idle now.[0m


[1;7;39;49m[2021-11-08T02:09:33,344327577-05:00][RUNNING][ROUND 2/6/21] object_size=4MB[0m


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:09:33,348087797-05:00] STAGE: Launch a Ceph cluster (CEPH_HOST: 10.10.2.3, OSD_HOST: 10.10.2.2) ...[0m
# ./benchmarks/bench-rados:183 - launch_ceph_cluster() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.3 bash -euo pipefail
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:09:33,357090841-05:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.3 bash -euo pipefail
10.10.2.3: b'\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T02:09:33,776708807-05:00] INFO: > Remove existing clusters\x1b[0m\n'
10.10.2.3: b'## bash:17 -  > basename -- /var/lib/ceph/9b956ac0-4061-11ec-aba1-c5f3c07a3dc7\n'
10.10.2.3: b'# bash:17 -  > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm --host 10.10.2.3 --host 10.10.2.2 rm-cluster --force --fsid 9b956ac0-4061-11ec-aba1-c5f3c07a3dc7\n'
10.10.2.3: b"\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T02:09:33,787884223-05:00] DEBUG: HOST_PARAMS: ['--host', '10.10.2.3', '--host', '10.10.2.2']\x1b[0m\n"
10.10.2.3: b"\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T02:09:33,791872558-05:00] DEBUG: CEPHADM_PARAMS: ['rm-cluster', '--force', '--fsid', '9b956ac0-4061-11ec-aba1-c5f3c07a3dc7']\x1b[0m\n"
10.10.2.3: b"# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:63 -  > /tmp/bench-rados/ceph-research/scripts/parallel-ssh --host 10.10.2.3 --host 10.10.2.2 'sudo python3 -u - rm-cluster --force --fsid 9b956ac0-4061-11ec-aba1-c5f3c07a3dc7'\n"
10.10.2.3: b'\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T02:09:33,800632166-05:00] DEBUG: command from stdin\x1b[0m\n'
10.10.2.3: b"# /tmp/bench-rados/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host 10.10.2.3 --host 10.10.2.2 'sudo python3 -u - rm-cluster --force --fsid 9b956ac0-4061-11ec-aba1-c5f3c07a3dc7'\n"
10.10.2.3: b'[1] 02:09:40 [SUCCESS] 10.10.2.3\n[2] 02:09:45 [SUCCESS] 10.10.2.2\n'
10.10.2.3: b'\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T02:09:45,942563805-05:00] INFO: > Deploy a new cluster\x1b[0m\n'
10.10.2.3: b'# bash:24 -  > /tmp/bench-rados/ceph-research/scripts/ceph-deploy -m 10.10.2.3 -o node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/10.10.2.2:/dev/nvme0n1\n'
10.10.2.3: b'\n\n\x1b[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T02:09:45,954266385-05:00] STAGE: Verify passwordless SSH to hosts, passwordless sudo via ljishen, and the existence of devices on hosts...\x1b[0m\n'
10.10.2.3: b'\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T02:09:45,959727860-05:00] INFO: > Check the host for the monitor\x1b[0m\n'
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:181 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@10.10.2.3 sudo --non-interactive --validate\n'
10.10.2.3: b'\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T02:09:46,110546112-05:00] INFO: > Check hosts for OSDs\x1b[0m\n'
10.10.2.3: b"\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T02:09:46,114951705-05:00] INFO: >> Check host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'\x1b[0m\n"
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:200 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us sudo --non-interactive --validate\n'
10.10.2.3: b"\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T02:09:46,261939731-05:00] INFO: >>> Check device '/dev/nvme0n1'\x1b[0m\n"
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:215 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us test -b /dev/nvme0n1\n'
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:226 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us lsblk --nodeps --noheadings --output TYPE /dev/nvme0n1\n'
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:227 - verify_parameters() > grep --word-regexp disk\n'
10.10.2.3: b'\n\n\x1b[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T02:09:46,542535227-05:00] STAGE: Reset devices on hosts...\x1b[0m\n'
10.10.2.3: b"\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T02:09:46,547865354-05:00] INFO: > Resetting device '/dev/nvme0n1' on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'\x1b[0m\n"
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:288 - reset_host_devices() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us sudo sh\n'
10.10.2.3: b'  Removing ceph--90aa1033--d241--4020--ab6a--9ee35903d6d4-osd--block--3831601f--d76c--49ba--8ba8--089de99f8166 (253:0)\n'
10.10.2.3: b'  Archiving volume group "ceph-90aa1033-d241-4020-ab6a-9ee35903d6d4" metadata (seqno 5).\n'
10.10.2.3: b'  Releasing logical volume "osd-block-3831601f-d76c-49ba-8ba8-089de99f8166"\n'
10.10.2.3: b'  Creating volume group backup "/etc/lvm/backup/ceph-90aa1033-d241-4020-ab6a-9ee35903d6d4" (seqno 6).\n'
10.10.2.3: b'  Logical volume "osd-block-3831601f-d76c-49ba-8ba8-089de99f8166" successfully removed\n'
10.10.2.3: b'  Removing physical volume "/dev/nvme0n1" from volume group "ceph-90aa1033-d241-4020-ab6a-9ee35903d6d4"\n'
10.10.2.3: b'  Volume group "ceph-90aa1033-d241-4020-ab6a-9ee35903d6d4" successfully removed\n'
10.10.2.3: b'\n\n\x1b[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T02:09:46,839680198-05:00] STAGE: Bootstrap a new cluster...\x1b[0m\n'
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:317 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.3 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults\n'
10.10.2.3: b'\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T02:09:46,848823670-05:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.3: b"\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T02:09:46,852216702-05:00] DEBUG: CEPHADM_PARAMS: ['bootstrap', '--config', '/tmp/bench-rados/ceph-research/scripts/ceph.conf', '--mon-ip', '10.10.2.3', '--output-dir', '/tmp/bench-rados/deployment_data_root/etc/ceph', '--ssh-user', 'ljishen', '--skip-dashboard', '--no-minimize-config', '--allow-overwrite', '--allow-fqdn-hostname', '--orphan-initial-daemons', '--skip-monitoring-stack', '--single-host-defaults']\x1b[0m\n"
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.3 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults\n'
10.10.2.3: b'Verifying podman|docker is present...\nVerifying lvm2 is present...\n'
10.10.2.3: b'Verifying time synchronization is in place...\n'
10.10.2.3: b'Unit ntp.service is enabled and running\n'
10.10.2.3: b'Repeating the final host check...\ndocker (/usr/bin/docker) is present\n'
10.10.2.3: b'systemctl is present\n'
10.10.2.3: b'lvcreate is present\n'
10.10.2.3: b'Unit ntp.service is enabled and running\n'
10.10.2.3: b'Host looks OK\n'
10.10.2.3: b'Cluster fsid: db39f1ae-4062-11ec-aba1-c5f3c07a3dc7\n'
10.10.2.3: b'Verifying IP 10.10.2.3 port 3300 ...\n'
10.10.2.3: b'Verifying IP 10.10.2.3 port 6789 ...\n'
10.10.2.3: b'Mon IP `10.10.2.3` is in CIDR network `10.10.2.0/24`\n'
10.10.2.3: b'- internal network (--cluster-network) has not been provided, OSD replication will default to the public_network\n'
10.10.2.3: b'Adjusting default settings to suit single-host cluster...\n'
10.10.2.3: b'Pulling container image quay.io/ceph/ceph:v16...\n'
10.10.2.3: b'Ceph version: ceph version 16.2.6 (ee28fb57e47e9f88813e24bbf4c14496ca299d31) pacific (stable)\n'
10.10.2.3: b'Extracting ceph user uid/gid from container image...\n'
10.10.2.3: b'Creating initial keys...\n'
10.10.2.3: b'Creating initial monmap...\n'
10.10.2.3: b'Creating mon...\n'
10.10.2.3: b'Waiting for mon to start...\n'
10.10.2.3: b'Waiting for mon...\n'
10.10.2.3: b'mon is available\nSetting mon public_network to 10.10.2.0/24\n'
10.10.2.3: b'Wrote config to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf\n'
10.10.2.3: b'Wrote keyring to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring\n'
10.10.2.3: b'Creating mgr...\n'
10.10.2.3: b'Verifying port 9283 ...\n'
10.10.2.3: b'Waiting for mgr to start...\nWaiting for mgr...\n'
10.10.2.3: b'mgr not available, waiting (1/15)...\n'
10.10.2.3: b'mgr not available, waiting (2/15)...\n'
10.10.2.3: b'mgr is available\n'
10.10.2.3: b'Enabling cephadm module...\n'
10.10.2.3: b'Waiting for the mgr to restart...\n'
10.10.2.3: b'Waiting for mgr epoch 5...\n'
10.10.2.3: b'mgr epoch 5 is available\nSetting orchestrator backend to cephadm...\n'
10.10.2.3: b'Generating ssh key...\n'
10.10.2.3: b'Wrote public SSH key to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub\n'
10.10.2.3: b'Adding key to ljishen@localhost authorized_keys...\n'
10.10.2.3: b'Adding host node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us...\n'
10.10.2.3: b'Deploying unmanaged mon service...\n'
10.10.2.3: b'Deploying unmanaged mgr service...\n'
10.10.2.3: b'You can access the Ceph CLI with:\n\n\tsudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --fsid db39f1ae-4062-11ec-aba1-c5f3c07a3dc7 -c /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf -k /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring\n\n'
10.10.2.3: b'Please consider enabling telemetry to help improve Ceph:\n\n\tceph telemetry on\n\nFor more information see:\n\n\thttps://docs.ceph.com/docs/pacific/mgr/telemetry/\n\nBootstrap complete.\n'
10.10.2.3: b'\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T02:10:53,428592763-05:00] INFO: Sleep for 20 seconds\x1b[0m\n'
10.10.2.3: b'\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T02:11:13,436077607-05:00] INFO: > Set the default number of monitor daemons to 1\x1b[0m\n# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:337 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1\n'
10.10.2.3: b'\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T02:11:13,446009707-05:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.3: b"\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T02:11:13,450154167-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mon', '1']\x1b[0m\n# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1\n"
10.10.2.3: b'Inferring fsid db39f1ae-4062-11ec-aba1-c5f3c07a3dc7\n'
10.10.2.3: b'Inferring config /var/lib/ceph/db39f1ae-4062-11ec-aba1-c5f3c07a3dc7/mon.node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.3: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.3: b'Scheduled mon update...\n'
10.10.2.3: b'\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T02:11:23,600391941-05:00] INFO: > Set the default number of manager daemons to 1\x1b[0m\n'
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:342 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1\n'
10.10.2.3: b'\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T02:11:23,610110688-05:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.3: b"\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T02:11:23,613980120-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mgr', '1']\x1b[0m\n"
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1\n'
10.10.2.3: b'Inferring fsid db39f1ae-4062-11ec-aba1-c5f3c07a3dc7\n'
10.10.2.3: b'Inferring config /var/lib/ceph/db39f1ae-4062-11ec-aba1-c5f3c07a3dc7/mon.node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.3: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.3: b'Scheduled mgr update...\n'
10.10.2.3: b'\n\n\x1b[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T02:11:33,706978678-05:00] STAGE: Add hosts to the cluster...\x1b[0m\n'
10.10.2.3: b"\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T02:11:33,713342095-05:00] INFO: > Adding host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'\x1b[0m\n"
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:361 - add_hosts() > ssh-copy-id -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -f -i /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\n'
10.10.2.3: b'/usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: "/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub"\n'
10.10.2.3: b'\nNumber of key(s) added: 1\n\nNow try logging into the machine, with:   "ssh -o \'GlobalKnownHostsFile=/dev/null\' -o \'LogLevel=ERROR\' -o \'PasswordAuthentication=no\' -o \'StrictHostKeyChecking=no\' -o \'UserKnownHostsFile=/dev/null\' \'ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\'"\nand check to make sure that only the key(s) you wanted were added.\n\n'
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:367 - add_hosts() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us 10.10.2.2\n'
10.10.2.3: b'\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T02:11:33,920067684-05:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.3: b"\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T02:11:33,923722911-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'host', 'add', 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us', '10.10.2.2']\x1b[0m\n"
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us 10.10.2.2\n'
10.10.2.3: b'Inferring fsid db39f1ae-4062-11ec-aba1-c5f3c07a3dc7\n'
10.10.2.3: b'Inferring config /var/lib/ceph/db39f1ae-4062-11ec-aba1-c5f3c07a3dc7/mon.node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.3: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.3: b"Added host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us' with addr '10.10.2.2'\n"
10.10.2.3: b'\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T02:11:44,427253574-05:00] INFO: Sleep for 20 seconds\x1b[0m\n'
10.10.2.3: b'\n\n\x1b[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T02:12:04,432267981-05:00] STAGE: Add OSDs to the cluster...\x1b[0m\n'
10.10.2.3: b"\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T02:12:04,439070987-05:00] INFO: > Adding devices ['/dev/nvme0n1'] on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us' as OSDs\x1b[0m\n"
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:388 - add_host_devices_as_osds() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1\n'
10.10.2.3: b'\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T02:12:04,448322332-05:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.3: b"\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T02:12:04,452180432-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'daemon', 'add', 'osd', 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1']\x1b[0m\n"
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1\n'
10.10.2.3: b'Inferring fsid db39f1ae-4062-11ec-aba1-c5f3c07a3dc7\n'
10.10.2.3: b'Inferring config /var/lib/ceph/db39f1ae-4062-11ec-aba1-c5f3c07a3dc7/mon.node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.3: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.3: b"Created osd(s) 0 on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'\n"
10.10.2.3: b'\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T02:12:30,008649089-05:00] INFO: > Sleep for 20 seconds\x1b[0m\n'
10.10.2.3: b'\n\n\x1b[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T02:12:50,014230503-05:00] STAGE: Show Cluster Status\x1b[0m\n'
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:408 - show_status() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status\n'
10.10.2.3: b'\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T02:12:50,024619485-05:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.3: b"\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T02:12:50,028267137-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', '--status']\x1b[0m\n"
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status\n'
10.10.2.3: b'Inferring fsid db39f1ae-4062-11ec-aba1-c5f3c07a3dc7\n'
10.10.2.3: b'Inferring config /var/lib/ceph/db39f1ae-4062-11ec-aba1-c5f3c07a3dc7/mon.node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.3: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.3: b'  cluster:\n    id:     db39f1ae-4062-11ec-aba1-c5f3c07a3dc7\n    health: HEALTH_WARN\n            1 pool(s) have no replicas configured\n \n  services:\n    mon: 1 daemons, quorum node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us (age 2m)\n    mgr: node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us.jyrhvz(active, since 2m)\n    osd: 1 osds: 1 up (since 19s), 1 in (since 41s)\n \n  data:\n    pools:   1 pools, 1 pgs\n    objects: 0 objects, 0 B\n    usage:   4.7 MiB used, 100 GiB / 100 GiB avail\n    pgs:     1 active+clean\n \n'
[1] 02:12:59 [SUCCESS] ljishen@10.10.2.3

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:09:33,776708807-05:00] INFO: > Remove existing clusters[0m
## bash:17 -  > basename -- /var/lib/ceph/9b956ac0-4061-11ec-aba1-c5f3c07a3dc7
# bash:17 -  > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm --host 10.10.2.3 --host 10.10.2.2 rm-cluster --force --fsid 9b956ac0-4061-11ec-aba1-c5f3c07a3dc7
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:09:33,787884223-05:00] DEBUG: HOST_PARAMS: ['--host', '10.10.2.3', '--host', '10.10.2.2'][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:09:33,791872558-05:00] DEBUG: CEPHADM_PARAMS: ['rm-cluster', '--force', '--fsid', '9b956ac0-4061-11ec-aba1-c5f3c07a3dc7'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:63 -  > /tmp/bench-rados/ceph-research/scripts/parallel-ssh --host 10.10.2.3 --host 10.10.2.2 'sudo python3 -u - rm-cluster --force --fsid 9b956ac0-4061-11ec-aba1-c5f3c07a3dc7'
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:09:33,800632166-05:00] DEBUG: command from stdin[0m
# /tmp/bench-rados/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host 10.10.2.3 --host 10.10.2.2 'sudo python3 -u - rm-cluster --force --fsid 9b956ac0-4061-11ec-aba1-c5f3c07a3dc7'
[1] 02:09:40 [SUCCESS] 10.10.2.3
[2] 02:09:45 [SUCCESS] 10.10.2.2

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:09:45,942563805-05:00] INFO: > Deploy a new cluster[0m
# bash:24 -  > /tmp/bench-rados/ceph-research/scripts/ceph-deploy -m 10.10.2.3 -o node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/10.10.2.2:/dev/nvme0n1


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:09:45,954266385-05:00] STAGE: Verify passwordless SSH to hosts, passwordless sudo via ljishen, and the existence of devices on hosts...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:09:45,959727860-05:00] INFO: > Check the host for the monitor[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:181 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@10.10.2.3 sudo --non-interactive --validate

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:09:46,110546112-05:00] INFO: > Check hosts for OSDs[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:09:46,114951705-05:00] INFO: >> Check host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:200 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us sudo --non-interactive --validate

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:09:46,261939731-05:00] INFO: >>> Check device '/dev/nvme0n1'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:215 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us test -b /dev/nvme0n1
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:226 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us lsblk --nodeps --noheadings --output TYPE /dev/nvme0n1
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:227 - verify_parameters() > grep --word-regexp disk


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:09:46,542535227-05:00] STAGE: Reset devices on hosts...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:09:46,547865354-05:00] INFO: > Resetting device '/dev/nvme0n1' on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:288 - reset_host_devices() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us sudo sh
  Removing ceph--90aa1033--d241--4020--ab6a--9ee35903d6d4-osd--block--3831601f--d76c--49ba--8ba8--089de99f8166 (253:0)
  Archiving volume group "ceph-90aa1033-d241-4020-ab6a-9ee35903d6d4" metadata (seqno 5).
  Releasing logical volume "osd-block-3831601f-d76c-49ba-8ba8-089de99f8166"
  Creating volume group backup "/etc/lvm/backup/ceph-90aa1033-d241-4020-ab6a-9ee35903d6d4" (seqno 6).
  Logical volume "osd-block-3831601f-d76c-49ba-8ba8-089de99f8166" successfully removed
  Removing physical volume "/dev/nvme0n1" from volume group "ceph-90aa1033-d241-4020-ab6a-9ee35903d6d4"
  Volume group "ceph-90aa1033-d241-4020-ab6a-9ee35903d6d4" successfully removed


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:09:46,839680198-05:00] STAGE: Bootstrap a new cluster...[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:317 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.3 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:09:46,848823670-05:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:09:46,852216702-05:00] DEBUG: CEPHADM_PARAMS: ['bootstrap', '--config', '/tmp/bench-rados/ceph-research/scripts/ceph.conf', '--mon-ip', '10.10.2.3', '--output-dir', '/tmp/bench-rados/deployment_data_root/etc/ceph', '--ssh-user', 'ljishen', '--skip-dashboard', '--no-minimize-config', '--allow-overwrite', '--allow-fqdn-hostname', '--orphan-initial-daemons', '--skip-monitoring-stack', '--single-host-defaults'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.3 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults
Verifying podman|docker is present...
Verifying lvm2 is present...
Verifying time synchronization is in place...
Unit ntp.service is enabled and running
Repeating the final host check...
docker (/usr/bin/docker) is present
systemctl is present
lvcreate is present
Unit ntp.service is enabled and running
Host looks OK
Cluster fsid: db39f1ae-4062-11ec-aba1-c5f3c07a3dc7
Verifying IP 10.10.2.3 port 3300 ...
Verifying IP 10.10.2.3 port 6789 ...
Mon IP `10.10.2.3` is in CIDR network `10.10.2.0/24`
- internal network (--cluster-network) has not been provided, OSD replication will default to the public_network
Adjusting default settings to suit single-host cluster...
Pulling container image quay.io/ceph/ceph:v16...
Ceph version: ceph version 16.2.6 (ee28fb57e47e9f88813e24bbf4c14496ca299d31) pacific (stable)
Extracting ceph user uid/gid from container image...
Creating initial keys...
Creating initial monmap...
Creating mon...
Waiting for mon to start...
Waiting for mon...
mon is available
Setting mon public_network to 10.10.2.0/24
Wrote config to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf
Wrote keyring to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring
Creating mgr...
Verifying port 9283 ...
Waiting for mgr to start...
Waiting for mgr...
mgr not available, waiting (1/15)...
mgr not available, waiting (2/15)...
mgr is available
Enabling cephadm module...
Waiting for the mgr to restart...
Waiting for mgr epoch 5...
mgr epoch 5 is available
Setting orchestrator backend to cephadm...
Generating ssh key...
Wrote public SSH key to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub
Adding key to ljishen@localhost authorized_keys...
Adding host node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us...
Deploying unmanaged mon service...
Deploying unmanaged mgr service...
You can access the Ceph CLI with:

	sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --fsid db39f1ae-4062-11ec-aba1-c5f3c07a3dc7 -c /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf -k /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring

Please consider enabling telemetry to help improve Ceph:

	ceph telemetry on

For more information see:

	https://docs.ceph.com/docs/pacific/mgr/telemetry/

Bootstrap complete.

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:10:53,428592763-05:00] INFO: Sleep for 20 seconds[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:11:13,436077607-05:00] INFO: > Set the default number of monitor daemons to 1[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:337 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:11:13,446009707-05:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:11:13,450154167-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mon', '1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1
Inferring fsid db39f1ae-4062-11ec-aba1-c5f3c07a3dc7
Inferring config /var/lib/ceph/db39f1ae-4062-11ec-aba1-c5f3c07a3dc7/mon.node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Scheduled mon update...

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:11:23,600391941-05:00] INFO: > Set the default number of manager daemons to 1[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:342 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:11:23,610110688-05:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:11:23,613980120-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mgr', '1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1
Inferring fsid db39f1ae-4062-11ec-aba1-c5f3c07a3dc7
Inferring config /var/lib/ceph/db39f1ae-4062-11ec-aba1-c5f3c07a3dc7/mon.node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Scheduled mgr update...


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:11:33,706978678-05:00] STAGE: Add hosts to the cluster...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:11:33,713342095-05:00] INFO: > Adding host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:361 - add_hosts() > ssh-copy-id -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -f -i /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us
/usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: "/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub"

Number of key(s) added: 1

Now try logging into the machine, with:   "ssh -o 'GlobalKnownHostsFile=/dev/null' -o 'LogLevel=ERROR' -o 'PasswordAuthentication=no' -o 'StrictHostKeyChecking=no' -o 'UserKnownHostsFile=/dev/null' 'ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'"
and check to make sure that only the key(s) you wanted were added.

# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:367 - add_hosts() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us 10.10.2.2
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:11:33,920067684-05:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:11:33,923722911-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'host', 'add', 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us', '10.10.2.2'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us 10.10.2.2
Inferring fsid db39f1ae-4062-11ec-aba1-c5f3c07a3dc7
Inferring config /var/lib/ceph/db39f1ae-4062-11ec-aba1-c5f3c07a3dc7/mon.node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Added host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us' with addr '10.10.2.2'

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:11:44,427253574-05:00] INFO: Sleep for 20 seconds[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:12:04,432267981-05:00] STAGE: Add OSDs to the cluster...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:12:04,439070987-05:00] INFO: > Adding devices ['/dev/nvme0n1'] on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us' as OSDs[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:388 - add_host_devices_as_osds() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:12:04,448322332-05:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:12:04,452180432-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'daemon', 'add', 'osd', 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1
Inferring fsid db39f1ae-4062-11ec-aba1-c5f3c07a3dc7
Inferring config /var/lib/ceph/db39f1ae-4062-11ec-aba1-c5f3c07a3dc7/mon.node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Created osd(s) 0 on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:12:30,008649089-05:00] INFO: > Sleep for 20 seconds[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:12:50,014230503-05:00] STAGE: Show Cluster Status[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:408 - show_status() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:12:50,024619485-05:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:12:50,028267137-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', '--status'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status
Inferring fsid db39f1ae-4062-11ec-aba1-c5f3c07a3dc7
Inferring config /var/lib/ceph/db39f1ae-4062-11ec-aba1-c5f3c07a3dc7/mon.node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
  cluster:
    id:     db39f1ae-4062-11ec-aba1-c5f3c07a3dc7
    health: HEALTH_WARN
            1 pool(s) have no replicas configured
 
  services:
    mon: 1 daemons, quorum node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us (age 2m)
    mgr: node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us.jyrhvz(active, since 2m)
    osd: 1 osds: 1 up (since 19s), 1 in (since 41s)
 
  data:
    pools:   1 pools, 1 pgs
    objects: 0 objects, 0 B
    usage:   4.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     1 active+clean
 

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:12:59,842276403-05:00] INFO: > Copy configuration files to local[0m
# ./benchmarks/bench-rados:220 - launch_ceph_cluster() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.3 sudo chown --changes --recursive ljishen /tmp/bench-rados/deployment_data_root/etc/ceph
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:12:59,849821007-05:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.3 sudo chown --changes --recursive ljishen /tmp/bench-rados/deployment_data_root/etc/ceph
[1] 02:13:00 [SUCCESS] ljishen@10.10.2.3
# ./benchmarks/bench-rados:223 - launch_ceph_cluster() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.3:/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf
# ./benchmarks/bench-rados:226 - launch_ceph_cluster() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.3:/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:13:00,323775602-05:00] STAGE: Update local ceph.conf as a client...[0m


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:13:00,327670775-05:00] STAGE: Configure Ceph pool...[0m
# ./benchmarks/bench-rados:257 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_max_backfills 32
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:13:00,348727482-05:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:13:00,351311954-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'db39f1ae-4062-11ec-aba1-c5f3c07a3dc7', '--', 'ceph', 'config', 'set', 'osd', 'osd_max_backfills', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid db39f1ae-4062-11ec-aba1-c5f3c07a3dc7 -- ceph config set osd osd_max_backfills 32
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:258 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_active 32
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:13:04,565541573-05:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:13:04,568462660-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'db39f1ae-4062-11ec-aba1-c5f3c07a3dc7', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_active', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid db39f1ae-4062-11ec-aba1-c5f3c07a3dc7 -- ceph config set osd osd_recovery_max_active 32
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:259 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_single_start 8
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:13:08,901809543-05:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:13:08,904442918-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'db39f1ae-4062-11ec-aba1-c5f3c07a3dc7', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_single_start', '8'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid db39f1ae-4062-11ec-aba1-c5f3c07a3dc7 -- ceph config set osd osd_recovery_max_single_start 8
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:260 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_op_priority 63
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:13:12,963113864-05:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:13:12,965736689-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'db39f1ae-4062-11ec-aba1-c5f3c07a3dc7', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_op_priority', '63'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid db39f1ae-4062-11ec-aba1-c5f3c07a3dc7 -- ceph config set osd osd_recovery_op_priority 63
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:262 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config show-with-defaults osd.0
# ./benchmarks/bench-rados:263 - configure_ceph_pool() > grep 'osd_max_backfills\|osd_recovery'
# ./benchmarks/bench-rados:263 - configure_ceph_pool() > column -t -s ' '
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
osd_max_backfills                        32         mon
osd_recovery_cost                        20971520   default
osd_recovery_delay_start                 0.000000   default
osd_recovery_max_active                  32         mon
osd_recovery_max_active_hdd              3          default
osd_recovery_max_active_ssd              10         default
osd_recovery_max_chunk                   8388608    default
osd_recovery_max_omap_entries_per_chunk  8096       default
osd_recovery_max_single_start            8          mon
osd_recovery_op_priority                 63         mon
osd_recovery_op_warn_multiple            16         default
osd_recovery_priority                    5          default
osd_recovery_retry_interval              30.000000  default
osd_recovery_sleep                       0.000000   default
osd_recovery_sleep_hdd                   0.100000   default
osd_recovery_sleep_hybrid                0.025000   default
osd_recovery_sleep_ssd                   0.000000   default
# ./benchmarks/bench-rados:265 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool create bench_rados 128 128 replicated --size 1 --pg-num-min 128
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:13:21,369664034-05:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:13:21,372490012-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'db39f1ae-4062-11ec-aba1-c5f3c07a3dc7', '--', 'ceph', 'osd', 'pool', 'create', 'bench_rados', '128', '128', 'replicated', '--size', '1', '--pg-num-min', '128'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid db39f1ae-4062-11ec-aba1-c5f3c07a3dc7 -- ceph osd pool create bench_rados 128 128 replicated --size 1 --pg-num-min 128
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
pool 'bench_rados' created
# ./benchmarks/bench-rados:267 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados min_size 1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:13:26,128061562-05:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:13:26,130914220-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'db39f1ae-4062-11ec-aba1-c5f3c07a3dc7', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'min_size', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid db39f1ae-4062-11ec-aba1-c5f3c07a3dc7 -- ceph osd pool set bench_rados min_size 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 min_size to 1
# ./benchmarks/bench-rados:268 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados pg_autoscale_mode off
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:13:31,698252813-05:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:13:31,701103788-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'db39f1ae-4062-11ec-aba1-c5f3c07a3dc7', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'pg_autoscale_mode', 'off'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid db39f1ae-4062-11ec-aba1-c5f3c07a3dc7 -- ceph osd pool set bench_rados pg_autoscale_mode off
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 pg_autoscale_mode to off
# ./benchmarks/bench-rados:269 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool application enable bench_rados benchmark
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:13:36,189394694-05:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:13:36,192224970-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'db39f1ae-4062-11ec-aba1-c5f3c07a3dc7', '--', 'ceph', 'osd', 'pool', 'application', 'enable', 'bench_rados', 'benchmark'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid db39f1ae-4062-11ec-aba1-c5f3c07a3dc7 -- ceph osd pool application enable bench_rados benchmark
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
enabled application 'benchmark' on pool 'bench_rados'
# ./benchmarks/bench-rados:270 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados noscrub 1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:13:41,517463230-05:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:13:41,520208315-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'db39f1ae-4062-11ec-aba1-c5f3c07a3dc7', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'noscrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid db39f1ae-4062-11ec-aba1-c5f3c07a3dc7 -- ceph osd pool set bench_rados noscrub 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 noscrub to 1
# ./benchmarks/bench-rados:271 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados nodeep-scrub 1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:13:46,113255043-05:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:13:46,116020136-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'db39f1ae-4062-11ec-aba1-c5f3c07a3dc7', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'nodeep-scrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid db39f1ae-4062-11ec-aba1-c5f3c07a3dc7 -- ceph osd pool set bench_rados nodeep-scrub 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 nodeep-scrub to 1
# ./benchmarks/bench-rados:272 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool ls detail
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:13:50,734047820-05:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:13:50,736988755-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'db39f1ae-4062-11ec-aba1-c5f3c07a3dc7', '--', 'ceph', 'osd', 'pool', 'ls', 'detail'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid db39f1ae-4062-11ec-aba1-c5f3c07a3dc7 -- ceph osd pool ls detail
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
pool 1 'device_health_metrics' replicated size 1 min_size 1 crush_rule 0 object_hash rjenkins pg_num 64 pgp_num 64 autoscale_mode on last_change 17 lfor 0/0/15 flags hashpspool stripe_width 0 pg_num_min 1 application mgr_devicehealth
pool 2 'bench_rados' replicated size 1 min_size 1 crush_rule 0 object_hash rjenkins pg_num 128 pgp_num 128 autoscale_mode off last_change 20 flags hashpspool,noscrub,nodeep-scrub stripe_width 0 pg_num_min 128 application benchmark

# ./benchmarks/bench-rados:273 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd df tree
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:13:55,022496912-05:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:13:55,025023174-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'db39f1ae-4062-11ec-aba1-c5f3c07a3dc7', '--', 'ceph', 'osd', 'df', 'tree'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid db39f1ae-4062-11ec-aba1-c5f3c07a3dc7 -- ceph osd df tree
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
ID  CLASS  WEIGHT   REWEIGHT  SIZE     RAW USE  DATA     OMAP  META     AVAIL    %USE  VAR   PGS  STATUS  TYPE NAME      
-1         0.09769         -  100 GiB  5.7 MiB  168 KiB   0 B  5.6 MiB  100 GiB  0.01  1.00    -          root default   
-3         0.09769         -  100 GiB  5.7 MiB  168 KiB   0 B  5.6 MiB  100 GiB  0.01  1.00    -              host node-1
 0    ssd  0.09769   1.00000  100 GiB  5.7 MiB  168 KiB   0 B  5.6 MiB  100 GiB  0.01  1.00  192      up          osd.0  
                       TOTAL  100 GiB  5.7 MiB  168 KiB   0 B  5.6 MiB  100 GiB  0.01                                    
MIN/MAX VAR: 1.00/1.00  STDDEV: 0


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:13:59,145291897-05:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:14:23,363894900-05:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:14:32,443119885-05:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:14:41,505060790-05:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:14:41,512858891-05:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:14:50,599865055-05:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:14:50,607674107-05:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:14:59,602545990-05:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:14:59,610186834-05:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:15:08,758458266-05:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:15:08,766302975-05:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:15:17,977899174-05:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:15:17,985646990-05:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:15:17,991577140-05:00] INFO: > The cluster is idle now.[0m


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:15:17,995233163-05:00] STAGE: Start benchmarking of mode write ...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:15:18,002182894-05:00] INFO: > Start system activity collection services[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:15:18,007811976-05:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:341 - rados_bench() > rm --force /mnt/sda8/ceph-research/scripts/output/sys_activity_4MB_write.dat.rados_bench_host.2
# ./benchmarks/bench-rados:342 - rados_bench() > sar_pid_rados_bench_host=240990
# ./benchmarks/bench-rados:342 - rados_bench() > S_TIME_FORMAT=ISO
# ./benchmarks/bench-rados:342 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/output/sys_activity_4MB_write.dat.rados_bench_host.2 2

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:15:18,015179356-05:00] INFO: >> for OSD host[0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:15:18,023814014-05:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.2 bash
10.10.2.2: b'67740\n'
[1] 02:15:18 [SUCCESS] ljishen@10.10.2.2
67740

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:15:18,209883029-05:00] INFO: > Run rados bench[0m
# ./benchmarks/bench-rados:370 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/output/rados_bench_4MB_write.log.2
## ./benchmarks/bench-rados:361 - rados_bench() > count_cpus 32-63,96-127
# ./benchmarks/bench-rados:361 - rados_bench() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 60 write --pool bench_rados -b 4194304 -O 4194304 --concurrent-ios 128 --show-time --write-object --write-omap --write-xattr --no-cleanup
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:15:18,229527724-05:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:15:18,232389630-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'db39f1ae-4062-11ec-aba1-c5f3c07a3dc7', '--', 'rados', 'bench', '60', 'write', '--pool', 'bench_rados', '-b', '4194304', '-O', '4194304', '--concurrent-ios', '128', '--show-time', '--write-object', '--write-omap', '--write-xattr', '--no-cleanup'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid db39f1ae-4062-11ec-aba1-c5f3c07a3dc7 -- rados bench 60 write --pool bench_rados -b 4194304 -O 4194304 --concurrent-ios 128 --show-time --write-object --write-omap --write-xattr --no-cleanup
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
hints = 1
2021-11-08T07:15:21.347966+0000 Maintaining 128 concurrent writes of 4194304 bytes to objects of size 4194304 for up to 60 seconds or 0 objects
2021-11-08T07:15:21.347980+0000 Object prefix: benchmark_data_node-0.bluefield2.ucsc-cmps10_7
2021-11-08T07:15:21.471676+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-11-08T07:15:21.471676+0000     0       0         0         0         0         0           -           0
2021-11-08T07:15:22.471776+0000     1      84        84         0         0         0           -           0
2021-11-08T07:15:23.471855+0000     2     127       145        18   35.9977        36     1.78067     1.67539
2021-11-08T07:15:24.471942+0000     3     127       190        63   83.9939       180     2.24343     2.05252
2021-11-08T07:15:25.472014+0000     4     127       265       138    137.99       300     2.15018     2.15007
2021-11-08T07:15:26.472087+0000     5     127       280       153   122.391        60     2.41896     2.17358
2021-11-08T07:15:27.472166+0000     6     127       307       180   119.991       108     3.11916     2.29352
2021-11-08T07:15:28.472246+0000     7     127       342       215   122.848       140     3.69926     2.48278
2021-11-08T07:15:29.472318+0000     8     127       395       268    133.99       212     3.83069     2.76049
2021-11-08T07:15:30.472391+0000     9     127       431       304   135.101       144      3.0442     2.84541
2021-11-08T07:15:31.472463+0000    10     127       511       384   153.589       320     2.29618     2.78069
2021-11-08T07:15:32.472542+0000    11     127       532       405   147.262        84      2.3501     2.75706
2021-11-08T07:15:33.472611+0000    12     127       556       429   142.989        96     2.98128     2.75851
2021-11-08T07:15:34.472684+0000    13     127       593       466   143.374       148     3.52095     2.80134
2021-11-08T07:15:35.472760+0000    14     127       632       505   144.275       156     4.07396     2.88148
2021-11-08T07:15:36.472826+0000    15     127       655       528    140.79        92     4.37063     2.94318
2021-11-08T07:15:37.472895+0000    16     127       676       549    137.24        84     4.24322      3.0006
2021-11-08T07:15:38.472973+0000    17     127       746       619   145.636       280     3.33543      3.0904
2021-11-08T07:15:39.473046+0000    18     127       787       660   146.656       164     2.47288     3.08919
2021-11-08T07:15:40.473117+0000    19     127       803       676   142.305        64     2.92162     3.08171
2021-11-08T07:15:41.473186+0000 min lat: 1.55274 max lat: 4.78874 avg lat: 3.0946
2021-11-08T07:15:41.473186+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-11-08T07:15:41.473186+0000    20     127       833       706    141.19       120     3.54952      3.0946
2021-11-08T07:15:42.473276+0000    21     127       867       740   140.942       136     4.01324      3.1282
2021-11-08T07:15:43.473355+0000    22     127       903       776    141.08       144     4.55178     3.18404
2021-11-08T07:15:44.473429+0000    23     127       919       792   137.729        64     4.69215     3.21539
2021-11-08T07:15:45.473497+0000    24     127       938       811   135.157        76     4.63051     3.25176
2021-11-08T07:15:46.473566+0000    25     127       971       844    135.03       132      4.6185     3.30732
2021-11-08T07:15:47.473645+0000    26     127      1003       876   134.759       128     4.72416     3.35903
2021-11-08T07:15:48.473721+0000    27     127      1033       906   134.212       120     4.51202     3.40397
2021-11-08T07:15:49.473792+0000    28     127      1049       922   131.705        64     4.54451     3.42708
2021-11-08T07:15:50.473874+0000    29     127      1070       943   130.059        84     4.71096     3.45719
2021-11-08T07:15:51.473944+0000    30     127      1108       981    130.79       152     4.77042     3.50958
2021-11-08T07:15:52.474014+0000    31     127      1139      1012   130.571       124     4.70339     3.54887
2021-11-08T07:15:53.474080+0000    32     127      1165      1038    129.74       104     4.76029     3.58045
2021-11-08T07:15:54.474151+0000    33     127      1181      1054   127.748        64     4.74975      3.5993
2021-11-08T07:15:55.474223+0000    34     127      1210      1083   127.402       116     4.70073     3.63069
2021-11-08T07:15:56.474290+0000    35     127      1244      1117   127.648       136      4.7581       3.665
2021-11-08T07:15:57.474357+0000    36     127      1275      1148   127.546       124      4.6504     3.69376
2021-11-08T07:15:58.474436+0000    37     127      1300      1173   126.802       100      4.6332     3.71576
2021-11-08T07:15:59.474507+0000    38     127      1317      1190   125.254        68     4.67497     3.72967
2021-11-08T07:16:00.474577+0000    39     127      1345      1218   124.914       112      4.6753     3.75229
2021-11-08T07:16:01.474643+0000 min lat: 1.55274 max lat: 5.10811 avg lat: 3.77952
2021-11-08T07:16:01.474643+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-11-08T07:16:01.474643+0000    40     127      1380      1253   125.291       140     4.68144     3.77952
2021-11-08T07:16:02.474722+0000    41     127      1415      1288   125.649       140     4.72761     3.80596
2021-11-08T07:16:03.474794+0000    42     127      1430      1303   124.086        60     4.67981     3.81637
2021-11-08T07:16:04.474865+0000    43     127      1448      1321   122.875        72     4.62253     3.82838
2021-11-08T07:16:05.474933+0000    44     127      1480      1353   122.991       128     4.81761     3.85256
2021-11-08T07:16:06.474998+0000    45     127      1514      1387    123.28       136     4.80723     3.87756
2021-11-08T07:16:07.475071+0000    46     127      1544      1417   123.208       120     4.84698     3.89842
2021-11-08T07:16:08.475147+0000    47     127      1559      1432   121.863        60     4.73611     3.90898
2021-11-08T07:16:09.475216+0000    48     127      1583      1456   121.325        96     4.68215     3.92526
2021-11-08T07:16:10.475299+0000    49     127      1615      1488   121.461       128      4.6602     3.94252
2021-11-08T07:16:11.475369+0000    50     127      1651      1524   121.911       144     4.55969     3.95979
2021-11-08T07:16:12.475454+0000    51     127      1679      1552   121.717       112     4.57734     3.97256
2021-11-08T07:16:13.475523+0000    52     127      1695      1568   120.607        64     4.59514     3.97923
2021-11-08T07:16:14.475600+0000    53     127      1721      1594   120.293       104     4.53944     3.98979
2021-11-08T07:16:15.475675+0000    54     127      1754      1627    120.51       132     4.60857     4.00382
2021-11-08T07:16:16.475743+0000    55     127      1787      1660   120.718       132     4.68622     4.01774
2021-11-08T07:16:17.475814+0000    56     127      1810      1683   120.206        92     4.56601     4.02696
2021-11-08T07:16:18.475888+0000    57     127      1826      1699   119.219        64     4.57471     4.03331
2021-11-08T07:16:19.475959+0000    58     127      1857      1730   119.302       124     4.63695     4.04615
2021-11-08T07:16:20.476030+0000    59     127      1893      1766    119.72       144     4.73484     4.06022
2021-11-08T07:16:21.476101+0000 min lat: 1.55274 max lat: 5.20934 avg lat: 4.07185
2021-11-08T07:16:21.476101+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-11-08T07:16:21.476101+0000    60     127      1925      1798   119.858       128     4.63229     4.07185
2021-11-08T07:16:22.476226+0000 Total time run:         60.3005
Total writes made:      1926
Write size:             4194304
Object size:            4194304
Bandwidth (MB/sec):     127.76
Stddev Bandwidth:       56.133
Max bandwidth (MB/sec): 320
Min bandwidth (MB/sec): 0
Average IOPS:           31
Stddev IOPS:            14.0332
Max IOPS:               80
Min IOPS:               0
Average Latency(s):     3.94104
Stddev Latency(s):      1.06908
Max latency(s):         5.20934
Min latency(s):         0.264084

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:16:23,181994737-05:00] INFO: > Stop system activity collection services[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:16:23,187915318-05:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:391 - rados_bench() > kill -INT 240990

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:16:23,193685988-05:00] INFO: >> for OSD host[0m
# ./benchmarks/bench-rados:396 - rados_bench() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.2 kill -INT 67740
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:16:23,201161862-05:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.2 kill -INT 67740
[1] 02:16:23 [SUCCESS] ljishen@10.10.2.2

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:16:23,384711217-05:00] INFO: >>> Copy system activity file from OSD host[0m
## ./benchmarks/bench-rados:403 - rados_bench() > basename -- /tmp/bench-rados/sys_activity_4MB_write.dat.osd_host.2
# ./benchmarks/bench-rados:403 - rados_bench() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.2:/tmp/bench-rados/sys_activity_4MB_write.dat.osd_host.2 /mnt/sda8/ceph-research/scripts/output/sys_activity_4MB_write.dat.osd_host.2


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:16:23,554977260-05:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:16:47,518311054-05:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 1.93k objects, 7.5 GiB
    usage:   23 GiB used, 77 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:16:47,526049713-05:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:16:56,713705608-05:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 1.93k objects, 7.5 GiB
    usage:   23 GiB used, 77 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:16:56,721925043-05:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:17:05,943455310-05:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 1.93k objects, 7.5 GiB
    usage:   23 GiB used, 77 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:17:05,950958375-05:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:17:15,036868156-05:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 1.93k objects, 7.5 GiB
    usage:   23 GiB used, 77 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:17:15,044381109-05:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:17:24,385581077-05:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 1.93k objects, 7.5 GiB
    usage:   23 GiB used, 77 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:17:24,393535493-05:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:17:24,399712057-05:00] INFO: > The cluster is idle now.[0m


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:17:24,403341279-05:00] STAGE: Start benchmarking of mode seq ...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:17:24,410448629-05:00] INFO: > Start system activity collection services[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:17:24,415855943-05:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:341 - rados_bench() > rm --force /mnt/sda8/ceph-research/scripts/output/sys_activity_4MB_seq.dat.rados_bench_host.2
# ./benchmarks/bench-rados:342 - rados_bench() > sar_pid_rados_bench_host=242337
# ./benchmarks/bench-rados:342 - rados_bench() > S_TIME_FORMAT=ISO
# ./benchmarks/bench-rados:342 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/output/sys_activity_4MB_seq.dat.rados_bench_host.2 2

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:17:24,423069492-05:00] INFO: >> for OSD host[0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:17:24,431805622-05:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.2 bash
10.10.2.2: b'67840\n'
[1] 02:17:24 [SUCCESS] ljishen@10.10.2.2
67840

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:17:24,618115983-05:00] INFO: > Run rados bench[0m
# ./benchmarks/bench-rados:379 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/output/rados_bench_4MB_seq.log.2
## ./benchmarks/bench-rados:376 - rados_bench() > count_cpus 32-63,96-127
# ./benchmarks/bench-rados:376 - rados_bench() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 128 --show-time
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:17:24,636912790-05:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:17:24,639526828-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'db39f1ae-4062-11ec-aba1-c5f3c07a3dc7', '--', 'rados', 'bench', '99999999', 'seq', '--pool', 'bench_rados', '--concurrent-ios', '128', '--show-time'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid db39f1ae-4062-11ec-aba1-c5f3c07a3dc7 -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 128 --show-time
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
hints = 1
2021-11-08T07:17:27.763786+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-11-08T07:17:27.763786+0000     0       0         0         0         0         0           -           0
2021-11-08T07:17:28.763885+0000     1     127       367       240   959.839       960    0.388821    0.366799
2021-11-08T07:17:29.763961+0000     2     127       689       562   1123.86      1288    0.393247    0.383485
2021-11-08T07:17:30.764053+0000     3     127      1011       884   1178.53      1288     0.39247     0.38847
2021-11-08T07:17:31.764352+0000     4     127      1335      1208   1207.81      1296    0.395672    0.390314
2021-11-08T07:17:32.764437+0000     5     127      1656      1529   1223.02      1284    0.399059    0.391682
2021-11-08T07:17:33.764542+0000 Total time run:       5.91346
Total reads made:     1926
Read size:            4194304
Object size:          4194304
Bandwidth (MB/sec):   1302.79
Average IOPS:         325
Stddev IOPS:          36.7995
Max IOPS:             324
Min IOPS:             240
Average Latency(s):   0.381154
Max latency(s):       0.404453
Min latency(s):       0.0754031

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:17:34,555246340-05:00] INFO: > Stop system activity collection services[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:17:34,561489380-05:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:391 - rados_bench() > kill -INT 242337

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:17:34,567536360-05:00] INFO: >> for OSD host[0m
# ./benchmarks/bench-rados:396 - rados_bench() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.2 kill -INT 67840
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:17:34,575058501-05:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.2 kill -INT 67840
[1] 02:17:34 [SUCCESS] ljishen@10.10.2.2

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:17:34,756650387-05:00] INFO: >>> Copy system activity file from OSD host[0m
## ./benchmarks/bench-rados:403 - rados_bench() > basename -- /tmp/bench-rados/sys_activity_4MB_seq.dat.osd_host.2
# ./benchmarks/bench-rados:403 - rados_bench() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.2:/tmp/bench-rados/sys_activity_4MB_seq.dat.osd_host.2 /mnt/sda8/ceph-research/scripts/output/sys_activity_4MB_seq.dat.osd_host.2


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:17:34,904665001-05:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:17:58,900791605-05:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 1.93k objects, 7.5 GiB
    usage:   23 GiB used, 77 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:17:58,908546375-05:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:18:08,003848272-05:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 1.93k objects, 7.5 GiB
    usage:   23 GiB used, 77 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:18:08,011406421-05:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:18:17,036045484-05:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 1.93k objects, 7.5 GiB
    usage:   23 GiB used, 77 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:18:17,043677162-05:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:18:26,447674326-05:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 1.93k objects, 7.5 GiB
    usage:   23 GiB used, 77 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:18:26,455369703-05:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:18:35,548853946-05:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 1.93k objects, 7.5 GiB
    usage:   23 GiB used, 77 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:18:35,556185558-05:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:18:35,562362293-05:00] INFO: > The cluster is idle now.[0m


[1;7;39;49m[2021-11-08T02:18:35,564610130-05:00][RUNNING][ROUND 3/6/21] object_size=4MB[0m


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:18:35,567942262-05:00] STAGE: Launch a Ceph cluster (CEPH_HOST: 10.10.2.3, OSD_HOST: 10.10.2.2) ...[0m
# ./benchmarks/bench-rados:183 - launch_ceph_cluster() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.3 bash -euo pipefail
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:18:35,577156553-05:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.3 bash -euo pipefail
10.10.2.3: b'\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T02:18:35,974780944-05:00] INFO: > Remove existing clusters\x1b[0m\n'
10.10.2.3: b'## bash:17 -  > basename -- /var/lib/ceph/db39f1ae-4062-11ec-aba1-c5f3c07a3dc7\n'
10.10.2.3: b'# bash:17 -  > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm --host 10.10.2.3 --host 10.10.2.2 rm-cluster --force --fsid db39f1ae-4062-11ec-aba1-c5f3c07a3dc7\n'
10.10.2.3: b"\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T02:18:35,985165306-05:00] DEBUG: HOST_PARAMS: ['--host', '10.10.2.3', '--host', '10.10.2.2']\x1b[0m\n"
10.10.2.3: b"\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T02:18:35,988145479-05:00] DEBUG: CEPHADM_PARAMS: ['rm-cluster', '--force', '--fsid', 'db39f1ae-4062-11ec-aba1-c5f3c07a3dc7']\x1b[0m\n"
10.10.2.3: b"# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:63 -  > /tmp/bench-rados/ceph-research/scripts/parallel-ssh --host 10.10.2.3 --host 10.10.2.2 'sudo python3 -u - rm-cluster --force --fsid db39f1ae-4062-11ec-aba1-c5f3c07a3dc7'\n"
10.10.2.3: b'\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T02:18:35,995837963-05:00] DEBUG: command from stdin\x1b[0m\n'
10.10.2.3: b"# /tmp/bench-rados/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host 10.10.2.3 --host 10.10.2.2 'sudo python3 -u - rm-cluster --force --fsid db39f1ae-4062-11ec-aba1-c5f3c07a3dc7'\n"
10.10.2.3: b'[1] 02:18:42 [SUCCESS] 10.10.2.3\n[2] 02:18:48 [SUCCESS] 10.10.2.2\n'
10.10.2.3: b'\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T02:18:48,634803490-05:00] INFO: > Deploy a new cluster\x1b[0m\n'
10.10.2.3: b'# bash:24 -  > /tmp/bench-rados/ceph-research/scripts/ceph-deploy -m 10.10.2.3 -o node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/10.10.2.2:/dev/nvme0n1\n'
10.10.2.3: b'\n\n\x1b[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T02:18:48,646026976-05:00] STAGE: Verify passwordless SSH to hosts, passwordless sudo via ljishen, and the existence of devices on hosts...\x1b[0m\n'
10.10.2.3: b'\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T02:18:48,650192576-05:00] INFO: > Check the host for the monitor\x1b[0m\n'
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:181 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@10.10.2.3 sudo --non-interactive --validate\n'
10.10.2.3: b'\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T02:18:48,798304687-05:00] INFO: > Check hosts for OSDs\x1b[0m\n'
10.10.2.3: b"\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T02:18:48,802836327-05:00] INFO: >> Check host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'\x1b[0m\n"
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:200 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us sudo --non-interactive --validate\n'
10.10.2.3: b"\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T02:18:48,949522438-05:00] INFO: >>> Check device '/dev/nvme0n1'\x1b[0m\n"
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:215 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us test -b /dev/nvme0n1\n'
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:226 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us lsblk --nodeps --noheadings --output TYPE /dev/nvme0n1\n'
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:227 - verify_parameters() > grep --word-regexp disk\n'
10.10.2.3: b'\n\n\x1b[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T02:18:49,226332420-05:00] STAGE: Reset devices on hosts...\x1b[0m\n'
10.10.2.3: b"\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T02:18:49,231424398-05:00] INFO: > Resetting device '/dev/nvme0n1' on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'\x1b[0m\n"
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:288 - reset_host_devices() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us sudo sh\n'
10.10.2.3: b'  Removing ceph--ed7fef94--1ef5--43c6--831f--e7f1534b015e-osd--block--1109ef11--b08b--4e20--9a86--4210ec623598 (253:0)\n'
10.10.2.3: b'  Archiving volume group "ceph-ed7fef94-1ef5-43c6-831f-e7f1534b015e" metadata (seqno 5).\n'
10.10.2.3: b'  Releasing logical volume "osd-block-1109ef11-b08b-4e20-9a86-4210ec623598"\n'
10.10.2.3: b'  Creating volume group backup "/etc/lvm/backup/ceph-ed7fef94-1ef5-43c6-831f-e7f1534b015e" (seqno 6).\n'
10.10.2.3: b'  Logical volume "osd-block-1109ef11-b08b-4e20-9a86-4210ec623598" successfully removed\n'
10.10.2.3: b'  Removing physical volume "/dev/nvme0n1" from volume group "ceph-ed7fef94-1ef5-43c6-831f-e7f1534b015e"\n'
10.10.2.3: b'  Volume group "ceph-ed7fef94-1ef5-43c6-831f-e7f1534b015e" successfully removed\n'
10.10.2.3: b'\n\n\x1b[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T02:18:49,551430732-05:00] STAGE: Bootstrap a new cluster...\x1b[0m\n'
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:317 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.3 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults\n'
10.10.2.3: b'\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T02:18:49,560616904-05:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.3: b"\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T02:18:49,564439857-05:00] DEBUG: CEPHADM_PARAMS: ['bootstrap', '--config', '/tmp/bench-rados/ceph-research/scripts/ceph.conf', '--mon-ip', '10.10.2.3', '--output-dir', '/tmp/bench-rados/deployment_data_root/etc/ceph', '--ssh-user', 'ljishen', '--skip-dashboard', '--no-minimize-config', '--allow-overwrite', '--allow-fqdn-hostname', '--orphan-initial-daemons', '--skip-monitoring-stack', '--single-host-defaults']\x1b[0m\n"
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.3 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults\n'
10.10.2.3: b'Verifying podman|docker is present...\nVerifying lvm2 is present...\n'
10.10.2.3: b'Verifying time synchronization is in place...\n'
10.10.2.3: b'Unit ntp.service is enabled and running\n'
10.10.2.3: b'Repeating the final host check...\ndocker (/usr/bin/docker) is present\n'
10.10.2.3: b'systemctl is present\n'
10.10.2.3: b'lvcreate is present\n'
10.10.2.3: b'Unit ntp.service is enabled and running\n'
10.10.2.3: b'Host looks OK\n'
10.10.2.3: b'Cluster fsid: 1eb49abe-4064-11ec-aba1-c5f3c07a3dc7\n'
10.10.2.3: b'Verifying IP 10.10.2.3 port 3300 ...\n'
10.10.2.3: b'Verifying IP 10.10.2.3 port 6789 ...\n'
10.10.2.3: b'Mon IP `10.10.2.3` is in CIDR network `10.10.2.0/24`\n- internal network (--cluster-network) has not been provided, OSD replication will default to the public_network\n'
10.10.2.3: b'Adjusting default settings to suit single-host cluster...\n'
10.10.2.3: b'Pulling container image quay.io/ceph/ceph:v16...\n'
10.10.2.3: b'Ceph version: ceph version 16.2.6 (ee28fb57e47e9f88813e24bbf4c14496ca299d31) pacific (stable)\n'
10.10.2.3: b'Extracting ceph user uid/gid from container image...\n'
10.10.2.3: b'Creating initial keys...\n'
10.10.2.3: b'Creating initial monmap...\n'
10.10.2.3: b'Creating mon...\n'
10.10.2.3: b'Waiting for mon to start...\n'
10.10.2.3: b'Waiting for mon...\n'
10.10.2.3: b'mon is available\nSetting mon public_network to 10.10.2.0/24\n'
10.10.2.3: b'Wrote config to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf\n'
10.10.2.3: b'Wrote keyring to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring\n'
10.10.2.3: b'Creating mgr...\n'
10.10.2.3: b'Verifying port 9283 ...\n'
10.10.2.3: b'Waiting for mgr to start...\n'
10.10.2.3: b'Waiting for mgr...\n'
10.10.2.3: b'mgr not available, waiting (1/15)...\n'
10.10.2.3: b'mgr not available, waiting (2/15)...\n'
10.10.2.3: b'mgr is available\n'
10.10.2.3: b'Enabling cephadm module...\n'
10.10.2.3: b'Waiting for the mgr to restart...\n'
10.10.2.3: b'Waiting for mgr epoch 5...\n'
10.10.2.3: b'mgr epoch 5 is available\nSetting orchestrator backend to cephadm...\n'
10.10.2.3: b'Generating ssh key...\n'
10.10.2.3: b'Wrote public SSH key to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub\n'
10.10.2.3: b'Adding key to ljishen@localhost authorized_keys...\n'
10.10.2.3: b'Adding host node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us...\n'
10.10.2.3: b'Deploying unmanaged mon service...\n'
10.10.2.3: b'Deploying unmanaged mgr service...\n'
10.10.2.3: b'You can access the Ceph CLI with:\n\n\tsudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --fsid 1eb49abe-4064-11ec-aba1-c5f3c07a3dc7 -c /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf -k /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring\n\nPlease consider enabling telemetry to help improve Ceph:\n\n\tceph telemetry on\n\nFor more information see:\n\n\thttps://docs.ceph.com/docs/pacific/mgr/telemetry/\n\n'
10.10.2.3: b'Bootstrap complete.\n'
10.10.2.3: b'\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T02:19:59,458062814-05:00] INFO: Sleep for 20 seconds\x1b[0m\n'
10.10.2.3: b'\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T02:20:19,465753572-05:00] INFO: > Set the default number of monitor daemons to 1\x1b[0m\n'
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:337 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1\n'
10.10.2.3: b'\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T02:20:19,476072542-05:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.3: b"\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T02:20:19,479958333-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mon', '1']\x1b[0m\n"
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1\n'
10.10.2.3: b'Inferring fsid 1eb49abe-4064-11ec-aba1-c5f3c07a3dc7\n'
10.10.2.3: b'Inferring config /var/lib/ceph/1eb49abe-4064-11ec-aba1-c5f3c07a3dc7/mon.node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.3: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.3: b'Scheduled mon update...\n'
10.10.2.3: b'\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T02:20:29,803885150-05:00] INFO: > Set the default number of manager daemons to 1\x1b[0m\n'
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:342 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1\n'
10.10.2.3: b'\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T02:20:29,813696802-05:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.3: b"\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T02:20:29,816949099-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mgr', '1']\x1b[0m\n"
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1\n'
10.10.2.3: b'Inferring fsid 1eb49abe-4064-11ec-aba1-c5f3c07a3dc7\n'
10.10.2.3: b'Inferring config /var/lib/ceph/1eb49abe-4064-11ec-aba1-c5f3c07a3dc7/mon.node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.3: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.3: b'Scheduled mgr update...\n'
10.10.2.3: b'\n\n\x1b[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T02:20:40,660465304-05:00] STAGE: Add hosts to the cluster...\x1b[0m\n'
10.10.2.3: b"\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T02:20:40,666232045-05:00] INFO: > Adding host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'\x1b[0m\n# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:361 - add_hosts() > ssh-copy-id -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -f -i /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\n"
10.10.2.3: b'/usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: "/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub"\n'
10.10.2.3: b'\nNumber of key(s) added: 1\n\nNow try logging into the machine, with:   "ssh -o \'GlobalKnownHostsFile=/dev/null\' -o \'LogLevel=ERROR\' -o \'PasswordAuthentication=no\' -o \'StrictHostKeyChecking=no\' -o \'UserKnownHostsFile=/dev/null\' \'ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\'"\nand check to make sure that only the key(s) you wanted were added.\n\n'
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:367 - add_hosts() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us 10.10.2.2\n'
10.10.2.3: b'\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T02:20:40,872932670-05:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.3: b"\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T02:20:40,876382239-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'host', 'add', 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us', '10.10.2.2']\x1b[0m\n"
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us 10.10.2.2\n'
10.10.2.3: b'Inferring fsid 1eb49abe-4064-11ec-aba1-c5f3c07a3dc7\n'
10.10.2.3: b'Inferring config /var/lib/ceph/1eb49abe-4064-11ec-aba1-c5f3c07a3dc7/mon.node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.3: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.3: b"Added host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us' with addr '10.10.2.2'\n"
10.10.2.3: b'\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T02:20:51,284917786-05:00] INFO: Sleep for 20 seconds\x1b[0m\n'
10.10.2.3: b'\n\n\x1b[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T02:21:11,289573605-05:00] STAGE: Add OSDs to the cluster...\x1b[0m\n'
10.10.2.3: b"\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T02:21:11,296084409-05:00] INFO: > Adding devices ['/dev/nvme0n1'] on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us' as OSDs\x1b[0m\n"
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:388 - add_host_devices_as_osds() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1\n'
10.10.2.3: b'\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T02:21:11,305741921-05:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.3: b"\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T02:21:11,309232556-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'daemon', 'add', 'osd', 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1']\x1b[0m\n"
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1\n'
10.10.2.3: b'Inferring fsid 1eb49abe-4064-11ec-aba1-c5f3c07a3dc7\n'
10.10.2.3: b'Inferring config /var/lib/ceph/1eb49abe-4064-11ec-aba1-c5f3c07a3dc7/mon.node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.3: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.3: b"Created osd(s) 0 on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'\n"
10.10.2.3: b'\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T02:21:37,572581985-05:00] INFO: > Sleep for 20 seconds\x1b[0m\n'
10.10.2.3: b'\n\n\x1b[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T02:21:57,578252815-05:00] STAGE: Show Cluster Status\x1b[0m\n'
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:408 - show_status() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status\n'
10.10.2.3: b'\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T02:21:57,587971041-05:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.3: b"\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T02:21:57,591725905-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', '--status']\x1b[0m\n"
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status\n'
10.10.2.3: b'Inferring fsid 1eb49abe-4064-11ec-aba1-c5f3c07a3dc7\n'
10.10.2.3: b'Inferring config /var/lib/ceph/1eb49abe-4064-11ec-aba1-c5f3c07a3dc7/mon.node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.3: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.3: b'  cluster:\n    id:     1eb49abe-4064-11ec-aba1-c5f3c07a3dc7\n    health: HEALTH_WARN\n            1 pool(s) have no replicas configured\n \n  services:\n    mon: 1 daemons, quorum node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us (age 2m)\n    mgr: node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us.hvsqjv(active, since 2m)\n    osd: 1 osds: 1 up (since 19s), 1 in (since 42s)\n \n  data:\n    pools:   1 pools, 1 pgs\n    objects: 0 objects, 0 B\n    usage:   4.7 MiB used, 100 GiB / 100 GiB avail\n    pgs:     1 active+clean\n \n'
[1] 02:22:07 [SUCCESS] ljishen@10.10.2.3

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:18:35,974780944-05:00] INFO: > Remove existing clusters[0m
## bash:17 -  > basename -- /var/lib/ceph/db39f1ae-4062-11ec-aba1-c5f3c07a3dc7
# bash:17 -  > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm --host 10.10.2.3 --host 10.10.2.2 rm-cluster --force --fsid db39f1ae-4062-11ec-aba1-c5f3c07a3dc7
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:18:35,985165306-05:00] DEBUG: HOST_PARAMS: ['--host', '10.10.2.3', '--host', '10.10.2.2'][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:18:35,988145479-05:00] DEBUG: CEPHADM_PARAMS: ['rm-cluster', '--force', '--fsid', 'db39f1ae-4062-11ec-aba1-c5f3c07a3dc7'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:63 -  > /tmp/bench-rados/ceph-research/scripts/parallel-ssh --host 10.10.2.3 --host 10.10.2.2 'sudo python3 -u - rm-cluster --force --fsid db39f1ae-4062-11ec-aba1-c5f3c07a3dc7'
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:18:35,995837963-05:00] DEBUG: command from stdin[0m
# /tmp/bench-rados/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host 10.10.2.3 --host 10.10.2.2 'sudo python3 -u - rm-cluster --force --fsid db39f1ae-4062-11ec-aba1-c5f3c07a3dc7'
[1] 02:18:42 [SUCCESS] 10.10.2.3
[2] 02:18:48 [SUCCESS] 10.10.2.2

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:18:48,634803490-05:00] INFO: > Deploy a new cluster[0m
# bash:24 -  > /tmp/bench-rados/ceph-research/scripts/ceph-deploy -m 10.10.2.3 -o node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/10.10.2.2:/dev/nvme0n1


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:18:48,646026976-05:00] STAGE: Verify passwordless SSH to hosts, passwordless sudo via ljishen, and the existence of devices on hosts...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:18:48,650192576-05:00] INFO: > Check the host for the monitor[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:181 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@10.10.2.3 sudo --non-interactive --validate

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:18:48,798304687-05:00] INFO: > Check hosts for OSDs[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:18:48,802836327-05:00] INFO: >> Check host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:200 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us sudo --non-interactive --validate

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:18:48,949522438-05:00] INFO: >>> Check device '/dev/nvme0n1'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:215 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us test -b /dev/nvme0n1
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:226 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us lsblk --nodeps --noheadings --output TYPE /dev/nvme0n1
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:227 - verify_parameters() > grep --word-regexp disk


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:18:49,226332420-05:00] STAGE: Reset devices on hosts...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:18:49,231424398-05:00] INFO: > Resetting device '/dev/nvme0n1' on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:288 - reset_host_devices() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us sudo sh
  Removing ceph--ed7fef94--1ef5--43c6--831f--e7f1534b015e-osd--block--1109ef11--b08b--4e20--9a86--4210ec623598 (253:0)
  Archiving volume group "ceph-ed7fef94-1ef5-43c6-831f-e7f1534b015e" metadata (seqno 5).
  Releasing logical volume "osd-block-1109ef11-b08b-4e20-9a86-4210ec623598"
  Creating volume group backup "/etc/lvm/backup/ceph-ed7fef94-1ef5-43c6-831f-e7f1534b015e" (seqno 6).
  Logical volume "osd-block-1109ef11-b08b-4e20-9a86-4210ec623598" successfully removed
  Removing physical volume "/dev/nvme0n1" from volume group "ceph-ed7fef94-1ef5-43c6-831f-e7f1534b015e"
  Volume group "ceph-ed7fef94-1ef5-43c6-831f-e7f1534b015e" successfully removed


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:18:49,551430732-05:00] STAGE: Bootstrap a new cluster...[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:317 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.3 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:18:49,560616904-05:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:18:49,564439857-05:00] DEBUG: CEPHADM_PARAMS: ['bootstrap', '--config', '/tmp/bench-rados/ceph-research/scripts/ceph.conf', '--mon-ip', '10.10.2.3', '--output-dir', '/tmp/bench-rados/deployment_data_root/etc/ceph', '--ssh-user', 'ljishen', '--skip-dashboard', '--no-minimize-config', '--allow-overwrite', '--allow-fqdn-hostname', '--orphan-initial-daemons', '--skip-monitoring-stack', '--single-host-defaults'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.3 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults
Verifying podman|docker is present...
Verifying lvm2 is present...
Verifying time synchronization is in place...
Unit ntp.service is enabled and running
Repeating the final host check...
docker (/usr/bin/docker) is present
systemctl is present
lvcreate is present
Unit ntp.service is enabled and running
Host looks OK
Cluster fsid: 1eb49abe-4064-11ec-aba1-c5f3c07a3dc7
Verifying IP 10.10.2.3 port 3300 ...
Verifying IP 10.10.2.3 port 6789 ...
Mon IP `10.10.2.3` is in CIDR network `10.10.2.0/24`
- internal network (--cluster-network) has not been provided, OSD replication will default to the public_network
Adjusting default settings to suit single-host cluster...
Pulling container image quay.io/ceph/ceph:v16...
Ceph version: ceph version 16.2.6 (ee28fb57e47e9f88813e24bbf4c14496ca299d31) pacific (stable)
Extracting ceph user uid/gid from container image...
Creating initial keys...
Creating initial monmap...
Creating mon...
Waiting for mon to start...
Waiting for mon...
mon is available
Setting mon public_network to 10.10.2.0/24
Wrote config to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf
Wrote keyring to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring
Creating mgr...
Verifying port 9283 ...
Waiting for mgr to start...
Waiting for mgr...
mgr not available, waiting (1/15)...
mgr not available, waiting (2/15)...
mgr is available
Enabling cephadm module...
Waiting for the mgr to restart...
Waiting for mgr epoch 5...
mgr epoch 5 is available
Setting orchestrator backend to cephadm...
Generating ssh key...
Wrote public SSH key to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub
Adding key to ljishen@localhost authorized_keys...
Adding host node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us...
Deploying unmanaged mon service...
Deploying unmanaged mgr service...
You can access the Ceph CLI with:

	sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --fsid 1eb49abe-4064-11ec-aba1-c5f3c07a3dc7 -c /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf -k /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring

Please consider enabling telemetry to help improve Ceph:

	ceph telemetry on

For more information see:

	https://docs.ceph.com/docs/pacific/mgr/telemetry/

Bootstrap complete.

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:19:59,458062814-05:00] INFO: Sleep for 20 seconds[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:20:19,465753572-05:00] INFO: > Set the default number of monitor daemons to 1[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:337 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:20:19,476072542-05:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:20:19,479958333-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mon', '1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1
Inferring fsid 1eb49abe-4064-11ec-aba1-c5f3c07a3dc7
Inferring config /var/lib/ceph/1eb49abe-4064-11ec-aba1-c5f3c07a3dc7/mon.node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Scheduled mon update...

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:20:29,803885150-05:00] INFO: > Set the default number of manager daemons to 1[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:342 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:20:29,813696802-05:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:20:29,816949099-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mgr', '1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1
Inferring fsid 1eb49abe-4064-11ec-aba1-c5f3c07a3dc7
Inferring config /var/lib/ceph/1eb49abe-4064-11ec-aba1-c5f3c07a3dc7/mon.node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Scheduled mgr update...


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:20:40,660465304-05:00] STAGE: Add hosts to the cluster...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:20:40,666232045-05:00] INFO: > Adding host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:361 - add_hosts() > ssh-copy-id -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -f -i /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us
/usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: "/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub"

Number of key(s) added: 1

Now try logging into the machine, with:   "ssh -o 'GlobalKnownHostsFile=/dev/null' -o 'LogLevel=ERROR' -o 'PasswordAuthentication=no' -o 'StrictHostKeyChecking=no' -o 'UserKnownHostsFile=/dev/null' 'ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'"
and check to make sure that only the key(s) you wanted were added.

# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:367 - add_hosts() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us 10.10.2.2
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:20:40,872932670-05:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:20:40,876382239-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'host', 'add', 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us', '10.10.2.2'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us 10.10.2.2
Inferring fsid 1eb49abe-4064-11ec-aba1-c5f3c07a3dc7
Inferring config /var/lib/ceph/1eb49abe-4064-11ec-aba1-c5f3c07a3dc7/mon.node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Added host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us' with addr '10.10.2.2'

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:20:51,284917786-05:00] INFO: Sleep for 20 seconds[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:21:11,289573605-05:00] STAGE: Add OSDs to the cluster...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:21:11,296084409-05:00] INFO: > Adding devices ['/dev/nvme0n1'] on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us' as OSDs[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:388 - add_host_devices_as_osds() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:21:11,305741921-05:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:21:11,309232556-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'daemon', 'add', 'osd', 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1
Inferring fsid 1eb49abe-4064-11ec-aba1-c5f3c07a3dc7
Inferring config /var/lib/ceph/1eb49abe-4064-11ec-aba1-c5f3c07a3dc7/mon.node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Created osd(s) 0 on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:21:37,572581985-05:00] INFO: > Sleep for 20 seconds[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:21:57,578252815-05:00] STAGE: Show Cluster Status[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:408 - show_status() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:21:57,587971041-05:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:21:57,591725905-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', '--status'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status
Inferring fsid 1eb49abe-4064-11ec-aba1-c5f3c07a3dc7
Inferring config /var/lib/ceph/1eb49abe-4064-11ec-aba1-c5f3c07a3dc7/mon.node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
  cluster:
    id:     1eb49abe-4064-11ec-aba1-c5f3c07a3dc7
    health: HEALTH_WARN
            1 pool(s) have no replicas configured
 
  services:
    mon: 1 daemons, quorum node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us (age 2m)
    mgr: node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us.hvsqjv(active, since 2m)
    osd: 1 osds: 1 up (since 19s), 1 in (since 42s)
 
  data:
    pools:   1 pools, 1 pgs
    objects: 0 objects, 0 B
    usage:   4.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     1 active+clean
 

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:22:07,591087523-05:00] INFO: > Copy configuration files to local[0m
# ./benchmarks/bench-rados:220 - launch_ceph_cluster() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.3 sudo chown --changes --recursive ljishen /tmp/bench-rados/deployment_data_root/etc/ceph
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:22:07,598641725-05:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.3 sudo chown --changes --recursive ljishen /tmp/bench-rados/deployment_data_root/etc/ceph
[1] 02:22:07 [SUCCESS] ljishen@10.10.2.3
# ./benchmarks/bench-rados:223 - launch_ceph_cluster() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.3:/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf
# ./benchmarks/bench-rados:226 - launch_ceph_cluster() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.3:/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:22:08,068295855-05:00] STAGE: Update local ceph.conf as a client...[0m


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:22:08,071937972-05:00] STAGE: Configure Ceph pool...[0m
# ./benchmarks/bench-rados:257 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_max_backfills 32
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:22:08,093158238-05:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:22:08,095825426-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '1eb49abe-4064-11ec-aba1-c5f3c07a3dc7', '--', 'ceph', 'config', 'set', 'osd', 'osd_max_backfills', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 1eb49abe-4064-11ec-aba1-c5f3c07a3dc7 -- ceph config set osd osd_max_backfills 32
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:258 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_active 32
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:22:12,210400168-05:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:22:12,213158378-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '1eb49abe-4064-11ec-aba1-c5f3c07a3dc7', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_active', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 1eb49abe-4064-11ec-aba1-c5f3c07a3dc7 -- ceph config set osd osd_recovery_max_active 32
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:259 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_single_start 8
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:22:16,345001476-05:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:22:16,347586249-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '1eb49abe-4064-11ec-aba1-c5f3c07a3dc7', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_single_start', '8'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 1eb49abe-4064-11ec-aba1-c5f3c07a3dc7 -- ceph config set osd osd_recovery_max_single_start 8
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:260 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_op_priority 63
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:22:20,555370698-05:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:22:20,558222544-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '1eb49abe-4064-11ec-aba1-c5f3c07a3dc7', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_op_priority', '63'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 1eb49abe-4064-11ec-aba1-c5f3c07a3dc7 -- ceph config set osd osd_recovery_op_priority 63
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:262 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config show-with-defaults osd.0
# ./benchmarks/bench-rados:263 - configure_ceph_pool() > grep 'osd_max_backfills\|osd_recovery'
# ./benchmarks/bench-rados:263 - configure_ceph_pool() > column -t -s ' '
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
osd_max_backfills                        32         mon
osd_recovery_cost                        20971520   default
osd_recovery_delay_start                 0.000000   default
osd_recovery_max_active                  32         mon
osd_recovery_max_active_hdd              3          default
osd_recovery_max_active_ssd              10         default
osd_recovery_max_chunk                   8388608    default
osd_recovery_max_omap_entries_per_chunk  8096       default
osd_recovery_max_single_start            8          mon
osd_recovery_op_priority                 3          default
osd_recovery_op_warn_multiple            16         default
osd_recovery_priority                    5          default
osd_recovery_retry_interval              30.000000  default
osd_recovery_sleep                       0.000000   default
osd_recovery_sleep_hdd                   0.100000   default
osd_recovery_sleep_hybrid                0.025000   default
osd_recovery_sleep_ssd                   0.000000   default
# ./benchmarks/bench-rados:265 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool create bench_rados 128 128 replicated --size 1 --pg-num-min 128
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:22:28,794086543-05:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:22:28,796972353-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '1eb49abe-4064-11ec-aba1-c5f3c07a3dc7', '--', 'ceph', 'osd', 'pool', 'create', 'bench_rados', '128', '128', 'replicated', '--size', '1', '--pg-num-min', '128'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 1eb49abe-4064-11ec-aba1-c5f3c07a3dc7 -- ceph osd pool create bench_rados 128 128 replicated --size 1 --pg-num-min 128
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
pool 'bench_rados' created
# ./benchmarks/bench-rados:267 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados min_size 1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:22:33,206034505-05:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:22:33,208795551-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '1eb49abe-4064-11ec-aba1-c5f3c07a3dc7', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'min_size', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 1eb49abe-4064-11ec-aba1-c5f3c07a3dc7 -- ceph osd pool set bench_rados min_size 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 min_size to 1
# ./benchmarks/bench-rados:268 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados pg_autoscale_mode off
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:22:37,599775184-05:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:22:37,602376939-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '1eb49abe-4064-11ec-aba1-c5f3c07a3dc7', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'pg_autoscale_mode', 'off'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 1eb49abe-4064-11ec-aba1-c5f3c07a3dc7 -- ceph osd pool set bench_rados pg_autoscale_mode off
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 pg_autoscale_mode to off
# ./benchmarks/bench-rados:269 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool application enable bench_rados benchmark
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:22:41,967662813-05:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:22:41,970383372-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '1eb49abe-4064-11ec-aba1-c5f3c07a3dc7', '--', 'ceph', 'osd', 'pool', 'application', 'enable', 'bench_rados', 'benchmark'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 1eb49abe-4064-11ec-aba1-c5f3c07a3dc7 -- ceph osd pool application enable bench_rados benchmark
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
enabled application 'benchmark' on pool 'bench_rados'
# ./benchmarks/bench-rados:270 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados noscrub 1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:22:46,909828439-05:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:22:46,912658234-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '1eb49abe-4064-11ec-aba1-c5f3c07a3dc7', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'noscrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 1eb49abe-4064-11ec-aba1-c5f3c07a3dc7 -- ceph osd pool set bench_rados noscrub 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 noscrub to 1
# ./benchmarks/bench-rados:271 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados nodeep-scrub 1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:22:51,997401627-05:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:22:52,000141482-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '1eb49abe-4064-11ec-aba1-c5f3c07a3dc7', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'nodeep-scrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 1eb49abe-4064-11ec-aba1-c5f3c07a3dc7 -- ceph osd pool set bench_rados nodeep-scrub 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 nodeep-scrub to 1
# ./benchmarks/bench-rados:272 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool ls detail
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:22:56,959812582-05:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:22:56,962586963-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '1eb49abe-4064-11ec-aba1-c5f3c07a3dc7', '--', 'ceph', 'osd', 'pool', 'ls', 'detail'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 1eb49abe-4064-11ec-aba1-c5f3c07a3dc7 -- ceph osd pool ls detail
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
pool 1 'device_health_metrics' replicated size 1 min_size 1 crush_rule 0 object_hash rjenkins pg_num 64 pgp_num 64 autoscale_mode on last_change 16 lfor 0/0/14 flags hashpspool stripe_width 0 pg_num_min 1 application mgr_devicehealth
pool 2 'bench_rados' replicated size 1 min_size 1 crush_rule 0 object_hash rjenkins pg_num 128 pgp_num 128 autoscale_mode off last_change 20 flags hashpspool,noscrub,nodeep-scrub stripe_width 0 pg_num_min 128 application benchmark

# ./benchmarks/bench-rados:273 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd df tree
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:23:01,150472524-05:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:23:01,153308861-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '1eb49abe-4064-11ec-aba1-c5f3c07a3dc7', '--', 'ceph', 'osd', 'df', 'tree'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 1eb49abe-4064-11ec-aba1-c5f3c07a3dc7 -- ceph osd df tree
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
ID  CLASS  WEIGHT   REWEIGHT  SIZE     RAW USE  DATA     OMAP  META     AVAIL    %USE  VAR   PGS  STATUS  TYPE NAME      
-1         0.09769         -  100 GiB  5.7 MiB  168 KiB   0 B  5.6 MiB  100 GiB  0.01  1.00    -          root default   
-3         0.09769         -  100 GiB  5.7 MiB  168 KiB   0 B  5.6 MiB  100 GiB  0.01  1.00    -              host node-1
 0    ssd  0.09769   1.00000  100 GiB  5.7 MiB  168 KiB   0 B  5.6 MiB  100 GiB  0.01  1.00  192      up          osd.0  
                       TOTAL  100 GiB  5.7 MiB  168 KiB   0 B  5.6 MiB  100 GiB  0.01                                    
MIN/MAX VAR: 1.00/1.00  STDDEV: 0


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:23:05,290803594-05:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:23:29,382639548-05:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:23:38,583975543-05:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:23:38,591919900-05:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:23:47,493265851-05:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:23:47,501194239-05:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:23:56,695124596-05:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:23:56,703048775-05:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:24:05,862370620-05:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:24:05,870070326-05:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:24:15,130102347-05:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:24:15,137905237-05:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:24:15,144220313-05:00] INFO: > The cluster is idle now.[0m


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:24:15,148101079-05:00] STAGE: Start benchmarking of mode write ...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:24:15,155264825-05:00] INFO: > Start system activity collection services[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:24:15,160824506-05:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:341 - rados_bench() > rm --force /mnt/sda8/ceph-research/scripts/output/sys_activity_4MB_write.dat.rados_bench_host.3
# ./benchmarks/bench-rados:342 - rados_bench() > sar_pid_rados_bench_host=247680
# ./benchmarks/bench-rados:342 - rados_bench() > S_TIME_FORMAT=ISO
# ./benchmarks/bench-rados:342 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/output/sys_activity_4MB_write.dat.rados_bench_host.3 2

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:24:15,167798384-05:00] INFO: >> for OSD host[0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:24:15,176599336-05:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.2 bash
10.10.2.2: b'71455\n'
[1] 02:24:15 [SUCCESS] ljishen@10.10.2.2
71455

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:24:15,362720183-05:00] INFO: > Run rados bench[0m
# ./benchmarks/bench-rados:370 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/output/rados_bench_4MB_write.log.3
## ./benchmarks/bench-rados:361 - rados_bench() > count_cpus 32-63,96-127
# ./benchmarks/bench-rados:361 - rados_bench() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 60 write --pool bench_rados -b 4194304 -O 4194304 --concurrent-ios 128 --show-time --write-object --write-omap --write-xattr --no-cleanup
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:24:15,382094069-05:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:24:15,384698319-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '1eb49abe-4064-11ec-aba1-c5f3c07a3dc7', '--', 'rados', 'bench', '60', 'write', '--pool', 'bench_rados', '-b', '4194304', '-O', '4194304', '--concurrent-ios', '128', '--show-time', '--write-object', '--write-omap', '--write-xattr', '--no-cleanup'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 1eb49abe-4064-11ec-aba1-c5f3c07a3dc7 -- rados bench 60 write --pool bench_rados -b 4194304 -O 4194304 --concurrent-ios 128 --show-time --write-object --write-omap --write-xattr --no-cleanup
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
hints = 1
2021-11-08T07:24:18.723657+0000 Maintaining 128 concurrent writes of 4194304 bytes to objects of size 4194304 for up to 60 seconds or 0 objects
2021-11-08T07:24:18.723670+0000 Object prefix: benchmark_data_node-0.bluefield2.ucsc-cmps10_7
2021-11-08T07:24:18.847796+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-11-08T07:24:18.847796+0000     0       0         0         0         0         0           -           0
2021-11-08T07:24:19.847910+0000     1      86        86         0         0         0           -           0
2021-11-08T07:24:20.847984+0000     2     127       145        18   35.9975        36     1.73854     1.63743
2021-11-08T07:24:21.848055+0000     3     127       192        65   86.6607       188     2.24267     2.00588
2021-11-08T07:24:22.848134+0000     4     127       268       141    140.99       304      2.1066     2.13254
2021-11-08T07:24:23.848210+0000     5     127       284       157   125.591        64     2.64681     2.17009
2021-11-08T07:24:24.848281+0000     6     127       308       181   120.658        96     3.08356     2.27319
2021-11-08T07:24:25.848351+0000     7     127       346       219   125.134       152     3.64707      2.4725
2021-11-08T07:24:26.848425+0000     8     127       401       274    136.99       220     3.65299     2.73365
2021-11-08T07:24:27.848507+0000     9     127       447       320   142.212       184     2.73003     2.78883
2021-11-08T07:24:28.848582+0000    10     127       522       395   157.988       300     2.27131      2.7089
2021-11-08T07:24:29.848652+0000    11     127       536       409   148.716        56     2.59155     2.70083
2021-11-08T07:24:30.848726+0000    12     127       563       436   145.323       108     3.13404     2.71965
2021-11-08T07:24:31.848793+0000    13     127       598       471   144.913       140     3.63206     2.77258
2021-11-08T07:24:32.848869+0000    14     127       636       509   145.418       152     4.21147     2.86228
2021-11-08T07:24:33.848937+0000    15     127       655       528    140.79        76     4.50776     2.91849
2021-11-08T07:24:34.849010+0000    16     127       696       569    142.24       164     3.86033     3.01019
2021-11-08T07:24:35.849086+0000    17     127       776       649   152.695       320     2.56748     3.03632
2021-11-08T07:24:36.849157+0000    18     127       792       665   147.767        64     2.46789     3.02373
2021-11-08T07:24:37.849228+0000    19     127       811       684    143.99        76     3.05882     3.02143
2021-11-08T07:24:38.849300+0000 min lat: 1.51846 max lat: 4.81245 avg lat: 3.0441
2021-11-08T07:24:38.849300+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-11-08T07:24:38.849300+0000    20     127       852       725   144.989       164     3.66499      3.0441
2021-11-08T07:24:39.849386+0000    21     127       887       760   144.751       140     4.18334     3.08659
2021-11-08T07:24:40.849459+0000    22     127       912       785   142.717       100     4.60112     3.12899
2021-11-08T07:24:41.849530+0000    23     127       926       799   138.946        56     4.25033     3.15284
2021-11-08T07:24:42.849606+0000    24     127       961       834    138.99       140     4.41206      3.2066
2021-11-08T07:24:43.849681+0000    25     127      1000       873    139.67       156     4.28321     3.26036
2021-11-08T07:24:44.849757+0000    26     127      1033       906   139.374       132     4.29298     3.30285
2021-11-08T07:24:45.849827+0000    27     127      1047       920   136.286        56     4.50379     3.32137
2021-11-08T07:24:46.849912+0000    28     127      1073       946   135.133       104      4.5045     3.35522
2021-11-08T07:24:47.849983+0000    29     127      1108       981     135.3       140     4.49225     3.39725
2021-11-08T07:24:48.850056+0000    30     127      1143      1016   135.457       140     4.50346     3.43676
2021-11-08T07:24:49.850135+0000    31     127      1169      1042   134.442       104     4.42342     3.46332
2021-11-08T07:24:50.850207+0000    32     127      1182      1055   131.865        52     4.38045     3.47537
2021-11-08T07:24:51.850277+0000    33     127      1218      1091   132.233       144     4.46193     3.50999
2021-11-08T07:24:52.850349+0000    34     127      1251      1124   132.226       132     4.45763     3.53955
2021-11-08T07:24:53.850419+0000    35     127      1289      1162    132.79       152     4.53094      3.5716
2021-11-08T07:24:54.850498+0000    36     127      1302      1175   130.546        52     4.55261     3.58245
2021-11-08T07:24:55.850575+0000    37     127      1327      1200    129.72       100      4.4491     3.60239
2021-11-08T07:24:56.850650+0000    38     127      1364      1237   130.201       148     4.49145     3.62973
2021-11-08T07:24:57.850721+0000    39     127      1398      1271   130.349       136     4.42488     3.65378
2021-11-08T07:24:58.850791+0000 min lat: 1.51846 max lat: 4.89032 avg lat: 3.66907
2021-11-08T07:24:58.850791+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-11-08T07:24:58.850791+0000    40     127      1421      1294    129.39        92     4.46217     3.66907
2021-11-08T07:24:59.850869+0000    41     127      1437      1310   127.795        64     4.43667     3.68006
2021-11-08T07:25:00.850950+0000    42     127      1472      1345   128.086       140     4.51503     3.70241
2021-11-08T07:25:01.851021+0000    43     127      1509      1382   128.549       148     4.48154     3.72484
2021-11-08T07:25:02.851099+0000    44     127      1543      1416   128.718       136     4.48584     3.74441
2021-11-08T07:25:03.851171+0000    45     127      1559      1432   127.279        64     4.28134     3.75222
2021-11-08T07:25:04.851242+0000    46     127      1581      1454   126.425        88     4.46601     3.76347
2021-11-08T07:25:05.851312+0000    47     127      1619      1492   126.969       152     4.41666     3.78149
2021-11-08T07:25:06.851384+0000    48     127      1654      1527   127.241       140     4.43979     3.79736
2021-11-08T07:25:07.851459+0000    49     127      1681      1554   126.848       108     4.25451     3.80869
2021-11-08T07:25:08.851530+0000    50     127      1697      1570   125.591        64     4.25821      3.8143
2021-11-08T07:25:09.851600+0000    51     127      1725      1598   125.324       112     4.47164     3.82739
2021-11-08T07:25:10.851673+0000    52     127      1763      1636   125.837       152     4.58806     3.84464
2021-11-08T07:25:11.851743+0000    53     127      1800      1673   126.255       148     4.52977     3.86055
2021-11-08T07:25:12.851816+0000    54     127      1813      1686    124.88        52     4.66856     3.86664
2021-11-08T07:25:13.851885+0000    55     127      1832      1705   123.991        76     4.47515     3.87542
2021-11-08T07:25:14.851955+0000    56     127      1869      1742   124.419       148     4.50429     3.88887
2021-11-08T07:25:15.852027+0000    57     127      1908      1781   124.973       156     4.45137     3.90273
2021-11-08T07:25:16.852098+0000    58     127      1930      1803   124.336        88     4.32159     3.91029
2021-11-08T07:25:17.852167+0000    59     127      1946      1819   123.313        64     4.43511     3.91652
2021-11-08T07:25:18.852234+0000 min lat: 1.51846 max lat: 4.99689 avg lat: 3.92882
2021-11-08T07:25:18.852234+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-11-08T07:25:18.852234+0000    60     127      1977      1850   123.324       124     4.64028     3.92882
2021-11-08T07:25:19.852345+0000 Total time run:         60.2338
Total writes made:      1978
Write size:             4194304
Object size:            4194304
Bandwidth (MB/sec):     131.355
Stddev Bandwidth:       61.1682
Max bandwidth (MB/sec): 320
Min bandwidth (MB/sec): 0
Average IOPS:           32
Stddev IOPS:            15.2921
Max IOPS:               80
Min IOPS:               0
Average Latency(s):     3.84389
Stddev Latency(s):      0.952941
Max latency(s):         4.99689
Min latency(s):         0.206288

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:25:20,640444615-05:00] INFO: > Stop system activity collection services[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:25:20,646308791-05:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:391 - rados_bench() > kill -INT 247680

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:25:20,652150955-05:00] INFO: >> for OSD host[0m
# ./benchmarks/bench-rados:396 - rados_bench() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.2 kill -INT 71455
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:25:20,659853947-05:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.2 kill -INT 71455
[1] 02:25:20 [SUCCESS] ljishen@10.10.2.2

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:25:20,840308693-05:00] INFO: >>> Copy system activity file from OSD host[0m
## ./benchmarks/bench-rados:403 - rados_bench() > basename -- /tmp/bench-rados/sys_activity_4MB_write.dat.osd_host.3
# ./benchmarks/bench-rados:403 - rados_bench() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.2:/tmp/bench-rados/sys_activity_4MB_write.dat.osd_host.3 /mnt/sda8/ceph-research/scripts/output/sys_activity_4MB_write.dat.osd_host.3


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:25:21,010237375-05:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:25:44,966059448-05:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 1.98k objects, 7.7 GiB
    usage:   23 GiB used, 77 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:25:44,974023011-05:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:25:54,252998398-05:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 1.98k objects, 7.7 GiB
    usage:   23 GiB used, 77 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:25:54,260737147-05:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:26:03,400723395-05:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 1.98k objects, 7.7 GiB
    usage:   23 GiB used, 77 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:26:03,408646482-05:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:26:12,645024449-05:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 1.98k objects, 7.7 GiB
    usage:   23 GiB used, 77 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:26:12,652761355-05:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:26:21,687247394-05:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 1.98k objects, 7.7 GiB
    usage:   23 GiB used, 77 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:26:21,695163187-05:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:26:21,701171074-05:00] INFO: > The cluster is idle now.[0m


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:26:21,704944788-05:00] STAGE: Start benchmarking of mode seq ...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:26:21,711811233-05:00] INFO: > Start system activity collection services[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:26:21,717489749-05:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:341 - rados_bench() > rm --force /mnt/sda8/ceph-research/scripts/output/sys_activity_4MB_seq.dat.rados_bench_host.3
# ./benchmarks/bench-rados:342 - rados_bench() > sar_pid_rados_bench_host=249065
# ./benchmarks/bench-rados:342 - rados_bench() > S_TIME_FORMAT=ISO
# ./benchmarks/bench-rados:342 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/output/sys_activity_4MB_seq.dat.rados_bench_host.3 2

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:26:21,724616384-05:00] INFO: >> for OSD host[0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:26:21,733114095-05:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.2 bash
10.10.2.2: b'71561\n'
[1] 02:26:21 [SUCCESS] ljishen@10.10.2.2
71561

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:26:21,921527817-05:00] INFO: > Run rados bench[0m
# ./benchmarks/bench-rados:379 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/output/rados_bench_4MB_seq.log.3
## ./benchmarks/bench-rados:376 - rados_bench() > count_cpus 32-63,96-127
# ./benchmarks/bench-rados:376 - rados_bench() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 128 --show-time
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:26:21,941132907-05:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:26:21,943742397-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '1eb49abe-4064-11ec-aba1-c5f3c07a3dc7', '--', 'rados', 'bench', '99999999', 'seq', '--pool', 'bench_rados', '--concurrent-ios', '128', '--show-time'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 1eb49abe-4064-11ec-aba1-c5f3c07a3dc7 -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 128 --show-time
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
hints = 1
2021-11-08T07:26:25.129806+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-11-08T07:26:25.129806+0000     0       0         0         0         0         0           -           0
2021-11-08T07:26:26.129906+0000     1     127       409       282   1127.82      1128    0.346226    0.328603
2021-11-08T07:26:27.130003+0000     2     127       776       649   1297.83      1468    0.343514    0.339765
2021-11-08T07:26:28.130103+0000     3     127      1140      1013   1350.51      1456    0.344796    0.343867
2021-11-08T07:26:29.130196+0000     4     127      1512      1385   1384.84      1488     0.34422    0.344096
2021-11-08T07:26:30.130290+0000     5     127      1880      1753   1402.25      1472    0.346892    0.344646
2021-11-08T07:26:31.130409+0000 Total time run:       5.33067
Total reads made:     1978
Read size:            4194304
Object size:          4194304
Bandwidth (MB/sec):   1484.24
Average IOPS:         371
Stddev IOPS:          38.4552
Max IOPS:             372
Min IOPS:             282
Average Latency(s):   0.334906
Max latency(s):       0.36465
Min latency(s):       0.0682219

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:26:31,903678176-05:00] INFO: > Stop system activity collection services[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:26:31,909456510-05:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:391 - rados_bench() > kill -INT 249065

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:26:31,915835856-05:00] INFO: >> for OSD host[0m
# ./benchmarks/bench-rados:396 - rados_bench() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.2 kill -INT 71561
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:26:31,924051244-05:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.2 kill -INT 71561
[1] 02:26:32 [SUCCESS] ljishen@10.10.2.2

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:26:32,105259402-05:00] INFO: >>> Copy system activity file from OSD host[0m
## ./benchmarks/bench-rados:403 - rados_bench() > basename -- /tmp/bench-rados/sys_activity_4MB_seq.dat.osd_host.3
# ./benchmarks/bench-rados:403 - rados_bench() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.2:/tmp/bench-rados/sys_activity_4MB_seq.dat.osd_host.3 /mnt/sda8/ceph-research/scripts/output/sys_activity_4MB_seq.dat.osd_host.3


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:26:32,253242741-05:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:26:56,293259701-05:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 1.98k objects, 7.7 GiB
    usage:   23 GiB used, 77 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:26:56,301153053-05:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:27:05,503107643-05:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 1.98k objects, 7.7 GiB
    usage:   23 GiB used, 77 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:27:05,510786681-05:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:27:14,631183146-05:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 1.98k objects, 7.7 GiB
    usage:   23 GiB used, 77 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:27:14,639060948-05:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:27:23,788230281-05:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 1.98k objects, 7.7 GiB
    usage:   23 GiB used, 77 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:27:23,796208192-05:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:27:32,906348048-05:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 1.98k objects, 7.7 GiB
    usage:   23 GiB used, 77 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:27:32,913785830-05:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:27:32,920246982-05:00] INFO: > The cluster is idle now.[0m


[1;7;39;49m[2021-11-08T02:27:32,923984388-05:00][RUNNING][ROUND 1/7/21] object_size=16MB[0m


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:27:32,927522267-05:00] STAGE: Launch a Ceph cluster (CEPH_HOST: 10.10.2.3, OSD_HOST: 10.10.2.2) ...[0m
# ./benchmarks/bench-rados:183 - launch_ceph_cluster() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.3 bash -euo pipefail
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:27:32,936826589-05:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.3 bash -euo pipefail
10.10.2.3: b'\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T02:27:33,367791598-05:00] INFO: > Remove existing clusters\x1b[0m\n'
10.10.2.3: b'## bash:17 -  > basename -- /var/lib/ceph/1eb49abe-4064-11ec-aba1-c5f3c07a3dc7\n'
10.10.2.3: b'# bash:17 -  > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm --host 10.10.2.3 --host 10.10.2.2 rm-cluster --force --fsid 1eb49abe-4064-11ec-aba1-c5f3c07a3dc7\n'
10.10.2.3: b"\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T02:27:33,377945697-05:00] DEBUG: HOST_PARAMS: ['--host', '10.10.2.3', '--host', '10.10.2.2']\x1b[0m\n"
10.10.2.3: b"\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T02:27:33,381439368-05:00] DEBUG: CEPHADM_PARAMS: ['rm-cluster', '--force', '--fsid', '1eb49abe-4064-11ec-aba1-c5f3c07a3dc7']\x1b[0m\n"
10.10.2.3: b"# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:63 -  > /tmp/bench-rados/ceph-research/scripts/parallel-ssh --host 10.10.2.3 --host 10.10.2.2 'sudo python3 -u - rm-cluster --force --fsid 1eb49abe-4064-11ec-aba1-c5f3c07a3dc7'\n"
10.10.2.3: b'\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T02:27:33,389811715-05:00] DEBUG: command from stdin\x1b[0m\n'
10.10.2.3: b"# /tmp/bench-rados/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host 10.10.2.3 --host 10.10.2.2 'sudo python3 -u - rm-cluster --force --fsid 1eb49abe-4064-11ec-aba1-c5f3c07a3dc7'\n"
10.10.2.3: b'[1] 02:27:39 [SUCCESS] 10.10.2.3\n[2] 02:27:45 [SUCCESS] 10.10.2.2\n'
10.10.2.3: b'\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T02:27:45,633516778-05:00] INFO: > Deploy a new cluster\x1b[0m\n'
10.10.2.3: b'# bash:24 -  > /tmp/bench-rados/ceph-research/scripts/ceph-deploy -m 10.10.2.3 -o node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/10.10.2.2:/dev/nvme0n1\n'
10.10.2.3: b'\n\n\x1b[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T02:27:45,645181627-05:00] STAGE: Verify passwordless SSH to hosts, passwordless sudo via ljishen, and the existence of devices on hosts...\x1b[0m\n'
10.10.2.3: b'\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T02:27:45,649644979-05:00] INFO: > Check the host for the monitor\x1b[0m\n'
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:181 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@10.10.2.3 sudo --non-interactive --validate\n'
10.10.2.3: b'\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T02:27:45,798409255-05:00] INFO: > Check hosts for OSDs\x1b[0m\n'
10.10.2.3: b"\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T02:27:45,803500130-05:00] INFO: >> Check host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'\x1b[0m\n"
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:200 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us sudo --non-interactive --validate\n'
10.10.2.3: b"\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T02:27:45,949901468-05:00] INFO: >>> Check device '/dev/nvme0n1'\x1b[0m\n"
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:215 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us test -b /dev/nvme0n1\n'
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:226 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us lsblk --nodeps --noheadings --output TYPE /dev/nvme0n1\n'
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:227 - verify_parameters() > grep --word-regexp disk\n'
10.10.2.3: b'\n\n\x1b[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T02:27:46,226014213-05:00] STAGE: Reset devices on hosts...\x1b[0m\n'
10.10.2.3: b"\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T02:27:46,230955276-05:00] INFO: > Resetting device '/dev/nvme0n1' on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'\x1b[0m\n"
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:288 - reset_host_devices() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us sudo sh\n'
10.10.2.3: b'  Removing ceph--04874bc9--019e--4f6e--966a--878fefd274ab-osd--block--2f8ca28a--c893--4ea9--aaac--72f889646d7b (253:0)\n'
10.10.2.3: b'  Archiving volume group "ceph-04874bc9-019e-4f6e-966a-878fefd274ab" metadata (seqno 5).\n'
10.10.2.3: b'  Releasing logical volume "osd-block-2f8ca28a-c893-4ea9-aaac-72f889646d7b"\n'
10.10.2.3: b'  Creating volume group backup "/etc/lvm/backup/ceph-04874bc9-019e-4f6e-966a-878fefd274ab" (seqno 6).\n'
10.10.2.3: b'  Logical volume "osd-block-2f8ca28a-c893-4ea9-aaac-72f889646d7b" successfully removed\n'
10.10.2.3: b'  Removing physical volume "/dev/nvme0n1" from volume group "ceph-04874bc9-019e-4f6e-966a-878fefd274ab"\n'
10.10.2.3: b'  Volume group "ceph-04874bc9-019e-4f6e-966a-878fefd274ab" successfully removed\n'
10.10.2.3: b'\n\n\x1b[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T02:27:46,563687691-05:00] STAGE: Bootstrap a new cluster...\x1b[0m\n'
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:317 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.3 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults\n'
10.10.2.3: b'\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T02:27:46,573551331-05:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.3: b"\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T02:27:46,577437764-05:00] DEBUG: CEPHADM_PARAMS: ['bootstrap', '--config', '/tmp/bench-rados/ceph-research/scripts/ceph.conf', '--mon-ip', '10.10.2.3', '--output-dir', '/tmp/bench-rados/deployment_data_root/etc/ceph', '--ssh-user', 'ljishen', '--skip-dashboard', '--no-minimize-config', '--allow-overwrite', '--allow-fqdn-hostname', '--orphan-initial-daemons', '--skip-monitoring-stack', '--single-host-defaults']\x1b[0m\n"
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.3 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults\n'
10.10.2.3: b'Verifying podman|docker is present...\nVerifying lvm2 is present...\n'
10.10.2.3: b'Verifying time synchronization is in place...\n'
10.10.2.3: b'Unit ntp.service is enabled and running\n'
10.10.2.3: b'Repeating the final host check...\n'
10.10.2.3: b'docker (/usr/bin/docker) is present\n'
10.10.2.3: b'systemctl is present\n'
10.10.2.3: b'lvcreate is present\n'
10.10.2.3: b'Unit ntp.service is enabled and running\n'
10.10.2.3: b'Host looks OK\n'
10.10.2.3: b'Cluster fsid: 5ecad068-4065-11ec-aba1-c5f3c07a3dc7\n'
10.10.2.3: b'Verifying IP 10.10.2.3 port 3300 ...\n'
10.10.2.3: b'Verifying IP 10.10.2.3 port 6789 ...\n'
10.10.2.3: b'Mon IP `10.10.2.3` is in CIDR network `10.10.2.0/24`\n'
10.10.2.3: b'- internal network (--cluster-network) has not been provided, OSD replication will default to the public_network\n'
10.10.2.3: b'Adjusting default settings to suit single-host cluster...\n'
10.10.2.3: b'Pulling container image quay.io/ceph/ceph:v16...\n'
10.10.2.3: b'Ceph version: ceph version 16.2.6 (ee28fb57e47e9f88813e24bbf4c14496ca299d31) pacific (stable)\n'
10.10.2.3: b'Extracting ceph user uid/gid from container image...\n'
10.10.2.3: b'Creating initial keys...\n'
10.10.2.3: b'Creating initial monmap...\n'
10.10.2.3: b'Creating mon...\n'
10.10.2.3: b'Waiting for mon to start...\n'
10.10.2.3: b'Waiting for mon...\n'
10.10.2.3: b'mon is available\n'
10.10.2.3: b'Setting mon public_network to 10.10.2.0/24\n'
10.10.2.3: b'Wrote config to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf\n'
10.10.2.3: b'Wrote keyring to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring\n'
10.10.2.3: b'Creating mgr...\n'
10.10.2.3: b'Verifying port 9283 ...\n'
10.10.2.3: b'Waiting for mgr to start...\nWaiting for mgr...\n'
10.10.2.3: b'mgr not available, waiting (1/15)...\n'
10.10.2.3: b'mgr not available, waiting (2/15)...\n'
10.10.2.3: b'mgr is available\n'
10.10.2.3: b'Enabling cephadm module...\n'
10.10.2.3: b'Waiting for the mgr to restart...\n'
10.10.2.3: b'Waiting for mgr epoch 5...\n'
10.10.2.3: b'mgr epoch 5 is available\n'
10.10.2.3: b'Setting orchestrator backend to cephadm...\n'
10.10.2.3: b'Generating ssh key...\n'
10.10.2.3: b'Wrote public SSH key to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub\n'
10.10.2.3: b'Adding key to ljishen@localhost authorized_keys...\n'
10.10.2.3: b'Adding host node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us...\n'
10.10.2.3: b'Deploying unmanaged mon service...\n'
10.10.2.3: b'Deploying unmanaged mgr service...\n'
10.10.2.3: b'You can access the Ceph CLI with:\n\n\tsudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --fsid 5ecad068-4065-11ec-aba1-c5f3c07a3dc7 -c /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf -k /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring\n\n'
10.10.2.3: b'Please consider enabling telemetry to help improve Ceph:\n\n\tceph telemetry on\n\nFor more information see:\n\n\thttps://docs.ceph.com/docs/pacific/mgr/telemetry/\n\n'
10.10.2.3: b'Bootstrap complete.\n'
10.10.2.3: b'\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T02:28:57,317550333-05:00] INFO: Sleep for 20 seconds\x1b[0m\n'
10.10.2.3: b'\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T02:29:17,324472320-05:00] INFO: > Set the default number of monitor daemons to 1\x1b[0m\n'
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:337 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1\n'
10.10.2.3: b'\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T02:29:17,333751258-05:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.3: b"\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T02:29:17,337542932-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mon', '1']\x1b[0m\n"
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1\n'
10.10.2.3: b'Inferring fsid 5ecad068-4065-11ec-aba1-c5f3c07a3dc7\n'
10.10.2.3: b'Inferring config /var/lib/ceph/5ecad068-4065-11ec-aba1-c5f3c07a3dc7/mon.node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.3: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.3: b'Scheduled mon update...\n'
10.10.2.3: b'\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T02:29:27,468961876-05:00] INFO: > Set the default number of manager daemons to 1\x1b[0m\n'
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:342 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1\n'
10.10.2.3: b'\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T02:29:27,479382016-05:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.3: b"\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T02:29:27,483001515-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mgr', '1']\x1b[0m\n"
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1\n'
10.10.2.3: b'Inferring fsid 5ecad068-4065-11ec-aba1-c5f3c07a3dc7\n'
10.10.2.3: b'Inferring config /var/lib/ceph/5ecad068-4065-11ec-aba1-c5f3c07a3dc7/mon.node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.3: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.3: b'Scheduled mgr update...\n'
10.10.2.3: b'\n\n\x1b[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T02:29:37,107411294-05:00] STAGE: Add hosts to the cluster...\x1b[0m\n'
10.10.2.3: b"\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T02:29:37,113288934-05:00] INFO: > Adding host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'\x1b[0m\n"
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:361 - add_hosts() > ssh-copy-id -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -f -i /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\n'
10.10.2.3: b'/usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: "/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub"\n'
10.10.2.3: b'\nNumber of key(s) added: 1\n\nNow try logging into the machine, with:   "ssh -o \'GlobalKnownHostsFile=/dev/null\' -o \'LogLevel=ERROR\' -o \'PasswordAuthentication=no\' -o \'StrictHostKeyChecking=no\' -o \'UserKnownHostsFile=/dev/null\' \'ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\'"\nand check to make sure that only the key(s) you wanted were added.\n\n'
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:367 - add_hosts() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us 10.10.2.2\n'
10.10.2.3: b'\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T02:29:37,315903124-05:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.3: b"\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T02:29:37,319685010-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'host', 'add', 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us', '10.10.2.2']\x1b[0m\n"
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us 10.10.2.2\n'
10.10.2.3: b'Inferring fsid 5ecad068-4065-11ec-aba1-c5f3c07a3dc7\n'
10.10.2.3: b'Inferring config /var/lib/ceph/5ecad068-4065-11ec-aba1-c5f3c07a3dc7/mon.node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.3: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.3: b"Added host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us' with addr '10.10.2.2'\n"
10.10.2.3: b'\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T02:29:47,230430537-05:00] INFO: Sleep for 20 seconds\x1b[0m\n'
10.10.2.3: b'\n\n\x1b[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T02:30:07,235522109-05:00] STAGE: Add OSDs to the cluster...\x1b[0m\n'
10.10.2.3: b"\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T02:30:07,242160524-05:00] INFO: > Adding devices ['/dev/nvme0n1'] on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us' as OSDs\x1b[0m\n"
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:388 - add_host_devices_as_osds() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1\n'
10.10.2.3: b'\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T02:30:07,251865485-05:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.3: b"\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T02:30:07,255562190-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'daemon', 'add', 'osd', 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1']\x1b[0m\n"
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1\n'
10.10.2.3: b'Inferring fsid 5ecad068-4065-11ec-aba1-c5f3c07a3dc7\n'
10.10.2.3: b'Inferring config /var/lib/ceph/5ecad068-4065-11ec-aba1-c5f3c07a3dc7/mon.node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.3: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.3: b"Created osd(s) 0 on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'\n"
10.10.2.3: b'\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T02:30:32,892571113-05:00] INFO: > Sleep for 20 seconds\x1b[0m\n'
10.10.2.3: b'\n\n\x1b[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T02:30:52,898183341-05:00] STAGE: Show Cluster Status\x1b[0m\n'
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:408 - show_status() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status\n'
10.10.2.3: b'\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T02:30:52,908272857-05:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.3: b"\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T02:30:52,912047499-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', '--status']\x1b[0m\n"
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status\n'
10.10.2.3: b'Inferring fsid 5ecad068-4065-11ec-aba1-c5f3c07a3dc7\n'
10.10.2.3: b'Inferring config /var/lib/ceph/5ecad068-4065-11ec-aba1-c5f3c07a3dc7/mon.node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.3: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.3: b'  cluster:\n    id:     5ecad068-4065-11ec-aba1-c5f3c07a3dc7\n    health: HEALTH_WARN\n            1 pool(s) have no replicas configured\n \n  services:\n    mon: 1 daemons, quorum node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us (age 2m)\n    mgr: node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us.nczypp(active, since 2m)\n    osd: 1 osds: 1 up (since 18s), 1 in (since 40s)\n \n  data:\n    pools:   1 pools, 1 pgs\n    objects: 0 objects, 0 B\n    usage:   4.7 MiB used, 100 GiB / 100 GiB avail\n    pgs:     1 active+clean\n \n'
[1] 02:31:01 [SUCCESS] ljishen@10.10.2.3

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:27:33,367791598-05:00] INFO: > Remove existing clusters[0m
## bash:17 -  > basename -- /var/lib/ceph/1eb49abe-4064-11ec-aba1-c5f3c07a3dc7
# bash:17 -  > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm --host 10.10.2.3 --host 10.10.2.2 rm-cluster --force --fsid 1eb49abe-4064-11ec-aba1-c5f3c07a3dc7
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:27:33,377945697-05:00] DEBUG: HOST_PARAMS: ['--host', '10.10.2.3', '--host', '10.10.2.2'][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:27:33,381439368-05:00] DEBUG: CEPHADM_PARAMS: ['rm-cluster', '--force', '--fsid', '1eb49abe-4064-11ec-aba1-c5f3c07a3dc7'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:63 -  > /tmp/bench-rados/ceph-research/scripts/parallel-ssh --host 10.10.2.3 --host 10.10.2.2 'sudo python3 -u - rm-cluster --force --fsid 1eb49abe-4064-11ec-aba1-c5f3c07a3dc7'
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:27:33,389811715-05:00] DEBUG: command from stdin[0m
# /tmp/bench-rados/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host 10.10.2.3 --host 10.10.2.2 'sudo python3 -u - rm-cluster --force --fsid 1eb49abe-4064-11ec-aba1-c5f3c07a3dc7'
[1] 02:27:39 [SUCCESS] 10.10.2.3
[2] 02:27:45 [SUCCESS] 10.10.2.2

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:27:45,633516778-05:00] INFO: > Deploy a new cluster[0m
# bash:24 -  > /tmp/bench-rados/ceph-research/scripts/ceph-deploy -m 10.10.2.3 -o node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/10.10.2.2:/dev/nvme0n1


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:27:45,645181627-05:00] STAGE: Verify passwordless SSH to hosts, passwordless sudo via ljishen, and the existence of devices on hosts...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:27:45,649644979-05:00] INFO: > Check the host for the monitor[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:181 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@10.10.2.3 sudo --non-interactive --validate

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:27:45,798409255-05:00] INFO: > Check hosts for OSDs[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:27:45,803500130-05:00] INFO: >> Check host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:200 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us sudo --non-interactive --validate

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:27:45,949901468-05:00] INFO: >>> Check device '/dev/nvme0n1'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:215 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us test -b /dev/nvme0n1
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:226 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us lsblk --nodeps --noheadings --output TYPE /dev/nvme0n1
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:227 - verify_parameters() > grep --word-regexp disk


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:27:46,226014213-05:00] STAGE: Reset devices on hosts...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:27:46,230955276-05:00] INFO: > Resetting device '/dev/nvme0n1' on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:288 - reset_host_devices() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us sudo sh
  Removing ceph--04874bc9--019e--4f6e--966a--878fefd274ab-osd--block--2f8ca28a--c893--4ea9--aaac--72f889646d7b (253:0)
  Archiving volume group "ceph-04874bc9-019e-4f6e-966a-878fefd274ab" metadata (seqno 5).
  Releasing logical volume "osd-block-2f8ca28a-c893-4ea9-aaac-72f889646d7b"
  Creating volume group backup "/etc/lvm/backup/ceph-04874bc9-019e-4f6e-966a-878fefd274ab" (seqno 6).
  Logical volume "osd-block-2f8ca28a-c893-4ea9-aaac-72f889646d7b" successfully removed
  Removing physical volume "/dev/nvme0n1" from volume group "ceph-04874bc9-019e-4f6e-966a-878fefd274ab"
  Volume group "ceph-04874bc9-019e-4f6e-966a-878fefd274ab" successfully removed


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:27:46,563687691-05:00] STAGE: Bootstrap a new cluster...[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:317 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.3 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:27:46,573551331-05:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:27:46,577437764-05:00] DEBUG: CEPHADM_PARAMS: ['bootstrap', '--config', '/tmp/bench-rados/ceph-research/scripts/ceph.conf', '--mon-ip', '10.10.2.3', '--output-dir', '/tmp/bench-rados/deployment_data_root/etc/ceph', '--ssh-user', 'ljishen', '--skip-dashboard', '--no-minimize-config', '--allow-overwrite', '--allow-fqdn-hostname', '--orphan-initial-daemons', '--skip-monitoring-stack', '--single-host-defaults'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.3 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults
Verifying podman|docker is present...
Verifying lvm2 is present...
Verifying time synchronization is in place...
Unit ntp.service is enabled and running
Repeating the final host check...
docker (/usr/bin/docker) is present
systemctl is present
lvcreate is present
Unit ntp.service is enabled and running
Host looks OK
Cluster fsid: 5ecad068-4065-11ec-aba1-c5f3c07a3dc7
Verifying IP 10.10.2.3 port 3300 ...
Verifying IP 10.10.2.3 port 6789 ...
Mon IP `10.10.2.3` is in CIDR network `10.10.2.0/24`
- internal network (--cluster-network) has not been provided, OSD replication will default to the public_network
Adjusting default settings to suit single-host cluster...
Pulling container image quay.io/ceph/ceph:v16...
Ceph version: ceph version 16.2.6 (ee28fb57e47e9f88813e24bbf4c14496ca299d31) pacific (stable)
Extracting ceph user uid/gid from container image...
Creating initial keys...
Creating initial monmap...
Creating mon...
Waiting for mon to start...
Waiting for mon...
mon is available
Setting mon public_network to 10.10.2.0/24
Wrote config to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf
Wrote keyring to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring
Creating mgr...
Verifying port 9283 ...
Waiting for mgr to start...
Waiting for mgr...
mgr not available, waiting (1/15)...
mgr not available, waiting (2/15)...
mgr is available
Enabling cephadm module...
Waiting for the mgr to restart...
Waiting for mgr epoch 5...
mgr epoch 5 is available
Setting orchestrator backend to cephadm...
Generating ssh key...
Wrote public SSH key to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub
Adding key to ljishen@localhost authorized_keys...
Adding host node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us...
Deploying unmanaged mon service...
Deploying unmanaged mgr service...
You can access the Ceph CLI with:

	sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --fsid 5ecad068-4065-11ec-aba1-c5f3c07a3dc7 -c /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf -k /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring

Please consider enabling telemetry to help improve Ceph:

	ceph telemetry on

For more information see:

	https://docs.ceph.com/docs/pacific/mgr/telemetry/

Bootstrap complete.

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:28:57,317550333-05:00] INFO: Sleep for 20 seconds[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:29:17,324472320-05:00] INFO: > Set the default number of monitor daemons to 1[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:337 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:29:17,333751258-05:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:29:17,337542932-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mon', '1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1
Inferring fsid 5ecad068-4065-11ec-aba1-c5f3c07a3dc7
Inferring config /var/lib/ceph/5ecad068-4065-11ec-aba1-c5f3c07a3dc7/mon.node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Scheduled mon update...

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:29:27,468961876-05:00] INFO: > Set the default number of manager daemons to 1[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:342 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:29:27,479382016-05:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:29:27,483001515-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mgr', '1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1
Inferring fsid 5ecad068-4065-11ec-aba1-c5f3c07a3dc7
Inferring config /var/lib/ceph/5ecad068-4065-11ec-aba1-c5f3c07a3dc7/mon.node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Scheduled mgr update...


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:29:37,107411294-05:00] STAGE: Add hosts to the cluster...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:29:37,113288934-05:00] INFO: > Adding host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:361 - add_hosts() > ssh-copy-id -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -f -i /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us
/usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: "/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub"

Number of key(s) added: 1

Now try logging into the machine, with:   "ssh -o 'GlobalKnownHostsFile=/dev/null' -o 'LogLevel=ERROR' -o 'PasswordAuthentication=no' -o 'StrictHostKeyChecking=no' -o 'UserKnownHostsFile=/dev/null' 'ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'"
and check to make sure that only the key(s) you wanted were added.

# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:367 - add_hosts() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us 10.10.2.2
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:29:37,315903124-05:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:29:37,319685010-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'host', 'add', 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us', '10.10.2.2'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us 10.10.2.2
Inferring fsid 5ecad068-4065-11ec-aba1-c5f3c07a3dc7
Inferring config /var/lib/ceph/5ecad068-4065-11ec-aba1-c5f3c07a3dc7/mon.node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Added host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us' with addr '10.10.2.2'

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:29:47,230430537-05:00] INFO: Sleep for 20 seconds[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:30:07,235522109-05:00] STAGE: Add OSDs to the cluster...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:30:07,242160524-05:00] INFO: > Adding devices ['/dev/nvme0n1'] on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us' as OSDs[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:388 - add_host_devices_as_osds() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:30:07,251865485-05:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:30:07,255562190-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'daemon', 'add', 'osd', 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1
Inferring fsid 5ecad068-4065-11ec-aba1-c5f3c07a3dc7
Inferring config /var/lib/ceph/5ecad068-4065-11ec-aba1-c5f3c07a3dc7/mon.node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Created osd(s) 0 on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:30:32,892571113-05:00] INFO: > Sleep for 20 seconds[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:30:52,898183341-05:00] STAGE: Show Cluster Status[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:408 - show_status() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:30:52,908272857-05:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:30:52,912047499-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', '--status'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status
Inferring fsid 5ecad068-4065-11ec-aba1-c5f3c07a3dc7
Inferring config /var/lib/ceph/5ecad068-4065-11ec-aba1-c5f3c07a3dc7/mon.node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
  cluster:
    id:     5ecad068-4065-11ec-aba1-c5f3c07a3dc7
    health: HEALTH_WARN
            1 pool(s) have no replicas configured
 
  services:
    mon: 1 daemons, quorum node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us (age 2m)
    mgr: node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us.nczypp(active, since 2m)
    osd: 1 osds: 1 up (since 18s), 1 in (since 40s)
 
  data:
    pools:   1 pools, 1 pgs
    objects: 0 objects, 0 B
    usage:   4.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     1 active+clean
 

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:31:01,150396456-05:00] INFO: > Copy configuration files to local[0m
# ./benchmarks/bench-rados:220 - launch_ceph_cluster() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.3 sudo chown --changes --recursive ljishen /tmp/bench-rados/deployment_data_root/etc/ceph
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:31:01,157867751-05:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.3 sudo chown --changes --recursive ljishen /tmp/bench-rados/deployment_data_root/etc/ceph
[1] 02:31:01 [SUCCESS] ljishen@10.10.2.3
# ./benchmarks/bench-rados:223 - launch_ceph_cluster() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.3:/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf
# ./benchmarks/bench-rados:226 - launch_ceph_cluster() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.3:/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:31:01,628717660-05:00] STAGE: Update local ceph.conf as a client...[0m


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:31:01,632327856-05:00] STAGE: Configure Ceph pool...[0m
# ./benchmarks/bench-rados:257 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_max_backfills 32
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:31:01,653125836-05:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:31:01,655683177-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '5ecad068-4065-11ec-aba1-c5f3c07a3dc7', '--', 'ceph', 'config', 'set', 'osd', 'osd_max_backfills', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 5ecad068-4065-11ec-aba1-c5f3c07a3dc7 -- ceph config set osd osd_max_backfills 32
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:258 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_active 32
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:31:05,827986295-05:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:31:05,830587479-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '5ecad068-4065-11ec-aba1-c5f3c07a3dc7', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_active', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 5ecad068-4065-11ec-aba1-c5f3c07a3dc7 -- ceph config set osd osd_recovery_max_active 32
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:259 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_single_start 8
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:31:09,965959414-05:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:31:09,968851155-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '5ecad068-4065-11ec-aba1-c5f3c07a3dc7', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_single_start', '8'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 5ecad068-4065-11ec-aba1-c5f3c07a3dc7 -- ceph config set osd osd_recovery_max_single_start 8
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:260 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_op_priority 63
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:31:14,225150822-05:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:31:14,227861923-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '5ecad068-4065-11ec-aba1-c5f3c07a3dc7', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_op_priority', '63'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 5ecad068-4065-11ec-aba1-c5f3c07a3dc7 -- ceph config set osd osd_recovery_op_priority 63
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:262 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config show-with-defaults osd.0
# ./benchmarks/bench-rados:263 - configure_ceph_pool() > grep 'osd_max_backfills\|osd_recovery'
# ./benchmarks/bench-rados:263 - configure_ceph_pool() > column -t -s ' '
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
osd_max_backfills                        32         mon
osd_recovery_cost                        20971520   default
osd_recovery_delay_start                 0.000000   default
osd_recovery_max_active                  32         mon
osd_recovery_max_active_hdd              3          default
osd_recovery_max_active_ssd              10         default
osd_recovery_max_chunk                   8388608    default
osd_recovery_max_omap_entries_per_chunk  8096       default
osd_recovery_max_single_start            8          mon
osd_recovery_op_priority                 63         mon
osd_recovery_op_warn_multiple            16         default
osd_recovery_priority                    5          default
osd_recovery_retry_interval              30.000000  default
osd_recovery_sleep                       0.000000   default
osd_recovery_sleep_hdd                   0.100000   default
osd_recovery_sleep_hybrid                0.025000   default
osd_recovery_sleep_ssd                   0.000000   default
# ./benchmarks/bench-rados:265 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool create bench_rados 128 128 replicated --size 1 --pg-num-min 128
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:31:22,551041510-05:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:31:22,553931338-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '5ecad068-4065-11ec-aba1-c5f3c07a3dc7', '--', 'ceph', 'osd', 'pool', 'create', 'bench_rados', '128', '128', 'replicated', '--size', '1', '--pg-num-min', '128'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 5ecad068-4065-11ec-aba1-c5f3c07a3dc7 -- ceph osd pool create bench_rados 128 128 replicated --size 1 --pg-num-min 128
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
pool 'bench_rados' created
# ./benchmarks/bench-rados:267 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados min_size 1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:31:26,801597890-05:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:31:26,804262493-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '5ecad068-4065-11ec-aba1-c5f3c07a3dc7', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'min_size', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 5ecad068-4065-11ec-aba1-c5f3c07a3dc7 -- ceph osd pool set bench_rados min_size 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 min_size to 1
# ./benchmarks/bench-rados:268 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados pg_autoscale_mode off
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:31:31,293807944-05:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:31:31,296784245-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '5ecad068-4065-11ec-aba1-c5f3c07a3dc7', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'pg_autoscale_mode', 'off'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 5ecad068-4065-11ec-aba1-c5f3c07a3dc7 -- ceph osd pool set bench_rados pg_autoscale_mode off
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 pg_autoscale_mode to off
# ./benchmarks/bench-rados:269 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool application enable bench_rados benchmark
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:31:35,630798749-05:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:31:35,633551558-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '5ecad068-4065-11ec-aba1-c5f3c07a3dc7', '--', 'ceph', 'osd', 'pool', 'application', 'enable', 'bench_rados', 'benchmark'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 5ecad068-4065-11ec-aba1-c5f3c07a3dc7 -- ceph osd pool application enable bench_rados benchmark
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
enabled application 'benchmark' on pool 'bench_rados'
# ./benchmarks/bench-rados:270 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados noscrub 1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:31:40,053210135-05:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:31:40,056194191-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '5ecad068-4065-11ec-aba1-c5f3c07a3dc7', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'noscrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 5ecad068-4065-11ec-aba1-c5f3c07a3dc7 -- ceph osd pool set bench_rados noscrub 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 noscrub to 1
# ./benchmarks/bench-rados:271 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados nodeep-scrub 1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:31:44,833614648-05:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:31:44,836685788-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '5ecad068-4065-11ec-aba1-c5f3c07a3dc7', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'nodeep-scrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 5ecad068-4065-11ec-aba1-c5f3c07a3dc7 -- ceph osd pool set bench_rados nodeep-scrub 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 nodeep-scrub to 1
# ./benchmarks/bench-rados:272 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool ls detail
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:31:49,666201078-05:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:31:49,669294920-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '5ecad068-4065-11ec-aba1-c5f3c07a3dc7', '--', 'ceph', 'osd', 'pool', 'ls', 'detail'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 5ecad068-4065-11ec-aba1-c5f3c07a3dc7 -- ceph osd pool ls detail
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
pool 1 'device_health_metrics' replicated size 1 min_size 1 crush_rule 0 object_hash rjenkins pg_num 64 pgp_num 64 autoscale_mode on last_change 18 lfor 0/0/15 flags hashpspool stripe_width 0 pg_num_min 1 application mgr_devicehealth
pool 2 'bench_rados' replicated size 1 min_size 1 crush_rule 0 object_hash rjenkins pg_num 128 pgp_num 128 autoscale_mode off last_change 20 flags hashpspool,noscrub,nodeep-scrub stripe_width 0 pg_num_min 128 application benchmark

# ./benchmarks/bench-rados:273 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd df tree
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:31:53,857777731-05:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:31:53,860779410-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '5ecad068-4065-11ec-aba1-c5f3c07a3dc7', '--', 'ceph', 'osd', 'df', 'tree'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 5ecad068-4065-11ec-aba1-c5f3c07a3dc7 -- ceph osd df tree
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
ID  CLASS  WEIGHT   REWEIGHT  SIZE     RAW USE  DATA     OMAP  META     AVAIL    %USE  VAR   PGS  STATUS  TYPE NAME      
-1         0.09769         -  100 GiB  5.7 MiB  168 KiB   0 B  5.6 MiB  100 GiB  0.01  1.00    -          root default   
-3         0.09769         -  100 GiB  5.7 MiB  168 KiB   0 B  5.6 MiB  100 GiB  0.01  1.00    -              host node-1
 0    ssd  0.09769   1.00000  100 GiB  5.7 MiB  168 KiB   0 B  5.6 MiB  100 GiB  0.01  1.00  192      up          osd.0  
                       TOTAL  100 GiB  5.7 MiB  168 KiB   0 B  5.6 MiB  100 GiB  0.01                                    
MIN/MAX VAR: 1.00/1.00  STDDEV: 0


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:31:57,975687261-05:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:32:21,992984594-05:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:32:31,112861078-05:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:32:40,161162627-05:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:32:40,169049536-05:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:32:49,214497780-05:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:32:49,221792021-05:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:32:58,367397172-05:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:32:58,375477396-05:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:33:07,562460017-05:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:33:07,570217983-05:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:33:16,767317893-05:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:33:16,775104343-05:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:33:16,781496904-05:00] INFO: > The cluster is idle now.[0m


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:33:16,785270679-05:00] STAGE: Start benchmarking of mode write ...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:33:16,792328916-05:00] INFO: > Start system activity collection services[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:33:16,797957878-05:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:341 - rados_bench() > rm --force /mnt/sda8/ceph-research/scripts/output/sys_activity_16MB_write.dat.rados_bench_host.1
# ./benchmarks/bench-rados:342 - rados_bench() > sar_pid_rados_bench_host=254687
# ./benchmarks/bench-rados:342 - rados_bench() > S_TIME_FORMAT=ISO
# ./benchmarks/bench-rados:342 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/output/sys_activity_16MB_write.dat.rados_bench_host.1 2

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:33:16,805165216-05:00] INFO: >> for OSD host[0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:33:16,813756964-05:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.2 bash
10.10.2.2: b'75187\n'
[1] 02:33:16 [SUCCESS] ljishen@10.10.2.2
75187

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:33:17,001540006-05:00] INFO: > Run rados bench[0m
# ./benchmarks/bench-rados:370 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/output/rados_bench_16MB_write.log.1
## ./benchmarks/bench-rados:361 - rados_bench() > count_cpus 32-63,96-127
# ./benchmarks/bench-rados:361 - rados_bench() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 60 write --pool bench_rados -b 16777216 -O 16777216 --concurrent-ios 128 --show-time --write-object --write-omap --write-xattr --no-cleanup
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:33:17,021046041-05:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:33:17,024036699-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '5ecad068-4065-11ec-aba1-c5f3c07a3dc7', '--', 'rados', 'bench', '60', 'write', '--pool', 'bench_rados', '-b', '16777216', '-O', '16777216', '--concurrent-ios', '128', '--show-time', '--write-object', '--write-omap', '--write-xattr', '--no-cleanup'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 5ecad068-4065-11ec-aba1-c5f3c07a3dc7 -- rados bench 60 write --pool bench_rados -b 16777216 -O 16777216 --concurrent-ios 128 --show-time --write-object --write-omap --write-xattr --no-cleanup
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
hints = 1
2021-11-08T07:33:20.363523+0000 Maintaining 128 concurrent writes of 16777216 bytes to objects of size 16777216 for up to 60 seconds or 0 objects
2021-11-08T07:33:20.363557+0000 Object prefix: benchmark_data_node-0.bluefield2.ucsc-cmps10_7
2021-11-08T07:33:21.031783+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-11-08T07:33:21.031783+0000     0       0         0         0         0         0           -           0
2021-11-08T07:33:22.031880+0000     1      19        19         0         0         0           -           0
2021-11-08T07:33:23.031951+0000     2      35        35         0         0         0           -           0
2021-11-08T07:33:24.032021+0000     3      45        45         0         0         0           -           0
2021-11-08T07:33:25.032089+0000     4      64        64         0         0         0           -           0
2021-11-08T07:33:26.032155+0000     5      69        69         0         0         0           -           0
2021-11-08T07:33:27.032222+0000     6      77        77         0         0         0           -           0
2021-11-08T07:33:28.032290+0000     7      88        88         0         0         0           -           0
2021-11-08T07:33:29.032356+0000     8     100       100         0         0         0           -           0
2021-11-08T07:33:30.032423+0000     9     113       113         0         0         0           -           0
2021-11-08T07:33:31.032491+0000    10     127       130         3   4.79969       4.8     9.93225     9.82424
2021-11-08T07:33:32.032563+0000    11     127       133         6    8.7267        48     10.6054       10.04
2021-11-08T07:33:33.032628+0000    12     127       142        15   19.9987       144      11.164     10.6131
2021-11-08T07:33:34.032708+0000    13     127       153        26   31.9979       176     11.6276     10.9587
2021-11-08T07:33:35.032794+0000    14     127       162        35   39.9973       144     12.1964     11.1971
2021-11-08T07:33:36.032871+0000    15     127       164        37    39.464        32     12.6619     11.2699
2021-11-08T07:33:37.032961+0000    16     127       175        48   47.9966       176      12.891     11.6136
2021-11-08T07:33:38.033040+0000    17     127       192        65   61.1722       272     13.0095     11.9593
2021-11-08T07:33:39.033119+0000    18     127       197        70   62.2178        80      12.929     12.0362
2021-11-08T07:33:40.033195+0000    19     127       207        80   67.3636       160     12.7951     12.1312
2021-11-08T07:33:41.033273+0000 min lat: 9.73921 max lat: 13.0743 avg lat: 12.2168
2021-11-08T07:33:41.033273+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-11-08T07:33:41.033273+0000    20     127       219        92   73.5947       192     12.8026     12.2168
2021-11-08T07:33:42.033353+0000    21     127       227       100    76.185       128     13.2308     12.2748
2021-11-08T07:33:43.033437+0000    22     127       229       102   74.1764        32     13.6502     12.2998
2021-11-08T07:33:44.033521+0000    23     127       240       113    78.603       176     14.0633      12.453
2021-11-08T07:33:45.033609+0000    24     127       251       124   82.6606       176     14.4921     12.6162
2021-11-08T07:33:46.033688+0000    25     127       259       132   84.4738       128     14.8103     12.7394
2021-11-08T07:33:47.033778+0000    26     127       262       135   83.0707        48     14.7657     12.7846
2021-11-08T07:33:48.033859+0000    27     127       272       145   85.9195       160      14.829     12.9257
2021-11-08T07:33:49.033943+0000    28     127       284       157   89.7076       192     14.8041     13.0693
2021-11-08T07:33:50.034019+0000    29     127       291       164    90.476       112     14.5191     13.1363
2021-11-08T07:33:51.034097+0000    30     127       295       168   89.5933        64     14.7846     13.1726
2021-11-08T07:33:52.034185+0000    31     127       306       179   92.3801       176     14.8183     13.2709
2021-11-08T07:33:53.034277+0000    32     127       316       189   94.4928       160     15.2621     13.3663
2021-11-08T07:33:54.034360+0000    33     127       323       196   95.0231       112     15.5587     13.4375
2021-11-08T07:33:55.034444+0000    34     127       327       200   94.1104        64     15.7886     13.4848
2021-11-08T07:33:56.034522+0000    35     127       338       211   96.4498       176     15.7165      13.603
2021-11-08T07:33:57.034604+0000    36     127       350       223   99.1035       192     15.7258     13.7172
2021-11-08T07:33:58.034680+0000    37     127       355       228    98.587        80      15.671      13.761
2021-11-08T07:33:59.034766+0000    38     127       360       233   98.0977        80     15.7033     13.8023
2021-11-08T07:34:00.034848+0000    39     127       371       244   100.095       176     15.7121     13.8893
2021-11-08T07:34:01.034933+0000 min lat: 9.73921 max lat: 15.8814 avg lat: 13.9674
2021-11-08T07:34:01.034933+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-11-08T07:34:01.034933+0000    40     127       382       255   101.992       176     15.6966     13.9674
2021-11-08T07:34:02.035017+0000    41     127       387       260   101.456        80     15.7227     14.0006
2021-11-08T07:34:03.035102+0000    42     127       393       266   101.325        96     15.7312     14.0404
2021-11-08T07:34:04.035185+0000    43     127       403       276    102.69       160     15.7424     14.1026
2021-11-08T07:34:05.035263+0000    44     127       414       287   104.356       176     15.8088     14.1671
2021-11-08T07:34:06.035338+0000    45     127       419       292   103.814        80     15.8331     14.1956
2021-11-08T07:34:07.035420+0000    46     127       425       298   103.644        96     15.8179     14.2283
2021-11-08T07:34:08.035500+0000    47     127       436       309   105.183       176     15.8042     14.2848
2021-11-08T07:34:09.035583+0000    48     127       447       320   106.658       176     15.7696     14.3368
2021-11-08T07:34:10.035664+0000    49     127       452       325   106.114        80     15.7585     14.3585
2021-11-08T07:34:11.035754+0000    50     127       458       331   105.912        96     15.7175     14.3831
2021-11-08T07:34:12.035845+0000    51     127       469       342   107.286       176     15.7634     14.4276
2021-11-08T07:34:13.035929+0000    52     127       480       353   108.607       176     15.7648     14.4695
2021-11-08T07:34:14.036008+0000    53     127       484       357   107.765        64     15.7237     14.4838
2021-11-08T07:34:15.036093+0000    54     127       492       365    108.14       128     15.6464       14.51
2021-11-08T07:34:16.036176+0000    55     127       502       375   109.082       160     15.7235     14.5417
2021-11-08T07:34:17.036261+0000    56     127       513       386   110.277       176     15.7613     14.5761
2021-11-08T07:34:18.036343+0000    57     127       516       389   109.184        48     15.6303     14.5847
2021-11-08T07:34:19.036425+0000    58     127       523       396   109.233       112     15.7017      14.605
2021-11-08T07:34:20.036505+0000    59     127       534       407   110.364       176     15.6799     14.6349
2021-11-08T07:34:21.036587+0000 min lat: 9.73921 max lat: 15.8814 avg lat: 14.6623
2021-11-08T07:34:21.036587+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-11-08T07:34:21.036587+0000    60     127       545       418   111.458       176      15.665     14.6623
2021-11-08T07:34:22.036701+0000 Total time run:         60.5927
Total writes made:      546
Write size:             16777216
Object size:            16777216
Bandwidth (MB/sec):     144.176
Stddev Bandwidth:       69.8519
Max bandwidth (MB/sec): 272
Min bandwidth (MB/sec): 0
Average IOPS:           9
Stddev IOPS:            4.37362
Max IOPS:               17
Min IOPS:               0
Average Latency(s):     12.9817
Stddev Latency(s):      3.954
Max latency(s):         15.8814
Min latency(s):         0.261915

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:34:22,820349364-05:00] INFO: > Stop system activity collection services[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:34:22,826389542-05:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:391 - rados_bench() > kill -INT 254687

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:34:22,832704507-05:00] INFO: >> for OSD host[0m
# ./benchmarks/bench-rados:396 - rados_bench() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.2 kill -INT 75187
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:34:22,840618167-05:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.2 kill -INT 75187
[1] 02:34:23 [SUCCESS] ljishen@10.10.2.2

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:34:23,024842501-05:00] INFO: >>> Copy system activity file from OSD host[0m
## ./benchmarks/bench-rados:403 - rados_bench() > basename -- /tmp/bench-rados/sys_activity_16MB_write.dat.osd_host.1
# ./benchmarks/bench-rados:403 - rados_bench() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.2:/tmp/bench-rados/sys_activity_16MB_write.dat.osd_host.1 /mnt/sda8/ceph-research/scripts/output/sys_activity_16MB_write.dat.osd_host.1


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:34:23,195066421-05:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:34:47,238015539-05:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 547 objects, 8.5 GiB
    usage:   26 GiB used, 74 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:34:47,245646035-05:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:34:56,226344406-05:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 547 objects, 8.5 GiB
    usage:   26 GiB used, 74 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:34:56,234193504-05:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:35:05,678750142-05:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 547 objects, 8.5 GiB
    usage:   26 GiB used, 74 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:35:05,686574403-05:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:35:14,823936808-05:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 547 objects, 8.5 GiB
    usage:   26 GiB used, 74 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:35:14,832144742-05:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:35:23,908860023-05:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 547 objects, 8.5 GiB
    usage:   26 GiB used, 74 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:35:23,917185038-05:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:35:23,923281311-05:00] INFO: > The cluster is idle now.[0m


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:35:23,927026773-05:00] STAGE: Start benchmarking of mode seq ...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:35:23,934142498-05:00] INFO: > Start system activity collection services[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:35:23,939789023-05:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:341 - rados_bench() > rm --force /mnt/sda8/ceph-research/scripts/output/sys_activity_16MB_seq.dat.rados_bench_host.1
# ./benchmarks/bench-rados:342 - rados_bench() > sar_pid_rados_bench_host=256037
# ./benchmarks/bench-rados:342 - rados_bench() > S_TIME_FORMAT=ISO
# ./benchmarks/bench-rados:342 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/output/sys_activity_16MB_seq.dat.rados_bench_host.1 2

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:35:23,947154319-05:00] INFO: >> for OSD host[0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:35:23,955994435-05:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.2 bash
10.10.2.2: b'75296\n'
[1] 02:35:24 [SUCCESS] ljishen@10.10.2.2
75296

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:35:24,142943557-05:00] INFO: > Run rados bench[0m
# ./benchmarks/bench-rados:379 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/output/rados_bench_16MB_seq.log.1
## ./benchmarks/bench-rados:376 - rados_bench() > count_cpus 32-63,96-127
# ./benchmarks/bench-rados:376 - rados_bench() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 128 --show-time
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:35:24,162257218-05:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:35:24,164964723-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '5ecad068-4065-11ec-aba1-c5f3c07a3dc7', '--', 'rados', 'bench', '99999999', 'seq', '--pool', 'bench_rados', '--concurrent-ios', '128', '--show-time'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 5ecad068-4065-11ec-aba1-c5f3c07a3dc7 -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 128 --show-time
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
hints = 1
2021-11-08T07:35:27.406892+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-11-08T07:35:27.406892+0000     0       0         0         0         0         0           -           0
2021-11-08T07:35:28.406996+0000     1      94        94         0         0         0           -           0
2021-11-08T07:35:29.407064+0000     2     127       179        52   415.941       416     1.46868     1.43947
2021-11-08T07:35:30.407153+0000     3     127       261       134   714.578      1312     1.54899      1.4945
2021-11-08T07:35:31.407229+0000     4     127       343       216   863.903      1312     1.54063     1.51601
2021-11-08T07:35:32.407298+0000     5     127       425       298   953.501      1312      1.5547     1.52604
2021-11-08T07:35:33.407364+0000     6     127       507       380   1013.23      1312     1.55218     1.53273
2021-11-08T07:35:34.407476+0000 Total time run:       6.68653
Total reads made:     546
Read size:            16777216
Object size:          16777216
Bandwidth (MB/sec):   1306.51
Average IOPS:         81
Stddev IOPS:          36.5677
Max IOPS:             82
Min IOPS:             0
Average Latency(s):   1.38537
Max latency(s):       1.57785
Min latency(s):       0.21086

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:35:35,267184025-05:00] INFO: > Stop system activity collection services[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:35:35,273317458-05:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:391 - rados_bench() > kill -INT 256037

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:35:35,279545009-05:00] INFO: >> for OSD host[0m
# ./benchmarks/bench-rados:396 - rados_bench() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.2 kill -INT 75296
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:35:35,287460973-05:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.2 kill -INT 75296
[1] 02:35:35 [SUCCESS] ljishen@10.10.2.2

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:35:35,468866643-05:00] INFO: >>> Copy system activity file from OSD host[0m
## ./benchmarks/bench-rados:403 - rados_bench() > basename -- /tmp/bench-rados/sys_activity_16MB_seq.dat.osd_host.1
# ./benchmarks/bench-rados:403 - rados_bench() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.2:/tmp/bench-rados/sys_activity_16MB_seq.dat.osd_host.1 /mnt/sda8/ceph-research/scripts/output/sys_activity_16MB_seq.dat.osd_host.1


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:35:35,621438143-05:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:35:59,615664950-05:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 547 objects, 8.5 GiB
    usage:   26 GiB used, 74 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:35:59,623454285-05:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:36:08,762390127-05:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 547 objects, 8.5 GiB
    usage:   26 GiB used, 74 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:36:08,770635703-05:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:36:17,860618278-05:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 547 objects, 8.5 GiB
    usage:   26 GiB used, 74 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:36:17,868644550-05:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:36:27,030102851-05:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 547 objects, 8.5 GiB
    usage:   26 GiB used, 74 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:36:27,038436462-05:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:36:36,208175407-05:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 547 objects, 8.5 GiB
    usage:   26 GiB used, 74 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:36:36,216055243-05:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:36:36,222742009-05:00] INFO: > The cluster is idle now.[0m


[1;7;39;49m[2021-11-08T02:36:36,225181188-05:00][RUNNING][ROUND 2/7/21] object_size=16MB[0m


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:36:36,229119423-05:00] STAGE: Launch a Ceph cluster (CEPH_HOST: 10.10.2.3, OSD_HOST: 10.10.2.2) ...[0m
# ./benchmarks/bench-rados:183 - launch_ceph_cluster() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.3 bash -euo pipefail
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:36:36,238362148-05:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.3 bash -euo pipefail
10.10.2.3: b'\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T02:36:36,674325619-05:00] INFO: > Remove existing clusters\x1b[0m\n'
10.10.2.3: b'## bash:17 -  > basename -- /var/lib/ceph/5ecad068-4065-11ec-aba1-c5f3c07a3dc7\n'
10.10.2.3: b'# bash:17 -  > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm --host 10.10.2.3 --host 10.10.2.2 rm-cluster --force --fsid 5ecad068-4065-11ec-aba1-c5f3c07a3dc7\n'
10.10.2.3: b"\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T02:36:36,684635231-05:00] DEBUG: HOST_PARAMS: ['--host', '10.10.2.3', '--host', '10.10.2.2']\x1b[0m\n"
10.10.2.3: b"\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T02:36:36,687912565-05:00] DEBUG: CEPHADM_PARAMS: ['rm-cluster', '--force', '--fsid', '5ecad068-4065-11ec-aba1-c5f3c07a3dc7']\x1b[0m\n"
10.10.2.3: b"# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:63 -  > /tmp/bench-rados/ceph-research/scripts/parallel-ssh --host 10.10.2.3 --host 10.10.2.2 'sudo python3 -u - rm-cluster --force --fsid 5ecad068-4065-11ec-aba1-c5f3c07a3dc7'\n"
10.10.2.3: b'\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T02:36:36,695734493-05:00] DEBUG: command from stdin\x1b[0m\n'
10.10.2.3: b"# /tmp/bench-rados/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host 10.10.2.3 --host 10.10.2.2 'sudo python3 -u - rm-cluster --force --fsid 5ecad068-4065-11ec-aba1-c5f3c07a3dc7'\n"
10.10.2.3: b'[1] 02:36:42 [SUCCESS] 10.10.2.3\n[2] 02:36:49 [SUCCESS] 10.10.2.2\n'
10.10.2.3: b'\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T02:36:49,738879770-05:00] INFO: > Deploy a new cluster\x1b[0m\n'
10.10.2.3: b'# bash:24 -  > /tmp/bench-rados/ceph-research/scripts/ceph-deploy -m 10.10.2.3 -o node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/10.10.2.2:/dev/nvme0n1\n'
10.10.2.3: b'\n\n\x1b[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T02:36:49,750421488-05:00] STAGE: Verify passwordless SSH to hosts, passwordless sudo via ljishen, and the existence of devices on hosts...\x1b[0m\n'
10.10.2.3: b'\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T02:36:49,755974305-05:00] INFO: > Check the host for the monitor\x1b[0m\n'
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:181 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@10.10.2.3 sudo --non-interactive --validate\n'
10.10.2.3: b'\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T02:36:49,902139730-05:00] INFO: > Check hosts for OSDs\x1b[0m\n'
10.10.2.3: b"\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T02:36:49,906524253-05:00] INFO: >> Check host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'\x1b[0m\n"
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:200 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us sudo --non-interactive --validate\n'
10.10.2.3: b"\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T02:36:50,054402379-05:00] INFO: >>> Check device '/dev/nvme0n1'\x1b[0m\n"
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:215 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us test -b /dev/nvme0n1\n'
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:226 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us lsblk --nodeps --noheadings --output TYPE /dev/nvme0n1\n'
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:227 - verify_parameters() > grep --word-regexp disk\n'
10.10.2.3: b'\n\n\x1b[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T02:36:50,334858355-05:00] STAGE: Reset devices on hosts...\x1b[0m\n'
10.10.2.3: b"\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T02:36:50,340778465-05:00] INFO: > Resetting device '/dev/nvme0n1' on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'\x1b[0m\n"
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:288 - reset_host_devices() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us sudo sh\n'
10.10.2.3: b'  Removing ceph--630d833a--1884--4c59--acef--76b37b72a0b4-osd--block--9442a45b--e3c1--4b4f--bb99--492e3aeb8401 (253:0)\n'
10.10.2.3: b'  Archiving volume group "ceph-630d833a-1884-4c59-acef-76b37b72a0b4" metadata (seqno 5).\n'
10.10.2.3: b'  Releasing logical volume "osd-block-9442a45b-e3c1-4b4f-bb99-492e3aeb8401"\n'
10.10.2.3: b'  Creating volume group backup "/etc/lvm/backup/ceph-630d833a-1884-4c59-acef-76b37b72a0b4" (seqno 6).\n'
10.10.2.3: b'  Logical volume "osd-block-9442a45b-e3c1-4b4f-bb99-492e3aeb8401" successfully removed\n'
10.10.2.3: b'  Removing physical volume "/dev/nvme0n1" from volume group "ceph-630d833a-1884-4c59-acef-76b37b72a0b4"\n'
10.10.2.3: b'  Volume group "ceph-630d833a-1884-4c59-acef-76b37b72a0b4" successfully removed\n'
10.10.2.3: b'\n\n\x1b[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T02:36:50,676158080-05:00] STAGE: Bootstrap a new cluster...\x1b[0m\n'
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:317 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.3 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults\n'
10.10.2.3: b'\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T02:36:50,685338683-05:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.3: b"\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T02:36:50,689112132-05:00] DEBUG: CEPHADM_PARAMS: ['bootstrap', '--config', '/tmp/bench-rados/ceph-research/scripts/ceph.conf', '--mon-ip', '10.10.2.3', '--output-dir', '/tmp/bench-rados/deployment_data_root/etc/ceph', '--ssh-user', 'ljishen', '--skip-dashboard', '--no-minimize-config', '--allow-overwrite', '--allow-fqdn-hostname', '--orphan-initial-daemons', '--skip-monitoring-stack', '--single-host-defaults']\x1b[0m\n"
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.3 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults\n'
10.10.2.3: b'Verifying podman|docker is present...\nVerifying lvm2 is present...\n'
10.10.2.3: b'Verifying time synchronization is in place...\n'
10.10.2.3: b'Unit ntp.service is enabled and running\n'
10.10.2.3: b'Repeating the final host check...\ndocker (/usr/bin/docker) is present\n'
10.10.2.3: b'systemctl is present\n'
10.10.2.3: b'lvcreate is present\n'
10.10.2.3: b'Unit ntp.service is enabled and running\n'
10.10.2.3: b'Host looks OK\n'
10.10.2.3: b'Cluster fsid: a31b0e80-4066-11ec-aba1-c5f3c07a3dc7\n'
10.10.2.3: b'Verifying IP 10.10.2.3 port 3300 ...\n'
10.10.2.3: b'Verifying IP 10.10.2.3 port 6789 ...\n'
10.10.2.3: b'Mon IP `10.10.2.3` is in CIDR network `10.10.2.0/24`\n- internal network (--cluster-network) has not been provided, OSD replication will default to the public_network\n'
10.10.2.3: b'Adjusting default settings to suit single-host cluster...\n'
10.10.2.3: b'Pulling container image quay.io/ceph/ceph:v16...\n'
10.10.2.3: b'Ceph version: ceph version 16.2.6 (ee28fb57e47e9f88813e24bbf4c14496ca299d31) pacific (stable)\n'
10.10.2.3: b'Extracting ceph user uid/gid from container image...\n'
10.10.2.3: b'Creating initial keys...\n'
10.10.2.3: b'Creating initial monmap...\n'
10.10.2.3: b'Creating mon...\n'
10.10.2.3: b'Waiting for mon to start...\nWaiting for mon...\n'
10.10.2.3: b'mon is available\nSetting mon public_network to 10.10.2.0/24\n'
10.10.2.3: b'Wrote config to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf\n'
10.10.2.3: b'Wrote keyring to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring\n'
10.10.2.3: b'Creating mgr...\n'
10.10.2.3: b'Verifying port 9283 ...\n'
10.10.2.3: b'Waiting for mgr to start...\nWaiting for mgr...\n'
10.10.2.3: b'mgr not available, waiting (1/15)...\n'
10.10.2.3: b'mgr not available, waiting (2/15)...\n'
10.10.2.3: b'mgr is available\n'
10.10.2.3: b'Enabling cephadm module...\n'
10.10.2.3: b'Waiting for the mgr to restart...\n'
10.10.2.3: b'Waiting for mgr epoch 5...\n'
10.10.2.3: b'mgr epoch 5 is available\nSetting orchestrator backend to cephadm...\n'
10.10.2.3: b'Generating ssh key...\n'
10.10.2.3: b'Wrote public SSH key to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub\n'
10.10.2.3: b'Adding key to ljishen@localhost authorized_keys...\n'
10.10.2.3: b'Adding host node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us...\n'
10.10.2.3: b'Deploying unmanaged mon service...\n'
10.10.2.3: b'Deploying unmanaged mgr service...\n'
10.10.2.3: b'You can access the Ceph CLI with:\n\n\tsudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --fsid a31b0e80-4066-11ec-aba1-c5f3c07a3dc7 -c /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf -k /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring\n\n'
10.10.2.3: b'Please consider enabling telemetry to help improve Ceph:\n\n\tceph telemetry on\n\nFor more information see:\n\n\thttps://docs.ceph.com/docs/pacific/mgr/telemetry/\n\n'
10.10.2.3: b'Bootstrap complete.\n'
10.10.2.3: b'\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T02:37:59,054105796-05:00] INFO: Sleep for 20 seconds\x1b[0m\n'
10.10.2.3: b'\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T02:38:19,060832471-05:00] INFO: > Set the default number of monitor daemons to 1\x1b[0m\n# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:337 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1\n'
10.10.2.3: b'\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T02:38:19,070290838-05:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.3: b"\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T02:38:19,073704499-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mon', '1']\x1b[0m\n"
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1\n'
10.10.2.3: b'Inferring fsid a31b0e80-4066-11ec-aba1-c5f3c07a3dc7\n'
10.10.2.3: b'Inferring config /var/lib/ceph/a31b0e80-4066-11ec-aba1-c5f3c07a3dc7/mon.node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.3: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.3: b'Scheduled mon update...\n'
10.10.2.3: b'\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T02:38:28,088247077-05:00] INFO: > Set the default number of manager daemons to 1\x1b[0m\n'
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:342 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1\n'
10.10.2.3: b'\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T02:38:28,098179307-05:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.3: b"\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T02:38:28,101282052-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mgr', '1']\x1b[0m\n"
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1\n'
10.10.2.3: b'Inferring fsid a31b0e80-4066-11ec-aba1-c5f3c07a3dc7\n'
10.10.2.3: b'Inferring config /var/lib/ceph/a31b0e80-4066-11ec-aba1-c5f3c07a3dc7/mon.node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.3: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.3: b'Scheduled mgr update...\n'
10.10.2.3: b'\n\n\x1b[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T02:38:38,737236121-05:00] STAGE: Add hosts to the cluster...\x1b[0m\n'
10.10.2.3: b"\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T02:38:38,743348273-05:00] INFO: > Adding host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'\x1b[0m\n"
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:361 - add_hosts() > ssh-copy-id -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -f -i /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\n'
10.10.2.3: b'/usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: "/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub"\n'
10.10.2.3: b'\nNumber of key(s) added: 1\n\nNow try logging into the machine, with:   "ssh -o \'GlobalKnownHostsFile=/dev/null\' -o \'LogLevel=ERROR\' -o \'PasswordAuthentication=no\' -o \'StrictHostKeyChecking=no\' -o \'UserKnownHostsFile=/dev/null\' \'ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\'"\nand check to make sure that only the key(s) you wanted were added.\n\n'
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:367 - add_hosts() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us 10.10.2.2\n'
10.10.2.3: b'\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T02:38:38,953344034-05:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.3: b"\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T02:38:38,956621218-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'host', 'add', 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us', '10.10.2.2']\x1b[0m\n"
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us 10.10.2.2\n'
10.10.2.3: b'Inferring fsid a31b0e80-4066-11ec-aba1-c5f3c07a3dc7\n'
10.10.2.3: b'Inferring config /var/lib/ceph/a31b0e80-4066-11ec-aba1-c5f3c07a3dc7/mon.node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.3: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.3: b"Added host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us' with addr '10.10.2.2'\n"
10.10.2.3: b'\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T02:38:49,478696663-05:00] INFO: Sleep for 20 seconds\x1b[0m\n'
10.10.2.3: b'\n\n\x1b[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T02:39:09,483143048-05:00] STAGE: Add OSDs to the cluster...\x1b[0m\n'
10.10.2.3: b"\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T02:39:09,489178025-05:00] INFO: > Adding devices ['/dev/nvme0n1'] on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us' as OSDs\x1b[0m\n"
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:388 - add_host_devices_as_osds() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1\n'
10.10.2.3: b'\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T02:39:09,499075139-05:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.3: b"\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T02:39:09,502430891-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'daemon', 'add', 'osd', 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1']\x1b[0m\n"
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1\n'
10.10.2.3: b'Inferring fsid a31b0e80-4066-11ec-aba1-c5f3c07a3dc7\n'
10.10.2.3: b'Inferring config /var/lib/ceph/a31b0e80-4066-11ec-aba1-c5f3c07a3dc7/mon.node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.3: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.3: b"Created osd(s) 0 on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'\n"
10.10.2.3: b'\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T02:39:35,332598938-05:00] INFO: > Sleep for 20 seconds\x1b[0m\n'
10.10.2.3: b'\n\n\x1b[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T02:39:55,338533891-05:00] STAGE: Show Cluster Status\x1b[0m\n'
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:408 - show_status() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status\n'
10.10.2.3: b'\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T02:39:55,348236499-05:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.3: b"\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T02:39:55,352467832-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', '--status']\x1b[0m\n"
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status\n'
10.10.2.3: b'Inferring fsid a31b0e80-4066-11ec-aba1-c5f3c07a3dc7\n'
10.10.2.3: b'Inferring config /var/lib/ceph/a31b0e80-4066-11ec-aba1-c5f3c07a3dc7/mon.node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.3: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.3: b'  cluster:\n    id:     a31b0e80-4066-11ec-aba1-c5f3c07a3dc7\n    health: HEALTH_WARN\n            1 pool(s) have no replicas configured\n \n  services:\n    mon: 1 daemons, quorum node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us (age 2m)\n    mgr: node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us.luylxi(active, since 2m)\n    osd: 1 osds: 1 up (since 18s), 1 in (since 42s)\n \n  data:\n    pools:   1 pools, 1 pgs\n    objects: 0 objects, 0 B\n    usage:   4.7 MiB used, 100 GiB / 100 GiB avail\n    pgs:     1 active+clean\n \n'
[1] 02:40:05 [SUCCESS] ljishen@10.10.2.3

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:36:36,674325619-05:00] INFO: > Remove existing clusters[0m
## bash:17 -  > basename -- /var/lib/ceph/5ecad068-4065-11ec-aba1-c5f3c07a3dc7
# bash:17 -  > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm --host 10.10.2.3 --host 10.10.2.2 rm-cluster --force --fsid 5ecad068-4065-11ec-aba1-c5f3c07a3dc7
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:36:36,684635231-05:00] DEBUG: HOST_PARAMS: ['--host', '10.10.2.3', '--host', '10.10.2.2'][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:36:36,687912565-05:00] DEBUG: CEPHADM_PARAMS: ['rm-cluster', '--force', '--fsid', '5ecad068-4065-11ec-aba1-c5f3c07a3dc7'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:63 -  > /tmp/bench-rados/ceph-research/scripts/parallel-ssh --host 10.10.2.3 --host 10.10.2.2 'sudo python3 -u - rm-cluster --force --fsid 5ecad068-4065-11ec-aba1-c5f3c07a3dc7'
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:36:36,695734493-05:00] DEBUG: command from stdin[0m
# /tmp/bench-rados/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host 10.10.2.3 --host 10.10.2.2 'sudo python3 -u - rm-cluster --force --fsid 5ecad068-4065-11ec-aba1-c5f3c07a3dc7'
[1] 02:36:42 [SUCCESS] 10.10.2.3
[2] 02:36:49 [SUCCESS] 10.10.2.2

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:36:49,738879770-05:00] INFO: > Deploy a new cluster[0m
# bash:24 -  > /tmp/bench-rados/ceph-research/scripts/ceph-deploy -m 10.10.2.3 -o node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/10.10.2.2:/dev/nvme0n1


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:36:49,750421488-05:00] STAGE: Verify passwordless SSH to hosts, passwordless sudo via ljishen, and the existence of devices on hosts...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:36:49,755974305-05:00] INFO: > Check the host for the monitor[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:181 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@10.10.2.3 sudo --non-interactive --validate

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:36:49,902139730-05:00] INFO: > Check hosts for OSDs[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:36:49,906524253-05:00] INFO: >> Check host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:200 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us sudo --non-interactive --validate

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:36:50,054402379-05:00] INFO: >>> Check device '/dev/nvme0n1'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:215 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us test -b /dev/nvme0n1
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:226 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us lsblk --nodeps --noheadings --output TYPE /dev/nvme0n1
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:227 - verify_parameters() > grep --word-regexp disk


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:36:50,334858355-05:00] STAGE: Reset devices on hosts...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:36:50,340778465-05:00] INFO: > Resetting device '/dev/nvme0n1' on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:288 - reset_host_devices() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us sudo sh
  Removing ceph--630d833a--1884--4c59--acef--76b37b72a0b4-osd--block--9442a45b--e3c1--4b4f--bb99--492e3aeb8401 (253:0)
  Archiving volume group "ceph-630d833a-1884-4c59-acef-76b37b72a0b4" metadata (seqno 5).
  Releasing logical volume "osd-block-9442a45b-e3c1-4b4f-bb99-492e3aeb8401"
  Creating volume group backup "/etc/lvm/backup/ceph-630d833a-1884-4c59-acef-76b37b72a0b4" (seqno 6).
  Logical volume "osd-block-9442a45b-e3c1-4b4f-bb99-492e3aeb8401" successfully removed
  Removing physical volume "/dev/nvme0n1" from volume group "ceph-630d833a-1884-4c59-acef-76b37b72a0b4"
  Volume group "ceph-630d833a-1884-4c59-acef-76b37b72a0b4" successfully removed


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:36:50,676158080-05:00] STAGE: Bootstrap a new cluster...[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:317 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.3 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:36:50,685338683-05:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:36:50,689112132-05:00] DEBUG: CEPHADM_PARAMS: ['bootstrap', '--config', '/tmp/bench-rados/ceph-research/scripts/ceph.conf', '--mon-ip', '10.10.2.3', '--output-dir', '/tmp/bench-rados/deployment_data_root/etc/ceph', '--ssh-user', 'ljishen', '--skip-dashboard', '--no-minimize-config', '--allow-overwrite', '--allow-fqdn-hostname', '--orphan-initial-daemons', '--skip-monitoring-stack', '--single-host-defaults'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.3 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults
Verifying podman|docker is present...
Verifying lvm2 is present...
Verifying time synchronization is in place...
Unit ntp.service is enabled and running
Repeating the final host check...
docker (/usr/bin/docker) is present
systemctl is present
lvcreate is present
Unit ntp.service is enabled and running
Host looks OK
Cluster fsid: a31b0e80-4066-11ec-aba1-c5f3c07a3dc7
Verifying IP 10.10.2.3 port 3300 ...
Verifying IP 10.10.2.3 port 6789 ...
Mon IP `10.10.2.3` is in CIDR network `10.10.2.0/24`
- internal network (--cluster-network) has not been provided, OSD replication will default to the public_network
Adjusting default settings to suit single-host cluster...
Pulling container image quay.io/ceph/ceph:v16...
Ceph version: ceph version 16.2.6 (ee28fb57e47e9f88813e24bbf4c14496ca299d31) pacific (stable)
Extracting ceph user uid/gid from container image...
Creating initial keys...
Creating initial monmap...
Creating mon...
Waiting for mon to start...
Waiting for mon...
mon is available
Setting mon public_network to 10.10.2.0/24
Wrote config to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf
Wrote keyring to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring
Creating mgr...
Verifying port 9283 ...
Waiting for mgr to start...
Waiting for mgr...
mgr not available, waiting (1/15)...
mgr not available, waiting (2/15)...
mgr is available
Enabling cephadm module...
Waiting for the mgr to restart...
Waiting for mgr epoch 5...
mgr epoch 5 is available
Setting orchestrator backend to cephadm...
Generating ssh key...
Wrote public SSH key to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub
Adding key to ljishen@localhost authorized_keys...
Adding host node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us...
Deploying unmanaged mon service...
Deploying unmanaged mgr service...
You can access the Ceph CLI with:

	sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --fsid a31b0e80-4066-11ec-aba1-c5f3c07a3dc7 -c /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf -k /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring

Please consider enabling telemetry to help improve Ceph:

	ceph telemetry on

For more information see:

	https://docs.ceph.com/docs/pacific/mgr/telemetry/

Bootstrap complete.

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:37:59,054105796-05:00] INFO: Sleep for 20 seconds[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:38:19,060832471-05:00] INFO: > Set the default number of monitor daemons to 1[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:337 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:38:19,070290838-05:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:38:19,073704499-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mon', '1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1
Inferring fsid a31b0e80-4066-11ec-aba1-c5f3c07a3dc7
Inferring config /var/lib/ceph/a31b0e80-4066-11ec-aba1-c5f3c07a3dc7/mon.node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Scheduled mon update...

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:38:28,088247077-05:00] INFO: > Set the default number of manager daemons to 1[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:342 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:38:28,098179307-05:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:38:28,101282052-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mgr', '1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1
Inferring fsid a31b0e80-4066-11ec-aba1-c5f3c07a3dc7
Inferring config /var/lib/ceph/a31b0e80-4066-11ec-aba1-c5f3c07a3dc7/mon.node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Scheduled mgr update...


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:38:38,737236121-05:00] STAGE: Add hosts to the cluster...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:38:38,743348273-05:00] INFO: > Adding host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:361 - add_hosts() > ssh-copy-id -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -f -i /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us
/usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: "/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub"

Number of key(s) added: 1

Now try logging into the machine, with:   "ssh -o 'GlobalKnownHostsFile=/dev/null' -o 'LogLevel=ERROR' -o 'PasswordAuthentication=no' -o 'StrictHostKeyChecking=no' -o 'UserKnownHostsFile=/dev/null' 'ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'"
and check to make sure that only the key(s) you wanted were added.

# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:367 - add_hosts() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us 10.10.2.2
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:38:38,953344034-05:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:38:38,956621218-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'host', 'add', 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us', '10.10.2.2'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us 10.10.2.2
Inferring fsid a31b0e80-4066-11ec-aba1-c5f3c07a3dc7
Inferring config /var/lib/ceph/a31b0e80-4066-11ec-aba1-c5f3c07a3dc7/mon.node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Added host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us' with addr '10.10.2.2'

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:38:49,478696663-05:00] INFO: Sleep for 20 seconds[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:39:09,483143048-05:00] STAGE: Add OSDs to the cluster...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:39:09,489178025-05:00] INFO: > Adding devices ['/dev/nvme0n1'] on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us' as OSDs[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:388 - add_host_devices_as_osds() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:39:09,499075139-05:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:39:09,502430891-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'daemon', 'add', 'osd', 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1
Inferring fsid a31b0e80-4066-11ec-aba1-c5f3c07a3dc7
Inferring config /var/lib/ceph/a31b0e80-4066-11ec-aba1-c5f3c07a3dc7/mon.node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Created osd(s) 0 on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:39:35,332598938-05:00] INFO: > Sleep for 20 seconds[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:39:55,338533891-05:00] STAGE: Show Cluster Status[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:408 - show_status() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:39:55,348236499-05:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:39:55,352467832-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', '--status'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status
Inferring fsid a31b0e80-4066-11ec-aba1-c5f3c07a3dc7
Inferring config /var/lib/ceph/a31b0e80-4066-11ec-aba1-c5f3c07a3dc7/mon.node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
  cluster:
    id:     a31b0e80-4066-11ec-aba1-c5f3c07a3dc7
    health: HEALTH_WARN
            1 pool(s) have no replicas configured
 
  services:
    mon: 1 daemons, quorum node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us (age 2m)
    mgr: node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us.luylxi(active, since 2m)
    osd: 1 osds: 1 up (since 18s), 1 in (since 42s)
 
  data:
    pools:   1 pools, 1 pgs
    objects: 0 objects, 0 B
    usage:   4.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     1 active+clean
 

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:40:05,597009178-05:00] INFO: > Copy configuration files to local[0m
# ./benchmarks/bench-rados:220 - launch_ceph_cluster() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.3 sudo chown --changes --recursive ljishen /tmp/bench-rados/deployment_data_root/etc/ceph
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:40:05,604531279-05:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.3 sudo chown --changes --recursive ljishen /tmp/bench-rados/deployment_data_root/etc/ceph
[1] 02:40:05 [SUCCESS] ljishen@10.10.2.3
# ./benchmarks/bench-rados:223 - launch_ceph_cluster() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.3:/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf
# ./benchmarks/bench-rados:226 - launch_ceph_cluster() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.3:/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:40:06,076293119-05:00] STAGE: Update local ceph.conf as a client...[0m


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:40:06,080275587-05:00] STAGE: Configure Ceph pool...[0m
# ./benchmarks/bench-rados:257 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_max_backfills 32
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:40:06,101561226-05:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:40:06,104247511-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'a31b0e80-4066-11ec-aba1-c5f3c07a3dc7', '--', 'ceph', 'config', 'set', 'osd', 'osd_max_backfills', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid a31b0e80-4066-11ec-aba1-c5f3c07a3dc7 -- ceph config set osd osd_max_backfills 32
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:258 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_active 32
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:40:10,369524940-05:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:40:10,372250649-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'a31b0e80-4066-11ec-aba1-c5f3c07a3dc7', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_active', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid a31b0e80-4066-11ec-aba1-c5f3c07a3dc7 -- ceph config set osd osd_recovery_max_active 32
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:259 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_single_start 8
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:40:14,596215962-05:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:40:14,599329683-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'a31b0e80-4066-11ec-aba1-c5f3c07a3dc7', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_single_start', '8'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid a31b0e80-4066-11ec-aba1-c5f3c07a3dc7 -- ceph config set osd osd_recovery_max_single_start 8
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:260 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_op_priority 63
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:40:18,882482343-05:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:40:18,885118082-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'a31b0e80-4066-11ec-aba1-c5f3c07a3dc7', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_op_priority', '63'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid a31b0e80-4066-11ec-aba1-c5f3c07a3dc7 -- ceph config set osd osd_recovery_op_priority 63
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:262 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config show-with-defaults osd.0
# ./benchmarks/bench-rados:263 - configure_ceph_pool() > grep 'osd_max_backfills\|osd_recovery'
# ./benchmarks/bench-rados:263 - configure_ceph_pool() > column -t -s ' '
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
osd_max_backfills                        32         mon
osd_recovery_cost                        20971520   default
osd_recovery_delay_start                 0.000000   default
osd_recovery_max_active                  32         mon
osd_recovery_max_active_hdd              3          default
osd_recovery_max_active_ssd              10         default
osd_recovery_max_chunk                   8388608    default
osd_recovery_max_omap_entries_per_chunk  8096       default
osd_recovery_max_single_start            8          mon
osd_recovery_op_priority                 63         mon
osd_recovery_op_warn_multiple            16         default
osd_recovery_priority                    5          default
osd_recovery_retry_interval              30.000000  default
osd_recovery_sleep                       0.000000   default
osd_recovery_sleep_hdd                   0.100000   default
osd_recovery_sleep_hybrid                0.025000   default
osd_recovery_sleep_ssd                   0.000000   default
# ./benchmarks/bench-rados:265 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool create bench_rados 128 128 replicated --size 1 --pg-num-min 128
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:40:27,348726411-05:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:40:27,351794104-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'a31b0e80-4066-11ec-aba1-c5f3c07a3dc7', '--', 'ceph', 'osd', 'pool', 'create', 'bench_rados', '128', '128', 'replicated', '--size', '1', '--pg-num-min', '128'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid a31b0e80-4066-11ec-aba1-c5f3c07a3dc7 -- ceph osd pool create bench_rados 128 128 replicated --size 1 --pg-num-min 128
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
pool 'bench_rados' created
# ./benchmarks/bench-rados:267 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados min_size 1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:40:31,774178469-05:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:40:31,777068778-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'a31b0e80-4066-11ec-aba1-c5f3c07a3dc7', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'min_size', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid a31b0e80-4066-11ec-aba1-c5f3c07a3dc7 -- ceph osd pool set bench_rados min_size 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 min_size to 1
# ./benchmarks/bench-rados:268 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados pg_autoscale_mode off
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:40:36,242836465-05:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:40:36,245579546-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'a31b0e80-4066-11ec-aba1-c5f3c07a3dc7', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'pg_autoscale_mode', 'off'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid a31b0e80-4066-11ec-aba1-c5f3c07a3dc7 -- ceph osd pool set bench_rados pg_autoscale_mode off
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 pg_autoscale_mode to off
# ./benchmarks/bench-rados:269 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool application enable bench_rados benchmark
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:40:40,855369106-05:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:40:40,858381836-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'a31b0e80-4066-11ec-aba1-c5f3c07a3dc7', '--', 'ceph', 'osd', 'pool', 'application', 'enable', 'bench_rados', 'benchmark'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid a31b0e80-4066-11ec-aba1-c5f3c07a3dc7 -- ceph osd pool application enable bench_rados benchmark
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
enabled application 'benchmark' on pool 'bench_rados'
# ./benchmarks/bench-rados:270 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados noscrub 1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:40:45,256621423-05:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:40:45,259308008-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'a31b0e80-4066-11ec-aba1-c5f3c07a3dc7', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'noscrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid a31b0e80-4066-11ec-aba1-c5f3c07a3dc7 -- ceph osd pool set bench_rados noscrub 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 noscrub to 1
# ./benchmarks/bench-rados:271 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados nodeep-scrub 1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:40:50,034348708-05:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:40:50,037083263-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'a31b0e80-4066-11ec-aba1-c5f3c07a3dc7', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'nodeep-scrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid a31b0e80-4066-11ec-aba1-c5f3c07a3dc7 -- ceph osd pool set bench_rados nodeep-scrub 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 nodeep-scrub to 1
# ./benchmarks/bench-rados:272 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool ls detail
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:40:54,994396792-05:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:40:54,997142859-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'a31b0e80-4066-11ec-aba1-c5f3c07a3dc7', '--', 'ceph', 'osd', 'pool', 'ls', 'detail'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid a31b0e80-4066-11ec-aba1-c5f3c07a3dc7 -- ceph osd pool ls detail
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
pool 1 'device_health_metrics' replicated size 1 min_size 1 crush_rule 0 object_hash rjenkins pg_num 64 pgp_num 64 autoscale_mode on last_change 17 lfor 0/0/14 flags hashpspool stripe_width 0 pg_num_min 1 application mgr_devicehealth
pool 2 'bench_rados' replicated size 1 min_size 1 crush_rule 0 object_hash rjenkins pg_num 128 pgp_num 128 autoscale_mode off last_change 20 flags hashpspool,noscrub,nodeep-scrub stripe_width 0 pg_num_min 128 application benchmark

# ./benchmarks/bench-rados:273 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd df tree
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:40:59,115839438-05:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:40:59,118618727-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'a31b0e80-4066-11ec-aba1-c5f3c07a3dc7', '--', 'ceph', 'osd', 'df', 'tree'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid a31b0e80-4066-11ec-aba1-c5f3c07a3dc7 -- ceph osd df tree
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
ID  CLASS  WEIGHT   REWEIGHT  SIZE     RAW USE  DATA     OMAP  META     AVAIL    %USE  VAR   PGS  STATUS  TYPE NAME      
-1         0.09769         -  100 GiB  5.7 MiB  168 KiB   0 B  5.6 MiB  100 GiB  0.01  1.00    -          root default   
-3         0.09769         -  100 GiB  5.7 MiB  168 KiB   0 B  5.6 MiB  100 GiB  0.01  1.00    -              host node-1
 0    ssd  0.09769   1.00000  100 GiB  5.7 MiB  168 KiB   0 B  5.6 MiB  100 GiB  0.01  1.00  192      up          osd.0  
                       TOTAL  100 GiB  5.7 MiB  168 KiB   0 B  5.6 MiB  100 GiB  0.01                                    
MIN/MAX VAR: 1.00/1.00  STDDEV: 0


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:41:03,496119439-05:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:41:27,627465326-05:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:41:36,748602004-05:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:41:45,874885739-05:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:41:45,883174476-05:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:41:55,294097284-05:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:41:55,302054826-05:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:42:04,514472866-05:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:42:04,522375735-05:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:42:13,614755058-05:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:42:13,622436600-05:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:42:22,931863887-05:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:42:22,939809647-05:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:42:22,945867749-05:00] INFO: > The cluster is idle now.[0m


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:42:22,949599514-05:00] STAGE: Start benchmarking of mode write ...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:42:22,956927500-05:00] INFO: > Start system activity collection services[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:42:22,962733015-05:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:341 - rados_bench() > rm --force /mnt/sda8/ceph-research/scripts/output/sys_activity_16MB_write.dat.rados_bench_host.2
# ./benchmarks/bench-rados:342 - rados_bench() > sar_pid_rados_bench_host=261610
# ./benchmarks/bench-rados:342 - rados_bench() > S_TIME_FORMAT=ISO
# ./benchmarks/bench-rados:342 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/output/sys_activity_16MB_write.dat.rados_bench_host.2 2

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:42:22,969952386-05:00] INFO: >> for OSD host[0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:42:22,978679158-05:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.2 bash
10.10.2.2: b'78909\n'
[1] 02:42:23 [SUCCESS] ljishen@10.10.2.2
78909

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:42:23,170788662-05:00] INFO: > Run rados bench[0m
# ./benchmarks/bench-rados:370 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/output/rados_bench_16MB_write.log.2
## ./benchmarks/bench-rados:361 - rados_bench() > count_cpus 32-63,96-127
# ./benchmarks/bench-rados:361 - rados_bench() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 60 write --pool bench_rados -b 16777216 -O 16777216 --concurrent-ios 128 --show-time --write-object --write-omap --write-xattr --no-cleanup
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:42:23,190382111-05:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:42:23,193094906-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'a31b0e80-4066-11ec-aba1-c5f3c07a3dc7', '--', 'rados', 'bench', '60', 'write', '--pool', 'bench_rados', '-b', '16777216', '-O', '16777216', '--concurrent-ios', '128', '--show-time', '--write-object', '--write-omap', '--write-xattr', '--no-cleanup'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid a31b0e80-4066-11ec-aba1-c5f3c07a3dc7 -- rados bench 60 write --pool bench_rados -b 16777216 -O 16777216 --concurrent-ios 128 --show-time --write-object --write-omap --write-xattr --no-cleanup
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
hints = 1
2021-11-08T07:42:26.421957+0000 Maintaining 128 concurrent writes of 16777216 bytes to objects of size 16777216 for up to 60 seconds or 0 objects
2021-11-08T07:42:26.421989+0000 Object prefix: benchmark_data_node-0.bluefield2.ucsc-cmps10_7
2021-11-08T07:42:27.089792+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-11-08T07:42:27.089792+0000     0       0         0         0         0         0           -           0
2021-11-08T07:42:28.089910+0000     1      18        18         0         0         0           -           0
2021-11-08T07:42:29.089980+0000     2      35        35         0         0         0           -           0
2021-11-08T07:42:30.090053+0000     3      45        45         0         0         0           -           0
2021-11-08T07:42:31.090130+0000     4      64        64         0         0         0           -           0
2021-11-08T07:42:32.090200+0000     5      69        69         0         0         0           -           0
2021-11-08T07:42:33.090271+0000     6      77        77         0         0         0           -           0
2021-11-08T07:42:34.090340+0000     7      88        88         0         0         0           -           0
2021-11-08T07:42:35.090410+0000     8     100       100         0         0         0           -           0
2021-11-08T07:42:36.090479+0000     9     111       111         0         0         0           -           0
2021-11-08T07:42:37.090549+0000    10     127       129         2   3.19978       3.2     9.93869     9.90462
2021-11-08T07:42:38.090631+0000    11     127       132         5   7.27221        48     10.3994     10.0819
2021-11-08T07:42:39.090704+0000    12     127       141        14   18.6653       144     11.2542     10.7177
2021-11-08T07:42:40.090774+0000    13     127       152        25   30.7671       176     11.7178     11.0613
2021-11-08T07:42:41.090846+0000    14     127       160        33   37.7116       128     12.2988       11.27
2021-11-08T07:42:42.090915+0000    15     127       164        37   39.4639        64      12.872     11.4063
2021-11-08T07:42:43.090986+0000    16     127       173        46   45.9967       144     12.9203     11.6899
2021-11-08T07:42:44.091058+0000    17     127       191        64    60.231       288     13.0544     12.0703
2021-11-08T07:42:45.091129+0000    18     127       197        70   62.2178        96     13.0932     12.1652
2021-11-08T07:42:46.091198+0000    19     127       206        79   66.5216       144     12.9122     12.2551
2021-11-08T07:42:47.091269+0000 min lat: 9.87055 max lat: 13.2115 avg lat: 12.3346
2021-11-08T07:42:47.091269+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-11-08T07:42:47.091269+0000    20     127       217        90   71.9949       176      12.893     12.3346
2021-11-08T07:42:48.091353+0000    21     127       226        99   75.4232       144     13.1108     12.3929
2021-11-08T07:42:49.091426+0000    22     127       229       102   74.1765        48       13.75     12.4273
2021-11-08T07:42:50.091498+0000    23     127       239       112   77.9075       160     14.0137     12.5613
2021-11-08T07:42:51.091570+0000    24     127       249       122   81.3275       160     14.4398      12.701
2021-11-08T07:42:52.091641+0000    25     127       258       131    83.834       144     14.6341     12.8317
2021-11-08T07:42:53.091713+0000    26     127       261       134   82.4556        48        14.7     12.8753
2021-11-08T07:42:54.091789+0000    27     127       271       144   85.3272       160      14.805     13.0104
2021-11-08T07:42:55.091863+0000    28     127       282       155   88.5651       176      14.781      13.138
2021-11-08T07:42:56.091937+0000    29     127       290       163   89.9246       128     14.4592     13.2123
2021-11-08T07:42:57.092008+0000    30     127       293       166    88.527        48     14.6243     13.2361
2021-11-08T07:42:58.092083+0000    31     127       304       177   91.3483       176     14.8199     13.3334
2021-11-08T07:42:59.092154+0000    32     127       314       187   93.4933       160     15.2529     13.4256
2021-11-08T07:43:00.092225+0000    33     127       323       196   95.0235       144     15.5784     13.5153
2021-11-08T07:43:01.092312+0000    34     127       325       198   93.1697        32     15.8695     13.5387
2021-11-08T07:43:02.092383+0000    35     127       336       209    95.536       176      15.807     13.6579
2021-11-08T07:43:03.092455+0000    36     127       348       221   98.2151       192     15.7792     13.7736
2021-11-08T07:43:04.092526+0000    37     127       355       228   98.5875       112     15.7171     13.8352
2021-11-08T07:43:05.092601+0000    38     127       359       232   97.6771        64     15.7273     13.8681
2021-11-08T07:43:06.092676+0000    39     127       370       243   99.6851       176     15.6722     13.9518
2021-11-08T07:43:07.092751+0000 min lat: 9.87055 max lat: 15.8695 avg lat: 14.0255
2021-11-08T07:43:07.092751+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-11-08T07:43:07.092751+0000    40     127       381       254   101.593       176     15.6397     14.0255
2021-11-08T07:43:08.092839+0000    41     127       387       260   101.456        96     15.6293     14.0628
2021-11-08T07:43:09.092922+0000    42     127       392       265   100.945        80     15.7256     14.0941
2021-11-08T07:43:10.092993+0000    43     127       402       275   102.318       160      15.704      14.153
2021-11-08T07:43:11.093070+0000    44     127       413       286   103.992       176     15.7399     14.2135
2021-11-08T07:43:12.093150+0000    45     127       419       292   103.815        96      15.706     14.2453
2021-11-08T07:43:13.093226+0000    46     127       424       297   103.297        80     15.6704     14.2696
2021-11-08T07:43:14.093296+0000    47     127       435       308   104.843       176     15.6599     14.3196
2021-11-08T07:43:15.093368+0000    48     127       447       320   106.659       192     15.6156     14.3688
2021-11-08T07:43:16.093442+0000    49     127       452       325   106.115        80     15.6424     14.3881
2021-11-08T07:43:17.093520+0000    50     127       457       330   105.592        80     15.6012     14.4067
2021-11-08T07:43:18.093590+0000    51     127       468       341   106.973       176     15.6603     14.4465
2021-11-08T07:43:19.093663+0000    52     127       479       352     108.3       176     15.6467     14.4841
2021-11-08T07:43:20.093739+0000    53     127       484       357   107.766        80     15.6012     14.5002
2021-11-08T07:43:21.093813+0000    54     127       490       363   107.548        96     15.6583     14.5194
2021-11-08T07:43:22.093887+0000    55     127       501       374   108.792       176     15.6842     14.5529
2021-11-08T07:43:23.093960+0000    56     127       512       385   109.992       176     15.7047     14.5863
2021-11-08T07:43:24.094035+0000    57     127       516       389   109.185        64     15.6684     14.5978
2021-11-08T07:43:25.094107+0000    58     127       523       396   109.233       112     15.7206     14.6177
2021-11-08T07:43:26.094178+0000    59     127       533       406   110.094       160     15.7076     14.6443
2021-11-08T07:43:27.094250+0000 min lat: 9.87055 max lat: 15.8695 avg lat: 14.6747
2021-11-08T07:43:27.094250+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-11-08T07:43:27.094250+0000    60     127       545       418   111.458       192     15.6729     14.6747
2021-11-08T07:43:28.094358+0000 Total time run:         60.7622
Total writes made:      546
Write size:             16777216
Object size:            16777216
Bandwidth (MB/sec):     143.774
Stddev Bandwidth:       69.4306
Max bandwidth (MB/sec): 288
Min bandwidth (MB/sec): 0
Average IOPS:           8
Stddev IOPS:            4.34475
Max IOPS:               18
Min IOPS:               0
Average Latency(s):     12.9903
Stddev Latency(s):      3.94514
Max latency(s):         15.8695
Min latency(s):         0.249929

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:43:28,901997692-05:00] INFO: > Stop system activity collection services[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:43:28,907801194-05:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:391 - rados_bench() > kill -INT 261610

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:43:28,913806555-05:00] INFO: >> for OSD host[0m
# ./benchmarks/bench-rados:396 - rados_bench() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.2 kill -INT 78909
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:43:28,921427533-05:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.2 kill -INT 78909
[1] 02:43:29 [SUCCESS] ljishen@10.10.2.2

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:43:29,104652397-05:00] INFO: >>> Copy system activity file from OSD host[0m
## ./benchmarks/bench-rados:403 - rados_bench() > basename -- /tmp/bench-rados/sys_activity_16MB_write.dat.osd_host.2
# ./benchmarks/bench-rados:403 - rados_bench() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.2:/tmp/bench-rados/sys_activity_16MB_write.dat.osd_host.2 /mnt/sda8/ceph-research/scripts/output/sys_activity_16MB_write.dat.osd_host.2


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:43:29,275590347-05:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:43:53,259078167-05:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 547 objects, 8.5 GiB
    usage:   26 GiB used, 74 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:43:53,267090392-05:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:44:02,530181002-05:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 547 objects, 8.5 GiB
    usage:   26 GiB used, 74 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:44:02,538386852-05:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:44:11,647947028-05:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 547 objects, 8.5 GiB
    usage:   26 GiB used, 74 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:44:11,656095240-05:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:44:20,776684853-05:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 547 objects, 8.5 GiB
    usage:   26 GiB used, 74 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:44:20,784531386-05:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:44:30,107717037-05:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 547 objects, 8.5 GiB
    usage:   26 GiB used, 74 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:44:30,115770730-05:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:44:30,121973184-05:00] INFO: > The cluster is idle now.[0m


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:44:30,125898685-05:00] STAGE: Start benchmarking of mode seq ...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:44:30,133201403-05:00] INFO: > Start system activity collection services[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:44:30,139067893-05:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:341 - rados_bench() > rm --force /mnt/sda8/ceph-research/scripts/output/sys_activity_16MB_seq.dat.rados_bench_host.2
# ./benchmarks/bench-rados:342 - rados_bench() > sar_pid_rados_bench_host=262969
# ./benchmarks/bench-rados:342 - rados_bench() > S_TIME_FORMAT=ISO
# ./benchmarks/bench-rados:342 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/output/sys_activity_16MB_seq.dat.rados_bench_host.2 2

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:44:30,146247338-05:00] INFO: >> for OSD host[0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:44:30,155475957-05:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.2 bash
10.10.2.2: b'79007\n'
[1] 02:44:30 [SUCCESS] ljishen@10.10.2.2
79007

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:44:30,341767984-05:00] INFO: > Run rados bench[0m
# ./benchmarks/bench-rados:379 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/output/rados_bench_16MB_seq.log.2
## ./benchmarks/bench-rados:376 - rados_bench() > count_cpus 32-63,96-127
# ./benchmarks/bench-rados:376 - rados_bench() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 128 --show-time
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:44:30,361232160-05:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:44:30,363978037-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'a31b0e80-4066-11ec-aba1-c5f3c07a3dc7', '--', 'rados', 'bench', '99999999', 'seq', '--pool', 'bench_rados', '--concurrent-ios', '128', '--show-time'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid a31b0e80-4066-11ec-aba1-c5f3c07a3dc7 -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 128 --show-time
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
hints = 1
2021-11-08T07:44:33.750874+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-11-08T07:44:33.750874+0000     0       0         0         0         0         0           -           0
2021-11-08T07:44:34.751005+0000     1     109       109         0         0         0           -           0
2021-11-08T07:44:35.751077+0000     2     127       208        81   647.894       648      1.2774      1.2317
2021-11-08T07:44:36.751158+0000     3     127       301       174   927.874      1488     1.35624     1.28819
2021-11-08T07:44:37.752079+0000     4     127       396       269   1075.64      1520     1.34945     1.31279
2021-11-08T07:44:38.753388+0000     5     127       491       364   1164.19      1520     1.34299     1.32292
2021-11-08T07:44:39.753482+0000 Total time run:       5.80672
Total reads made:     546
Read size:            16777216
Object size:          16777216
Bandwidth (MB/sec):   1504.46
Average IOPS:         94
Stddev IOPS:          43.108
Max IOPS:             95
Min IOPS:             0
Average Latency(s):   1.20383
Max latency(s):       1.37873
Min latency(s):       0.235819

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:44:40,492620632-05:00] INFO: > Stop system activity collection services[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:44:40,499029214-05:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:391 - rados_bench() > kill -INT 262969

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:44:40,505467793-05:00] INFO: >> for OSD host[0m
# ./benchmarks/bench-rados:396 - rados_bench() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.2 kill -INT 79007
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:44:40,513170345-05:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.2 kill -INT 79007
[1] 02:44:40 [SUCCESS] ljishen@10.10.2.2

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:44:40,696594305-05:00] INFO: >>> Copy system activity file from OSD host[0m
## ./benchmarks/bench-rados:403 - rados_bench() > basename -- /tmp/bench-rados/sys_activity_16MB_seq.dat.osd_host.2
# ./benchmarks/bench-rados:403 - rados_bench() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.2:/tmp/bench-rados/sys_activity_16MB_seq.dat.osd_host.2 /mnt/sda8/ceph-research/scripts/output/sys_activity_16MB_seq.dat.osd_host.2


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:44:40,849097039-05:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:45:05,068597945-05:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 547 objects, 8.5 GiB
    usage:   26 GiB used, 74 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:45:05,076590153-05:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:45:14,327854204-05:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 547 objects, 8.5 GiB
    usage:   26 GiB used, 74 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:45:14,336023977-05:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:45:23,446429070-05:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 547 objects, 8.5 GiB
    usage:   26 GiB used, 74 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:45:23,454241760-05:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:45:32,639675990-05:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 547 objects, 8.5 GiB
    usage:   26 GiB used, 74 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:45:32,647757907-05:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:45:41,758543834-05:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 547 objects, 8.5 GiB
    usage:   26 GiB used, 74 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:45:41,766215508-05:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:45:41,772914146-05:00] INFO: > The cluster is idle now.[0m


[1;7;39;49m[2021-11-08T02:45:41,775460166-05:00][RUNNING][ROUND 3/7/21] object_size=16MB[0m


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:45:41,779115779-05:00] STAGE: Launch a Ceph cluster (CEPH_HOST: 10.10.2.3, OSD_HOST: 10.10.2.2) ...[0m
# ./benchmarks/bench-rados:183 - launch_ceph_cluster() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.3 bash -euo pipefail
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:45:41,788341261-05:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.3 bash -euo pipefail
10.10.2.3: b'\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T02:45:42,180072897-05:00] INFO: > Remove existing clusters\x1b[0m\n'
10.10.2.3: b'## bash:17 -  > basename -- /var/lib/ceph/a31b0e80-4066-11ec-aba1-c5f3c07a3dc7\n'
10.10.2.3: b'# bash:17 -  > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm --host 10.10.2.3 --host 10.10.2.2 rm-cluster --force --fsid a31b0e80-4066-11ec-aba1-c5f3c07a3dc7\n'
10.10.2.3: b"\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T02:45:42,191857512-05:00] DEBUG: HOST_PARAMS: ['--host', '10.10.2.3', '--host', '10.10.2.2']\x1b[0m\n"
10.10.2.3: b"\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T02:45:42,195315427-05:00] DEBUG: CEPHADM_PARAMS: ['rm-cluster', '--force', '--fsid', 'a31b0e80-4066-11ec-aba1-c5f3c07a3dc7']\x1b[0m\n"
10.10.2.3: b"# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:63 -  > /tmp/bench-rados/ceph-research/scripts/parallel-ssh --host 10.10.2.3 --host 10.10.2.2 'sudo python3 -u - rm-cluster --force --fsid a31b0e80-4066-11ec-aba1-c5f3c07a3dc7'\n"
10.10.2.3: b'\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T02:45:42,203825964-05:00] DEBUG: command from stdin\x1b[0m\n'
10.10.2.3: b"# /tmp/bench-rados/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host 10.10.2.3 --host 10.10.2.2 'sudo python3 -u - rm-cluster --force --fsid a31b0e80-4066-11ec-aba1-c5f3c07a3dc7'\n"
10.10.2.3: b'[1] 02:45:48 [SUCCESS] 10.10.2.3\n[2] 02:45:55 [SUCCESS] 10.10.2.2\n'
10.10.2.3: b'\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T02:45:55,161094716-05:00] INFO: > Deploy a new cluster\x1b[0m\n'
10.10.2.3: b'# bash:24 -  > /tmp/bench-rados/ceph-research/scripts/ceph-deploy -m 10.10.2.3 -o node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/10.10.2.2:/dev/nvme0n1\n'
10.10.2.3: b'\n\n\x1b[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T02:45:55,172645130-05:00] STAGE: Verify passwordless SSH to hosts, passwordless sudo via ljishen, and the existence of devices on hosts...\x1b[0m\n'
10.10.2.3: b'\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T02:45:55,176866534-05:00] INFO: > Check the host for the monitor\x1b[0m\n'
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:181 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@10.10.2.3 sudo --non-interactive --validate\n'
10.10.2.3: b'\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T02:45:55,327226285-05:00] INFO: > Check hosts for OSDs\x1b[0m\n'
10.10.2.3: b"\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T02:45:55,332034567-05:00] INFO: >> Check host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'\x1b[0m\n"
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:200 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us sudo --non-interactive --validate\n'
10.10.2.3: b"\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T02:45:55,481579881-05:00] INFO: >>> Check device '/dev/nvme0n1'\x1b[0m\n"
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:215 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us test -b /dev/nvme0n1\n'
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:226 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us lsblk --nodeps --noheadings --output TYPE /dev/nvme0n1\n'
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:227 - verify_parameters() > grep --word-regexp disk\n'
10.10.2.3: b'\n\n\x1b[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T02:45:55,758346879-05:00] STAGE: Reset devices on hosts...\x1b[0m\n'
10.10.2.3: b"\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T02:45:55,763402298-05:00] INFO: > Resetting device '/dev/nvme0n1' on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'\x1b[0m\n"
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:288 - reset_host_devices() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us sudo sh\n'
10.10.2.3: b'  Removing ceph--cc7f20ff--ac5e--4ce2--b131--559875b40438-osd--block--9c9fc2be--f039--40eb--b487--b52c96cab9f5 (253:0)\n'
10.10.2.3: b'  Archiving volume group "ceph-cc7f20ff-ac5e-4ce2-b131-559875b40438" metadata (seqno 5).\n  Releasing logical volume "osd-block-9c9fc2be-f039-40eb-b487-b52c96cab9f5"\n'
10.10.2.3: b'  Creating volume group backup "/etc/lvm/backup/ceph-cc7f20ff-ac5e-4ce2-b131-559875b40438" (seqno 6).\n'
10.10.2.3: b'  Logical volume "osd-block-9c9fc2be-f039-40eb-b487-b52c96cab9f5" successfully removed\n'
10.10.2.3: b'  Removing physical volume "/dev/nvme0n1" from volume group "ceph-cc7f20ff-ac5e-4ce2-b131-559875b40438"\n'
10.10.2.3: b'  Volume group "ceph-cc7f20ff-ac5e-4ce2-b131-559875b40438" successfully removed\n'
10.10.2.3: b'\n\n\x1b[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T02:45:56,135697810-05:00] STAGE: Bootstrap a new cluster...\x1b[0m\n'
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:317 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.3 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults\n'
10.10.2.3: b'\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T02:45:56,145280952-05:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.3: b"\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T02:45:56,148644448-05:00] DEBUG: CEPHADM_PARAMS: ['bootstrap', '--config', '/tmp/bench-rados/ceph-research/scripts/ceph.conf', '--mon-ip', '10.10.2.3', '--output-dir', '/tmp/bench-rados/deployment_data_root/etc/ceph', '--ssh-user', 'ljishen', '--skip-dashboard', '--no-minimize-config', '--allow-overwrite', '--allow-fqdn-hostname', '--orphan-initial-daemons', '--skip-monitoring-stack', '--single-host-defaults']\x1b[0m\n"
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.3 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults\n'
10.10.2.3: b'Verifying podman|docker is present...\nVerifying lvm2 is present...\n'
10.10.2.3: b'Verifying time synchronization is in place...\n'
10.10.2.3: b'Unit ntp.service is enabled and running\n'
10.10.2.3: b'Repeating the final host check...\n'
10.10.2.3: b'docker (/usr/bin/docker) is present\n'
10.10.2.3: b'systemctl is present\n'
10.10.2.3: b'lvcreate is present\n'
10.10.2.3: b'Unit ntp.service is enabled and running\n'
10.10.2.3: b'Host looks OK\n'
10.10.2.3: b'Cluster fsid: e83a8710-4067-11ec-aba1-c5f3c07a3dc7\n'
10.10.2.3: b'Verifying IP 10.10.2.3 port 3300 ...\n'
10.10.2.3: b'Verifying IP 10.10.2.3 port 6789 ...\n'
10.10.2.3: b'Mon IP `10.10.2.3` is in CIDR network `10.10.2.0/24`\n'
10.10.2.3: b'- internal network (--cluster-network) has not been provided, OSD replication will default to the public_network\n'
10.10.2.3: b'Adjusting default settings to suit single-host cluster...\n'
10.10.2.3: b'Pulling container image quay.io/ceph/ceph:v16...\n'
10.10.2.3: b'Ceph version: ceph version 16.2.6 (ee28fb57e47e9f88813e24bbf4c14496ca299d31) pacific (stable)\n'
10.10.2.3: b'Extracting ceph user uid/gid from container image...\n'
10.10.2.3: b'Creating initial keys...\n'
10.10.2.3: b'Creating initial monmap...\n'
10.10.2.3: b'Creating mon...\n'
10.10.2.3: b'Waiting for mon to start...\n'
10.10.2.3: b'Waiting for mon...\n'
10.10.2.3: b'mon is available\n'
10.10.2.3: b'Setting mon public_network to 10.10.2.0/24\n'
10.10.2.3: b'Wrote config to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf\n'
10.10.2.3: b'Wrote keyring to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring\n'
10.10.2.3: b'Creating mgr...\n'
10.10.2.3: b'Verifying port 9283 ...\n'
10.10.2.3: b'Waiting for mgr to start...\n'
10.10.2.3: b'Waiting for mgr...\n'
10.10.2.3: b'mgr not available, waiting (1/15)...\n'
10.10.2.3: b'mgr not available, waiting (2/15)...\n'
10.10.2.3: b'mgr is available\n'
10.10.2.3: b'Enabling cephadm module...\n'
10.10.2.3: b'Waiting for the mgr to restart...\n'
10.10.2.3: b'Waiting for mgr epoch 5...\n'
10.10.2.3: b'mgr epoch 5 is available\n'
10.10.2.3: b'Setting orchestrator backend to cephadm...\n'
10.10.2.3: b'Generating ssh key...\n'
10.10.2.3: b'Wrote public SSH key to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub\nAdding key to ljishen@localhost authorized_keys...\n'
10.10.2.3: b'Adding host node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us...\n'
10.10.2.3: b'Deploying unmanaged mon service...\n'
10.10.2.3: b'Deploying unmanaged mgr service...\n'
10.10.2.3: b'You can access the Ceph CLI with:\n\n\tsudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --fsid e83a8710-4067-11ec-aba1-c5f3c07a3dc7 -c /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf -k /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring\n\nPlease consider enabling telemetry to help improve Ceph:\n\n\tceph telemetry on\n\nFor more information see:\n\n\thttps://docs.ceph.com/docs/pacific/mgr/telemetry/\n\nBootstrap complete.\n'
10.10.2.3: b'\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T02:47:06,045105047-05:00] INFO: Sleep for 20 seconds\x1b[0m\n'
10.10.2.3: b'\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T02:47:26,051931855-05:00] INFO: > Set the default number of monitor daemons to 1\x1b[0m\n# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:337 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1\n'
10.10.2.3: b'\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T02:47:26,062064343-05:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.3: b"\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T02:47:26,065995340-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mon', '1']\x1b[0m\n"
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1\n'
10.10.2.3: b'Inferring fsid e83a8710-4067-11ec-aba1-c5f3c07a3dc7\n'
10.10.2.3: b'Inferring config /var/lib/ceph/e83a8710-4067-11ec-aba1-c5f3c07a3dc7/mon.node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.3: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.3: b'Scheduled mon update...\n'
10.10.2.3: b'\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T02:47:34,727493836-05:00] INFO: > Set the default number of manager daemons to 1\x1b[0m\n'
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:342 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1\n'
10.10.2.3: b'\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T02:47:34,736928747-05:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.3: b"\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T02:47:34,740151789-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mgr', '1']\x1b[0m\n"
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1\n'
10.10.2.3: b'Inferring fsid e83a8710-4067-11ec-aba1-c5f3c07a3dc7\n'
10.10.2.3: b'Inferring config /var/lib/ceph/e83a8710-4067-11ec-aba1-c5f3c07a3dc7/mon.node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.3: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.3: b'Scheduled mgr update...\n'
10.10.2.3: b'\n\n\x1b[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T02:47:45,634549303-05:00] STAGE: Add hosts to the cluster...\x1b[0m\n'
10.10.2.3: b"\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T02:47:45,640358424-05:00] INFO: > Adding host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'\x1b[0m\n"
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:361 - add_hosts() > ssh-copy-id -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -f -i /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\n'
10.10.2.3: b'/usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: "/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub"\n'
10.10.2.3: b'\nNumber of key(s) added: 1\n\nNow try logging into the machine, with:   "ssh -o \'GlobalKnownHostsFile=/dev/null\' -o \'LogLevel=ERROR\' -o \'PasswordAuthentication=no\' -o \'StrictHostKeyChecking=no\' -o \'UserKnownHostsFile=/dev/null\' \'ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\'"\nand check to make sure that only the key(s) you wanted were added.\n\n'
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:367 - add_hosts() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us 10.10.2.2\n'
10.10.2.3: b'\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T02:47:45,844296838-05:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.3: b"\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T02:47:45,847588509-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'host', 'add', 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us', '10.10.2.2']\x1b[0m\n"
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us 10.10.2.2\n'
10.10.2.3: b'Inferring fsid e83a8710-4067-11ec-aba1-c5f3c07a3dc7\n'
10.10.2.3: b'Inferring config /var/lib/ceph/e83a8710-4067-11ec-aba1-c5f3c07a3dc7/mon.node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.3: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.3: b"Added host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us' with addr '10.10.2.2'\n"
10.10.2.3: b'\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T02:47:56,016242772-05:00] INFO: Sleep for 20 seconds\x1b[0m\n'
10.10.2.3: b'\n\n\x1b[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T02:48:16,021159169-05:00] STAGE: Add OSDs to the cluster...\x1b[0m\n'
10.10.2.3: b"\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T02:48:16,027132890-05:00] INFO: > Adding devices ['/dev/nvme0n1'] on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us' as OSDs\x1b[0m\n"
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:388 - add_host_devices_as_osds() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1\n'
10.10.2.3: b'\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T02:48:16,036561670-05:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.3: b"\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T02:48:16,040075400-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'daemon', 'add', 'osd', 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1']\x1b[0m\n"
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1\n'
10.10.2.3: b'Inferring fsid e83a8710-4067-11ec-aba1-c5f3c07a3dc7\n'
10.10.2.3: b'Inferring config /var/lib/ceph/e83a8710-4067-11ec-aba1-c5f3c07a3dc7/mon.node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.3: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.3: b"Created osd(s) 0 on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'\n"
10.10.2.3: b'\n\x1b[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T02:48:41,677503295-05:00] INFO: > Sleep for 20 seconds\x1b[0m\n'
10.10.2.3: b'\n\n\x1b[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T02:49:01,683021729-05:00] STAGE: Show Cluster Status\x1b[0m\n'
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:408 - show_status() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status\n'
10.10.2.3: b'\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T02:49:01,691941058-05:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.3: b"\x1b[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-11-08T02:49:01,695528397-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', '--status']\x1b[0m\n"
10.10.2.3: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status\n'
10.10.2.3: b'Inferring fsid e83a8710-4067-11ec-aba1-c5f3c07a3dc7\n'
10.10.2.3: b'Inferring config /var/lib/ceph/e83a8710-4067-11ec-aba1-c5f3c07a3dc7/mon.node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.3: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.3: b'  cluster:\n    id:     e83a8710-4067-11ec-aba1-c5f3c07a3dc7\n    health: HEALTH_WARN\n            1 pool(s) have no replicas configured\n \n  services:\n    mon: 1 daemons, quorum node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us (age 2m)\n    mgr: node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us.afseso(active, since 2m)\n    osd: 1 osds: 1 up (since 18s), 1 in (since 41s)\n \n  data:\n    pools:   1 pools, 1 pgs\n    objects: 0 objects, 0 B\n    usage:   4.7 MiB used, 100 GiB / 100 GiB avail\n    pgs:     1 active+clean\n \n'
[1] 02:49:11 [SUCCESS] ljishen@10.10.2.3

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:45:42,180072897-05:00] INFO: > Remove existing clusters[0m
## bash:17 -  > basename -- /var/lib/ceph/a31b0e80-4066-11ec-aba1-c5f3c07a3dc7
# bash:17 -  > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm --host 10.10.2.3 --host 10.10.2.2 rm-cluster --force --fsid a31b0e80-4066-11ec-aba1-c5f3c07a3dc7
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:45:42,191857512-05:00] DEBUG: HOST_PARAMS: ['--host', '10.10.2.3', '--host', '10.10.2.2'][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:45:42,195315427-05:00] DEBUG: CEPHADM_PARAMS: ['rm-cluster', '--force', '--fsid', 'a31b0e80-4066-11ec-aba1-c5f3c07a3dc7'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:63 -  > /tmp/bench-rados/ceph-research/scripts/parallel-ssh --host 10.10.2.3 --host 10.10.2.2 'sudo python3 -u - rm-cluster --force --fsid a31b0e80-4066-11ec-aba1-c5f3c07a3dc7'
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:45:42,203825964-05:00] DEBUG: command from stdin[0m
# /tmp/bench-rados/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host 10.10.2.3 --host 10.10.2.2 'sudo python3 -u - rm-cluster --force --fsid a31b0e80-4066-11ec-aba1-c5f3c07a3dc7'
[1] 02:45:48 [SUCCESS] 10.10.2.3
[2] 02:45:55 [SUCCESS] 10.10.2.2

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:45:55,161094716-05:00] INFO: > Deploy a new cluster[0m
# bash:24 -  > /tmp/bench-rados/ceph-research/scripts/ceph-deploy -m 10.10.2.3 -o node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/10.10.2.2:/dev/nvme0n1


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:45:55,172645130-05:00] STAGE: Verify passwordless SSH to hosts, passwordless sudo via ljishen, and the existence of devices on hosts...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:45:55,176866534-05:00] INFO: > Check the host for the monitor[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:181 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@10.10.2.3 sudo --non-interactive --validate

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:45:55,327226285-05:00] INFO: > Check hosts for OSDs[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:45:55,332034567-05:00] INFO: >> Check host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:200 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us sudo --non-interactive --validate

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:45:55,481579881-05:00] INFO: >>> Check device '/dev/nvme0n1'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:215 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us test -b /dev/nvme0n1
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:226 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us lsblk --nodeps --noheadings --output TYPE /dev/nvme0n1
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:227 - verify_parameters() > grep --word-regexp disk


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:45:55,758346879-05:00] STAGE: Reset devices on hosts...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:45:55,763402298-05:00] INFO: > Resetting device '/dev/nvme0n1' on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:288 - reset_host_devices() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us sudo sh
  Removing ceph--cc7f20ff--ac5e--4ce2--b131--559875b40438-osd--block--9c9fc2be--f039--40eb--b487--b52c96cab9f5 (253:0)
  Archiving volume group "ceph-cc7f20ff-ac5e-4ce2-b131-559875b40438" metadata (seqno 5).
  Releasing logical volume "osd-block-9c9fc2be-f039-40eb-b487-b52c96cab9f5"
  Creating volume group backup "/etc/lvm/backup/ceph-cc7f20ff-ac5e-4ce2-b131-559875b40438" (seqno 6).
  Logical volume "osd-block-9c9fc2be-f039-40eb-b487-b52c96cab9f5" successfully removed
  Removing physical volume "/dev/nvme0n1" from volume group "ceph-cc7f20ff-ac5e-4ce2-b131-559875b40438"
  Volume group "ceph-cc7f20ff-ac5e-4ce2-b131-559875b40438" successfully removed


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:45:56,135697810-05:00] STAGE: Bootstrap a new cluster...[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:317 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.3 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:45:56,145280952-05:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:45:56,148644448-05:00] DEBUG: CEPHADM_PARAMS: ['bootstrap', '--config', '/tmp/bench-rados/ceph-research/scripts/ceph.conf', '--mon-ip', '10.10.2.3', '--output-dir', '/tmp/bench-rados/deployment_data_root/etc/ceph', '--ssh-user', 'ljishen', '--skip-dashboard', '--no-minimize-config', '--allow-overwrite', '--allow-fqdn-hostname', '--orphan-initial-daemons', '--skip-monitoring-stack', '--single-host-defaults'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.3 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults
Verifying podman|docker is present...
Verifying lvm2 is present...
Verifying time synchronization is in place...
Unit ntp.service is enabled and running
Repeating the final host check...
docker (/usr/bin/docker) is present
systemctl is present
lvcreate is present
Unit ntp.service is enabled and running
Host looks OK
Cluster fsid: e83a8710-4067-11ec-aba1-c5f3c07a3dc7
Verifying IP 10.10.2.3 port 3300 ...
Verifying IP 10.10.2.3 port 6789 ...
Mon IP `10.10.2.3` is in CIDR network `10.10.2.0/24`
- internal network (--cluster-network) has not been provided, OSD replication will default to the public_network
Adjusting default settings to suit single-host cluster...
Pulling container image quay.io/ceph/ceph:v16...
Ceph version: ceph version 16.2.6 (ee28fb57e47e9f88813e24bbf4c14496ca299d31) pacific (stable)
Extracting ceph user uid/gid from container image...
Creating initial keys...
Creating initial monmap...
Creating mon...
Waiting for mon to start...
Waiting for mon...
mon is available
Setting mon public_network to 10.10.2.0/24
Wrote config to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf
Wrote keyring to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring
Creating mgr...
Verifying port 9283 ...
Waiting for mgr to start...
Waiting for mgr...
mgr not available, waiting (1/15)...
mgr not available, waiting (2/15)...
mgr is available
Enabling cephadm module...
Waiting for the mgr to restart...
Waiting for mgr epoch 5...
mgr epoch 5 is available
Setting orchestrator backend to cephadm...
Generating ssh key...
Wrote public SSH key to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub
Adding key to ljishen@localhost authorized_keys...
Adding host node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us...
Deploying unmanaged mon service...
Deploying unmanaged mgr service...
You can access the Ceph CLI with:

	sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --fsid e83a8710-4067-11ec-aba1-c5f3c07a3dc7 -c /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf -k /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring

Please consider enabling telemetry to help improve Ceph:

	ceph telemetry on

For more information see:

	https://docs.ceph.com/docs/pacific/mgr/telemetry/

Bootstrap complete.

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:47:06,045105047-05:00] INFO: Sleep for 20 seconds[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:47:26,051931855-05:00] INFO: > Set the default number of monitor daemons to 1[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:337 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:47:26,062064343-05:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:47:26,065995340-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mon', '1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1
Inferring fsid e83a8710-4067-11ec-aba1-c5f3c07a3dc7
Inferring config /var/lib/ceph/e83a8710-4067-11ec-aba1-c5f3c07a3dc7/mon.node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Scheduled mon update...

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:47:34,727493836-05:00] INFO: > Set the default number of manager daemons to 1[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:342 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:47:34,736928747-05:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:47:34,740151789-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mgr', '1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1
Inferring fsid e83a8710-4067-11ec-aba1-c5f3c07a3dc7
Inferring config /var/lib/ceph/e83a8710-4067-11ec-aba1-c5f3c07a3dc7/mon.node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Scheduled mgr update...


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:47:45,634549303-05:00] STAGE: Add hosts to the cluster...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:47:45,640358424-05:00] INFO: > Adding host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:361 - add_hosts() > ssh-copy-id -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -f -i /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us
/usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: "/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub"

Number of key(s) added: 1

Now try logging into the machine, with:   "ssh -o 'GlobalKnownHostsFile=/dev/null' -o 'LogLevel=ERROR' -o 'PasswordAuthentication=no' -o 'StrictHostKeyChecking=no' -o 'UserKnownHostsFile=/dev/null' 'ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'"
and check to make sure that only the key(s) you wanted were added.

# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:367 - add_hosts() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us 10.10.2.2
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:47:45,844296838-05:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:47:45,847588509-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'host', 'add', 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us', '10.10.2.2'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us 10.10.2.2
Inferring fsid e83a8710-4067-11ec-aba1-c5f3c07a3dc7
Inferring config /var/lib/ceph/e83a8710-4067-11ec-aba1-c5f3c07a3dc7/mon.node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Added host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us' with addr '10.10.2.2'

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:47:56,016242772-05:00] INFO: Sleep for 20 seconds[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:48:16,021159169-05:00] STAGE: Add OSDs to the cluster...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:48:16,027132890-05:00] INFO: > Adding devices ['/dev/nvme0n1'] on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us' as OSDs[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:388 - add_host_devices_as_osds() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:48:16,036561670-05:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:48:16,040075400-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'daemon', 'add', 'osd', 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1
Inferring fsid e83a8710-4067-11ec-aba1-c5f3c07a3dc7
Inferring config /var/lib/ceph/e83a8710-4067-11ec-aba1-c5f3c07a3dc7/mon.node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Created osd(s) 0 on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:48:41,677503295-05:00] INFO: > Sleep for 20 seconds[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:49:01,683021729-05:00] STAGE: Show Cluster Status[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:408 - show_status() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:49:01,691941058-05:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:49:01,695528397-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', '--status'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status
Inferring fsid e83a8710-4067-11ec-aba1-c5f3c07a3dc7
Inferring config /var/lib/ceph/e83a8710-4067-11ec-aba1-c5f3c07a3dc7/mon.node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
  cluster:
    id:     e83a8710-4067-11ec-aba1-c5f3c07a3dc7
    health: HEALTH_WARN
            1 pool(s) have no replicas configured
 
  services:
    mon: 1 daemons, quorum node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us (age 2m)
    mgr: node-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us.afseso(active, since 2m)
    osd: 1 osds: 1 up (since 18s), 1 in (since 41s)
 
  data:
    pools:   1 pools, 1 pgs
    objects: 0 objects, 0 B
    usage:   4.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     1 active+clean
 

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:49:11,154107526-05:00] INFO: > Copy configuration files to local[0m
# ./benchmarks/bench-rados:220 - launch_ceph_cluster() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.3 sudo chown --changes --recursive ljishen /tmp/bench-rados/deployment_data_root/etc/ceph
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:49:11,161890099-05:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.3 sudo chown --changes --recursive ljishen /tmp/bench-rados/deployment_data_root/etc/ceph
[1] 02:49:11 [SUCCESS] ljishen@10.10.2.3
# ./benchmarks/bench-rados:223 - launch_ceph_cluster() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.3:/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf
# ./benchmarks/bench-rados:226 - launch_ceph_cluster() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.3:/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:49:11,628265758-05:00] STAGE: Update local ceph.conf as a client...[0m


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:49:11,631977045-05:00] STAGE: Configure Ceph pool...[0m
# ./benchmarks/bench-rados:257 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_max_backfills 32
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:49:11,653332579-05:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:49:11,655985431-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'e83a8710-4067-11ec-aba1-c5f3c07a3dc7', '--', 'ceph', 'config', 'set', 'osd', 'osd_max_backfills', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid e83a8710-4067-11ec-aba1-c5f3c07a3dc7 -- ceph config set osd osd_max_backfills 32
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:258 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_active 32
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:49:15,670603524-05:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:49:15,673321448-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'e83a8710-4067-11ec-aba1-c5f3c07a3dc7', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_active', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid e83a8710-4067-11ec-aba1-c5f3c07a3dc7 -- ceph config set osd osd_recovery_max_active 32
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:259 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_single_start 8
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:49:19,876708301-05:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:49:19,879442526-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'e83a8710-4067-11ec-aba1-c5f3c07a3dc7', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_single_start', '8'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid e83a8710-4067-11ec-aba1-c5f3c07a3dc7 -- ceph config set osd osd_recovery_max_single_start 8
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:260 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_op_priority 63
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:49:24,189116105-05:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:49:24,191713142-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'e83a8710-4067-11ec-aba1-c5f3c07a3dc7', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_op_priority', '63'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid e83a8710-4067-11ec-aba1-c5f3c07a3dc7 -- ceph config set osd osd_recovery_op_priority 63
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:262 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config show-with-defaults osd.0
# ./benchmarks/bench-rados:263 - configure_ceph_pool() > grep 'osd_max_backfills\|osd_recovery'
# ./benchmarks/bench-rados:263 - configure_ceph_pool() > column -t -s ' '
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
osd_max_backfills                        32         mon
osd_recovery_cost                        20971520   default
osd_recovery_delay_start                 0.000000   default
osd_recovery_max_active                  32         mon
osd_recovery_max_active_hdd              3          default
osd_recovery_max_active_ssd              10         default
osd_recovery_max_chunk                   8388608    default
osd_recovery_max_omap_entries_per_chunk  8096       default
osd_recovery_max_single_start            8          mon
osd_recovery_op_priority                 63         mon
osd_recovery_op_warn_multiple            16         default
osd_recovery_priority                    5          default
osd_recovery_retry_interval              30.000000  default
osd_recovery_sleep                       0.000000   default
osd_recovery_sleep_hdd                   0.100000   default
osd_recovery_sleep_hybrid                0.025000   default
osd_recovery_sleep_ssd                   0.000000   default
# ./benchmarks/bench-rados:265 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool create bench_rados 128 128 replicated --size 1 --pg-num-min 128
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:49:32,837731868-05:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:49:32,840615425-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'e83a8710-4067-11ec-aba1-c5f3c07a3dc7', '--', 'ceph', 'osd', 'pool', 'create', 'bench_rados', '128', '128', 'replicated', '--size', '1', '--pg-num-min', '128'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid e83a8710-4067-11ec-aba1-c5f3c07a3dc7 -- ceph osd pool create bench_rados 128 128 replicated --size 1 --pg-num-min 128
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
pool 'bench_rados' created
# ./benchmarks/bench-rados:267 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados min_size 1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:49:37,852814327-05:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:49:37,855453122-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'e83a8710-4067-11ec-aba1-c5f3c07a3dc7', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'min_size', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid e83a8710-4067-11ec-aba1-c5f3c07a3dc7 -- ceph osd pool set bench_rados min_size 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 min_size to 1
# ./benchmarks/bench-rados:268 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados pg_autoscale_mode off
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:49:42,832203057-05:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:49:42,834931011-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'e83a8710-4067-11ec-aba1-c5f3c07a3dc7', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'pg_autoscale_mode', 'off'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid e83a8710-4067-11ec-aba1-c5f3c07a3dc7 -- ceph osd pool set bench_rados pg_autoscale_mode off
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 pg_autoscale_mode to off
# ./benchmarks/bench-rados:269 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool application enable bench_rados benchmark
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:49:47,812223991-05:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:49:47,814879598-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'e83a8710-4067-11ec-aba1-c5f3c07a3dc7', '--', 'ceph', 'osd', 'pool', 'application', 'enable', 'bench_rados', 'benchmark'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid e83a8710-4067-11ec-aba1-c5f3c07a3dc7 -- ceph osd pool application enable bench_rados benchmark
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
enabled application 'benchmark' on pool 'bench_rados'
# ./benchmarks/bench-rados:270 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados noscrub 1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:49:53,155920038-05:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:49:53,158565346-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'e83a8710-4067-11ec-aba1-c5f3c07a3dc7', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'noscrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid e83a8710-4067-11ec-aba1-c5f3c07a3dc7 -- ceph osd pool set bench_rados noscrub 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 noscrub to 1
# ./benchmarks/bench-rados:271 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados nodeep-scrub 1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:49:57,887048021-05:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:49:57,889807013-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'e83a8710-4067-11ec-aba1-c5f3c07a3dc7', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'nodeep-scrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid e83a8710-4067-11ec-aba1-c5f3c07a3dc7 -- ceph osd pool set bench_rados nodeep-scrub 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 nodeep-scrub to 1
# ./benchmarks/bench-rados:272 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool ls detail
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:50:03,027627506-05:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:50:03,030293523-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'e83a8710-4067-11ec-aba1-c5f3c07a3dc7', '--', 'ceph', 'osd', 'pool', 'ls', 'detail'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid e83a8710-4067-11ec-aba1-c5f3c07a3dc7 -- ceph osd pool ls detail
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
pool 1 'device_health_metrics' replicated size 1 min_size 1 crush_rule 0 object_hash rjenkins pg_num 64 pgp_num 64 autoscale_mode on last_change 17 lfor 0/0/14 flags hashpspool stripe_width 0 pg_num_min 1 application mgr_devicehealth
pool 2 'bench_rados' replicated size 1 min_size 1 crush_rule 0 object_hash rjenkins pg_num 128 pgp_num 128 autoscale_mode off last_change 20 flags hashpspool,noscrub,nodeep-scrub stripe_width 0 pg_num_min 128 application benchmark

# ./benchmarks/bench-rados:273 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd df tree
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:50:07,203933631-05:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:50:07,206602383-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'e83a8710-4067-11ec-aba1-c5f3c07a3dc7', '--', 'ceph', 'osd', 'df', 'tree'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid e83a8710-4067-11ec-aba1-c5f3c07a3dc7 -- ceph osd df tree
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
ID  CLASS  WEIGHT   REWEIGHT  SIZE     RAW USE  DATA     OMAP  META     AVAIL    %USE  VAR   PGS  STATUS  TYPE NAME      
-1         0.09769         -  100 GiB  5.7 MiB  168 KiB   0 B  5.6 MiB  100 GiB  0.01  1.00    -          root default   
-3         0.09769         -  100 GiB  5.7 MiB  168 KiB   0 B  5.6 MiB  100 GiB  0.01  1.00    -              host node-1
 0    ssd  0.09769   1.00000  100 GiB  5.7 MiB  168 KiB   0 B  5.6 MiB  100 GiB  0.01  1.00  192      up          osd.0  
                       TOTAL  100 GiB  5.7 MiB  168 KiB   0 B  5.6 MiB  100 GiB  0.01                                    
MIN/MAX VAR: 1.00/1.00  STDDEV: 0


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:50:11,521058871-05:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:50:35,631143385-05:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:50:44,794325731-05:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:50:44,802281511-05:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:50:54,256899860-05:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:50:54,264885305-05:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:51:03,483238764-05:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:51:03,491010687-05:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:51:12,631555393-05:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:51:12,639790881-05:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:51:21,724172476-05:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 100 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:51:21,731712342-05:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:51:21,737958067-05:00] INFO: > The cluster is idle now.[0m


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:51:21,741807255-05:00] STAGE: Start benchmarking of mode write ...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:51:21,749285795-05:00] INFO: > Start system activity collection services[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:51:21,755399061-05:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:341 - rados_bench() > rm --force /mnt/sda8/ceph-research/scripts/output/sys_activity_16MB_write.dat.rados_bench_host.3
# ./benchmarks/bench-rados:342 - rados_bench() > sar_pid_rados_bench_host=268372
# ./benchmarks/bench-rados:342 - rados_bench() > S_TIME_FORMAT=ISO
# ./benchmarks/bench-rados:342 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/output/sys_activity_16MB_write.dat.rados_bench_host.3 2

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:51:21,762704164-05:00] INFO: >> for OSD host[0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:51:21,771500169-05:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.2 bash
10.10.2.2: b'82628\n'
[1] 02:51:21 [SUCCESS] ljishen@10.10.2.2
82628

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:51:21,962258461-05:00] INFO: > Run rados bench[0m
# ./benchmarks/bench-rados:370 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/output/rados_bench_16MB_write.log.3
## ./benchmarks/bench-rados:361 - rados_bench() > count_cpus 32-63,96-127
# ./benchmarks/bench-rados:361 - rados_bench() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 60 write --pool bench_rados -b 16777216 -O 16777216 --concurrent-ios 128 --show-time --write-object --write-omap --write-xattr --no-cleanup
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:51:21,981787932-05:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:51:21,984635070-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'e83a8710-4067-11ec-aba1-c5f3c07a3dc7', '--', 'rados', 'bench', '60', 'write', '--pool', 'bench_rados', '-b', '16777216', '-O', '16777216', '--concurrent-ios', '128', '--show-time', '--write-object', '--write-omap', '--write-xattr', '--no-cleanup'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid e83a8710-4067-11ec-aba1-c5f3c07a3dc7 -- rados bench 60 write --pool bench_rados -b 16777216 -O 16777216 --concurrent-ios 128 --show-time --write-object --write-omap --write-xattr --no-cleanup
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
hints = 1
2021-11-08T07:51:25.239943+0000 Maintaining 128 concurrent writes of 16777216 bytes to objects of size 16777216 for up to 60 seconds or 0 objects
2021-11-08T07:51:25.239978+0000 Object prefix: benchmark_data_node-0.bluefield2.ucsc-cmps10_7
2021-11-08T07:51:25.909743+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-11-08T07:51:25.909743+0000     0       0         0         0         0         0           -           0
2021-11-08T07:51:26.909848+0000     1      19        19         0         0         0           -           0
2021-11-08T07:51:27.909923+0000     2      35        35         0         0         0           -           0
2021-11-08T07:51:28.909994+0000     3      44        44         0         0         0           -           0
2021-11-08T07:51:29.910063+0000     4      63        63         0         0         0           -           0
2021-11-08T07:51:30.910136+0000     5      69        69         0         0         0           -           0
2021-11-08T07:51:31.910205+0000     6      76        76         0         0         0           -           0
2021-11-08T07:51:32.910276+0000     7      87        87         0         0         0           -           0
2021-11-08T07:51:33.910341+0000     8     100       100         0         0         0           -           0
2021-11-08T07:51:34.910409+0000     9     111       111         0         0         0           -           0
2021-11-08T07:51:35.910487+0000    10     127       129         2   3.19978       3.2     9.94034     9.91714
2021-11-08T07:51:36.910561+0000    11     127       132         5   7.27222        48     10.4896     10.1102
2021-11-08T07:51:37.910629+0000    12     127       141        14   18.6654       144     11.2636      10.738
2021-11-08T07:51:38.910697+0000    13     127       152        25   30.7671       176     11.6602     11.0683
2021-11-08T07:51:39.910766+0000    14     127       160        33   37.7117       128     12.2506     11.2647
2021-11-08T07:51:40.910839+0000    15     127       164        37   39.4639        64     12.7417     11.3961
2021-11-08T07:51:41.910910+0000    16     127       174        47   46.9967       160     12.9262     11.7095
2021-11-08T07:51:42.910982+0000    17     127       191        64   60.2311       272     13.0237     12.0562
2021-11-08T07:51:43.911049+0000    18     127       197        70   62.2179        96     13.0718     12.1489
2021-11-08T07:51:44.911118+0000    19     127       205        78   65.6796       128     12.8859     12.2299
2021-11-08T07:51:45.911185+0000 min lat: 9.89394 max lat: 13.1792 avg lat: 12.3137
2021-11-08T07:51:45.911185+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-11-08T07:51:45.911185+0000    20     127       216        89    71.195       176     12.8939     12.3137
2021-11-08T07:51:46.911270+0000    21     127       226        99   75.4233       160     13.1362     12.3812
2021-11-08T07:51:47.911340+0000    22     127       229       102   74.1766        48     13.6181      12.412
2021-11-08T07:51:48.911409+0000    23     127       238       111    77.212       144     14.0167     12.5335
2021-11-08T07:51:49.911485+0000    24     127       249       122   81.3276       176     14.4612     12.6892
2021-11-08T07:51:50.911562+0000    25     127       258       131   83.8341       144     14.6994     12.8228
2021-11-08T07:51:51.911635+0000    26     127       261       134   82.4557        48     14.7144     12.8666
2021-11-08T07:51:52.911706+0000    27     127       270       143   84.7347       144     14.9623     12.9976
2021-11-08T07:51:53.911775+0000    28     127       281       154   87.9938       176     14.9296     13.1354
2021-11-08T07:51:54.911850+0000    29     127       290       163   89.9247       144     14.5926     13.2274
2021-11-08T07:51:55.911920+0000    30     127       293       166   88.5271        48     14.6808     13.2534
2021-11-08T07:51:56.912000+0000    31     127       303       176   90.8322       160     14.8911     13.3463
2021-11-08T07:51:57.912086+0000    32     127       314       187   93.4933       176     15.2553     13.4475
2021-11-08T07:51:58.912158+0000    33     127       322       195   94.5387       128     15.4266     13.5269
2021-11-08T07:51:59.912234+0000    34     127       325       198   93.1698        48     15.9071     13.5611
2021-11-08T07:52:00.912301+0000    35     127       335       208   95.0789       160     15.8534     13.6695
2021-11-08T07:52:01.912369+0000    36     127       347       220   97.7708       192      15.795     13.7861
2021-11-08T07:52:02.912439+0000    37     127       355       228   98.5875       128     15.8098     13.8567
2021-11-08T07:52:03.912508+0000    38     127       357       230   96.8352        32     15.8073     13.8741
2021-11-08T07:52:04.912581+0000    39     127       368       241   98.8647       176     15.8069     13.9635
2021-11-08T07:52:05.912649+0000 min lat: 9.89394 max lat: 15.92 avg lat: 14.042
2021-11-08T07:52:05.912649+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-11-08T07:52:05.912649+0000    40     127       379       252   100.793       176     15.7531      14.042
2021-11-08T07:52:06.912725+0000    41     127       387       260   101.456       128     15.7365     14.0948
2021-11-08T07:52:07.912795+0000    42     127       390       263   100.183        48     15.8124     14.1139
2021-11-08T07:52:08.912867+0000    43     127       401       274   101.946       176     15.7276     14.1787
2021-11-08T07:52:09.912942+0000    44     127       411       284   103.265       160     15.8043     14.2342
2021-11-08T07:52:10.913017+0000    45     127       419       292   103.815       128      15.855     14.2777
2021-11-08T07:52:11.913093+0000    46     127       423       296   102.949        64      15.796     14.2976
2021-11-08T07:52:12.913166+0000    47     127       434       307   104.503       176     15.7319     14.3498
2021-11-08T07:52:13.913235+0000    48     127       445       318   105.992       176     15.6837     14.3967
2021-11-08T07:52:14.913309+0000    49     127       451       324   105.788        96     15.6579     14.4205
2021-11-08T07:52:15.913384+0000    50     127       456       329   105.272        80     15.6988       14.44
2021-11-08T07:52:16.913460+0000    51     127       467       340   106.659       176     15.6899     14.4802
2021-11-08T07:52:17.913537+0000    52     127       478       351   107.992       176     15.7488     14.5187
2021-11-08T07:52:18.913605+0000    53     127       483       356   107.464        80     15.7837     14.5359
2021-11-08T07:52:19.913676+0000    54     127       488       361   106.955        80     15.7378     14.5529
2021-11-08T07:52:20.913750+0000    55     127       499       372    108.21       176     15.7584     14.5876
2021-11-08T07:52:21.913820+0000    56     127       510       383   109.421       176     15.7378     14.6215
2021-11-08T07:52:22.913891+0000    57     127       515       388   108.904        80     15.8011     14.6359
2021-11-08T07:52:23.913958+0000    58     127       521       394   108.682        96     15.7584      14.653
2021-11-08T07:52:24.914029+0000    59     127       531       404   109.551       160     15.7828     14.6804
2021-11-08T07:52:25.914105+0000 min lat: 9.89394 max lat: 15.92 avg lat: 14.7112
2021-11-08T07:52:25.914105+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-11-08T07:52:25.914105+0000    60     127       543       416   110.925       192      15.705     14.7112
2021-11-08T07:52:26.914220+0000 Total time run:         60.254
Total writes made:      544
Write size:             16777216
Object size:            16777216
Bandwidth (MB/sec):     144.455
Stddev Bandwidth:       69.0446
Max bandwidth (MB/sec): 272
Min bandwidth (MB/sec): 0
Average IOPS:           9
Stddev IOPS:            4.32062
Max IOPS:               17
Min IOPS:               0
Average Latency(s):     13.0277
Stddev Latency(s):      3.9614
Max latency(s):         15.92
Min latency(s):         0.256847

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:52:27,770253899-05:00] INFO: > Stop system activity collection services[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:52:27,776532457-05:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:391 - rados_bench() > kill -INT 268372

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:52:27,782702240-05:00] INFO: >> for OSD host[0m
# ./benchmarks/bench-rados:396 - rados_bench() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.2 kill -INT 82628
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:52:27,790128421-05:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.2 kill -INT 82628
[1] 02:52:27 [SUCCESS] ljishen@10.10.2.2

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:52:27,973134046-05:00] INFO: >>> Copy system activity file from OSD host[0m
## ./benchmarks/bench-rados:403 - rados_bench() > basename -- /tmp/bench-rados/sys_activity_16MB_write.dat.osd_host.3
# ./benchmarks/bench-rados:403 - rados_bench() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.2:/tmp/bench-rados/sys_activity_16MB_write.dat.osd_host.3 /mnt/sda8/ceph-research/scripts/output/sys_activity_16MB_write.dat.osd_host.3


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:52:28,148265104-05:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:52:52,263541144-05:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 545 objects, 8.5 GiB
    usage:   26 GiB used, 74 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:52:52,271266089-05:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:53:01,544129858-05:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 545 objects, 8.5 GiB
    usage:   26 GiB used, 74 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:53:01,552133278-05:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:53:10,648312091-05:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 545 objects, 8.5 GiB
    usage:   26 GiB used, 74 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:53:10,656612822-05:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:53:19,938779362-05:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 545 objects, 8.5 GiB
    usage:   26 GiB used, 74 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:53:19,947568142-05:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:53:29,132586498-05:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 545 objects, 8.5 GiB
    usage:   26 GiB used, 74 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:53:29,140788813-05:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:53:29,147152352-05:00] INFO: > The cluster is idle now.[0m


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:53:29,150884037-05:00] STAGE: Start benchmarking of mode seq ...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:53:29,158525975-05:00] INFO: > Start system activity collection services[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:53:29,164652356-05:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:341 - rados_bench() > rm --force /mnt/sda8/ceph-research/scripts/output/sys_activity_16MB_seq.dat.rados_bench_host.3
# ./benchmarks/bench-rados:342 - rados_bench() > sar_pid_rados_bench_host=269722
# ./benchmarks/bench-rados:342 - rados_bench() > S_TIME_FORMAT=ISO
# ./benchmarks/bench-rados:342 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/output/sys_activity_16MB_seq.dat.rados_bench_host.3 2

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:53:29,172074450-05:00] INFO: >> for OSD host[0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:53:29,180972076-05:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.2 bash
10.10.2.2: b'82730\n'
[1] 02:53:29 [SUCCESS] ljishen@10.10.2.2
82730

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:53:29,370896546-05:00] INFO: > Run rados bench[0m
# ./benchmarks/bench-rados:379 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/output/rados_bench_16MB_seq.log.3
## ./benchmarks/bench-rados:376 - rados_bench() > count_cpus 32-63,96-127
# ./benchmarks/bench-rados:376 - rados_bench() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 128 --show-time
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:53:29,390322181-05:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:53:29,392994099-05:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'e83a8710-4067-11ec-aba1-c5f3c07a3dc7', '--', 'rados', 'bench', '99999999', 'seq', '--pool', 'bench_rados', '--concurrent-ios', '128', '--show-time'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid e83a8710-4067-11ec-aba1-c5f3c07a3dc7 -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 128 --show-time
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
hints = 1
2021-11-08T07:53:32.668015+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-11-08T07:53:32.668015+0000     0       0         0         0         0         0           -           0
2021-11-08T07:53:33.668129+0000     1      97        97         0         0         0           -           0
2021-11-08T07:53:34.668201+0000     2     127       185        58   463.929       464     1.41294     1.37864
2021-11-08T07:53:35.668283+0000     3     127       269       142   757.236      1344     1.52108      1.4364
2021-11-08T07:53:36.669614+0000     4     127       353       226   903.612      1344     1.52294     1.46995
2021-11-08T07:53:37.669685+0000     5     127       436       309   988.446      1328     1.52663     1.48533
2021-11-08T07:53:38.669758+0000     6     127       519       392   1045.01      1328     1.53785     1.49564
2021-11-08T07:53:39.669854+0000 Total time run:       6.52005
Total reads made:     544
Read size:            16777216
Object size:          16777216
Bandwidth (MB/sec):   1334.96
Average IOPS:         83
Stddev IOPS:          36.7954
Max IOPS:             84
Min IOPS:             0
Average Latency(s):   1.35421
Max latency(s):       1.55165
Min latency(s):       0.222802

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:53:40,417707745-05:00] INFO: > Stop system activity collection services[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:53:40,424264196-05:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:391 - rados_bench() > kill -INT 269722

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:53:40,430514571-05:00] INFO: >> for OSD host[0m
# ./benchmarks/bench-rados:396 - rados_bench() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.2 kill -INT 82730
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:53:40,438158423-05:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.2 kill -INT 82730
[1] 02:53:40 [SUCCESS] ljishen@10.10.2.2

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:53:40,620841690-05:00] INFO: >>> Copy system activity file from OSD host[0m
## ./benchmarks/bench-rados:403 - rados_bench() > basename -- /tmp/bench-rados/sys_activity_16MB_seq.dat.osd_host.3
# ./benchmarks/bench-rados:403 - rados_bench() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.2:/tmp/bench-rados/sys_activity_16MB_seq.dat.osd_host.3 /mnt/sda8/ceph-research/scripts/output/sys_activity_16MB_seq.dat.osd_host.3


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:53:40,773240588-05:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:54:04,872167383-05:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 545 objects, 8.5 GiB
    usage:   26 GiB used, 74 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:54:04,880375920-05:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:54:14,141145541-05:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 545 objects, 8.5 GiB
    usage:   26 GiB used, 74 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:54:14,149592397-05:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:54:23,393511598-05:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 545 objects, 8.5 GiB
    usage:   26 GiB used, 74 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:54:23,401712561-05:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:54:32,627072651-05:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 545 objects, 8.5 GiB
    usage:   26 GiB used, 74 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:54:32,635886038-05:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:54:41,728750878-05:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 545 objects, 8.5 GiB
    usage:   26 GiB used, 74 GiB / 100 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:54:41,736834128-05:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-11-08T02:54:41,743591369-05:00] INFO: > The cluster is idle now.[0m
